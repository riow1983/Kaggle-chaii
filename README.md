# Kaggle-chaii
![input file image](https://github.com/riow1983/Kaggle-chaii/blob/master/png/20210823.png)<br>
https://www.kaggle.com/c/chaii-hindi-and-tamil-question-answering<br>
ã©ã‚“ãªã‚³ãƒ³ãƒš?:<br>
é–‹å‚¬æœŸé–“: 2021-08-11 ~ 2021-11-15<br>
[çµæœ]()<br>  
<br>
<br>
<br>
***
## å®Ÿé¨“ç®¡ç†ãƒ†ãƒ¼ãƒ–ãƒ«
|commitSHA|comment|Local CV|Public LB|
|----|----|----|----|
|-|-|-|0.010|
|6bb70140768d5dec90205db8b0568746124f5568|for loop incorporated in the function for memory efficiency|-|Error|
|8c807fec919722adf8eacb7822ddbfd3c0627487|sentence seperation deprecated|-|0.002|
|aa0b23e093ebbc085a56e354e764d95c3b31bb9f|replaced pipeline w/ torch-native way in the inference loop|-|0.005|
|9c381dbda9b2e142ab2ea1f32fcda596f4eb28d0|replaced pipeline w/ torch-native way in the inference loop|-|0.005|
|c85cec65d19ea00a6942e63934cd7f1288bc2460|pre-trained model (mBERT) w/ fine-tuning being done on this notebook|-|0.005|
|f4d2f7179bc1bc0c11bb03558ccf19a6a0ee3a00|indi-bert w/o fine-tuning|-|0.006|
|d421e45e0d00c77123ed61e7fef6b680aae9f19d|xlm-roberta-large-squad2 w/o fine-tuning|-|0.571|
|652b874625c0200035e39a79ab8144469b174a58|xlm-roberta-large-squad2 w/ fine-tuning (pipeline inference)|-|0.008|
|495c5de2a4b7582eccc3da4e1831572255e11004|xlm-roberta-large-squad2 w/ fine-tuning (naive torch inference)|-|Error|
|30955f14a9a0da8823b9e68df53443505ccacd96|copy & paste|-|0.792|
|5097e95f17ac09bb806088ba626ddb33f180ce5e|huggingface trainer API example reproduced faithfully|0.654|0.728|
|e316f40c44c52d204978b2c34a7b66320cb7687e|inference using fine-tuned model trained on Colab w/ 3 epochs|0.655|0.696|
|89a9c10ef578370e83f6ed05d86c24fa9fd4b262|--max_length 512 --doc_stride 80 --num_train_epochs 1|0.650|0.719|
|3c37a320af36ffe503d4839805c0cb7435601946|--drop_tamil_train true --CV 0.685 --LB 0.708|0.685|0.708|
|8c3d0c8487e6e7f99dc2277b7cc120259acb80a2|--max_seq_length 512 --doc_stride 80|-|0.775|
|7a15ddf4444426ad241d591b48610ec83311674a|--ensemble [2,1,1,1,1]|-|0.789|
|84a040e7d17bb3831a5f9ca5a7d9aa8081352043|--ensemble [1,2,1,1,1]|-|0.787|
|7495f69317823feeb8824da564a2d3bafad89479|riow1983/chaii-qa-5-fold-xlmroberta-torch-fit-7 used|-|0.770|
|1fae73a8fca470057b8650a6dbc8cd79fbbc2fc0|external data concatenated before 5-fold splitting|-|0.779|
|5f4c5aa7bb7bfe98f17a61f8ec1704d133a7987d|Models updated (tamil from train dropped)|-|0.771|
|356b83219e70108a9f6b47e850d640ac4c47e19b|Models updated (2 epochs for all folds)|-|0.783|
<br>

## Late Submissions
|commitSHA|comment|Local CV|Private LB|Public LB|
|----|----|----|----|----|
<br>


## My Assets
[notebookå‘½åè¦å‰‡]  
- kagglenb001-hoge.ipynb: Kaggle platformä¸Šã§æ–°è¦ä½œæˆã•ã‚ŒãŸKaggle notebook (kernel).
- localnb001-hoge.ipynb: localã§æ–°è¦ä½œæˆã•ã‚ŒãŸnotebook. 
- k2lnb001-hoge.ipynb: kagglenb001-hoge.ipynbã‚’localã«pullã—localã§å¤‰æ›´ã‚’åŠ ãˆã‚‹ã‚‚ã®.
- l2knb001-hoge.ipynb: localnb001-hoge.ipynbã‚’Kaggle platformã«pushã—ãŸã‚‚ã®.

#### Code
ä½œæˆã—ãŸnotebookç­‰ã®èª¬æ˜  
|name|url|input|output|status|comment|
|----|----|----|----|----|----|
|localnb001-export-transformers|[URL](https://github.com/riow1983/Kaggle-chaii/blob/master/notebooks/localnb001-export-transformers.ipynb)|-|[localnb001-export-transformers](https://www.kaggle.com/riow1983/localnb001-export-transformers)|Done|`bert-base-multilingual-cased`ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’Kaggle Datasetã¨ã—ã¦export|
|kagglenb001-chaii-eda|[URL](https://www.kaggle.com/riow1983/kagglenb001-chaii-eda)|[localnb001-export-transformers](https://www.kaggle.com/riow1983/localnb001-export-transformers)<br>[indic-bert](https://www.kaggle.com/ajax0564/indicbert)|submission.csv|Done|`bert-base-multilingual-cased`ã«ã‚ˆã‚‹äºˆæ¸¬ (w/o fine-tuning)|
|localnb002-fine-tune|[URL](https://github.com/riow1983/Kaggle-chaii/blob/master/notebooks/localnb002-fine-tune.ipynb)|`../input/chaii-hindi-and-tamil-question-answering/train.csv`|localnb002|Done|`bert-base-multilingual-cased`ã®fine-tuning|
|l2knb001-fine-tune|[URL](https://www.kaggle.com/riow1983/l2knb001-fine-tune)|localnb001, localnb002|submission.csv|ä½œæˆä¸­|fine-tuned `bert-base-multilingual-cased`ã«ã‚ˆã‚‹inference|
|kagglenb002-fine-tune|[URL](https://www.kaggle.com/riow1983/kagglenb002-fine-tune)|localnbf001, localnb002|submission.csv|ä½œæˆä¸­|kagglenb001ã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ãŸfine-tuned `bert-base-multilingual-cased`ã«ã‚ˆã‚‹inference|
|reproduction-of-0-792-notebook|[URL](https://www.kaggle.com/riow1983/reproduction-of-0-792-notebook)|[kishalmandal/5foldsroberta](https://www.kaggle.com/kishalmandal/5foldsroberta), [nguyenduongthanh/xlm-roberta-large-squad-v2](https://www.kaggle.com/nguyenduongthanh/xlm-roberta-large-squad-v2)|submission.csv|Done|[Reproduction of 0.792 notebook](https://www.kaggle.com/tkm2261/reproduction-of-0-792-notebook)ã®ã‚³ãƒ”ãƒ¼|
|ChAII - EDA & Baseline|[URL](https://www.kaggle.com/riow1983/chaii-eda-baseline)|[thedrcat/hf-datasets](https://www.kaggle.com/thedrcat/hf-datasets), [nbroad/xlm-roberta-squad2](https://www.kaggle.com/nbroad/xlm-roberta-squad2)|chaii-bert-trained, chaii-qa, runs, submission.csv|Done|[ChAII - EDA & Baseline](https://www.kaggle.com/thedrcat/chaii-eda-baseline)ã®ã‚³ãƒ”ãƒ¼|
|k2lnb001-chaii-eda-baseline-train|-|./input/hf-datasets, ./input/xlm-roberta-squad2|./notebooks/k2lnb001-chaii-eda-baseline-train/chaii-bert-trained, ./notebooks/k2lnb001-chaii-eda-baseline-train/chaii-qa, ./notebooks/k2lnb001-chaii-eda-baseline-train/runs|Done|[ChAII - EDA & Baseline](https://www.kaggle.com/riow1983/chaii-eda-baseline)ã‹ã‚‰inferenceéƒ¨åˆ†ã‚’é™¤å¤–ã—ãŸã‚‚ã®|
|kagglenb003-chaii-eda-baseline-inference|-|[thedrcat/hf-datasets](https://www.kaggle.com/thedrcat/hf-datasets), [nbroad/xlm-roberta-squad2](https://www.kaggle.com/nbroad/xlm-roberta-squad2), [riow1983/k2lnb001-chaii-eda-baseline-train](https://www.kaggle.com/riow1983/k2lnb001-chaii-eda-baseline-train)|submission.csv|Done|[ChAII - EDA & Baseline](https://www.kaggle.com/riow1983/chaii-eda-baseline)ã‹ã‚‰trainéƒ¨åˆ†ã‚’é™¤å¤–ã—ãŸã‚‚ã®|
<br>





***
## å‚è€ƒè³‡æ–™
#### Snipets
```python
# huggingface modelã‚’PyTorch nn.Moduleã§è¨“ç·´ã—ãŸå¾Œsave (& load) ã™ã‚‹æ–¹æ³•:
# reference: https://github.com/riow1983/Kaggle-Coleridge-Initiative#2021-04-25
model = MyModel(num_classes).to(device)
optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=1e-2)
output_model = './models/model_hoge.pth'

# save
def save(model, optimizer):
    # save
    torch.save({
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict()
    }, output_model)

save(model, optimizer)

# load
checkpoint = torch.load(output_model, map_location='cpu')
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
```
```python
# ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚’ç¢ºä¿ã—ãªãŒã‚‰ç‰¹å®šé•·ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’å¾—ã‚‹ãŸã‚ã®ãƒã‚¸ã‚·ãƒ§ãƒ³ã‚’å–å¾—ã™ã‚‹ãƒ«ãƒ¼ãƒ—

max_len = 512
context_len = 5121
overlap=60

for i in range(0, context_len, max_len-overlap):
    print("start_position:", i)
    print("end_position:", min(i + max_len, context_len-1))
    print()
```
```python
# ip_addressã”ã¨ã®æœ€é »å‡ºmalware_typeã‚’è¡¨ç¤ºã™ã‚‹Seriesã‚’å–å¾—ã™ã‚‹æ–¹æ³•

def md(s):
    c = Counter(s)
    return c.most_common(1)[0][0]

df.groupby('ip_address')['malware_type'].agg(md)
```
<br>


#### Papers
|name|url|status|comment|
|----|----|----|----|
|BERT Based Multilingual Machine Comprehension in English and Hindi|[URL](https://arxiv.org/pdf/2006.01432.pdf)|æœªèª­|-|
|Unsupervised Cross-lingual Representation Learning at Scale|[URL](https://arxiv.org/pdf/1911.02116.pdf)|æœªèª­|XLM-RoBERTaã®è«–æ–‡|
|RETHINKING EMBEDDING COUPLING IN PRE-TRAINED LANGUAGE MODELS|[URL](https://openreview.net/pdf?id=xpFFI_NtgpW)|æœªèª­|mBERTã®æ”¹è‰¯ç‰ˆ"RemBERT"ã®è«–æ–‡.<br>XLM-RoBERTaã‚’å‡Œé§•. [ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³](https://www.kaggle.com/c/chaii-hindi-and-tamil-question-answering/discussion/267827)ã§å–ã‚Šä¸Šã’ã‚‰ã‚Œã¦ã„ã‚‹.|
<br>


#### Blogs / Qiita / etc.
|name|url|status|comment|
|----|----|----|----|
|BERTå…¥é–€|[URL](https://www.slideshare.net/matsukenbook/bert-217710964)|Done|Kaggle inferenceæ™‚ (=internetã‚ªãƒ•æ™‚),<br>:hugs: transformersã®ãƒ¡ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—ã™ã‚‹æ–¹é‡ãŒå‚è€ƒã«ãªã£ãŸ|
|Unicode block|[URL](https://en.wikipedia.org/wiki/Unicode_block)|Done|å„è¨€èªæ–‡å­—ã®block rangeãŒç¤ºã•ã‚Œã¦ã„ã‚‹|
|(æ­£è¦è¡¨ç¾) iHateRegex|[URL](https://ihateregex.io/expr/hyphen-word-break/)|Done|æ­£è¦è¡¨ç¾ã®å‹•ä½œç¢ºèªãŒãƒ–ãƒ©ã‚¦ã‚¶ä¸Šã§ã§ãã‚‹|
|The Unicode Standard, Version 13.0 > Devanagari|[URL](https://unicode.org/charts/PDF/U0900.pdf)|Done|Devanagariã®unicodeã®è§£èª¬æ›¸|
|Adapting BERT question answering for the medical domain|[URL](https://medium.com/analytics-vidhya/adapting-bert-question-answering-for-the-medical-domain-2085ada8ceb1)|Done|Dealing with a long contextã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«512ãƒˆãƒ¼ã‚¯ãƒ³é•·ä»¥å†…ã«contextã‚’åˆ†å‰²ã™ã‚‹æ‰‹æ³•ãŒè¨€è‘‰ã§èª¬æ˜ã•ã‚Œã¦ã„ã‚‹|
|Question Answering with a fine-tuned BERT|[URL](https://towardsdatascience.com/question-answering-with-a-fine-tuned-bert-bc4dafd45626)|Done|fine-tunedã¨ã„ã†ã“ã¨ã§fine-tuneã™ã‚‹å®Ÿè£…ä¾‹ã§ã¯ãªã, fine-tuneæ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«ã®åˆ©ç”¨ä¾‹ã«ãªã£ã¦ã„ã‚‹ã“ã¨ã«æ³¨æ„|
|ã€çªç„¶GitHubã«pushã§ããªããªã£ãŸã€‘ ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆã§è§£æ±ºï¼šThe requested URL returned error: 403|[URL](https://zenn.dev/yuri0427/articles/9587ae6a578ee9)|Done|GitHubé€£æºã«ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰æ–¹å¼ãŒä½¿ãˆãªããªã£ãŸ2021å¹´8æœˆ13æ—¥ä»¥é™ã¯ã“ã®ãƒˆãƒ¼ã‚¯ãƒ³æ–¹å¼ã«ãªã‚‹|
|(:hugs:) ValueError: char_to_token() is not available when using Python based tokenizers|[URL](https://www.gitmemory.com/issue/huggingface/transformers/12201/862549850)|Done|:hugs: `char_to_token`ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ã†ãªã‚‰fast tokenizerã‚’ä½¿ã†ã¹ã—|
|(pandas) pandasã§DataFrameã®ã‚»ãƒ«ã«listã‚’ä»£å…¥ã™ã‚‹|[URL](https://linus-mk.hatenablog.com/entry/pandas_insert_list_into_cell)|Done|`object`å‹ã®Seriesã«å¯¾ã—ã¦`.at`ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ã†ã¹ã—|
|HTML ç‰¹æ®Šæ–‡å­—|[URL](https://qiita.com/inabe49/items/303afa114b0204da8a24)|Done|çŸ¢å°ã‚„ã‚®ãƒªã‚·ã‚¢æ–‡å­—ãªã©ã®è¨˜æ³•é›†|
|pandasã§jsonlã‚’æ‰±ã„ãŸã„æ™‚|[URL](https://qiita.com/meshidenn/items/3ff72396fe85044bc74f)|Done|jsonlã¨ã¯|
|ã‚¨ãƒ©ãƒ¼ã‚’å‡ºã•ãšã«æœ€é »å€¤ã‚’å¾—ãŸã„ã¨ã|[URL](https://qiita.com/tmitani/items/bd77eb08f1da7c283fed)|Done|statistics.modeã§ã¯ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹å±€é¢ã‚‚collections.Counterã§è§£æ±ºå¯èƒ½|
<br>


#### Official Documentation or Tutorial
|name|url|status|comment|
|----|----|----|----|
|(:hugs:) Fine-tuning a pretrained model|[URL](https://huggingface.co/transformers/training.html)|å‚ç…§ä¸­|Trainer APIã‚„ç´ ã®PyTorchã«ã‚ˆã‚‹:hugs: pretrained modelã®fine-tuningå®Ÿè£…ä¾‹|
|(:hugs:) Fine-tuning with custom datasets|[URL](https://huggingface.co/transformers/custom_datasets.html#fine-tuning-with-custom-datasets)|å‚ç…§ä¸­|IMDb(Sequence Classification), W-NUT(Token Classification = NER), SQuAD2.0(QA) ãã‚Œãã‚Œã«ã¤ã„ã¦ã®fine-tuningå®Ÿè£…ä¾‹|
|(PyTorch) DATASETS & DATALOADERS|[URL](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)|å‚ç…§ä¸­|PyTorch Dataset, DataLoaderã®å…¬å¼å®Ÿè£…è§£èª¬|
|(PyTorch) Custom loss functions|[URL](https://discuss.pytorch.org/t/custom-loss-functions/29387)|Done|PyTorchã§Custom loss functionã‚’å®Ÿè£…ã™ã‚‹æ–¹æ³•|
|(:hugs:) BERT|[URL](https://huggingface.co/transformers/model_doc/bert.html?highlight=bertforquestionanswering#berttokenizer)|å‚ç…§ä¸­|:hugs:BERTãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸|
|(:hugs:) RoBERTa|[URL](https://huggingface.co/transformers/model_doc/roberta.html)|å‚ç…§ä¸­|:hugs:RoBERTaãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸|
|(:hugs:) XLM-RoBERTa|[URL](https://huggingface.co/transformers/model_doc/xlmroberta.html)|å‚ç…§ä¸­|:hugs:XLM-RoBERTaãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸|
|(:hugs:) Tokenizer|[URL](https://huggingface.co/transformers/main_classes/tokenizer.html#tokenizer)|å‚ç…§ä¸­|:hugs:Tokenizerãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸|
|(:hugs:) Preprocessing data|[URL](https://huggingface.co/transformers/preprocessing.html)|å‚ç…§ä¸­|tokenizerã«batch sentence Aã¨batch sentence Bã‚’å…¥åŠ›ã—ãŸå ´åˆã«å‡ºåŠ›ã•ã‚Œã‚‹ã‚‚ã®ã‚’ç¢ºèªã§ãã‚‹|
|(:hugs:) Extractive Question Answering|[URL](https://huggingface.co/transformers/task_summary.html#extractive-question-answering)|Done|`pipeline`ã«ã‚ˆã‚‹inferenceã¨`torch`ã«ã‚ˆã‚‹inferenceä¸¡è€…ã®ä¾‹ãŒã‚·ãƒ³ãƒ—ãƒ«ã«ã¾ã¨ã¾ã£ã¦ã„ã‚‹|
<br>

#### StackOverflow
|name|url|status|comment|
|----|----|----|----|
|BERT tokenizer & model download|[URL](https://stackoverflow.com/questions/59701981/bert-tokenizer-model-download)|Done|:hugs: transformersã®ãƒ¡ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ä¾‹.<br>ãŸã ã—æœ€è¿‘ã¯æ ¼ç´å…ˆãŒHugging Face Platformã«å¤‰æ›´ã«ãªã£ãŸæ¨¡æ§˜.|
|python pandas: apply a function with arguments to a series|[URL](https://stackoverflow.com/questions/12182744/python-pandas-apply-a-function-with-arguments-to-a-series)|Done|applyå¯¾è±¡ã®é–¢æ•°ã¸ã®å¼•æ•°ã®æ¸¡ã—æ–¹|
|Python: Find a substring in a string and returning the index of the substring|[URL](https://stackoverflow.com/questions/21842885/python-find-a-substring-in-a-string-and-returning-the-index-of-the-substring)|Done|`string`å‹ã®findãƒ¡ã‚½ãƒƒãƒ‰ã§ã§ãã‚‹|
|(pandas) Pandas drop duplicates on one column and keep only rows with the most frequent value in another column|[URL](https://stackoverflow.com/questions/63319148/pandas-drop-duplicates-on-one-column-and-keep-only-rows-with-the-most-frequent-v)|Done|å‡ºç¾å›æ•°ãŒæœ€é »ã®ã‚‚ã®ã‚’æ®‹ã—ã¦drop duplicatesã™ã‚‹æ–¹æ³•|
|(:hugs:) Transformers v4.x: Convert slow tokenizer to fast tokenizer|[URL](https://stackoverflow.com/questions/65431837/transformers-v4-x-convert-slow-tokenizer-to-fast-tokenizer)|Done|`XLMRobertaTokenizer`ã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã«ã¯`pip install transformers[sentencepiece]`ã¨ã—ã¦ã‚„ã‚‹å¿…è¦ã‚ã‚Š|


#### GitHub
|name|url|status|comment|
|----|----|----|----|
|Kaggle-Coleridge-Initiative|[URL](https://github.com/riow1983/Kaggle-Coleridge-Initiative)|Done|Coleridgeã‚³ãƒ³ãƒšæ™‚ã«ã¤ã‘ã¦ã„ãŸKaggeæ—¥è¨˜|
|å€‹äººã‚¢ã‚¯ã‚»ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ç”¨ã™ã‚‹|[URL](https://docs.github.com/ja/github/authenticating-to-github/keeping-your-account-and-data-secure/creating-a-personal-access-token)|Done|GitHubé€£æºã«ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰æ–¹å¼ãŒä½¿ãˆãªããªã£ãŸ2021å¹´8æœˆ13æ—¥ä»¥é™ã¯ã“ã®ãƒˆãƒ¼ã‚¯ãƒ³æ–¹å¼ã«ãªã‚‹|
|(PyTorch) How to slice a BatchEncoding object into desired batch sizes?|[URL](https://github.com/huggingface/tokenizers/issues/577)|Done|`tokenizer`ã¯batchã«ã—ãŸtextã‚’å—ã‘å–ã‚Œã‚‹ä»•æ§˜|
|IndicBERT|[URL](https://github.com/AI4Bharat/indic-bert)|æœªç¢ºèª|[ai4bharat/indic-bert](https://huggingface.co/ai4bharat/indic-bert)ã«fine-tuneing cliãŒä»˜ã„ãŸã‚‚ã®(?)|
|Visualize your ğŸ¤— Hugging Face data with ğŸ‹â€â™€ï¸ Weights & Biases|[URL](https://github.com/wandb/examples/blob/master/colabs/huggingface/Visualize_your_Hugging_Face_data_with_Weights_%26_Biases.ipynb)|å®Ÿè¡Œã‚¨ãƒ©ãƒ¼|Colabã§å®Ÿè¡Œã—ã¦ã¿ãŸãŒãƒ‡ãƒ¼ã‚¿ã®ãƒ­ãƒ¼ãƒ‰ã§ã‚¨ãƒ©ãƒ¼ãŒå‡ºã‚‹|
<br>

#### Hugging Face Platform
|name|url|status|comment|
|----|----|----|----|
|bert-base-multilingual-cased|[URL](https://huggingface.co/bert-base-multilingual-cased)|Done|`config.json`, `pytorch_model.bin`, `vocab.txt`ã‚’å–å¾—<br>kagglenb001-chaii-edaã«ã¦ä½¿ç”¨|
|ai4bharat/indic-bert|[URL](https://huggingface.co/ai4bharat/indic-bert)|Done|kagglenb001-chaii-edaã«ã¦ä½¿ç”¨|
|deepset/xlm-roberta-large-squad2|[URL](https://huggingface.co/deepset/xlm-roberta-large-squad2)|Done|kagglenb001-chaii-edaã«ã¦ä½¿ç”¨.<br>tokenizerã«ã¯`XLMRobertaTokenizer`ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã“ã¨ãŒ[ã“ã¡ã‚‰](https://public-mlflow.deepset.ai/#/experiments/124/runs/3a540e3f3ecf4dd98eae8fc6d457ff20)ã§ç¢ºèªã§ãã‚‹<br>|
<br>

#### Kaggle Notebooks
|name|url|status|comment|
|----|----|----|----|
|coleridge_regex_electra|[URL](https://www.kaggle.com/nbroad/no-training-question-answering-model/data?scriptVersionId=66240356)|å‚è€ƒ|Coleridgeã‚³ãƒ³ãƒšã®47th solution.<br>transformers pipelineã‚’ä½¿ã£ãŸQAå®Ÿè£…ä¾‹ãŒéå¸¸ã«åˆ†ã‹ã‚Šã‚„ã™ã, æœ¬ã‚³ãƒ³ãƒšã§ã‚‚å‚è€ƒã¨ã—ãŸ.|
|chaii-QA: multi-lingual pretrained baseline|[URL](https://www.kaggle.com/nbroad/chaii-qa-multi-lingual-pretrained-baseline)|å‚ç…§ä¸­|:hugs:è£½ãƒãƒ«ãƒãƒªãƒ³ã‚¬ãƒ«ç³»pre-trainedãƒ¢ãƒ‡ãƒ«ã‚’pipelineãƒ¡ã‚½ãƒƒãƒ‰ã§å®Ÿè¡Œã®ã†ãˆsubmitã—ã¦ã¿ãŸçµæœ`xlm-roberta-large-squad2`ãŒæœ€è‰¯ã¨ã®ã“ã¨|
|Intro to Hugging Face datasets :hugs:|[URL](https://www.kaggle.com/nbroad/intro-to-hugging-face-datasets/notebook)|èª­äº†|å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã¯tensorã‚’æ ¼ç´ã—ãŸè¾æ›¸({'input_ids':tensor(), 'attention_mask':tensor(), 'offset_mapping':tensor(), start_positions':tensor(), 'end_positions':tensor()})ã§ã‚ã‚Œã°è‰¯ã„.<br>:hugs: `datasets`ã¯ãƒ‡ãƒ¼ã‚¿åŠ å·¥ã«pandasã‚’å¿…è¦ã¨ã—ãªã„ã»ã©æŸ”è»Ÿæ€§ãŒã‚ã‚Šãã†.|
<br>


#### Kaggle Datasets
|name|url|status|comment|
|----|----|----|----|
|xlm roberta squad2|[URL](https://www.kaggle.com/nbroad/xlm-roberta-squad2)|Done|[deepset/xlm-roberta-large-squad2](deepset/xlm-roberta-large-squad2)ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸã‚‚ã®|
<br>

#### Kaggle Discussion
|name|url|status|comment|
|----|----|----|----|
|Recipe for winning?|[URL](https://www.kaggle.com/c/chaii-hindi-and-tamil-question-answering/discussion/264917#1482290)|Done|å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ãƒ©ãƒ™ãƒ«ãŒé©å½“(ç‰¹ã«tã‚¿ãƒŸãƒ«èª)ã‹ã¤ãƒ‡ãƒ¼ã‚¿é‡ãŒå°‘ãªã„ä»¶ã«ã¤ã„ã¦æ‰‹å‹•ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®æœ‰åˆ©æ€§ã«è¨€åŠã—ãŸã‚‚ã®.<br>ã‚‚ã—ãã¯å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€åˆ‡ä½¿ã‚ãšå¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã ã‘ã§fine-tuneã—ãŸã»ã†ãŒã„ã„ã¨ã„ã†æ„è¦‹ã‚‚.|
|Hindi & Tamil QA papers / datasets|[URL](https://www.kaggle.com/c/chaii-hindi-and-tamil-question-answering/discussion/264344)|æœªç¢ºèª|ä½¿ãˆãã†ãªå¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã‚„pre-trainedãƒ¢ãƒ‡ãƒ«ã®ç´¹ä»‹|
|Useful Resources for the competition|[URL](https://www.kaggle.com/c/chaii-hindi-and-tamil-question-answering/discussion/264795)|Done|ã‚¿ã‚¤ãƒˆãƒ«ã¨ã¯è£è…¹ã«å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ãƒ©ãƒ™ãƒ«ãŒã„ã„åŠ æ¸›ã ã¨ã„ã†æŒ‡æ‘˜ã¾ã¨ã‚ã¨ãã“ã«èµ·å› ã™ã‚‹å€«ç†çš„å•é¡Œã«ã¤ã„ã¦ã®ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³ã¾ã¨ã‚ãŒç§€é€¸|
<br>



***
## Diary

#### 2021-08-21  
ã‚³ãƒ³ãƒšå‚åŠ . 
<br>
<br>
<br>

#### 2021-08-22
kagglenb001-chaii-edaã‚’ä½œæˆ.
<br>
<br>
<br>

#### 2021-08-24
kagglenb001-chaii-edaã«ã‚ˆã‚‹submitå®Œäº†.
<br>
<br>
<br>

#### 2021-09-13
localnb002-fine-tuneã®preprocessingã‚³ãƒ¼ãƒ‰ãŒã‚ˆã†ã‚„ãã²ã¨æ®µè½ã—, trainé–‹å§‹.
<br>
<br>
<br>

#### 2021-09-14
fine-tuned mBERTã§submitã™ã‚‹ã‚‚zero-shotã‚ˆã‚Šã‚‚LBæ‚ªåŒ–(0.010 &rArr; 0.002).<br>
inferenceã‚’pipelineã§å®Ÿæ–½ã—ã¦ã„ãŸãŒ, [torch-nativeãªæ–¹æ³•](https://huggingface.co/transformers/task_summary.html#extractive-question-answering)ã‚‚è©¦ã—ãŸã„.<br>
ãã®æ¬¡ä½•ã™ã‚‹ã‹:
- pipelineæ–¹å¼ã§trainã™ã‚‹éš›, Hindiãƒ¢ãƒ‡ãƒ«ã¨Tamilãƒ¢ãƒ‡ãƒ«ã«åˆ†ã‘ã¦ãã‚Œãã‚Œã§inferenceã™ã‚‹ &rArr; [issue#2](https://github.com/riow1983/Kaggle-chaii/issues/2)
- Jaccardé–¢æ•°ãªã©lossé–¢æ•°ã«çµ„ã¿è¾¼ã‚“ã§ã¡ã‚ƒã‚“ã¨trainã™ã‚‹ &rArr; [issue#3](https://github.com/riow1983/Kaggle-chaii/issues/3)
- mBERTã®æ çµ„ã¿ã§, Hindi Question - Tamil Answer ãªã©è‡ªå‹•ç¿»è¨³ãªã©ã‚’åˆ©ç”¨ã—ã¦data augmentationã—ã¦trainã™ã‚‹
- Hindi - Tamil ã«ç‰¹åŒ–ã—ãŸpre-trainedãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã‚‹å½¢ã§ã©ã“ã‹ã«è½ã¡ã¦ãªã„ã‹
- è«¦ã‚ã¦ãƒ›ã‚¹ãƒˆã®tutorial notebookã®è»é–€ã«ä¸‹ã‚‹ã‹ &rArr; [issue#4](https://github.com/riow1983/Kaggle-chaii/issues/4)
<br>
<br>
<br>

#### 2021-09-15
>inferenceã‚’pipelineã§å®Ÿæ–½ã—ã¦ã„ãŸãŒ, [torch-nativeãªæ–¹æ³•](https://huggingface.co/transformers/task_summary.html#extractive-question-answering)ã‚‚è©¦ã—ãŸã„.

ã«ã¤ã„ã¦å®Ÿè¡Œã—ãŸ. çµæœã¯ä»¥ä¸‹ã®é€šã‚Š:
|Notebook|fine-tuningæ–¹å¼|inferenceæ™‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²ã—ã¦ã‚‹ã‹|inferenceæ–¹å¼|Public LB|
|----|----|----|----|----|
|kagglenb002-fine-tune|torch-native(ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²)|No|pipeline|0.002|
|kagglenb002-fine-tune|torch-native(ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²)|No|torch-native|0.005|
|l2knb001-fine-tune|torch-native(ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²)|Yes|pipeline|Error|
|l2knb001-fine-tune|torch-native(ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²)|Yes|torch-native|0.005|
|kagglenb001-chaii-eda|-|No|pipeline|0.010|
|kagglenb001-chaii-eda|~~pipeline(ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãã®ã¾ã¾)~~ -<br>trainã—ã¦ã„ãŸã¨æ€ã„ãã‚„ã—ã¦ã„ãªã‹ã£ãŸ|No|pipeline|0.005|
<br>
kagglenb001-chaii-edaã«ã¤ã„ã¦ã¯ï¼’å›submitã—ã¦ã„ã‚‹ãŒã„ãšã‚Œã‚‚fine-tuningã—ã¦ã„ãªã„ã‚‚ã®ã§ã‚ã‚‹. ã«ã‚‚é–¢ã‚ã‚‰ãšLBãŒç•°ãªã£ã¦ã„ã‚‹ã®ã¯ãƒ¢ãƒ‡ãƒ«ã®ãƒ©ãƒ³ãƒ€ãƒ æ€§ãŒèµ·å› ã—ã¦ã„ã‚‹ã‚‚ã®ã¨æ€ã‚ã‚Œã‚‹. (pre-trainedãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹åº¦ã«output(answer span)ãŒå¤‰åŒ–ã™ã‚‹ã“ã¨ã¯ç¢ºèªã—ãŸ.)
<br>
<br>
<br>

#### 2021-10-05
> It seems like the data is generated by a model. especially for Tamil where even the answer text and start index are incorrect. So, what are we actually learning here?
[what are we learning?](https://www.kaggle.com/c/chaii-hindi-and-tamil-question-answering/discussion/267124)

ã¨ã®ã“ã¨ã§ç‰¹ã«Tamilèªã®trainã®æ•™å¸«ãƒ©ãƒ™ãƒ«ã¯ã„ã„åŠ æ¸›ã‚‰ã—ã„. fine-tuningã§Tamil trainã‚’é™¤å¤–ã—ã¦ã¿ã‚ˆã†ã‹.
<br>
<br>
<br>

#### 2021-10-19
[notebooks/chaii-qa-5-fold-xlmroberta-torch-fit.ipynb](https://github.com/riow1983/Kaggle-chaii/blob/master/notebooks/chaii-qa-5-fold-xlmroberta-torch-fit.ipynb)ã§foldã”ã¨ã®valid lossã‚’ç¢ºèª. fold 5ã§valid lossãŒ0ã«ãªã£ã¦ã„ã‚‹ã®ã¯ãªãœã‹.<br>
[inference notebook](https://www.kaggle.com/riow1983/reproduction-of-0-792-notebook/notebook?scriptVersionId=77460400)ã§fold 5ã®çµæœã‚’ä½¿ç”¨ã—ãªã„submitã‚’ã—ã¦ã¿ã‚‹. -> LB: 0.783 ï¼¿|ï¿£|â—‹

```
--------------------------------------------------
FOLD: 1
--------------------------------------------------
----Validation Results Summary----
Epoch: [0] Valid Loss: 0.62316
0 Epoch, Best epoch was updated! Valid Loss: 0.62316

----Validation Results Summary----
Epoch: [1] Valid Loss: 0.71733


--------------------------------------------------
FOLD: 2
--------------------------------------------------
----Validation Results Summary----
Epoch: [0] Valid Loss: 0.57144
0 Epoch, Best epoch was updated! Valid Loss: 0.57144

----Validation Results Summary----
Epoch: [1] Valid Loss: 0.70074


--------------------------------------------------
FOLD: 3
--------------------------------------------------
----Validation Results Summary----
Epoch: [0] Valid Loss: 0.63953
0 Epoch, Best epoch was updated! Valid Loss: 0.63953

----Validation Results Summary----
Epoch: [1] Valid Loss: 0.77846


--------------------------------------------------
FOLD: 4
--------------------------------------------------
----Validation Results Summary----
Epoch: [0] Valid Loss: 0.62968
0 Epoch, Best epoch was updated! Valid Loss: 0.62968

----Validation Results Summary----
Epoch: [1] Valid Loss: 0.73707


--------------------------------------------------
FOLD: 5
--------------------------------------------------
----Validation Results Summary----
Epoch: [0] Valid Loss: 0.00000
0 Epoch, Best epoch was updated! Valid Loss: 0.00000

----Validation Results Summary----
Epoch: [1] Valid Loss: 0.00000
```

ã¾ãŸepochæ•°ã¯1ã§å……åˆ†ãªã®ã‹ã‚‚ã—ã‚Œãªã„. ã“ã‚Œã‚‚ã‚‚ã†ä¸€å›ã‚„ã£ã¦ã¿ã‚‹?



