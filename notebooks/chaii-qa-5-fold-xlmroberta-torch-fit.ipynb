{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"name":"chaii-qa-5-fold-xlmroberta-torch-fit.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"5c5c53bdc5db4d5bb3131540881362a7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_cfab0d413e1e484192935cb271bb445f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_69f119f7e86a412d96379a4c91db69de","IPY_MODEL_05f6575034f4459fad752763d377dcbe","IPY_MODEL_3dc3a2f8727346329a17108f5723597d"]}},"cfab0d413e1e484192935cb271bb445f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"69f119f7e86a412d96379a4c91db69de":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d3fcdaa804e14986884d5b24a0df61be","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c0df1ef7ce6046dc8f5eb9684cd9fd8a"}},"05f6575034f4459fad752763d377dcbe":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_6334c7e5244641b0b1160cf551206691","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":606,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":606,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a5aa032868284e60800de8548f5093e3"}},"3dc3a2f8727346329a17108f5723597d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_eaeebe5de19844ec95e52099447e930c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 606/606 [00:00&lt;00:00, 20.5kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d9d559fabd0844aaa0a5ea6693501d72"}},"d3fcdaa804e14986884d5b24a0df61be":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c0df1ef7ce6046dc8f5eb9684cd9fd8a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6334c7e5244641b0b1160cf551206691":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a5aa032868284e60800de8548f5093e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"eaeebe5de19844ec95e52099447e930c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d9d559fabd0844aaa0a5ea6693501d72":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"07e9782e0356466fb8d9c9ea085cee79":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4f1b0fff485e474296f4e9f33f0c7dbd","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_810166135c3f48f5914c2600421e92ab","IPY_MODEL_8523fd8459c34a6b9d3917699a1d1f40","IPY_MODEL_29862227e7f543768cd95384aea8123d"]}},"4f1b0fff485e474296f4e9f33f0c7dbd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"810166135c3f48f5914c2600421e92ab":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_82cfa58a26ea4104a99643a6405afc93","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c803908cebdd4975adde33f4ad5778af"}},"8523fd8459c34a6b9d3917699a1d1f40":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_6bafb1630ce54c62a28b120b73c879e1","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":179,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":179,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a1bfeecbdc5a42bb9bd83dc6afe337d7"}},"29862227e7f543768cd95384aea8123d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e409f4c9bc404955b3396776634ed13f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 179/179 [00:00&lt;00:00, 7.06kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e4f4c2901fc4410382751e763ce775de"}},"82cfa58a26ea4104a99643a6405afc93":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c803908cebdd4975adde33f4ad5778af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6bafb1630ce54c62a28b120b73c879e1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a1bfeecbdc5a42bb9bd83dc6afe337d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e409f4c9bc404955b3396776634ed13f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e4f4c2901fc4410382751e763ce775de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"abaee2d39aae476eb2fae12c3faed028":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_91e309bb3f3d44bf9ef21be77dbd294b","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_127aff325ee84ea389c83b06c9033fdf","IPY_MODEL_e77cf9ff29574af3bdfee828620d1602","IPY_MODEL_0b57515be5ba474eafb4b6df237fb331"]}},"91e309bb3f3d44bf9ef21be77dbd294b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"127aff325ee84ea389c83b06c9033fdf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_240b3df9dc32467ca4839fe8c5fd8950","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d2aac14280f84cccb2d1b877636ae275"}},"e77cf9ff29574af3bdfee828620d1602":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_2b6a7abda3ba46bc95b811d94f62e272","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":5069051,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":5069051,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4f4f04d8233f4a3cbd8955deb2a6c484"}},"0b57515be5ba474eafb4b6df237fb331":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f0ef22ede61549fcb5fa4547c8ab6791","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 4.83M/4.83M [00:01&lt;00:00, 6.36MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9461f6ee3a8b4f7a9226d62b38302b3d"}},"240b3df9dc32467ca4839fe8c5fd8950":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d2aac14280f84cccb2d1b877636ae275":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2b6a7abda3ba46bc95b811d94f62e272":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"4f4f04d8233f4a3cbd8955deb2a6c484":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f0ef22ede61549fcb5fa4547c8ab6791":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"9461f6ee3a8b4f7a9226d62b38302b3d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a84b11799a164f929883288adb8b096a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_fb84277845b947f1b428df3db441b630","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_db9f7d908db0441fa10e495160f09b9a","IPY_MODEL_d2a90d7b38e241cba13160ab9aeb178e","IPY_MODEL_74fde70c5615493884afa661589bc1a5"]}},"fb84277845b947f1b428df3db441b630":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"db9f7d908db0441fa10e495160f09b9a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ce8d968006c44338af47ec2d0e5ba791","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bfb1bcc868cc40e982a5a5c5f101f059"}},"d2a90d7b38e241cba13160ab9aeb178e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_f96250164b2a4a368df68a7eee64c50d","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":150,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":150,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e0691c2695264d8b8777bafbd143dcc8"}},"74fde70c5615493884afa661589bc1a5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_857237a158b142ae8dc30e38ab774cea","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 150/150 [00:00&lt;00:00, 5.73kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fa6df4a64a0342eda7973615249c6163"}},"ce8d968006c44338af47ec2d0e5ba791":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"bfb1bcc868cc40e982a5a5c5f101f059":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f96250164b2a4a368df68a7eee64c50d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e0691c2695264d8b8777bafbd143dcc8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"857237a158b142ae8dc30e38ab774cea":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"fa6df4a64a0342eda7973615249c6163":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"29564eac28f4472e961bb9eb30db9ba4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_cb6e60b18c414658ac87e76a5adf8639","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_08587f4c0d7f4863a6a8c8c03b596f76","IPY_MODEL_b94e152275794a6597437068d1891799","IPY_MODEL_1275f41ca8844afcaee8be0237359564"]}},"cb6e60b18c414658ac87e76a5adf8639":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"08587f4c0d7f4863a6a8c8c03b596f76":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_384af44eb3cd4c17b853ec0cc353b2a2","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9331f39ef8ed4936837cb713f1a86a18"}},"b94e152275794a6597437068d1891799":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_430264c5168e40f1850aca8cb4ddc6be","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":2239666418,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":2239666418,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_dc54f20f45ca4e8385c0ae9469d6466c"}},"1275f41ca8844afcaee8be0237359564":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9452aba62430415db767b0a399ff610e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 2.09G/2.09G [00:38&lt;00:00, 52.9MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_98fcdf55b2cf429281759b32477bd453"}},"384af44eb3cd4c17b853ec0cc353b2a2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"9331f39ef8ed4936837cb713f1a86a18":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"430264c5168e40f1850aca8cb4ddc6be":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"dc54f20f45ca4e8385c0ae9469d6466c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9452aba62430415db767b0a399ff610e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"98fcdf55b2cf429281759b32477bd453":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RkuY4GVbQ1k4","executionInfo":{"status":"ok","timestamp":1634374084918,"user_tz":-540,"elapsed":1377,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"1798f1c2-6b18-4116-886f-fdbd421a1dda"},"source":["# Kaggle or Colab\n","import sys\n","import os\n","if 'kaggle_web_client' in sys.modules:\n","    # Do something\n","    pass\n","elif 'google.colab' in sys.modules:\n","    # Do something\n","    from google.colab import drive\n","    drive.mount(\"/content/drive\")\n","\n","    comp_name_official = \"chaii-hindi-and-tamil-question-answering\"\n","    comp_name_local = \"Kaggle-chaii\"\n","\n","    !pip install --upgrade --force-reinstall --no-deps kaggle\n","    import json\n","    f = open(\"/content/drive/MyDrive/colab_notebooks/kaggle/kaggle.json\", \"r\")\n","    json_data = json.load(f)\n","    os.environ[\"KAGGLE_USERNAME\"] = json_data[\"username\"]\n","    os.environ[\"KAGGLE_KEY\"] = json_data[\"key\"]\n","\n","    %cd /content/drive/MyDrive/colab_notebooks/kaggle/{comp_name_local}/notebooks\n","\n","    dname = \"chaii-qa-5-fold-xlmroberta-torch-fit\"\n","    !mkdir {dname}\n","    #%cd /content/drive/MyDrive/colab_notebooks/kaggle/{comp_name_local}/notebooks/{dname}"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Collecting kaggle\n","  Using cached kaggle-1.5.12-py3-none-any.whl\n","Installing collected packages: kaggle\n","  Attempting uninstall: kaggle\n","    Found existing installation: kaggle 1.5.12\n","    Uninstalling kaggle-1.5.12:\n","      Successfully uninstalled kaggle-1.5.12\n","Successfully installed kaggle-1.5.12\n","/content/drive/MyDrive/colab_notebooks/kaggle/Kaggle-chaii/notebooks\n","mkdir: cannot create directory ‘chaii-qa-5-fold-xlmroberta-torch-fit’: File exists\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a0xZ62k7SB1g","executionInfo":{"status":"ok","timestamp":1634374087441,"user_tz":-540,"elapsed":2526,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"924353f0-e114-4007-daa3-674aa9e9b542"},"source":["if 'kaggle_web_client' in sys.modules:\n","    # Do something\n","    pass\n","elif 'google.colab' in sys.modules:\n","    #!pip install transformers\n","    !pip install transformers[sentencepiece]\n","\n","    # import yaml\n","    # with open(f'./config_notebook/config.yaml') as file:\n","    #     cfg = yaml.load(file, Loader=yaml.FullLoader) # Loader is recommended\n","    # print(\"Config:\\n\", cfg)"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.7/dist-packages (4.11.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2019.12.20)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (0.0.46)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (0.10.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (4.62.3)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (21.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (4.8.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.3.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2.23.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (6.0)\n","Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (0.0.19)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (1.19.5)\n","Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (0.1.96)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.17.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers[sentencepiece]) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers[sentencepiece]) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers[sentencepiece]) (3.6.0)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers[sentencepiece]) (1.15.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (2021.5.30)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[sentencepiece]) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[sentencepiece]) (1.0.1)\n"]}]},{"cell_type":"markdown","metadata":{"id":"uQdNDkrBP4hj"},"source":["<h2>chaii QA - 5 Fold XLMRoberta Finetuning in Torch w/o Trainer API</h2>\n","    \n","<h3><span \"style: color=#444\">Introduction</span></h3>\n","\n","The kernel implements 5-Fold XLMRoberta QA Model without using the Trainer API from HuggingFace.\n","\n","This is a three part kernel,\n","\n","- [External Data - MLQA, XQUAD Preprocessing](https://www.kaggle.com/rhtsingh/external-data-mlqa-xquad-preprocessing) which preprocesses the Hindi Corpus of MLQA and XQUAD. I have used these data for training.\n","\n","- [chaii QA - 5 Fold XLMRoberta Torch | FIT](https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-fit/edit) This kernel is the current kernel and used for Finetuning (FIT) on competition + external data.\n","\n","- [chaii QA - 5 Fold XLMRoberta Torch | Infer](https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-infer) The Inference kernel where we ensemble our 5 Fold XLMRoberta Models and do the submission.\n","\n","<h3><span \"style: color=#444\">Techniques</span></h3>\n","\n","The kernel has implementation for below techniques, click on the links to learn more -\n","\n"," - [External Data Preprocessing + Training](https://www.kaggle.com/rhtsingh/external-data-mlqa-xquad-preprocessing)\n"," \n"," - [Mixed Precision Training using APEX](https://www.kaggle.com/rhtsingh/swa-apex-amp-interpreting-transformers-in-torch)\n"," \n"," - [Gradient Accumulation](https://www.kaggle.com/rhtsingh/speeding-up-transformer-w-optimization-strategies)\n"," \n"," - [Gradient Clipping](https://www.kaggle.com/rhtsingh/swa-apex-amp-interpreting-transformers-in-torch)\n"," \n"," - [Grouped Layerwise Learning Rate Decay](https://www.kaggle.com/rhtsingh/guide-to-huggingface-schedulers-differential-lrs)\n"," \n"," - [Utilizing Intermediate Transformer Representations](https://www.kaggle.com/rhtsingh/utilizing-transformer-representations-efficiently)\n"," \n"," - [Layer Initialization](https://www.kaggle.com/rhtsingh/on-stability-of-few-sample-transformer-fine-tuning)\n"," \n"," - [5-Fold Training](https://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-fit)\n"," \n"," - etc.\n"," \n","<h3><span \"style: color=#444\">References</span></h3>\n","I would like to acknowledge below kagglers work and resources without whom this kernel wouldn't have been possible,  \n","\n","- [roberta inference 5 folds by Abhishek](https://www.kaggle.com/abhishek/roberta-inference-5-folds)\n","\n","- [ChAII - EDA & Baseline by Darek](https://www.kaggle.com/thedrcat/chaii-eda-baseline) due to whom i found the great notebook below from HuggingFace,\n","\n","- [How to fine-tune a model on question answering](https://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb)\n","\n","- [HuggingFace QA Examples](https://github.com/huggingface/transformers/tree/master/examples/pytorch/question-answering)\n","\n","- [TensorFlow 2.0 Question Answering - Collections of gold solutions](https://www.kaggle.com/c/tensorflow2-question-answering/discussion/127409) these solutions are gold and highly recommend everyone to give a detailed read."]},{"cell_type":"markdown","metadata":{"id":"vI-eiJRPP4hw"},"source":["<h3><span style=\"color=#444\">Note</span></h3>\n","\n","The below points are worth noting,\n","\n"," - I haven't used FP16 because due to some reason this fails and model never starts training.\n"," - These are the original hyperparamters and setting that I have used for training my models.\n"," - I tried few pooling layers but none of them performed better than simple one.\n"," - Gradient clipping reduces model performance.\n"," - <span style=\"color:#2E86C1\">Training 5 Folds at once will give OOM issue on Kaggle and Colab. I have trained one fold at a time i.e. After 1st fold is completed I save the model as a dataset then train second fold. Repeat this process until all 5 folds are completed. Training each fold takes around 50 minuts on Kaggle.</span>\n"," - <span style=\"color:#2E86C1\">I have modified the preprocessing code a lot from original used in HuggingFace notebook since I am not using HuggingFace Datasets API as well. So I had to handle the sequence-ids specially since they contain None values which torch doesn't support.</span>"]},{"cell_type":"markdown","metadata":{"id":"8ykS_qCDP4hz"},"source":["### Install APEX"]},{"cell_type":"code","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","id":"aCY6yvR6ET3s","execution":{"iopub.status.busy":"2021-08-17T20:22:25.164246Z","iopub.execute_input":"2021-08-17T20:22:25.164729Z","iopub.status.idle":"2021-08-17T20:22:25.170948Z","shell.execute_reply.started":"2021-08-17T20:22:25.164627Z","shell.execute_reply":"2021-08-17T20:22:25.169352Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1634374087445,"user_tz":-540,"elapsed":15,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["# %%writefile setup.sh\n","# export CUDA_HOME=/usr/local/cuda-10.1\n","# git clone https://github.com/NVIDIA/apex\n","# cd apex\n","# pip install -v --disable-pip-version-check --no-cache-dir ./"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"-l2Jsav9ET3v","execution":{"iopub.status.busy":"2021-08-17T20:22:26.935257Z","iopub.execute_input":"2021-08-17T20:22:26.935919Z","iopub.status.idle":"2021-08-17T20:22:26.939334Z","shell.execute_reply.started":"2021-08-17T20:22:26.935881Z","shell.execute_reply":"2021-08-17T20:22:26.938488Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1634374087448,"user_tz":-540,"elapsed":17,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["# %%capture\n","# !sh setup.sh"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xFbbPlPgP4h7"},"source":["### Import Dependencies"]},{"cell_type":"code","metadata":{"id":"E4l6PirHET3x","execution":{"iopub.status.busy":"2021-08-17T20:23:50.232396Z","iopub.execute_input":"2021-08-17T20:23:50.232892Z","iopub.status.idle":"2021-08-17T20:24:01.076652Z","shell.execute_reply.started":"2021-08-17T20:23:50.232861Z","shell.execute_reply":"2021-08-17T20:24:01.075658Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634374090294,"user_tz":-540,"elapsed":2862,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"ea6f724e-14f0-4d6c-b949-602c53c5bc0a"},"source":["#import os\n","import gc\n","gc.enable()\n","import math\n","#mport json\n","import time\n","import random\n","import multiprocessing\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=UserWarning)\n","\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm, trange\n","from sklearn import model_selection\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn import Parameter\n","import torch.optim as optim\n","from torch.utils.data import (\n","    Dataset, DataLoader,\n","    SequentialSampler, RandomSampler\n",")\n","from torch.utils.data.distributed import DistributedSampler\n","\n","try:\n","    from apex import amp\n","    APEX_INSTALLED = True\n","except ImportError:\n","    APEX_INSTALLED = False\n","\n","import transformers\n","from transformers import (\n","    WEIGHTS_NAME,\n","    AdamW,\n","    AutoConfig,\n","    AutoModel,\n","    AutoTokenizer,\n","    get_cosine_schedule_with_warmup,\n","    get_linear_schedule_with_warmup,\n","    logging,\n","    MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n",")\n","logging.set_verbosity_warning()\n","logging.set_verbosity_error()\n","\n","def fix_all_seeds(seed):\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","def optimal_num_of_loader_workers():\n","    num_cpus = multiprocessing.cpu_count()\n","    num_gpus = torch.cuda.device_count()\n","    optimal_value = min(num_cpus, num_gpus*4) if num_gpus else num_cpus - 1\n","    return optimal_value\n","\n","print(f\"Apex AMP Installed :: {APEX_INSTALLED}\")\n","MODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())\n","MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Apex AMP Installed :: False\n"]}]},{"cell_type":"markdown","metadata":{"id":"y0OvcnMaP4h9"},"source":["### Training Configuration"]},{"cell_type":"code","metadata":{"id":"LUx5XplNET3y","execution":{"iopub.status.busy":"2021-08-17T20:24:13.391322Z","iopub.execute_input":"2021-08-17T20:24:13.391696Z","iopub.status.idle":"2021-08-17T20:24:13.398397Z","shell.execute_reply.started":"2021-08-17T20:24:13.391662Z","shell.execute_reply":"2021-08-17T20:24:13.39732Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1634374090296,"user_tz":-540,"elapsed":34,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["class Config:\n","    # model\n","    model_type = 'xlm_roberta'\n","    model_name_or_path = \"deepset/xlm-roberta-large-squad2\"\n","    config_name = \"deepset/xlm-roberta-large-squad2\"\n","    fp16 = True if APEX_INSTALLED else False\n","    fp16_opt_level = \"O1\"\n","    gradient_accumulation_steps = 2\n","\n","    # tokenizer\n","    tokenizer_name = \"deepset/xlm-roberta-large-squad2\"\n","    max_seq_length = 384 #512 #384\n","    doc_stride = 128 #80 #128\n","\n","    # train\n","    epochs = 1 #7 #1\n","    train_batch_size = 4 #2 #4\n","    eval_batch_size = 8 #4 #8\n","\n","    # optimizer\n","    optimizer_type = 'AdamW'\n","    learning_rate = 1.5e-5\n","    weight_decay = 1e-2\n","    epsilon = 1e-8\n","    max_grad_norm = 1.0\n","\n","    # scheduler\n","    decay_name = 'linear-warmup'\n","    warmup_ratio = 0.1\n","\n","    # logging\n","    logging_steps = 10\n","\n","    # evaluate\n","    output_dir = dname #'output'\n","    seed = 2021"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HYno9o1OP4h-"},"source":["### Data Factory"]},{"cell_type":"code","metadata":{"id":"X_eRZQrzET3z","execution":{"iopub.status.busy":"2021-08-17T20:24:26.939105Z","iopub.execute_input":"2021-08-17T20:24:26.939437Z","iopub.status.idle":"2021-08-17T20:24:27.776039Z","shell.execute_reply.started":"2021-08-17T20:24:26.939409Z","shell.execute_reply":"2021-08-17T20:24:27.775204Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1634374093996,"user_tz":-540,"elapsed":3733,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["train = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/train.csv')\n","test = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/test.csv')\n","external_mlqa = pd.read_csv('../input/mlqa-hindi-processed/mlqa_hindi.csv')\n","external_xquad = pd.read_csv('../input/mlqa-hindi-processed/xquad.csv')\n","external_train = pd.concat([external_mlqa, external_xquad])\n","\n","def create_folds(data, num_splits):\n","    data[\"kfold\"] = -1\n","    kf = model_selection.StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=2021)\n","    for f, (t_, v_) in enumerate(kf.split(X=data, y=data['language'])):\n","        data.loc[v_, 'kfold'] = f\n","    return data\n","\n","#### RIOW\n","# train = create_folds(train, num_splits=5)\n","# external_train[\"kfold\"] = -1\n","# external_train['id'] = list(np.arange(1, len(external_train)+1))\n","# train = pd.concat([train, external_train]).reset_index(drop=True)\n","\n","external_train['id'] = list(np.arange(1, len(external_train)+1))\n","\n","# Drop tamil\n","train = train[train[\"language\"]!=\"tamil\"].reset_index(drop=True)\n","\n","train = pd.concat([train, external_train]).reset_index(drop=True)\n","train = create_folds(train, num_splits=5)\n","#### RIOWRIOW\n","\n","def convert_answers(row):\n","    return {'answer_start': [row[0]], 'text': [row[1]]}\n","\n","train['answers'] = train[['answer_start', 'answer_text']].apply(convert_answers, axis=1)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dAoAJUVaP4iA"},"source":["### Covert Examples to Features (Preprocess)"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-12T15:50:26.947214Z","iopub.execute_input":"2021-08-12T15:50:26.947589Z","iopub.status.idle":"2021-08-12T15:50:26.960916Z","shell.execute_reply.started":"2021-08-12T15:50:26.947551Z","shell.execute_reply":"2021-08-12T15:50:26.959064Z"},"id":"dxbZdct1ET3z","trusted":true,"executionInfo":{"status":"ok","timestamp":1634374093997,"user_tz":-540,"elapsed":51,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["def prepare_train_features(args, example, tokenizer):\n","    example[\"question\"] = example[\"question\"].lstrip()\n","    tokenized_example = tokenizer(\n","        example[\"question\"],\n","        example[\"context\"],\n","        truncation=\"only_second\",\n","        max_length=args.max_seq_length,\n","        stride=args.doc_stride,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True,\n","        padding=\"max_length\",\n","    )\n","\n","    sample_mapping = tokenized_example.pop(\"overflow_to_sample_mapping\")\n","    offset_mapping = tokenized_example.pop(\"offset_mapping\")\n","\n","    features = []\n","    for i, offsets in enumerate(offset_mapping):\n","        feature = {}\n","\n","        input_ids = tokenized_example[\"input_ids\"][i]\n","        attention_mask = tokenized_example[\"attention_mask\"][i]\n","\n","        feature['input_ids'] = input_ids\n","        feature['attention_mask'] = attention_mask\n","        feature['offset_mapping'] = offsets\n","\n","        cls_index = input_ids.index(tokenizer.cls_token_id)\n","        sequence_ids = tokenized_example.sequence_ids(i)\n","\n","        sample_index = sample_mapping[i]\n","        answers = example[\"answers\"]\n","\n","        if len(answers[\"answer_start\"]) == 0:\n","            feature[\"start_position\"] = cls_index\n","            feature[\"end_position\"] = cls_index\n","        else:\n","            start_char = answers[\"answer_start\"][0]\n","            end_char = start_char + len(answers[\"text\"][0])\n","\n","            token_start_index = 0\n","            while sequence_ids[token_start_index] != 1:\n","                token_start_index += 1\n","\n","            token_end_index = len(input_ids) - 1\n","            while sequence_ids[token_end_index] != 1:\n","                token_end_index -= 1\n","\n","            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n","                feature[\"start_position\"] = cls_index\n","                feature[\"end_position\"] = cls_index\n","            else:\n","                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n","                    token_start_index += 1\n","                feature[\"start_position\"] = token_start_index - 1\n","                while offsets[token_end_index][1] >= end_char:\n","                    token_end_index -= 1\n","                feature[\"end_position\"] = token_end_index + 1\n","\n","        features.append(feature)\n","    return features"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a3SIS_xAP4iC"},"source":["### Dataset Retriever"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-12T15:50:26.962738Z","iopub.execute_input":"2021-08-12T15:50:26.963118Z","iopub.status.idle":"2021-08-12T15:50:26.97542Z","shell.execute_reply.started":"2021-08-12T15:50:26.963075Z","shell.execute_reply":"2021-08-12T15:50:26.974431Z"},"id":"6TuzHdjmET30","trusted":true,"executionInfo":{"status":"ok","timestamp":1634374093998,"user_tz":-540,"elapsed":50,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["class DatasetRetriever(Dataset):\n","    def __init__(self, features, mode='train'):\n","        super(DatasetRetriever, self).__init__()\n","        self.features = features\n","        self.mode = mode\n","        \n","    def __len__(self):\n","        return len(self.features)\n","    \n","    def __getitem__(self, item):   \n","        feature = self.features[item]\n","        if self.mode == 'train':\n","            return {\n","                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n","                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n","                'offset_mapping':torch.tensor(feature['offset_mapping'], dtype=torch.long),\n","                'start_position':torch.tensor(feature['start_position'], dtype=torch.long),\n","                'end_position':torch.tensor(feature['end_position'], dtype=torch.long)\n","            }\n","        else:\n","            return {\n","                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n","                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n","                'offset_mapping':feature['offset_mapping'],\n","                'sequence_ids':feature['sequence_ids'],\n","                'id':feature['example_id'],\n","                'context': feature['context'],\n","                'question': feature['question']\n","            }"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tpH-2nPqP4iE"},"source":["### Model"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-12T15:50:26.976977Z","iopub.execute_input":"2021-08-12T15:50:26.977627Z","iopub.status.idle":"2021-08-12T15:50:26.990227Z","shell.execute_reply.started":"2021-08-12T15:50:26.97747Z","shell.execute_reply":"2021-08-12T15:50:26.989443Z"},"id":"9OxhKqxcET31","trusted":true,"executionInfo":{"status":"ok","timestamp":1634374093999,"user_tz":-540,"elapsed":50,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["class Model(nn.Module):\n","    def __init__(self, modelname_or_path, config):\n","        super(Model, self).__init__()\n","        self.config = config\n","        self.xlm_roberta = AutoModel.from_pretrained(modelname_or_path, config=config)\n","        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","        self._init_weights(self.qa_outputs)\n","        \n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","\n","    def forward(\n","        self, \n","        input_ids, \n","        attention_mask=None, \n","        # token_type_ids=None\n","    ):\n","        outputs = self.xlm_roberta(\n","            input_ids,\n","            attention_mask=attention_mask,\n","        )\n","\n","        sequence_output = outputs[0]\n","        pooled_output = outputs[1]\n","        \n","        # sequence_output = self.dropout(sequence_output)\n","        qa_logits = self.qa_outputs(sequence_output)\n","        \n","        start_logits, end_logits = qa_logits.split(1, dim=-1)\n","        start_logits = start_logits.squeeze(-1)\n","        end_logits = end_logits.squeeze(-1)\n","    \n","        return start_logits, end_logits"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xdMx6plqP4iG"},"source":["### Loss"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-12T15:50:26.991891Z","iopub.execute_input":"2021-08-12T15:50:26.99237Z","iopub.status.idle":"2021-08-12T15:50:27.000188Z","shell.execute_reply.started":"2021-08-12T15:50:26.992334Z","shell.execute_reply":"2021-08-12T15:50:26.999374Z"},"id":"SxuNrJqqET32","trusted":true,"executionInfo":{"status":"ok","timestamp":1634374094001,"user_tz":-540,"elapsed":48,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["def loss_fn(preds, labels):\n","    start_preds, end_preds = preds\n","    start_labels, end_labels = labels\n","    \n","    start_loss = nn.CrossEntropyLoss(ignore_index=-1)(start_preds, start_labels)\n","    end_loss = nn.CrossEntropyLoss(ignore_index=-1)(end_preds, end_labels)\n","    total_loss = (start_loss + end_loss) / 2\n","    return total_loss"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jUMtX08cP4iH"},"source":["### Grouped Layerwise Learning Rate Decay"]},{"cell_type":"code","metadata":{"id":"vf6HVcu2ET34","execution":{"iopub.status.busy":"2021-08-17T20:25:36.36033Z","iopub.execute_input":"2021-08-17T20:25:36.361058Z","iopub.status.idle":"2021-08-17T20:25:36.381524Z","shell.execute_reply.started":"2021-08-17T20:25:36.361009Z","shell.execute_reply":"2021-08-17T20:25:36.380328Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1634374094002,"user_tz":-540,"elapsed":48,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["def get_optimizer_grouped_parameters(args, model):\n","    no_decay = [\"bias\", \"LayerNorm.weight\"]\n","    group1=['layer.0.','layer.1.','layer.2.','layer.3.']\n","    group2=['layer.4.','layer.5.','layer.6.','layer.7.']    \n","    group3=['layer.8.','layer.9.','layer.10.','layer.11.']\n","    group_all=['layer.0.','layer.1.','layer.2.','layer.3.','layer.4.','layer.5.','layer.6.','layer.7.','layer.8.','layer.9.','layer.10.','layer.11.']\n","    optimizer_grouped_parameters = [\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': args.weight_decay},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': args.weight_decay, 'lr': args.learning_rate/2.6},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': args.weight_decay, 'lr': args.learning_rate},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': args.weight_decay, 'lr': args.learning_rate*2.6},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': 0.0},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': 0.0, 'lr': args.learning_rate/2.6},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': 0.0, 'lr': args.learning_rate},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': 0.0, 'lr': args.learning_rate*2.6},\n","        {'params': [p for n, p in model.named_parameters() if args.model_type not in n], 'lr':args.learning_rate*20, \"weight_decay\": 0.0},\n","    ]\n","    return optimizer_grouped_parameters"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oXz_0fIQP4iI"},"source":["### Metric Logger"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-12T15:50:27.021442Z","iopub.execute_input":"2021-08-12T15:50:27.021952Z","iopub.status.idle":"2021-08-12T15:50:27.03234Z","shell.execute_reply.started":"2021-08-12T15:50:27.021912Z","shell.execute_reply":"2021-08-12T15:50:27.03156Z"},"id":"bkFB-iMcET34","trusted":true,"executionInfo":{"status":"ok","timestamp":1634374094003,"user_tz":-540,"elapsed":47,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["class AverageMeter(object):\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","        self.max = 0\n","        self.min = 1e5\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","        if val > self.max:\n","            self.max = val\n","        if val < self.min:\n","            self.min = val"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y946gQxtP4iJ"},"source":["### Utilities"]},{"cell_type":"code","metadata":{"id":"spFRutV0ET34","execution":{"iopub.status.busy":"2021-08-17T20:26:30.014223Z","iopub.execute_input":"2021-08-17T20:26:30.014623Z","iopub.status.idle":"2021-08-17T20:26:30.030566Z","shell.execute_reply.started":"2021-08-17T20:26:30.01459Z","shell.execute_reply":"2021-08-17T20:26:30.029723Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1634374094004,"user_tz":-540,"elapsed":47,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["def make_model(args):\n","    config = AutoConfig.from_pretrained(args.config_name)\n","    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name)\n","    model = Model(args.model_name_or_path, config=config)\n","    return config, tokenizer, model\n","\n","def make_optimizer(args, model):\n","    # optimizer_grouped_parameters = get_optimizer_grouped_parameters(args, model)\n","    no_decay = [\"bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\n","            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","            \"weight_decay\": args.weight_decay,\n","        },\n","        {\n","            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n","            \"weight_decay\": 0.0,\n","        },\n","    ]\n","    if args.optimizer_type == \"AdamW\":\n","        optimizer = AdamW(\n","            optimizer_grouped_parameters,\n","            lr=args.learning_rate,\n","            eps=args.epsilon,\n","            correct_bias=True\n","        )\n","        return optimizer\n","\n","def make_scheduler(\n","    args, optimizer, \n","    num_warmup_steps, \n","    num_training_steps\n","):\n","    if args.decay_name == \"cosine-warmup\":\n","        scheduler = get_cosine_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=num_warmup_steps,\n","            num_training_steps=num_training_steps\n","        )\n","    else:\n","        scheduler = get_linear_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=num_warmup_steps,\n","            num_training_steps=num_training_steps\n","        )\n","    return scheduler    \n","\n","def make_loader(\n","    args, data, \n","    tokenizer, fold\n","):\n","    train_set, valid_set = data[data['kfold']!=fold], data[data['kfold']==fold]\n","    \n","    train_features, valid_features = [[] for _ in range(2)]\n","    for i, row in train_set.iterrows():\n","        train_features += prepare_train_features(args, row, tokenizer)\n","    for i, row in valid_set.iterrows():\n","        valid_features += prepare_train_features(args, row, tokenizer)\n","\n","    train_dataset = DatasetRetriever(train_features)\n","    valid_dataset = DatasetRetriever(valid_features)\n","    print(f\"Num examples Train= {len(train_dataset)}, Num examples Valid={len(valid_dataset)}\")\n","    \n","    train_sampler = RandomSampler(train_dataset)\n","    valid_sampler = SequentialSampler(valid_dataset)\n","\n","    train_dataloader = DataLoader(\n","        train_dataset,\n","        batch_size=args.train_batch_size,\n","        sampler=train_sampler,\n","        num_workers=optimal_num_of_loader_workers(),\n","        pin_memory=True,\n","        drop_last=False \n","    )\n","\n","    valid_dataloader = DataLoader(\n","        valid_dataset,\n","        batch_size=args.eval_batch_size, \n","        sampler=valid_sampler,\n","        num_workers=optimal_num_of_loader_workers(),\n","        pin_memory=True, \n","        drop_last=False\n","    )\n","\n","    return train_dataloader, valid_dataloader"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i-zfYJGxP4iK"},"source":["### Trainer"]},{"cell_type":"code","metadata":{"id":"iFLvh1VQET35","execution":{"iopub.status.busy":"2021-08-17T20:26:36.310455Z","iopub.execute_input":"2021-08-17T20:26:36.310827Z","iopub.status.idle":"2021-08-17T20:26:36.325575Z","shell.execute_reply.started":"2021-08-17T20:26:36.310796Z","shell.execute_reply":"2021-08-17T20:26:36.324417Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1634374094005,"user_tz":-540,"elapsed":45,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["class Trainer:\n","    def __init__(\n","        self, model, tokenizer, \n","        optimizer, scheduler\n","    ):\n","        self.model = model\n","        self.tokenizer = tokenizer\n","        self.optimizer = optimizer\n","        self.scheduler = scheduler\n","\n","    def train(\n","        self, args, \n","        train_dataloader, \n","        epoch, result_dict\n","    ):\n","        count = 0\n","        losses = AverageMeter()\n","        \n","        self.model.zero_grad()\n","        self.model.train()\n","        \n","        fix_all_seeds(args.seed)\n","        \n","        for batch_idx, batch_data in enumerate(train_dataloader):\n","            input_ids, attention_mask, targets_start, targets_end = \\\n","                batch_data['input_ids'], batch_data['attention_mask'], \\\n","                    batch_data['start_position'], batch_data['end_position']\n","            \n","            input_ids, attention_mask, targets_start, targets_end = \\\n","                input_ids.cuda(), attention_mask.cuda(), targets_start.cuda(), targets_end.cuda()\n","\n","            outputs_start, outputs_end = self.model(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","            )\n","            \n","            loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n","            loss = loss / args.gradient_accumulation_steps\n","\n","            if args.fp16:\n","                with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n","                    scaled_loss.backward()\n","            else:\n","                loss.backward()\n","\n","            count += input_ids.size(0)\n","            losses.update(loss.item(), input_ids.size(0))\n","\n","            # if args.fp16:\n","            #     torch.nn.utils.clip_grad_norm_(amp.master_params(self.optimizer), args.max_grad_norm)\n","            # else:\n","            #     torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.max_grad_norm)\n","\n","            if batch_idx % args.gradient_accumulation_steps == 0 or batch_idx == len(train_dataloader) - 1:\n","                self.optimizer.step()\n","                self.scheduler.step()\n","                self.optimizer.zero_grad()\n","\n","            if (batch_idx % args.logging_steps == 0) or (batch_idx+1)==len(train_dataloader):\n","                _s = str(len(str(len(train_dataloader.sampler))))\n","                ret = [\n","                    ('Epoch: {:0>2} [{: >' + _s + '}/{} ({: >3.0f}%)]').format(epoch, count, len(train_dataloader.sampler), 100 * count / len(train_dataloader.sampler)),\n","                    'Train Loss: {: >4.5f}'.format(losses.avg),\n","                ]\n","                print(', '.join(ret))\n","\n","        result_dict['train_loss'].append(losses.avg)\n","        return result_dict"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7pmOF0dxP4iL"},"source":["### Evaluator"]},{"cell_type":"code","metadata":{"id":"1a8kG2UYET36","execution":{"iopub.status.busy":"2021-08-17T20:26:39.517516Z","iopub.execute_input":"2021-08-17T20:26:39.517886Z","iopub.status.idle":"2021-08-17T20:26:39.528591Z","shell.execute_reply.started":"2021-08-17T20:26:39.517856Z","shell.execute_reply":"2021-08-17T20:26:39.527569Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1634374094006,"user_tz":-540,"elapsed":45,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["class Evaluator:\n","    def __init__(self, model):\n","        self.model = model\n","    \n","    def save(self, result, output_dir):\n","        with open(f'{output_dir}/result_dict.json', 'w') as f:\n","            f.write(json.dumps(result, sort_keys=True, indent=4, ensure_ascii=False))\n","\n","    def evaluate(self, valid_dataloader, epoch, result_dict):\n","        losses = AverageMeter()\n","        for batch_idx, batch_data in enumerate(valid_dataloader):\n","            self.model = self.model.eval()\n","            input_ids, attention_mask, targets_start, targets_end = \\\n","                batch_data['input_ids'], batch_data['attention_mask'], \\\n","                    batch_data['start_position'], batch_data['end_position']\n","            \n","            input_ids, attention_mask, targets_start, targets_end = \\\n","                input_ids.cuda(), attention_mask.cuda(), targets_start.cuda(), targets_end.cuda()\n","            \n","            with torch.no_grad():            \n","                outputs_start, outputs_end = self.model(\n","                    input_ids=input_ids,\n","                    attention_mask=attention_mask,\n","                )\n","                \n","                loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n","                losses.update(loss.item(), input_ids.size(0))\n","                \n","        print('----Validation Results Summary----')\n","        print('Epoch: [{}] Valid Loss: {: >4.5f}'.format(epoch, losses.avg))\n","        result_dict['val_loss'].append(losses.avg)        \n","        return result_dict"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cAZPoRJeP4iM"},"source":["### Initialize Training"]},{"cell_type":"code","metadata":{"id":"v-gUDyq2ET37","execution":{"iopub.status.busy":"2021-08-17T20:26:44.41837Z","iopub.execute_input":"2021-08-17T20:26:44.418722Z","iopub.status.idle":"2021-08-17T20:26:44.428918Z","shell.execute_reply.started":"2021-08-17T20:26:44.418691Z","shell.execute_reply":"2021-08-17T20:26:44.428086Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1634374094006,"user_tz":-540,"elapsed":42,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["def init_training(args, data, fold):\n","    fix_all_seeds(args.seed)\n","    \n","    if not os.path.exists(args.output_dir):\n","        os.makedirs(args.output_dir)\n","    \n","    # model\n","    model_config, tokenizer, model = make_model(args)\n","    if torch.cuda.device_count() >= 1:\n","        print('Model pushed to {} GPU(s), type {}.'.format(\n","            torch.cuda.device_count(), \n","            torch.cuda.get_device_name(0))\n","        )\n","        model = model.cuda() \n","    else:\n","        raise ValueError('CPU training is not supported')\n","    \n","    # data loaders\n","    train_dataloader, valid_dataloader = make_loader(args, data, tokenizer, fold)\n","\n","    # optimizer\n","    optimizer = make_optimizer(args, model)\n","\n","    # scheduler\n","    num_training_steps = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps) * args.epochs\n","    if args.warmup_ratio > 0:\n","        num_warmup_steps = int(args.warmup_ratio * num_training_steps)\n","    else:\n","        num_warmup_steps = 0\n","    print(f\"Total Training Steps: {num_training_steps}, Total Warmup Steps: {num_warmup_steps}\")\n","    scheduler = make_scheduler(args, optimizer, num_warmup_steps, num_training_steps)\n","\n","    # mixed precision training with NVIDIA Apex\n","    if args.fp16:\n","        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n","    \n","    result_dict = {\n","        'epoch':[], \n","        'train_loss': [], \n","        'val_loss' : [], \n","        'best_val_loss': np.inf\n","    }\n","\n","    return (\n","        model, model_config, tokenizer, optimizer, scheduler, \n","        train_dataloader, valid_dataloader, result_dict\n","    )"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g6ZBX-5aP4iN"},"source":["### Run"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-12T15:50:27.097353Z","iopub.execute_input":"2021-08-12T15:50:27.097724Z","iopub.status.idle":"2021-08-12T15:50:27.112113Z","shell.execute_reply.started":"2021-08-12T15:50:27.097688Z","shell.execute_reply":"2021-08-12T15:50:27.111224Z"},"id":"39ei5Bm5ET37","trusted":true,"executionInfo":{"status":"ok","timestamp":1634374094007,"user_tz":-540,"elapsed":42,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["def run(data, fold):\n","    args = Config()\n","    model, model_config, tokenizer, optimizer, scheduler, train_dataloader, \\\n","        valid_dataloader, result_dict = init_training(args, data, fold)\n","    \n","    trainer = Trainer(model, tokenizer, optimizer, scheduler)\n","    evaluator = Evaluator(model)\n","\n","    train_time_list = []\n","    valid_time_list = []\n","\n","    for epoch in range(args.epochs):\n","        result_dict['epoch'].append(epoch)\n","\n","        # Train\n","        torch.cuda.synchronize()\n","        tic1 = time.time()\n","        result_dict = trainer.train(\n","            args, train_dataloader, \n","            epoch, result_dict\n","        )\n","        torch.cuda.synchronize()\n","        tic2 = time.time() \n","        train_time_list.append(tic2 - tic1)\n","        \n","        # Evaluate\n","        torch.cuda.synchronize()\n","        tic3 = time.time()\n","        result_dict = evaluator.evaluate(\n","            valid_dataloader, epoch, result_dict\n","        )\n","        torch.cuda.synchronize()\n","        tic4 = time.time() \n","        valid_time_list.append(tic4 - tic3)\n","            \n","        output_dir = os.path.join(args.output_dir, f\"checkpoint-fold-{fold}\")\n","        if result_dict['val_loss'][-1] < result_dict['best_val_loss']:\n","            print(\"{} Epoch, Best epoch was updated! Valid Loss: {: >4.5f}\".format(epoch, result_dict['val_loss'][-1]))\n","            result_dict[\"best_val_loss\"] = result_dict['val_loss'][-1]        \n","            \n","            os.makedirs(output_dir, exist_ok=True)\n","            torch.save(model.state_dict(), f\"{output_dir}/pytorch_model.bin\")\n","            model_config.save_pretrained(output_dir)\n","            tokenizer.save_pretrained(output_dir)\n","            print(f\"Saving model checkpoint to {output_dir}.\")\n","            \n","        print()\n","\n","    evaluator.save(result_dict, output_dir)\n","    \n","    print(f\"Total Training Time: {np.sum(train_time_list)}secs, Average Training Time per Epoch: {np.mean(train_time_list)}secs.\")\n","    print(f\"Total Validation Time: {np.sum(valid_time_list)}secs, Average Validation Time per Epoch: {np.mean(valid_time_list)}secs.\")\n","    \n","    torch.cuda.empty_cache()\n","    del trainer, evaluator\n","    del model, model_config, tokenizer\n","    del optimizer, scheduler\n","    del train_dataloader, valid_dataloader, result_dict\n","    gc.collect()"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"mPaGnnCnbhWl","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["5c5c53bdc5db4d5bb3131540881362a7","cfab0d413e1e484192935cb271bb445f","69f119f7e86a412d96379a4c91db69de","05f6575034f4459fad752763d377dcbe","3dc3a2f8727346329a17108f5723597d","d3fcdaa804e14986884d5b24a0df61be","c0df1ef7ce6046dc8f5eb9684cd9fd8a","6334c7e5244641b0b1160cf551206691","a5aa032868284e60800de8548f5093e3","eaeebe5de19844ec95e52099447e930c","d9d559fabd0844aaa0a5ea6693501d72","07e9782e0356466fb8d9c9ea085cee79","4f1b0fff485e474296f4e9f33f0c7dbd","810166135c3f48f5914c2600421e92ab","8523fd8459c34a6b9d3917699a1d1f40","29862227e7f543768cd95384aea8123d","82cfa58a26ea4104a99643a6405afc93","c803908cebdd4975adde33f4ad5778af","6bafb1630ce54c62a28b120b73c879e1","a1bfeecbdc5a42bb9bd83dc6afe337d7","e409f4c9bc404955b3396776634ed13f","e4f4c2901fc4410382751e763ce775de","abaee2d39aae476eb2fae12c3faed028","91e309bb3f3d44bf9ef21be77dbd294b","127aff325ee84ea389c83b06c9033fdf","e77cf9ff29574af3bdfee828620d1602","0b57515be5ba474eafb4b6df237fb331","240b3df9dc32467ca4839fe8c5fd8950","d2aac14280f84cccb2d1b877636ae275","2b6a7abda3ba46bc95b811d94f62e272","4f4f04d8233f4a3cbd8955deb2a6c484","f0ef22ede61549fcb5fa4547c8ab6791","9461f6ee3a8b4f7a9226d62b38302b3d","a84b11799a164f929883288adb8b096a","fb84277845b947f1b428df3db441b630","db9f7d908db0441fa10e495160f09b9a","d2a90d7b38e241cba13160ab9aeb178e","74fde70c5615493884afa661589bc1a5","ce8d968006c44338af47ec2d0e5ba791","bfb1bcc868cc40e982a5a5c5f101f059","f96250164b2a4a368df68a7eee64c50d","e0691c2695264d8b8777bafbd143dcc8","857237a158b142ae8dc30e38ab774cea","fa6df4a64a0342eda7973615249c6163","29564eac28f4472e961bb9eb30db9ba4","cb6e60b18c414658ac87e76a5adf8639","08587f4c0d7f4863a6a8c8c03b596f76","b94e152275794a6597437068d1891799","1275f41ca8844afcaee8be0237359564","384af44eb3cd4c17b853ec0cc353b2a2","9331f39ef8ed4936837cb713f1a86a18","430264c5168e40f1850aca8cb4ddc6be","dc54f20f45ca4e8385c0ae9469d6466c","9452aba62430415db767b0a399ff610e","98fcdf55b2cf429281759b32477bd453"]},"executionInfo":{"status":"ok","timestamp":1634385652980,"user_tz":-540,"elapsed":1446187,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"8c67300b-765e-4051-997d-812dff142225"},"source":["for fold in range(1,6):\n","    print();print()\n","    print('-'*50)\n","    print(f'FOLD: {fold}')\n","    print('-'*50)\n","    run(train, fold)"],"execution_count":19,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","\n","--------------------------------------------------\n","FOLD: 1\n","--------------------------------------------------\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5c5c53bdc5db4d5bb3131540881362a7","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/606 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"07e9782e0356466fb8d9c9ea085cee79","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/179 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"abaee2d39aae476eb2fae12c3faed028","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/4.83M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a84b11799a164f929883288adb8b096a","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/150 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"29564eac28f4472e961bb9eb30db9ba4","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/2.09G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"output_type":"stream","name":"stdout","text":["Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n","Num examples Train= 14281, Num examples Valid=3277\n","Total Training Steps: 1786, Total Warmup Steps: 178\n","Epoch: 00 [    4/14281 (  0%)], Train Loss: 2.87143\n","Epoch: 00 [   44/14281 (  0%)], Train Loss: 2.88651\n","Epoch: 00 [   84/14281 (  1%)], Train Loss: 2.84771\n","Epoch: 00 [  124/14281 (  1%)], Train Loss: 2.80989\n","Epoch: 00 [  164/14281 (  1%)], Train Loss: 2.73662\n","Epoch: 00 [  204/14281 (  1%)], Train Loss: 2.65609\n","Epoch: 00 [  244/14281 (  2%)], Train Loss: 2.55573\n","Epoch: 00 [  284/14281 (  2%)], Train Loss: 2.44054\n","Epoch: 00 [  324/14281 (  2%)], Train Loss: 2.30642\n","Epoch: 00 [  364/14281 (  3%)], Train Loss: 2.16926\n","Epoch: 00 [  404/14281 (  3%)], Train Loss: 2.03016\n","Epoch: 00 [  444/14281 (  3%)], Train Loss: 1.93318\n","Epoch: 00 [  484/14281 (  3%)], Train Loss: 1.82131\n","Epoch: 00 [  524/14281 (  4%)], Train Loss: 1.73045\n","Epoch: 00 [  564/14281 (  4%)], Train Loss: 1.64794\n","Epoch: 00 [  604/14281 (  4%)], Train Loss: 1.57145\n","Epoch: 00 [  644/14281 (  5%)], Train Loss: 1.51284\n","Epoch: 00 [  684/14281 (  5%)], Train Loss: 1.44902\n","Epoch: 00 [  724/14281 (  5%)], Train Loss: 1.39844\n","Epoch: 00 [  764/14281 (  5%)], Train Loss: 1.34777\n","Epoch: 00 [  804/14281 (  6%)], Train Loss: 1.31281\n","Epoch: 00 [  844/14281 (  6%)], Train Loss: 1.27169\n","Epoch: 00 [  884/14281 (  6%)], Train Loss: 1.22823\n","Epoch: 00 [  924/14281 (  6%)], Train Loss: 1.19672\n","Epoch: 00 [  964/14281 (  7%)], Train Loss: 1.16252\n","Epoch: 00 [ 1004/14281 (  7%)], Train Loss: 1.13264\n","Epoch: 00 [ 1044/14281 (  7%)], Train Loss: 1.10872\n","Epoch: 00 [ 1084/14281 (  8%)], Train Loss: 1.08516\n","Epoch: 00 [ 1124/14281 (  8%)], Train Loss: 1.06420\n","Epoch: 00 [ 1164/14281 (  8%)], Train Loss: 1.04291\n","Epoch: 00 [ 1204/14281 (  8%)], Train Loss: 1.02297\n","Epoch: 00 [ 1244/14281 (  9%)], Train Loss: 0.99879\n","Epoch: 00 [ 1284/14281 (  9%)], Train Loss: 0.98278\n","Epoch: 00 [ 1324/14281 (  9%)], Train Loss: 0.96884\n","Epoch: 00 [ 1364/14281 ( 10%)], Train Loss: 0.95404\n","Epoch: 00 [ 1404/14281 ( 10%)], Train Loss: 0.94326\n","Epoch: 00 [ 1444/14281 ( 10%)], Train Loss: 0.92631\n","Epoch: 00 [ 1484/14281 ( 10%)], Train Loss: 0.91188\n","Epoch: 00 [ 1524/14281 ( 11%)], Train Loss: 0.90729\n","Epoch: 00 [ 1564/14281 ( 11%)], Train Loss: 0.89947\n","Epoch: 00 [ 1604/14281 ( 11%)], Train Loss: 0.89051\n","Epoch: 00 [ 1644/14281 ( 12%)], Train Loss: 0.88362\n","Epoch: 00 [ 1684/14281 ( 12%)], Train Loss: 0.87501\n","Epoch: 00 [ 1724/14281 ( 12%)], Train Loss: 0.86351\n","Epoch: 00 [ 1764/14281 ( 12%)], Train Loss: 0.85370\n","Epoch: 00 [ 1804/14281 ( 13%)], Train Loss: 0.84226\n","Epoch: 00 [ 1844/14281 ( 13%)], Train Loss: 0.83239\n","Epoch: 00 [ 1884/14281 ( 13%)], Train Loss: 0.82015\n","Epoch: 00 [ 1924/14281 ( 13%)], Train Loss: 0.81571\n","Epoch: 00 [ 1964/14281 ( 14%)], Train Loss: 0.81536\n","Epoch: 00 [ 2004/14281 ( 14%)], Train Loss: 0.81069\n","Epoch: 00 [ 2044/14281 ( 14%)], Train Loss: 0.80121\n","Epoch: 00 [ 2084/14281 ( 15%)], Train Loss: 0.79573\n","Epoch: 00 [ 2124/14281 ( 15%)], Train Loss: 0.79131\n","Epoch: 00 [ 2164/14281 ( 15%)], Train Loss: 0.78734\n","Epoch: 00 [ 2204/14281 ( 15%)], Train Loss: 0.77789\n","Epoch: 00 [ 2244/14281 ( 16%)], Train Loss: 0.77218\n","Epoch: 00 [ 2284/14281 ( 16%)], Train Loss: 0.76858\n","Epoch: 00 [ 2324/14281 ( 16%)], Train Loss: 0.76783\n","Epoch: 00 [ 2364/14281 ( 17%)], Train Loss: 0.76361\n","Epoch: 00 [ 2404/14281 ( 17%)], Train Loss: 0.75801\n","Epoch: 00 [ 2444/14281 ( 17%)], Train Loss: 0.75498\n","Epoch: 00 [ 2484/14281 ( 17%)], Train Loss: 0.75133\n","Epoch: 00 [ 2524/14281 ( 18%)], Train Loss: 0.74705\n","Epoch: 00 [ 2564/14281 ( 18%)], Train Loss: 0.74051\n","Epoch: 00 [ 2604/14281 ( 18%)], Train Loss: 0.73300\n","Epoch: 00 [ 2644/14281 ( 19%)], Train Loss: 0.73344\n","Epoch: 00 [ 2684/14281 ( 19%)], Train Loss: 0.72886\n","Epoch: 00 [ 2724/14281 ( 19%)], Train Loss: 0.72430\n","Epoch: 00 [ 2764/14281 ( 19%)], Train Loss: 0.72055\n","Epoch: 00 [ 2804/14281 ( 20%)], Train Loss: 0.71976\n","Epoch: 00 [ 2844/14281 ( 20%)], Train Loss: 0.71454\n","Epoch: 00 [ 2884/14281 ( 20%)], Train Loss: 0.70772\n","Epoch: 00 [ 2924/14281 ( 20%)], Train Loss: 0.70256\n","Epoch: 00 [ 2964/14281 ( 21%)], Train Loss: 0.69830\n","Epoch: 00 [ 3004/14281 ( 21%)], Train Loss: 0.69786\n","Epoch: 00 [ 3044/14281 ( 21%)], Train Loss: 0.69407\n","Epoch: 00 [ 3084/14281 ( 22%)], Train Loss: 0.69016\n","Epoch: 00 [ 3124/14281 ( 22%)], Train Loss: 0.68582\n","Epoch: 00 [ 3164/14281 ( 22%)], Train Loss: 0.68094\n","Epoch: 00 [ 3204/14281 ( 22%)], Train Loss: 0.67712\n","Epoch: 00 [ 3244/14281 ( 23%)], Train Loss: 0.67081\n","Epoch: 00 [ 3284/14281 ( 23%)], Train Loss: 0.66913\n","Epoch: 00 [ 3324/14281 ( 23%)], Train Loss: 0.66383\n","Epoch: 00 [ 3364/14281 ( 24%)], Train Loss: 0.65988\n","Epoch: 00 [ 3404/14281 ( 24%)], Train Loss: 0.65608\n","Epoch: 00 [ 3444/14281 ( 24%)], Train Loss: 0.65341\n","Epoch: 00 [ 3484/14281 ( 24%)], Train Loss: 0.64958\n","Epoch: 00 [ 3524/14281 ( 25%)], Train Loss: 0.64637\n","Epoch: 00 [ 3564/14281 ( 25%)], Train Loss: 0.64532\n","Epoch: 00 [ 3604/14281 ( 25%)], Train Loss: 0.64246\n","Epoch: 00 [ 3644/14281 ( 26%)], Train Loss: 0.64108\n","Epoch: 00 [ 3684/14281 ( 26%)], Train Loss: 0.63813\n","Epoch: 00 [ 3724/14281 ( 26%)], Train Loss: 0.63584\n","Epoch: 00 [ 3764/14281 ( 26%)], Train Loss: 0.63284\n","Epoch: 00 [ 3804/14281 ( 27%)], Train Loss: 0.63074\n","Epoch: 00 [ 3844/14281 ( 27%)], Train Loss: 0.62754\n","Epoch: 00 [ 3884/14281 ( 27%)], Train Loss: 0.62762\n","Epoch: 00 [ 3924/14281 ( 27%)], Train Loss: 0.62680\n","Epoch: 00 [ 3964/14281 ( 28%)], Train Loss: 0.62388\n","Epoch: 00 [ 4004/14281 ( 28%)], Train Loss: 0.62031\n","Epoch: 00 [ 4044/14281 ( 28%)], Train Loss: 0.61894\n","Epoch: 00 [ 4084/14281 ( 29%)], Train Loss: 0.61827\n","Epoch: 00 [ 4124/14281 ( 29%)], Train Loss: 0.61590\n","Epoch: 00 [ 4164/14281 ( 29%)], Train Loss: 0.61342\n","Epoch: 00 [ 4204/14281 ( 29%)], Train Loss: 0.61234\n","Epoch: 00 [ 4244/14281 ( 30%)], Train Loss: 0.61039\n","Epoch: 00 [ 4284/14281 ( 30%)], Train Loss: 0.60877\n","Epoch: 00 [ 4324/14281 ( 30%)], Train Loss: 0.60853\n","Epoch: 00 [ 4364/14281 ( 31%)], Train Loss: 0.60498\n","Epoch: 00 [ 4404/14281 ( 31%)], Train Loss: 0.60090\n","Epoch: 00 [ 4444/14281 ( 31%)], Train Loss: 0.59958\n","Epoch: 00 [ 4484/14281 ( 31%)], Train Loss: 0.59735\n","Epoch: 00 [ 4524/14281 ( 32%)], Train Loss: 0.59620\n","Epoch: 00 [ 4564/14281 ( 32%)], Train Loss: 0.59508\n","Epoch: 00 [ 4604/14281 ( 32%)], Train Loss: 0.59739\n","Epoch: 00 [ 4644/14281 ( 33%)], Train Loss: 0.59492\n","Epoch: 00 [ 4684/14281 ( 33%)], Train Loss: 0.59295\n","Epoch: 00 [ 4724/14281 ( 33%)], Train Loss: 0.59079\n","Epoch: 00 [ 4764/14281 ( 33%)], Train Loss: 0.58993\n","Epoch: 00 [ 4804/14281 ( 34%)], Train Loss: 0.58824\n","Epoch: 00 [ 4844/14281 ( 34%)], Train Loss: 0.58642\n","Epoch: 00 [ 4884/14281 ( 34%)], Train Loss: 0.58439\n","Epoch: 00 [ 4924/14281 ( 34%)], Train Loss: 0.58358\n","Epoch: 00 [ 4964/14281 ( 35%)], Train Loss: 0.58152\n","Epoch: 00 [ 5004/14281 ( 35%)], Train Loss: 0.57922\n","Epoch: 00 [ 5044/14281 ( 35%)], Train Loss: 0.57925\n","Epoch: 00 [ 5084/14281 ( 36%)], Train Loss: 0.57763\n","Epoch: 00 [ 5124/14281 ( 36%)], Train Loss: 0.57646\n","Epoch: 00 [ 5164/14281 ( 36%)], Train Loss: 0.57560\n","Epoch: 00 [ 5204/14281 ( 36%)], Train Loss: 0.57342\n","Epoch: 00 [ 5244/14281 ( 37%)], Train Loss: 0.57351\n","Epoch: 00 [ 5284/14281 ( 37%)], Train Loss: 0.57209\n","Epoch: 00 [ 5324/14281 ( 37%)], Train Loss: 0.57015\n","Epoch: 00 [ 5364/14281 ( 38%)], Train Loss: 0.56769\n","Epoch: 00 [ 5404/14281 ( 38%)], Train Loss: 0.56515\n","Epoch: 00 [ 5444/14281 ( 38%)], Train Loss: 0.56419\n","Epoch: 00 [ 5484/14281 ( 38%)], Train Loss: 0.56327\n","Epoch: 00 [ 5524/14281 ( 39%)], Train Loss: 0.56240\n","Epoch: 00 [ 5564/14281 ( 39%)], Train Loss: 0.56106\n","Epoch: 00 [ 5604/14281 ( 39%)], Train Loss: 0.55890\n","Epoch: 00 [ 5644/14281 ( 40%)], Train Loss: 0.55648\n","Epoch: 00 [ 5684/14281 ( 40%)], Train Loss: 0.55561\n","Epoch: 00 [ 5724/14281 ( 40%)], Train Loss: 0.55412\n","Epoch: 00 [ 5764/14281 ( 40%)], Train Loss: 0.55259\n","Epoch: 00 [ 5804/14281 ( 41%)], Train Loss: 0.55143\n","Epoch: 00 [ 5844/14281 ( 41%)], Train Loss: 0.54955\n","Epoch: 00 [ 5884/14281 ( 41%)], Train Loss: 0.54818\n","Epoch: 00 [ 5924/14281 ( 41%)], Train Loss: 0.54730\n","Epoch: 00 [ 5964/14281 ( 42%)], Train Loss: 0.54539\n","Epoch: 00 [ 6004/14281 ( 42%)], Train Loss: 0.54471\n","Epoch: 00 [ 6044/14281 ( 42%)], Train Loss: 0.54276\n","Epoch: 00 [ 6084/14281 ( 43%)], Train Loss: 0.54258\n","Epoch: 00 [ 6124/14281 ( 43%)], Train Loss: 0.54215\n","Epoch: 00 [ 6164/14281 ( 43%)], Train Loss: 0.54034\n","Epoch: 00 [ 6204/14281 ( 43%)], Train Loss: 0.54016\n","Epoch: 00 [ 6244/14281 ( 44%)], Train Loss: 0.53960\n","Epoch: 00 [ 6284/14281 ( 44%)], Train Loss: 0.53827\n","Epoch: 00 [ 6324/14281 ( 44%)], Train Loss: 0.53730\n","Epoch: 00 [ 6364/14281 ( 45%)], Train Loss: 0.53761\n","Epoch: 00 [ 6404/14281 ( 45%)], Train Loss: 0.53631\n","Epoch: 00 [ 6444/14281 ( 45%)], Train Loss: 0.53541\n","Epoch: 00 [ 6484/14281 ( 45%)], Train Loss: 0.53382\n","Epoch: 00 [ 6524/14281 ( 46%)], Train Loss: 0.53286\n","Epoch: 00 [ 6564/14281 ( 46%)], Train Loss: 0.53247\n","Epoch: 00 [ 6604/14281 ( 46%)], Train Loss: 0.53323\n","Epoch: 00 [ 6644/14281 ( 47%)], Train Loss: 0.53350\n","Epoch: 00 [ 6684/14281 ( 47%)], Train Loss: 0.53291\n","Epoch: 00 [ 6724/14281 ( 47%)], Train Loss: 0.53239\n","Epoch: 00 [ 6764/14281 ( 47%)], Train Loss: 0.53158\n","Epoch: 00 [ 6804/14281 ( 48%)], Train Loss: 0.52984\n","Epoch: 00 [ 6844/14281 ( 48%)], Train Loss: 0.52922\n","Epoch: 00 [ 6884/14281 ( 48%)], Train Loss: 0.52861\n","Epoch: 00 [ 6924/14281 ( 48%)], Train Loss: 0.52782\n","Epoch: 00 [ 6964/14281 ( 49%)], Train Loss: 0.52807\n","Epoch: 00 [ 7004/14281 ( 49%)], Train Loss: 0.52705\n","Epoch: 00 [ 7044/14281 ( 49%)], Train Loss: 0.52675\n","Epoch: 00 [ 7084/14281 ( 50%)], Train Loss: 0.52498\n","Epoch: 00 [ 7124/14281 ( 50%)], Train Loss: 0.52430\n","Epoch: 00 [ 7164/14281 ( 50%)], Train Loss: 0.52346\n","Epoch: 00 [ 7204/14281 ( 50%)], Train Loss: 0.52228\n","Epoch: 00 [ 7244/14281 ( 51%)], Train Loss: 0.52191\n","Epoch: 00 [ 7284/14281 ( 51%)], Train Loss: 0.52024\n","Epoch: 00 [ 7324/14281 ( 51%)], Train Loss: 0.52018\n","Epoch: 00 [ 7364/14281 ( 52%)], Train Loss: 0.51858\n","Epoch: 00 [ 7404/14281 ( 52%)], Train Loss: 0.51713\n","Epoch: 00 [ 7444/14281 ( 52%)], Train Loss: 0.51665\n","Epoch: 00 [ 7484/14281 ( 52%)], Train Loss: 0.51576\n","Epoch: 00 [ 7524/14281 ( 53%)], Train Loss: 0.51471\n","Epoch: 00 [ 7564/14281 ( 53%)], Train Loss: 0.51435\n","Epoch: 00 [ 7604/14281 ( 53%)], Train Loss: 0.51289\n","Epoch: 00 [ 7644/14281 ( 54%)], Train Loss: 0.51346\n","Epoch: 00 [ 7684/14281 ( 54%)], Train Loss: 0.51313\n","Epoch: 00 [ 7724/14281 ( 54%)], Train Loss: 0.51332\n","Epoch: 00 [ 7764/14281 ( 54%)], Train Loss: 0.51330\n","Epoch: 00 [ 7804/14281 ( 55%)], Train Loss: 0.51205\n","Epoch: 00 [ 7844/14281 ( 55%)], Train Loss: 0.51187\n","Epoch: 00 [ 7884/14281 ( 55%)], Train Loss: 0.51076\n","Epoch: 00 [ 7924/14281 ( 55%)], Train Loss: 0.51129\n","Epoch: 00 [ 7964/14281 ( 56%)], Train Loss: 0.51117\n","Epoch: 00 [ 8004/14281 ( 56%)], Train Loss: 0.51160\n","Epoch: 00 [ 8044/14281 ( 56%)], Train Loss: 0.51023\n","Epoch: 00 [ 8084/14281 ( 57%)], Train Loss: 0.51003\n","Epoch: 00 [ 8124/14281 ( 57%)], Train Loss: 0.50917\n","Epoch: 00 [ 8164/14281 ( 57%)], Train Loss: 0.50930\n","Epoch: 00 [ 8204/14281 ( 57%)], Train Loss: 0.50969\n","Epoch: 00 [ 8244/14281 ( 58%)], Train Loss: 0.51008\n","Epoch: 00 [ 8284/14281 ( 58%)], Train Loss: 0.50896\n","Epoch: 00 [ 8324/14281 ( 58%)], Train Loss: 0.50739\n","Epoch: 00 [ 8364/14281 ( 59%)], Train Loss: 0.50691\n","Epoch: 00 [ 8404/14281 ( 59%)], Train Loss: 0.50708\n","Epoch: 00 [ 8444/14281 ( 59%)], Train Loss: 0.50589\n","Epoch: 00 [ 8484/14281 ( 59%)], Train Loss: 0.50525\n","Epoch: 00 [ 8524/14281 ( 60%)], Train Loss: 0.50576\n","Epoch: 00 [ 8564/14281 ( 60%)], Train Loss: 0.50479\n","Epoch: 00 [ 8604/14281 ( 60%)], Train Loss: 0.50420\n","Epoch: 00 [ 8644/14281 ( 61%)], Train Loss: 0.50359\n","Epoch: 00 [ 8684/14281 ( 61%)], Train Loss: 0.50326\n","Epoch: 00 [ 8724/14281 ( 61%)], Train Loss: 0.50348\n","Epoch: 00 [ 8764/14281 ( 61%)], Train Loss: 0.50247\n","Epoch: 00 [ 8804/14281 ( 62%)], Train Loss: 0.50217\n","Epoch: 00 [ 8844/14281 ( 62%)], Train Loss: 0.50166\n","Epoch: 00 [ 8884/14281 ( 62%)], Train Loss: 0.50135\n","Epoch: 00 [ 8924/14281 ( 62%)], Train Loss: 0.50124\n","Epoch: 00 [ 8964/14281 ( 63%)], Train Loss: 0.50050\n","Epoch: 00 [ 9004/14281 ( 63%)], Train Loss: 0.49996\n","Epoch: 00 [ 9044/14281 ( 63%)], Train Loss: 0.49953\n","Epoch: 00 [ 9084/14281 ( 64%)], Train Loss: 0.49807\n","Epoch: 00 [ 9124/14281 ( 64%)], Train Loss: 0.49729\n","Epoch: 00 [ 9164/14281 ( 64%)], Train Loss: 0.49734\n","Epoch: 00 [ 9204/14281 ( 64%)], Train Loss: 0.49639\n","Epoch: 00 [ 9244/14281 ( 65%)], Train Loss: 0.49617\n","Epoch: 00 [ 9284/14281 ( 65%)], Train Loss: 0.49481\n","Epoch: 00 [ 9324/14281 ( 65%)], Train Loss: 0.49347\n","Epoch: 00 [ 9364/14281 ( 66%)], Train Loss: 0.49309\n","Epoch: 00 [ 9404/14281 ( 66%)], Train Loss: 0.49384\n","Epoch: 00 [ 9444/14281 ( 66%)], Train Loss: 0.49381\n","Epoch: 00 [ 9484/14281 ( 66%)], Train Loss: 0.49339\n","Epoch: 00 [ 9524/14281 ( 67%)], Train Loss: 0.49272\n","Epoch: 00 [ 9564/14281 ( 67%)], Train Loss: 0.49265\n","Epoch: 00 [ 9604/14281 ( 67%)], Train Loss: 0.49227\n","Epoch: 00 [ 9644/14281 ( 68%)], Train Loss: 0.49137\n","Epoch: 00 [ 9684/14281 ( 68%)], Train Loss: 0.49102\n","Epoch: 00 [ 9724/14281 ( 68%)], Train Loss: 0.49029\n","Epoch: 00 [ 9764/14281 ( 68%)], Train Loss: 0.48909\n","Epoch: 00 [ 9804/14281 ( 69%)], Train Loss: 0.48824\n","Epoch: 00 [ 9844/14281 ( 69%)], Train Loss: 0.48758\n","Epoch: 00 [ 9884/14281 ( 69%)], Train Loss: 0.48723\n","Epoch: 00 [ 9924/14281 ( 69%)], Train Loss: 0.48717\n","Epoch: 00 [ 9964/14281 ( 70%)], Train Loss: 0.48727\n","Epoch: 00 [10004/14281 ( 70%)], Train Loss: 0.48662\n","Epoch: 00 [10044/14281 ( 70%)], Train Loss: 0.48652\n","Epoch: 00 [10084/14281 ( 71%)], Train Loss: 0.48595\n","Epoch: 00 [10124/14281 ( 71%)], Train Loss: 0.48530\n","Epoch: 00 [10164/14281 ( 71%)], Train Loss: 0.48499\n","Epoch: 00 [10204/14281 ( 71%)], Train Loss: 0.48398\n","Epoch: 00 [10244/14281 ( 72%)], Train Loss: 0.48307\n","Epoch: 00 [10284/14281 ( 72%)], Train Loss: 0.48237\n","Epoch: 00 [10324/14281 ( 72%)], Train Loss: 0.48259\n","Epoch: 00 [10364/14281 ( 73%)], Train Loss: 0.48192\n","Epoch: 00 [10404/14281 ( 73%)], Train Loss: 0.48149\n","Epoch: 00 [10444/14281 ( 73%)], Train Loss: 0.48037\n","Epoch: 00 [10484/14281 ( 73%)], Train Loss: 0.47970\n","Epoch: 00 [10524/14281 ( 74%)], Train Loss: 0.47874\n","Epoch: 00 [10564/14281 ( 74%)], Train Loss: 0.47840\n","Epoch: 00 [10604/14281 ( 74%)], Train Loss: 0.47806\n","Epoch: 00 [10644/14281 ( 75%)], Train Loss: 0.47784\n","Epoch: 00 [10684/14281 ( 75%)], Train Loss: 0.47697\n","Epoch: 00 [10724/14281 ( 75%)], Train Loss: 0.47618\n","Epoch: 00 [10764/14281 ( 75%)], Train Loss: 0.47585\n","Epoch: 00 [10804/14281 ( 76%)], Train Loss: 0.47609\n","Epoch: 00 [10844/14281 ( 76%)], Train Loss: 0.47566\n","Epoch: 00 [10884/14281 ( 76%)], Train Loss: 0.47471\n","Epoch: 00 [10924/14281 ( 76%)], Train Loss: 0.47400\n","Epoch: 00 [10964/14281 ( 77%)], Train Loss: 0.47322\n","Epoch: 00 [11004/14281 ( 77%)], Train Loss: 0.47310\n","Epoch: 00 [11044/14281 ( 77%)], Train Loss: 0.47265\n","Epoch: 00 [11084/14281 ( 78%)], Train Loss: 0.47142\n","Epoch: 00 [11124/14281 ( 78%)], Train Loss: 0.47077\n","Epoch: 00 [11164/14281 ( 78%)], Train Loss: 0.47104\n","Epoch: 00 [11204/14281 ( 78%)], Train Loss: 0.47070\n","Epoch: 00 [11244/14281 ( 79%)], Train Loss: 0.46996\n","Epoch: 00 [11284/14281 ( 79%)], Train Loss: 0.46941\n","Epoch: 00 [11324/14281 ( 79%)], Train Loss: 0.46909\n","Epoch: 00 [11364/14281 ( 80%)], Train Loss: 0.46904\n","Epoch: 00 [11404/14281 ( 80%)], Train Loss: 0.46847\n","Epoch: 00 [11444/14281 ( 80%)], Train Loss: 0.46821\n","Epoch: 00 [11484/14281 ( 80%)], Train Loss: 0.46841\n","Epoch: 00 [11524/14281 ( 81%)], Train Loss: 0.46737\n","Epoch: 00 [11564/14281 ( 81%)], Train Loss: 0.46689\n","Epoch: 00 [11604/14281 ( 81%)], Train Loss: 0.46615\n","Epoch: 00 [11644/14281 ( 82%)], Train Loss: 0.46599\n","Epoch: 00 [11684/14281 ( 82%)], Train Loss: 0.46572\n","Epoch: 00 [11724/14281 ( 82%)], Train Loss: 0.46508\n","Epoch: 00 [11764/14281 ( 82%)], Train Loss: 0.46484\n","Epoch: 00 [11804/14281 ( 83%)], Train Loss: 0.46459\n","Epoch: 00 [11844/14281 ( 83%)], Train Loss: 0.46382\n","Epoch: 00 [11884/14281 ( 83%)], Train Loss: 0.46327\n","Epoch: 00 [11924/14281 ( 83%)], Train Loss: 0.46286\n","Epoch: 00 [11964/14281 ( 84%)], Train Loss: 0.46197\n","Epoch: 00 [12004/14281 ( 84%)], Train Loss: 0.46168\n","Epoch: 00 [12044/14281 ( 84%)], Train Loss: 0.46099\n","Epoch: 00 [12084/14281 ( 85%)], Train Loss: 0.46043\n","Epoch: 00 [12124/14281 ( 85%)], Train Loss: 0.45983\n","Epoch: 00 [12164/14281 ( 85%)], Train Loss: 0.45897\n","Epoch: 00 [12204/14281 ( 85%)], Train Loss: 0.45895\n","Epoch: 00 [12244/14281 ( 86%)], Train Loss: 0.45852\n","Epoch: 00 [12284/14281 ( 86%)], Train Loss: 0.45818\n","Epoch: 00 [12324/14281 ( 86%)], Train Loss: 0.45819\n","Epoch: 00 [12364/14281 ( 87%)], Train Loss: 0.45762\n","Epoch: 00 [12404/14281 ( 87%)], Train Loss: 0.45719\n","Epoch: 00 [12444/14281 ( 87%)], Train Loss: 0.45653\n","Epoch: 00 [12484/14281 ( 87%)], Train Loss: 0.45643\n","Epoch: 00 [12524/14281 ( 88%)], Train Loss: 0.45658\n","Epoch: 00 [12564/14281 ( 88%)], Train Loss: 0.45629\n","Epoch: 00 [12604/14281 ( 88%)], Train Loss: 0.45561\n","Epoch: 00 [12644/14281 ( 89%)], Train Loss: 0.45518\n","Epoch: 00 [12684/14281 ( 89%)], Train Loss: 0.45450\n","Epoch: 00 [12724/14281 ( 89%)], Train Loss: 0.45395\n","Epoch: 00 [12764/14281 ( 89%)], Train Loss: 0.45361\n","Epoch: 00 [12804/14281 ( 90%)], Train Loss: 0.45357\n","Epoch: 00 [12844/14281 ( 90%)], Train Loss: 0.45325\n","Epoch: 00 [12884/14281 ( 90%)], Train Loss: 0.45292\n","Epoch: 00 [12924/14281 ( 90%)], Train Loss: 0.45250\n","Epoch: 00 [12964/14281 ( 91%)], Train Loss: 0.45219\n","Epoch: 00 [13004/14281 ( 91%)], Train Loss: 0.45163\n","Epoch: 00 [13044/14281 ( 91%)], Train Loss: 0.45085\n","Epoch: 00 [13084/14281 ( 92%)], Train Loss: 0.45118\n","Epoch: 00 [13124/14281 ( 92%)], Train Loss: 0.45067\n","Epoch: 00 [13164/14281 ( 92%)], Train Loss: 0.44993\n","Epoch: 00 [13204/14281 ( 92%)], Train Loss: 0.44963\n","Epoch: 00 [13244/14281 ( 93%)], Train Loss: 0.44891\n","Epoch: 00 [13284/14281 ( 93%)], Train Loss: 0.44886\n","Epoch: 00 [13324/14281 ( 93%)], Train Loss: 0.44814\n","Epoch: 00 [13364/14281 ( 94%)], Train Loss: 0.44765\n","Epoch: 00 [13404/14281 ( 94%)], Train Loss: 0.44698\n","Epoch: 00 [13444/14281 ( 94%)], Train Loss: 0.44667\n","Epoch: 00 [13484/14281 ( 94%)], Train Loss: 0.44637\n","Epoch: 00 [13524/14281 ( 95%)], Train Loss: 0.44573\n","Epoch: 00 [13564/14281 ( 95%)], Train Loss: 0.44508\n","Epoch: 00 [13604/14281 ( 95%)], Train Loss: 0.44533\n","Epoch: 00 [13644/14281 ( 96%)], Train Loss: 0.44508\n","Epoch: 00 [13684/14281 ( 96%)], Train Loss: 0.44451\n","Epoch: 00 [13724/14281 ( 96%)], Train Loss: 0.44395\n","Epoch: 00 [13764/14281 ( 96%)], Train Loss: 0.44354\n","Epoch: 00 [13804/14281 ( 97%)], Train Loss: 0.44383\n","Epoch: 00 [13844/14281 ( 97%)], Train Loss: 0.44331\n","Epoch: 00 [13884/14281 ( 97%)], Train Loss: 0.44297\n","Epoch: 00 [13924/14281 ( 98%)], Train Loss: 0.44301\n","Epoch: 00 [13964/14281 ( 98%)], Train Loss: 0.44251\n","Epoch: 00 [14004/14281 ( 98%)], Train Loss: 0.44217\n","Epoch: 00 [14044/14281 ( 98%)], Train Loss: 0.44165\n","Epoch: 00 [14084/14281 ( 99%)], Train Loss: 0.44190\n","Epoch: 00 [14124/14281 ( 99%)], Train Loss: 0.44152\n","Epoch: 00 [14164/14281 ( 99%)], Train Loss: 0.44115\n","Epoch: 00 [14204/14281 ( 99%)], Train Loss: 0.44071\n","Epoch: 00 [14244/14281 (100%)], Train Loss: 0.44015\n","Epoch: 00 [14281/14281 (100%)], Train Loss: 0.43971\n","----Validation Results Summary----\n","Epoch: [0] Valid Loss: 0.74995\n","0 Epoch, Best epoch was updated! Valid Loss: 0.74995\n","Saving model checkpoint to chaii-qa-5-fold-xlmroberta-torch-fit/checkpoint-fold-1.\n","\n","Total Training Time: 1984.789630651474secs, Average Training Time per Epoch: 1984.789630651474secs.\n","Total Validation Time: 137.90667963027954secs, Average Validation Time per Epoch: 137.90667963027954secs.\n","\n","\n","--------------------------------------------------\n","FOLD: 2\n","--------------------------------------------------\n","Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n","Num examples Train= 13514, Num examples Valid=4044\n","Total Training Steps: 1690, Total Warmup Steps: 169\n","Epoch: 00 [    4/13514 (  0%)], Train Loss: 2.90795\n","Epoch: 00 [   44/13514 (  0%)], Train Loss: 2.86742\n","Epoch: 00 [   84/13514 (  1%)], Train Loss: 2.83108\n","Epoch: 00 [  124/13514 (  1%)], Train Loss: 2.78966\n","Epoch: 00 [  164/13514 (  1%)], Train Loss: 2.71875\n","Epoch: 00 [  204/13514 (  2%)], Train Loss: 2.62483\n","Epoch: 00 [  244/13514 (  2%)], Train Loss: 2.53048\n","Epoch: 00 [  284/13514 (  2%)], Train Loss: 2.41792\n","Epoch: 00 [  324/13514 (  2%)], Train Loss: 2.28025\n","Epoch: 00 [  364/13514 (  3%)], Train Loss: 2.15253\n","Epoch: 00 [  404/13514 (  3%)], Train Loss: 2.01561\n","Epoch: 00 [  444/13514 (  3%)], Train Loss: 1.91328\n","Epoch: 00 [  484/13514 (  4%)], Train Loss: 1.81336\n","Epoch: 00 [  524/13514 (  4%)], Train Loss: 1.73286\n","Epoch: 00 [  564/13514 (  4%)], Train Loss: 1.64820\n","Epoch: 00 [  604/13514 (  4%)], Train Loss: 1.58398\n","Epoch: 00 [  644/13514 (  5%)], Train Loss: 1.52170\n","Epoch: 00 [  684/13514 (  5%)], Train Loss: 1.46548\n","Epoch: 00 [  724/13514 (  5%)], Train Loss: 1.41342\n","Epoch: 00 [  764/13514 (  6%)], Train Loss: 1.36347\n","Epoch: 00 [  804/13514 (  6%)], Train Loss: 1.32518\n","Epoch: 00 [  844/13514 (  6%)], Train Loss: 1.29256\n","Epoch: 00 [  884/13514 (  7%)], Train Loss: 1.25082\n","Epoch: 00 [  924/13514 (  7%)], Train Loss: 1.21749\n","Epoch: 00 [  964/13514 (  7%)], Train Loss: 1.18818\n","Epoch: 00 [ 1004/13514 (  7%)], Train Loss: 1.16052\n","Epoch: 00 [ 1044/13514 (  8%)], Train Loss: 1.13587\n","Epoch: 00 [ 1084/13514 (  8%)], Train Loss: 1.11656\n","Epoch: 00 [ 1124/13514 (  8%)], Train Loss: 1.09067\n","Epoch: 00 [ 1164/13514 (  9%)], Train Loss: 1.06905\n","Epoch: 00 [ 1204/13514 (  9%)], Train Loss: 1.05497\n","Epoch: 00 [ 1244/13514 (  9%)], Train Loss: 1.03260\n","Epoch: 00 [ 1284/13514 ( 10%)], Train Loss: 1.01698\n","Epoch: 00 [ 1324/13514 ( 10%)], Train Loss: 0.99239\n","Epoch: 00 [ 1364/13514 ( 10%)], Train Loss: 0.97712\n","Epoch: 00 [ 1404/13514 ( 10%)], Train Loss: 0.96014\n","Epoch: 00 [ 1444/13514 ( 11%)], Train Loss: 0.95099\n","Epoch: 00 [ 1484/13514 ( 11%)], Train Loss: 0.94083\n","Epoch: 00 [ 1524/13514 ( 11%)], Train Loss: 0.92987\n","Epoch: 00 [ 1564/13514 ( 12%)], Train Loss: 0.92135\n","Epoch: 00 [ 1604/13514 ( 12%)], Train Loss: 0.90790\n","Epoch: 00 [ 1644/13514 ( 12%)], Train Loss: 0.89332\n","Epoch: 00 [ 1684/13514 ( 12%)], Train Loss: 0.88026\n","Epoch: 00 [ 1724/13514 ( 13%)], Train Loss: 0.87210\n","Epoch: 00 [ 1764/13514 ( 13%)], Train Loss: 0.86746\n","Epoch: 00 [ 1804/13514 ( 13%)], Train Loss: 0.85663\n","Epoch: 00 [ 1844/13514 ( 14%)], Train Loss: 0.84658\n","Epoch: 00 [ 1884/13514 ( 14%)], Train Loss: 0.83611\n","Epoch: 00 [ 1924/13514 ( 14%)], Train Loss: 0.82796\n","Epoch: 00 [ 1964/13514 ( 15%)], Train Loss: 0.82388\n","Epoch: 00 [ 2004/13514 ( 15%)], Train Loss: 0.81368\n","Epoch: 00 [ 2044/13514 ( 15%)], Train Loss: 0.80961\n","Epoch: 00 [ 2084/13514 ( 15%)], Train Loss: 0.80383\n","Epoch: 00 [ 2124/13514 ( 16%)], Train Loss: 0.79744\n","Epoch: 00 [ 2164/13514 ( 16%)], Train Loss: 0.79114\n","Epoch: 00 [ 2204/13514 ( 16%)], Train Loss: 0.78619\n","Epoch: 00 [ 2244/13514 ( 17%)], Train Loss: 0.77576\n","Epoch: 00 [ 2284/13514 ( 17%)], Train Loss: 0.77006\n","Epoch: 00 [ 2324/13514 ( 17%)], Train Loss: 0.76191\n","Epoch: 00 [ 2364/13514 ( 17%)], Train Loss: 0.75802\n","Epoch: 00 [ 2404/13514 ( 18%)], Train Loss: 0.75218\n","Epoch: 00 [ 2444/13514 ( 18%)], Train Loss: 0.74980\n","Epoch: 00 [ 2484/13514 ( 18%)], Train Loss: 0.74534\n","Epoch: 00 [ 2524/13514 ( 19%)], Train Loss: 0.74107\n","Epoch: 00 [ 2564/13514 ( 19%)], Train Loss: 0.73709\n","Epoch: 00 [ 2604/13514 ( 19%)], Train Loss: 0.73478\n","Epoch: 00 [ 2644/13514 ( 20%)], Train Loss: 0.73034\n","Epoch: 00 [ 2684/13514 ( 20%)], Train Loss: 0.72479\n","Epoch: 00 [ 2724/13514 ( 20%)], Train Loss: 0.72110\n","Epoch: 00 [ 2764/13514 ( 20%)], Train Loss: 0.71915\n","Epoch: 00 [ 2804/13514 ( 21%)], Train Loss: 0.71797\n","Epoch: 00 [ 2844/13514 ( 21%)], Train Loss: 0.71462\n","Epoch: 00 [ 2884/13514 ( 21%)], Train Loss: 0.70880\n","Epoch: 00 [ 2924/13514 ( 22%)], Train Loss: 0.70804\n","Epoch: 00 [ 2964/13514 ( 22%)], Train Loss: 0.71019\n","Epoch: 00 [ 3004/13514 ( 22%)], Train Loss: 0.70772\n","Epoch: 00 [ 3044/13514 ( 23%)], Train Loss: 0.70676\n","Epoch: 00 [ 3084/13514 ( 23%)], Train Loss: 0.70243\n","Epoch: 00 [ 3124/13514 ( 23%)], Train Loss: 0.69719\n","Epoch: 00 [ 3164/13514 ( 23%)], Train Loss: 0.69174\n","Epoch: 00 [ 3204/13514 ( 24%)], Train Loss: 0.68902\n","Epoch: 00 [ 3244/13514 ( 24%)], Train Loss: 0.68488\n","Epoch: 00 [ 3284/13514 ( 24%)], Train Loss: 0.68027\n","Epoch: 00 [ 3324/13514 ( 25%)], Train Loss: 0.67580\n","Epoch: 00 [ 3364/13514 ( 25%)], Train Loss: 0.67515\n","Epoch: 00 [ 3404/13514 ( 25%)], Train Loss: 0.67604\n","Epoch: 00 [ 3444/13514 ( 25%)], Train Loss: 0.67533\n","Epoch: 00 [ 3484/13514 ( 26%)], Train Loss: 0.67334\n","Epoch: 00 [ 3524/13514 ( 26%)], Train Loss: 0.66931\n","Epoch: 00 [ 3564/13514 ( 26%)], Train Loss: 0.66595\n","Epoch: 00 [ 3604/13514 ( 27%)], Train Loss: 0.66313\n","Epoch: 00 [ 3644/13514 ( 27%)], Train Loss: 0.66122\n","Epoch: 00 [ 3684/13514 ( 27%)], Train Loss: 0.66035\n","Epoch: 00 [ 3724/13514 ( 28%)], Train Loss: 0.66059\n","Epoch: 00 [ 3764/13514 ( 28%)], Train Loss: 0.65723\n","Epoch: 00 [ 3804/13514 ( 28%)], Train Loss: 0.65603\n","Epoch: 00 [ 3844/13514 ( 28%)], Train Loss: 0.65353\n","Epoch: 00 [ 3884/13514 ( 29%)], Train Loss: 0.65291\n","Epoch: 00 [ 3924/13514 ( 29%)], Train Loss: 0.65157\n","Epoch: 00 [ 3964/13514 ( 29%)], Train Loss: 0.64915\n","Epoch: 00 [ 4004/13514 ( 30%)], Train Loss: 0.64745\n","Epoch: 00 [ 4044/13514 ( 30%)], Train Loss: 0.64434\n","Epoch: 00 [ 4084/13514 ( 30%)], Train Loss: 0.64143\n","Epoch: 00 [ 4124/13514 ( 31%)], Train Loss: 0.63747\n","Epoch: 00 [ 4164/13514 ( 31%)], Train Loss: 0.63608\n","Epoch: 00 [ 4204/13514 ( 31%)], Train Loss: 0.63552\n","Epoch: 00 [ 4244/13514 ( 31%)], Train Loss: 0.63473\n","Epoch: 00 [ 4284/13514 ( 32%)], Train Loss: 0.63469\n","Epoch: 00 [ 4324/13514 ( 32%)], Train Loss: 0.63528\n","Epoch: 00 [ 4364/13514 ( 32%)], Train Loss: 0.63401\n","Epoch: 00 [ 4404/13514 ( 33%)], Train Loss: 0.63465\n","Epoch: 00 [ 4444/13514 ( 33%)], Train Loss: 0.63351\n","Epoch: 00 [ 4484/13514 ( 33%)], Train Loss: 0.63173\n","Epoch: 00 [ 4524/13514 ( 33%)], Train Loss: 0.62933\n","Epoch: 00 [ 4564/13514 ( 34%)], Train Loss: 0.62760\n","Epoch: 00 [ 4604/13514 ( 34%)], Train Loss: 0.62643\n","Epoch: 00 [ 4644/13514 ( 34%)], Train Loss: 0.62409\n","Epoch: 00 [ 4684/13514 ( 35%)], Train Loss: 0.62219\n","Epoch: 00 [ 4724/13514 ( 35%)], Train Loss: 0.62034\n","Epoch: 00 [ 4764/13514 ( 35%)], Train Loss: 0.61811\n","Epoch: 00 [ 4804/13514 ( 36%)], Train Loss: 0.61760\n","Epoch: 00 [ 4844/13514 ( 36%)], Train Loss: 0.61598\n","Epoch: 00 [ 4884/13514 ( 36%)], Train Loss: 0.61246\n","Epoch: 00 [ 4924/13514 ( 36%)], Train Loss: 0.60992\n","Epoch: 00 [ 4964/13514 ( 37%)], Train Loss: 0.60854\n","Epoch: 00 [ 5004/13514 ( 37%)], Train Loss: 0.60635\n","Epoch: 00 [ 5044/13514 ( 37%)], Train Loss: 0.60401\n","Epoch: 00 [ 5084/13514 ( 38%)], Train Loss: 0.60176\n","Epoch: 00 [ 5124/13514 ( 38%)], Train Loss: 0.60009\n","Epoch: 00 [ 5164/13514 ( 38%)], Train Loss: 0.59804\n","Epoch: 00 [ 5204/13514 ( 39%)], Train Loss: 0.59642\n","Epoch: 00 [ 5244/13514 ( 39%)], Train Loss: 0.59444\n","Epoch: 00 [ 5284/13514 ( 39%)], Train Loss: 0.59342\n","Epoch: 00 [ 5324/13514 ( 39%)], Train Loss: 0.59337\n","Epoch: 00 [ 5364/13514 ( 40%)], Train Loss: 0.59150\n","Epoch: 00 [ 5404/13514 ( 40%)], Train Loss: 0.59090\n","Epoch: 00 [ 5444/13514 ( 40%)], Train Loss: 0.59123\n","Epoch: 00 [ 5484/13514 ( 41%)], Train Loss: 0.58902\n","Epoch: 00 [ 5524/13514 ( 41%)], Train Loss: 0.58624\n","Epoch: 00 [ 5564/13514 ( 41%)], Train Loss: 0.58456\n","Epoch: 00 [ 5604/13514 ( 41%)], Train Loss: 0.58262\n","Epoch: 00 [ 5644/13514 ( 42%)], Train Loss: 0.58219\n","Epoch: 00 [ 5684/13514 ( 42%)], Train Loss: 0.58138\n","Epoch: 00 [ 5724/13514 ( 42%)], Train Loss: 0.58025\n","Epoch: 00 [ 5764/13514 ( 43%)], Train Loss: 0.57958\n","Epoch: 00 [ 5804/13514 ( 43%)], Train Loss: 0.57870\n","Epoch: 00 [ 5844/13514 ( 43%)], Train Loss: 0.57736\n","Epoch: 00 [ 5884/13514 ( 44%)], Train Loss: 0.57796\n","Epoch: 00 [ 5924/13514 ( 44%)], Train Loss: 0.57592\n","Epoch: 00 [ 5964/13514 ( 44%)], Train Loss: 0.57426\n","Epoch: 00 [ 6004/13514 ( 44%)], Train Loss: 0.57508\n","Epoch: 00 [ 6044/13514 ( 45%)], Train Loss: 0.57325\n","Epoch: 00 [ 6084/13514 ( 45%)], Train Loss: 0.57221\n","Epoch: 00 [ 6124/13514 ( 45%)], Train Loss: 0.57102\n","Epoch: 00 [ 6164/13514 ( 46%)], Train Loss: 0.56892\n","Epoch: 00 [ 6204/13514 ( 46%)], Train Loss: 0.56808\n","Epoch: 00 [ 6244/13514 ( 46%)], Train Loss: 0.56650\n","Epoch: 00 [ 6284/13514 ( 46%)], Train Loss: 0.56572\n","Epoch: 00 [ 6324/13514 ( 47%)], Train Loss: 0.56499\n","Epoch: 00 [ 6364/13514 ( 47%)], Train Loss: 0.56536\n","Epoch: 00 [ 6404/13514 ( 47%)], Train Loss: 0.56514\n","Epoch: 00 [ 6444/13514 ( 48%)], Train Loss: 0.56485\n","Epoch: 00 [ 6484/13514 ( 48%)], Train Loss: 0.56420\n","Epoch: 00 [ 6524/13514 ( 48%)], Train Loss: 0.56355\n","Epoch: 00 [ 6564/13514 ( 49%)], Train Loss: 0.56324\n","Epoch: 00 [ 6604/13514 ( 49%)], Train Loss: 0.56270\n","Epoch: 00 [ 6644/13514 ( 49%)], Train Loss: 0.56179\n","Epoch: 00 [ 6684/13514 ( 49%)], Train Loss: 0.56166\n","Epoch: 00 [ 6724/13514 ( 50%)], Train Loss: 0.56103\n","Epoch: 00 [ 6764/13514 ( 50%)], Train Loss: 0.55982\n","Epoch: 00 [ 6804/13514 ( 50%)], Train Loss: 0.55959\n","Epoch: 00 [ 6844/13514 ( 51%)], Train Loss: 0.55886\n","Epoch: 00 [ 6884/13514 ( 51%)], Train Loss: 0.55676\n","Epoch: 00 [ 6924/13514 ( 51%)], Train Loss: 0.55652\n","Epoch: 00 [ 6964/13514 ( 52%)], Train Loss: 0.55625\n","Epoch: 00 [ 7004/13514 ( 52%)], Train Loss: 0.55451\n","Epoch: 00 [ 7044/13514 ( 52%)], Train Loss: 0.55298\n","Epoch: 00 [ 7084/13514 ( 52%)], Train Loss: 0.55344\n","Epoch: 00 [ 7124/13514 ( 53%)], Train Loss: 0.55279\n","Epoch: 00 [ 7164/13514 ( 53%)], Train Loss: 0.55239\n","Epoch: 00 [ 7204/13514 ( 53%)], Train Loss: 0.55199\n","Epoch: 00 [ 7244/13514 ( 54%)], Train Loss: 0.55161\n","Epoch: 00 [ 7284/13514 ( 54%)], Train Loss: 0.55107\n","Epoch: 00 [ 7324/13514 ( 54%)], Train Loss: 0.54961\n","Epoch: 00 [ 7364/13514 ( 54%)], Train Loss: 0.54884\n","Epoch: 00 [ 7404/13514 ( 55%)], Train Loss: 0.54829\n","Epoch: 00 [ 7444/13514 ( 55%)], Train Loss: 0.54683\n","Epoch: 00 [ 7484/13514 ( 55%)], Train Loss: 0.54563\n","Epoch: 00 [ 7524/13514 ( 56%)], Train Loss: 0.54437\n","Epoch: 00 [ 7564/13514 ( 56%)], Train Loss: 0.54293\n","Epoch: 00 [ 7604/13514 ( 56%)], Train Loss: 0.54216\n","Epoch: 00 [ 7644/13514 ( 57%)], Train Loss: 0.54287\n","Epoch: 00 [ 7684/13514 ( 57%)], Train Loss: 0.54196\n","Epoch: 00 [ 7724/13514 ( 57%)], Train Loss: 0.54132\n","Epoch: 00 [ 7764/13514 ( 57%)], Train Loss: 0.54017\n","Epoch: 00 [ 7804/13514 ( 58%)], Train Loss: 0.53991\n","Epoch: 00 [ 7844/13514 ( 58%)], Train Loss: 0.53939\n","Epoch: 00 [ 7884/13514 ( 58%)], Train Loss: 0.53995\n","Epoch: 00 [ 7924/13514 ( 59%)], Train Loss: 0.53862\n","Epoch: 00 [ 7964/13514 ( 59%)], Train Loss: 0.53804\n","Epoch: 00 [ 8004/13514 ( 59%)], Train Loss: 0.53831\n","Epoch: 00 [ 8044/13514 ( 60%)], Train Loss: 0.53741\n","Epoch: 00 [ 8084/13514 ( 60%)], Train Loss: 0.53664\n","Epoch: 00 [ 8124/13514 ( 60%)], Train Loss: 0.53575\n","Epoch: 00 [ 8164/13514 ( 60%)], Train Loss: 0.53522\n","Epoch: 00 [ 8204/13514 ( 61%)], Train Loss: 0.53462\n","Epoch: 00 [ 8244/13514 ( 61%)], Train Loss: 0.53373\n","Epoch: 00 [ 8284/13514 ( 61%)], Train Loss: 0.53289\n","Epoch: 00 [ 8324/13514 ( 62%)], Train Loss: 0.53181\n","Epoch: 00 [ 8364/13514 ( 62%)], Train Loss: 0.53054\n","Epoch: 00 [ 8404/13514 ( 62%)], Train Loss: 0.52910\n","Epoch: 00 [ 8444/13514 ( 62%)], Train Loss: 0.52766\n","Epoch: 00 [ 8484/13514 ( 63%)], Train Loss: 0.52721\n","Epoch: 00 [ 8524/13514 ( 63%)], Train Loss: 0.52630\n","Epoch: 00 [ 8564/13514 ( 63%)], Train Loss: 0.52538\n","Epoch: 00 [ 8604/13514 ( 64%)], Train Loss: 0.52465\n","Epoch: 00 [ 8644/13514 ( 64%)], Train Loss: 0.52391\n","Epoch: 00 [ 8684/13514 ( 64%)], Train Loss: 0.52443\n","Epoch: 00 [ 8724/13514 ( 65%)], Train Loss: 0.52322\n","Epoch: 00 [ 8764/13514 ( 65%)], Train Loss: 0.52281\n","Epoch: 00 [ 8804/13514 ( 65%)], Train Loss: 0.52214\n","Epoch: 00 [ 8844/13514 ( 65%)], Train Loss: 0.52126\n","Epoch: 00 [ 8884/13514 ( 66%)], Train Loss: 0.52118\n","Epoch: 00 [ 8924/13514 ( 66%)], Train Loss: 0.52000\n","Epoch: 00 [ 8964/13514 ( 66%)], Train Loss: 0.51946\n","Epoch: 00 [ 9004/13514 ( 67%)], Train Loss: 0.51907\n","Epoch: 00 [ 9044/13514 ( 67%)], Train Loss: 0.51884\n","Epoch: 00 [ 9084/13514 ( 67%)], Train Loss: 0.51790\n","Epoch: 00 [ 9124/13514 ( 68%)], Train Loss: 0.51703\n","Epoch: 00 [ 9164/13514 ( 68%)], Train Loss: 0.51583\n","Epoch: 00 [ 9204/13514 ( 68%)], Train Loss: 0.51585\n","Epoch: 00 [ 9244/13514 ( 68%)], Train Loss: 0.51513\n","Epoch: 00 [ 9284/13514 ( 69%)], Train Loss: 0.51440\n","Epoch: 00 [ 9324/13514 ( 69%)], Train Loss: 0.51394\n","Epoch: 00 [ 9364/13514 ( 69%)], Train Loss: 0.51327\n","Epoch: 00 [ 9404/13514 ( 70%)], Train Loss: 0.51243\n","Epoch: 00 [ 9444/13514 ( 70%)], Train Loss: 0.51202\n","Epoch: 00 [ 9484/13514 ( 70%)], Train Loss: 0.51184\n","Epoch: 00 [ 9524/13514 ( 70%)], Train Loss: 0.51154\n","Epoch: 00 [ 9564/13514 ( 71%)], Train Loss: 0.51041\n","Epoch: 00 [ 9604/13514 ( 71%)], Train Loss: 0.50970\n","Epoch: 00 [ 9644/13514 ( 71%)], Train Loss: 0.50900\n","Epoch: 00 [ 9684/13514 ( 72%)], Train Loss: 0.50942\n","Epoch: 00 [ 9724/13514 ( 72%)], Train Loss: 0.50912\n","Epoch: 00 [ 9764/13514 ( 72%)], Train Loss: 0.50850\n","Epoch: 00 [ 9804/13514 ( 73%)], Train Loss: 0.50802\n","Epoch: 00 [ 9844/13514 ( 73%)], Train Loss: 0.50784\n","Epoch: 00 [ 9884/13514 ( 73%)], Train Loss: 0.50664\n","Epoch: 00 [ 9924/13514 ( 73%)], Train Loss: 0.50597\n","Epoch: 00 [ 9964/13514 ( 74%)], Train Loss: 0.50554\n","Epoch: 00 [10004/13514 ( 74%)], Train Loss: 0.50557\n","Epoch: 00 [10044/13514 ( 74%)], Train Loss: 0.50498\n","Epoch: 00 [10084/13514 ( 75%)], Train Loss: 0.50519\n","Epoch: 00 [10124/13514 ( 75%)], Train Loss: 0.50518\n","Epoch: 00 [10164/13514 ( 75%)], Train Loss: 0.50417\n","Epoch: 00 [10204/13514 ( 76%)], Train Loss: 0.50326\n","Epoch: 00 [10244/13514 ( 76%)], Train Loss: 0.50327\n","Epoch: 00 [10284/13514 ( 76%)], Train Loss: 0.50247\n","Epoch: 00 [10324/13514 ( 76%)], Train Loss: 0.50140\n","Epoch: 00 [10364/13514 ( 77%)], Train Loss: 0.50087\n","Epoch: 00 [10404/13514 ( 77%)], Train Loss: 0.50093\n","Epoch: 00 [10444/13514 ( 77%)], Train Loss: 0.50008\n","Epoch: 00 [10484/13514 ( 78%)], Train Loss: 0.50016\n","Epoch: 00 [10524/13514 ( 78%)], Train Loss: 0.49966\n","Epoch: 00 [10564/13514 ( 78%)], Train Loss: 0.49903\n","Epoch: 00 [10604/13514 ( 78%)], Train Loss: 0.49842\n","Epoch: 00 [10644/13514 ( 79%)], Train Loss: 0.49770\n","Epoch: 00 [10684/13514 ( 79%)], Train Loss: 0.49722\n","Epoch: 00 [10724/13514 ( 79%)], Train Loss: 0.49632\n","Epoch: 00 [10764/13514 ( 80%)], Train Loss: 0.49651\n","Epoch: 00 [10804/13514 ( 80%)], Train Loss: 0.49603\n","Epoch: 00 [10844/13514 ( 80%)], Train Loss: 0.49517\n","Epoch: 00 [10884/13514 ( 81%)], Train Loss: 0.49439\n","Epoch: 00 [10924/13514 ( 81%)], Train Loss: 0.49407\n","Epoch: 00 [10964/13514 ( 81%)], Train Loss: 0.49320\n","Epoch: 00 [11004/13514 ( 81%)], Train Loss: 0.49240\n","Epoch: 00 [11044/13514 ( 82%)], Train Loss: 0.49196\n","Epoch: 00 [11084/13514 ( 82%)], Train Loss: 0.49107\n","Epoch: 00 [11124/13514 ( 82%)], Train Loss: 0.49095\n","Epoch: 00 [11164/13514 ( 83%)], Train Loss: 0.49091\n","Epoch: 00 [11204/13514 ( 83%)], Train Loss: 0.49051\n","Epoch: 00 [11244/13514 ( 83%)], Train Loss: 0.49024\n","Epoch: 00 [11284/13514 ( 83%)], Train Loss: 0.48966\n","Epoch: 00 [11324/13514 ( 84%)], Train Loss: 0.48908\n","Epoch: 00 [11364/13514 ( 84%)], Train Loss: 0.48878\n","Epoch: 00 [11404/13514 ( 84%)], Train Loss: 0.48833\n","Epoch: 00 [11444/13514 ( 85%)], Train Loss: 0.48772\n","Epoch: 00 [11484/13514 ( 85%)], Train Loss: 0.48721\n","Epoch: 00 [11524/13514 ( 85%)], Train Loss: 0.48634\n","Epoch: 00 [11564/13514 ( 86%)], Train Loss: 0.48600\n","Epoch: 00 [11604/13514 ( 86%)], Train Loss: 0.48492\n","Epoch: 00 [11644/13514 ( 86%)], Train Loss: 0.48408\n","Epoch: 00 [11684/13514 ( 86%)], Train Loss: 0.48352\n","Epoch: 00 [11724/13514 ( 87%)], Train Loss: 0.48350\n","Epoch: 00 [11764/13514 ( 87%)], Train Loss: 0.48279\n","Epoch: 00 [11804/13514 ( 87%)], Train Loss: 0.48254\n","Epoch: 00 [11844/13514 ( 88%)], Train Loss: 0.48188\n","Epoch: 00 [11884/13514 ( 88%)], Train Loss: 0.48160\n","Epoch: 00 [11924/13514 ( 88%)], Train Loss: 0.48125\n","Epoch: 00 [11964/13514 ( 89%)], Train Loss: 0.48133\n","Epoch: 00 [12004/13514 ( 89%)], Train Loss: 0.48096\n","Epoch: 00 [12044/13514 ( 89%)], Train Loss: 0.48047\n","Epoch: 00 [12084/13514 ( 89%)], Train Loss: 0.48078\n","Epoch: 00 [12124/13514 ( 90%)], Train Loss: 0.48050\n","Epoch: 00 [12164/13514 ( 90%)], Train Loss: 0.48076\n","Epoch: 00 [12204/13514 ( 90%)], Train Loss: 0.48035\n","Epoch: 00 [12244/13514 ( 91%)], Train Loss: 0.47975\n","Epoch: 00 [12284/13514 ( 91%)], Train Loss: 0.47912\n","Epoch: 00 [12324/13514 ( 91%)], Train Loss: 0.47841\n","Epoch: 00 [12364/13514 ( 91%)], Train Loss: 0.47823\n","Epoch: 00 [12404/13514 ( 92%)], Train Loss: 0.47785\n","Epoch: 00 [12444/13514 ( 92%)], Train Loss: 0.47827\n","Epoch: 00 [12484/13514 ( 92%)], Train Loss: 0.47776\n","Epoch: 00 [12524/13514 ( 93%)], Train Loss: 0.47785\n","Epoch: 00 [12564/13514 ( 93%)], Train Loss: 0.47729\n","Epoch: 00 [12604/13514 ( 93%)], Train Loss: 0.47721\n","Epoch: 00 [12644/13514 ( 94%)], Train Loss: 0.47706\n","Epoch: 00 [12684/13514 ( 94%)], Train Loss: 0.47674\n","Epoch: 00 [12724/13514 ( 94%)], Train Loss: 0.47622\n","Epoch: 00 [12764/13514 ( 94%)], Train Loss: 0.47534\n","Epoch: 00 [12804/13514 ( 95%)], Train Loss: 0.47520\n","Epoch: 00 [12844/13514 ( 95%)], Train Loss: 0.47462\n","Epoch: 00 [12884/13514 ( 95%)], Train Loss: 0.47488\n","Epoch: 00 [12924/13514 ( 96%)], Train Loss: 0.47403\n","Epoch: 00 [12964/13514 ( 96%)], Train Loss: 0.47406\n","Epoch: 00 [13004/13514 ( 96%)], Train Loss: 0.47378\n","Epoch: 00 [13044/13514 ( 97%)], Train Loss: 0.47303\n","Epoch: 00 [13084/13514 ( 97%)], Train Loss: 0.47334\n","Epoch: 00 [13124/13514 ( 97%)], Train Loss: 0.47303\n","Epoch: 00 [13164/13514 ( 97%)], Train Loss: 0.47235\n","Epoch: 00 [13204/13514 ( 98%)], Train Loss: 0.47202\n","Epoch: 00 [13244/13514 ( 98%)], Train Loss: 0.47184\n","Epoch: 00 [13284/13514 ( 98%)], Train Loss: 0.47131\n","Epoch: 00 [13324/13514 ( 99%)], Train Loss: 0.47086\n","Epoch: 00 [13364/13514 ( 99%)], Train Loss: 0.47052\n","Epoch: 00 [13404/13514 ( 99%)], Train Loss: 0.47033\n","Epoch: 00 [13444/13514 ( 99%)], Train Loss: 0.47012\n","Epoch: 00 [13484/13514 (100%)], Train Loss: 0.46968\n","Epoch: 00 [13514/13514 (100%)], Train Loss: 0.46971\n","----Validation Results Summary----\n","Epoch: [0] Valid Loss: 0.60262\n","0 Epoch, Best epoch was updated! Valid Loss: 0.60262\n","Saving model checkpoint to chaii-qa-5-fold-xlmroberta-torch-fit/checkpoint-fold-2.\n","\n","Total Training Time: 1878.3860218524933secs, Average Training Time per Epoch: 1878.3860218524933secs.\n","Total Validation Time: 170.06197714805603secs, Average Validation Time per Epoch: 170.06197714805603secs.\n","\n","\n","--------------------------------------------------\n","FOLD: 3\n","--------------------------------------------------\n","Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n","Num examples Train= 14322, Num examples Valid=3236\n","Total Training Steps: 1791, Total Warmup Steps: 179\n","Epoch: 00 [    4/14322 (  0%)], Train Loss: 2.88127\n","Epoch: 00 [   44/14322 (  0%)], Train Loss: 2.88450\n","Epoch: 00 [   84/14322 (  1%)], Train Loss: 2.84687\n","Epoch: 00 [  124/14322 (  1%)], Train Loss: 2.80678\n","Epoch: 00 [  164/14322 (  1%)], Train Loss: 2.74134\n","Epoch: 00 [  204/14322 (  1%)], Train Loss: 2.66433\n","Epoch: 00 [  244/14322 (  2%)], Train Loss: 2.55922\n","Epoch: 00 [  284/14322 (  2%)], Train Loss: 2.44619\n","Epoch: 00 [  324/14322 (  2%)], Train Loss: 2.31728\n","Epoch: 00 [  364/14322 (  3%)], Train Loss: 2.15850\n","Epoch: 00 [  404/14322 (  3%)], Train Loss: 2.02014\n","Epoch: 00 [  444/14322 (  3%)], Train Loss: 1.88812\n","Epoch: 00 [  484/14322 (  3%)], Train Loss: 1.78723\n","Epoch: 00 [  524/14322 (  4%)], Train Loss: 1.69820\n","Epoch: 00 [  564/14322 (  4%)], Train Loss: 1.62612\n","Epoch: 00 [  604/14322 (  4%)], Train Loss: 1.56351\n","Epoch: 00 [  644/14322 (  4%)], Train Loss: 1.49347\n","Epoch: 00 [  684/14322 (  5%)], Train Loss: 1.43838\n","Epoch: 00 [  724/14322 (  5%)], Train Loss: 1.39332\n","Epoch: 00 [  764/14322 (  5%)], Train Loss: 1.34038\n","Epoch: 00 [  804/14322 (  6%)], Train Loss: 1.30549\n","Epoch: 00 [  844/14322 (  6%)], Train Loss: 1.26517\n","Epoch: 00 [  884/14322 (  6%)], Train Loss: 1.23290\n","Epoch: 00 [  924/14322 (  6%)], Train Loss: 1.20787\n","Epoch: 00 [  964/14322 (  7%)], Train Loss: 1.17728\n","Epoch: 00 [ 1004/14322 (  7%)], Train Loss: 1.15547\n","Epoch: 00 [ 1044/14322 (  7%)], Train Loss: 1.12842\n","Epoch: 00 [ 1084/14322 (  8%)], Train Loss: 1.10492\n","Epoch: 00 [ 1124/14322 (  8%)], Train Loss: 1.07813\n","Epoch: 00 [ 1164/14322 (  8%)], Train Loss: 1.05333\n","Epoch: 00 [ 1204/14322 (  8%)], Train Loss: 1.03544\n","Epoch: 00 [ 1244/14322 (  9%)], Train Loss: 1.01694\n","Epoch: 00 [ 1284/14322 (  9%)], Train Loss: 1.00345\n","Epoch: 00 [ 1324/14322 (  9%)], Train Loss: 0.98715\n","Epoch: 00 [ 1364/14322 ( 10%)], Train Loss: 0.97543\n","Epoch: 00 [ 1404/14322 ( 10%)], Train Loss: 0.96216\n","Epoch: 00 [ 1444/14322 ( 10%)], Train Loss: 0.94754\n","Epoch: 00 [ 1484/14322 ( 10%)], Train Loss: 0.93498\n","Epoch: 00 [ 1524/14322 ( 11%)], Train Loss: 0.92302\n","Epoch: 00 [ 1564/14322 ( 11%)], Train Loss: 0.91114\n","Epoch: 00 [ 1604/14322 ( 11%)], Train Loss: 0.90267\n","Epoch: 00 [ 1644/14322 ( 11%)], Train Loss: 0.88961\n","Epoch: 00 [ 1684/14322 ( 12%)], Train Loss: 0.87720\n","Epoch: 00 [ 1724/14322 ( 12%)], Train Loss: 0.86833\n","Epoch: 00 [ 1764/14322 ( 12%)], Train Loss: 0.85758\n","Epoch: 00 [ 1804/14322 ( 13%)], Train Loss: 0.84929\n","Epoch: 00 [ 1844/14322 ( 13%)], Train Loss: 0.83691\n","Epoch: 00 [ 1884/14322 ( 13%)], Train Loss: 0.83160\n","Epoch: 00 [ 1924/14322 ( 13%)], Train Loss: 0.82509\n","Epoch: 00 [ 1964/14322 ( 14%)], Train Loss: 0.81869\n","Epoch: 00 [ 2004/14322 ( 14%)], Train Loss: 0.81152\n","Epoch: 00 [ 2044/14322 ( 14%)], Train Loss: 0.80537\n","Epoch: 00 [ 2084/14322 ( 15%)], Train Loss: 0.80082\n","Epoch: 00 [ 2124/14322 ( 15%)], Train Loss: 0.79383\n","Epoch: 00 [ 2164/14322 ( 15%)], Train Loss: 0.78851\n","Epoch: 00 [ 2204/14322 ( 15%)], Train Loss: 0.78054\n","Epoch: 00 [ 2244/14322 ( 16%)], Train Loss: 0.77058\n","Epoch: 00 [ 2284/14322 ( 16%)], Train Loss: 0.75961\n","Epoch: 00 [ 2324/14322 ( 16%)], Train Loss: 0.75636\n","Epoch: 00 [ 2364/14322 ( 17%)], Train Loss: 0.75335\n","Epoch: 00 [ 2404/14322 ( 17%)], Train Loss: 0.74649\n","Epoch: 00 [ 2444/14322 ( 17%)], Train Loss: 0.74164\n","Epoch: 00 [ 2484/14322 ( 17%)], Train Loss: 0.73561\n","Epoch: 00 [ 2524/14322 ( 18%)], Train Loss: 0.73008\n","Epoch: 00 [ 2564/14322 ( 18%)], Train Loss: 0.72381\n","Epoch: 00 [ 2604/14322 ( 18%)], Train Loss: 0.71943\n","Epoch: 00 [ 2644/14322 ( 18%)], Train Loss: 0.71432\n","Epoch: 00 [ 2684/14322 ( 19%)], Train Loss: 0.71064\n","Epoch: 00 [ 2724/14322 ( 19%)], Train Loss: 0.70392\n","Epoch: 00 [ 2764/14322 ( 19%)], Train Loss: 0.70100\n","Epoch: 00 [ 2804/14322 ( 20%)], Train Loss: 0.69519\n","Epoch: 00 [ 2844/14322 ( 20%)], Train Loss: 0.69033\n","Epoch: 00 [ 2884/14322 ( 20%)], Train Loss: 0.68578\n","Epoch: 00 [ 2924/14322 ( 20%)], Train Loss: 0.68066\n","Epoch: 00 [ 2964/14322 ( 21%)], Train Loss: 0.67701\n","Epoch: 00 [ 3004/14322 ( 21%)], Train Loss: 0.67152\n","Epoch: 00 [ 3044/14322 ( 21%)], Train Loss: 0.66633\n","Epoch: 00 [ 3084/14322 ( 22%)], Train Loss: 0.66313\n","Epoch: 00 [ 3124/14322 ( 22%)], Train Loss: 0.65920\n","Epoch: 00 [ 3164/14322 ( 22%)], Train Loss: 0.65493\n","Epoch: 00 [ 3204/14322 ( 22%)], Train Loss: 0.65166\n","Epoch: 00 [ 3244/14322 ( 23%)], Train Loss: 0.64945\n","Epoch: 00 [ 3284/14322 ( 23%)], Train Loss: 0.64821\n","Epoch: 00 [ 3324/14322 ( 23%)], Train Loss: 0.64582\n","Epoch: 00 [ 3364/14322 ( 23%)], Train Loss: 0.64452\n","Epoch: 00 [ 3404/14322 ( 24%)], Train Loss: 0.64237\n","Epoch: 00 [ 3444/14322 ( 24%)], Train Loss: 0.63947\n","Epoch: 00 [ 3484/14322 ( 24%)], Train Loss: 0.63800\n","Epoch: 00 [ 3524/14322 ( 25%)], Train Loss: 0.63611\n","Epoch: 00 [ 3564/14322 ( 25%)], Train Loss: 0.63144\n","Epoch: 00 [ 3604/14322 ( 25%)], Train Loss: 0.62870\n","Epoch: 00 [ 3644/14322 ( 25%)], Train Loss: 0.62817\n","Epoch: 00 [ 3684/14322 ( 26%)], Train Loss: 0.62445\n","Epoch: 00 [ 3724/14322 ( 26%)], Train Loss: 0.62262\n","Epoch: 00 [ 3764/14322 ( 26%)], Train Loss: 0.61971\n","Epoch: 00 [ 3804/14322 ( 27%)], Train Loss: 0.61768\n","Epoch: 00 [ 3844/14322 ( 27%)], Train Loss: 0.61390\n","Epoch: 00 [ 3884/14322 ( 27%)], Train Loss: 0.61142\n","Epoch: 00 [ 3924/14322 ( 27%)], Train Loss: 0.60738\n","Epoch: 00 [ 3964/14322 ( 28%)], Train Loss: 0.60464\n","Epoch: 00 [ 4004/14322 ( 28%)], Train Loss: 0.60203\n","Epoch: 00 [ 4044/14322 ( 28%)], Train Loss: 0.59902\n","Epoch: 00 [ 4084/14322 ( 29%)], Train Loss: 0.59783\n","Epoch: 00 [ 4124/14322 ( 29%)], Train Loss: 0.59552\n","Epoch: 00 [ 4164/14322 ( 29%)], Train Loss: 0.59382\n","Epoch: 00 [ 4204/14322 ( 29%)], Train Loss: 0.59248\n","Epoch: 00 [ 4244/14322 ( 30%)], Train Loss: 0.58988\n","Epoch: 00 [ 4284/14322 ( 30%)], Train Loss: 0.58698\n","Epoch: 00 [ 4324/14322 ( 30%)], Train Loss: 0.58565\n","Epoch: 00 [ 4364/14322 ( 30%)], Train Loss: 0.58313\n","Epoch: 00 [ 4404/14322 ( 31%)], Train Loss: 0.58026\n","Epoch: 00 [ 4444/14322 ( 31%)], Train Loss: 0.57710\n","Epoch: 00 [ 4484/14322 ( 31%)], Train Loss: 0.57610\n","Epoch: 00 [ 4524/14322 ( 32%)], Train Loss: 0.57312\n","Epoch: 00 [ 4564/14322 ( 32%)], Train Loss: 0.57246\n","Epoch: 00 [ 4604/14322 ( 32%)], Train Loss: 0.57402\n","Epoch: 00 [ 4644/14322 ( 32%)], Train Loss: 0.57431\n","Epoch: 00 [ 4684/14322 ( 33%)], Train Loss: 0.57382\n","Epoch: 00 [ 4724/14322 ( 33%)], Train Loss: 0.57214\n","Epoch: 00 [ 4764/14322 ( 33%)], Train Loss: 0.57164\n","Epoch: 00 [ 4804/14322 ( 34%)], Train Loss: 0.56950\n","Epoch: 00 [ 4844/14322 ( 34%)], Train Loss: 0.56819\n","Epoch: 00 [ 4884/14322 ( 34%)], Train Loss: 0.56771\n","Epoch: 00 [ 4924/14322 ( 34%)], Train Loss: 0.56612\n","Epoch: 00 [ 4964/14322 ( 35%)], Train Loss: 0.56372\n","Epoch: 00 [ 5004/14322 ( 35%)], Train Loss: 0.56198\n","Epoch: 00 [ 5044/14322 ( 35%)], Train Loss: 0.55999\n","Epoch: 00 [ 5084/14322 ( 35%)], Train Loss: 0.55933\n","Epoch: 00 [ 5124/14322 ( 36%)], Train Loss: 0.55786\n","Epoch: 00 [ 5164/14322 ( 36%)], Train Loss: 0.55597\n","Epoch: 00 [ 5204/14322 ( 36%)], Train Loss: 0.55552\n","Epoch: 00 [ 5244/14322 ( 37%)], Train Loss: 0.55466\n","Epoch: 00 [ 5284/14322 ( 37%)], Train Loss: 0.55519\n","Epoch: 00 [ 5324/14322 ( 37%)], Train Loss: 0.55315\n","Epoch: 00 [ 5364/14322 ( 37%)], Train Loss: 0.55172\n","Epoch: 00 [ 5404/14322 ( 38%)], Train Loss: 0.55062\n","Epoch: 00 [ 5444/14322 ( 38%)], Train Loss: 0.54750\n","Epoch: 00 [ 5484/14322 ( 38%)], Train Loss: 0.54724\n","Epoch: 00 [ 5524/14322 ( 39%)], Train Loss: 0.54636\n","Epoch: 00 [ 5564/14322 ( 39%)], Train Loss: 0.54492\n","Epoch: 00 [ 5604/14322 ( 39%)], Train Loss: 0.54411\n","Epoch: 00 [ 5644/14322 ( 39%)], Train Loss: 0.54283\n","Epoch: 00 [ 5684/14322 ( 40%)], Train Loss: 0.54179\n","Epoch: 00 [ 5724/14322 ( 40%)], Train Loss: 0.54096\n","Epoch: 00 [ 5764/14322 ( 40%)], Train Loss: 0.54049\n","Epoch: 00 [ 5804/14322 ( 41%)], Train Loss: 0.53929\n","Epoch: 00 [ 5844/14322 ( 41%)], Train Loss: 0.53892\n","Epoch: 00 [ 5884/14322 ( 41%)], Train Loss: 0.53721\n","Epoch: 00 [ 5924/14322 ( 41%)], Train Loss: 0.53544\n","Epoch: 00 [ 5964/14322 ( 42%)], Train Loss: 0.53377\n","Epoch: 00 [ 6004/14322 ( 42%)], Train Loss: 0.53301\n","Epoch: 00 [ 6044/14322 ( 42%)], Train Loss: 0.53179\n","Epoch: 00 [ 6084/14322 ( 42%)], Train Loss: 0.53097\n","Epoch: 00 [ 6124/14322 ( 43%)], Train Loss: 0.53057\n","Epoch: 00 [ 6164/14322 ( 43%)], Train Loss: 0.52896\n","Epoch: 00 [ 6204/14322 ( 43%)], Train Loss: 0.52818\n","Epoch: 00 [ 6244/14322 ( 44%)], Train Loss: 0.52753\n","Epoch: 00 [ 6284/14322 ( 44%)], Train Loss: 0.52650\n","Epoch: 00 [ 6324/14322 ( 44%)], Train Loss: 0.52659\n","Epoch: 00 [ 6364/14322 ( 44%)], Train Loss: 0.52442\n","Epoch: 00 [ 6404/14322 ( 45%)], Train Loss: 0.52290\n","Epoch: 00 [ 6444/14322 ( 45%)], Train Loss: 0.52372\n","Epoch: 00 [ 6484/14322 ( 45%)], Train Loss: 0.52225\n","Epoch: 00 [ 6524/14322 ( 46%)], Train Loss: 0.52121\n","Epoch: 00 [ 6564/14322 ( 46%)], Train Loss: 0.52033\n","Epoch: 00 [ 6604/14322 ( 46%)], Train Loss: 0.51838\n","Epoch: 00 [ 6644/14322 ( 46%)], Train Loss: 0.51771\n","Epoch: 00 [ 6684/14322 ( 47%)], Train Loss: 0.51743\n","Epoch: 00 [ 6724/14322 ( 47%)], Train Loss: 0.51657\n","Epoch: 00 [ 6764/14322 ( 47%)], Train Loss: 0.51718\n","Epoch: 00 [ 6804/14322 ( 48%)], Train Loss: 0.51627\n","Epoch: 00 [ 6844/14322 ( 48%)], Train Loss: 0.51509\n","Epoch: 00 [ 6884/14322 ( 48%)], Train Loss: 0.51412\n","Epoch: 00 [ 6924/14322 ( 48%)], Train Loss: 0.51433\n","Epoch: 00 [ 6964/14322 ( 49%)], Train Loss: 0.51293\n","Epoch: 00 [ 7004/14322 ( 49%)], Train Loss: 0.51168\n","Epoch: 00 [ 7044/14322 ( 49%)], Train Loss: 0.51067\n","Epoch: 00 [ 7084/14322 ( 49%)], Train Loss: 0.50945\n","Epoch: 00 [ 7124/14322 ( 50%)], Train Loss: 0.50904\n","Epoch: 00 [ 7164/14322 ( 50%)], Train Loss: 0.50759\n","Epoch: 00 [ 7204/14322 ( 50%)], Train Loss: 0.50675\n","Epoch: 00 [ 7244/14322 ( 51%)], Train Loss: 0.50658\n","Epoch: 00 [ 7284/14322 ( 51%)], Train Loss: 0.50498\n","Epoch: 00 [ 7324/14322 ( 51%)], Train Loss: 0.50529\n","Epoch: 00 [ 7364/14322 ( 51%)], Train Loss: 0.50597\n","Epoch: 00 [ 7404/14322 ( 52%)], Train Loss: 0.50627\n","Epoch: 00 [ 7444/14322 ( 52%)], Train Loss: 0.50552\n","Epoch: 00 [ 7484/14322 ( 52%)], Train Loss: 0.50449\n","Epoch: 00 [ 7524/14322 ( 53%)], Train Loss: 0.50368\n","Epoch: 00 [ 7564/14322 ( 53%)], Train Loss: 0.50364\n","Epoch: 00 [ 7604/14322 ( 53%)], Train Loss: 0.50390\n","Epoch: 00 [ 7644/14322 ( 53%)], Train Loss: 0.50245\n","Epoch: 00 [ 7684/14322 ( 54%)], Train Loss: 0.50186\n","Epoch: 00 [ 7724/14322 ( 54%)], Train Loss: 0.50224\n","Epoch: 00 [ 7764/14322 ( 54%)], Train Loss: 0.50184\n","Epoch: 00 [ 7804/14322 ( 54%)], Train Loss: 0.50090\n","Epoch: 00 [ 7844/14322 ( 55%)], Train Loss: 0.50023\n","Epoch: 00 [ 7884/14322 ( 55%)], Train Loss: 0.50071\n","Epoch: 00 [ 7924/14322 ( 55%)], Train Loss: 0.50050\n","Epoch: 00 [ 7964/14322 ( 56%)], Train Loss: 0.49957\n","Epoch: 00 [ 8004/14322 ( 56%)], Train Loss: 0.49827\n","Epoch: 00 [ 8044/14322 ( 56%)], Train Loss: 0.49838\n","Epoch: 00 [ 8084/14322 ( 56%)], Train Loss: 0.49773\n","Epoch: 00 [ 8124/14322 ( 57%)], Train Loss: 0.49685\n","Epoch: 00 [ 8164/14322 ( 57%)], Train Loss: 0.49703\n","Epoch: 00 [ 8204/14322 ( 57%)], Train Loss: 0.49682\n","Epoch: 00 [ 8244/14322 ( 58%)], Train Loss: 0.49704\n","Epoch: 00 [ 8284/14322 ( 58%)], Train Loss: 0.49606\n","Epoch: 00 [ 8324/14322 ( 58%)], Train Loss: 0.49638\n","Epoch: 00 [ 8364/14322 ( 58%)], Train Loss: 0.49552\n","Epoch: 00 [ 8404/14322 ( 59%)], Train Loss: 0.49540\n","Epoch: 00 [ 8444/14322 ( 59%)], Train Loss: 0.49560\n","Epoch: 00 [ 8484/14322 ( 59%)], Train Loss: 0.49498\n","Epoch: 00 [ 8524/14322 ( 60%)], Train Loss: 0.49476\n","Epoch: 00 [ 8564/14322 ( 60%)], Train Loss: 0.49398\n","Epoch: 00 [ 8604/14322 ( 60%)], Train Loss: 0.49356\n","Epoch: 00 [ 8644/14322 ( 60%)], Train Loss: 0.49304\n","Epoch: 00 [ 8684/14322 ( 61%)], Train Loss: 0.49157\n","Epoch: 00 [ 8724/14322 ( 61%)], Train Loss: 0.49175\n","Epoch: 00 [ 8764/14322 ( 61%)], Train Loss: 0.49170\n","Epoch: 00 [ 8804/14322 ( 61%)], Train Loss: 0.49059\n","Epoch: 00 [ 8844/14322 ( 62%)], Train Loss: 0.49023\n","Epoch: 00 [ 8884/14322 ( 62%)], Train Loss: 0.49014\n","Epoch: 00 [ 8924/14322 ( 62%)], Train Loss: 0.48978\n","Epoch: 00 [ 8964/14322 ( 63%)], Train Loss: 0.48937\n","Epoch: 00 [ 9004/14322 ( 63%)], Train Loss: 0.48956\n","Epoch: 00 [ 9044/14322 ( 63%)], Train Loss: 0.48931\n","Epoch: 00 [ 9084/14322 ( 63%)], Train Loss: 0.48821\n","Epoch: 00 [ 9124/14322 ( 64%)], Train Loss: 0.48738\n","Epoch: 00 [ 9164/14322 ( 64%)], Train Loss: 0.48739\n","Epoch: 00 [ 9204/14322 ( 64%)], Train Loss: 0.48708\n","Epoch: 00 [ 9244/14322 ( 65%)], Train Loss: 0.48606\n","Epoch: 00 [ 9284/14322 ( 65%)], Train Loss: 0.48539\n","Epoch: 00 [ 9324/14322 ( 65%)], Train Loss: 0.48498\n","Epoch: 00 [ 9364/14322 ( 65%)], Train Loss: 0.48545\n","Epoch: 00 [ 9404/14322 ( 66%)], Train Loss: 0.48556\n","Epoch: 00 [ 9444/14322 ( 66%)], Train Loss: 0.48549\n","Epoch: 00 [ 9484/14322 ( 66%)], Train Loss: 0.48526\n","Epoch: 00 [ 9524/14322 ( 66%)], Train Loss: 0.48500\n","Epoch: 00 [ 9564/14322 ( 67%)], Train Loss: 0.48444\n","Epoch: 00 [ 9604/14322 ( 67%)], Train Loss: 0.48513\n","Epoch: 00 [ 9644/14322 ( 67%)], Train Loss: 0.48547\n","Epoch: 00 [ 9684/14322 ( 68%)], Train Loss: 0.48507\n","Epoch: 00 [ 9724/14322 ( 68%)], Train Loss: 0.48496\n","Epoch: 00 [ 9764/14322 ( 68%)], Train Loss: 0.48528\n","Epoch: 00 [ 9804/14322 ( 68%)], Train Loss: 0.48494\n","Epoch: 00 [ 9844/14322 ( 69%)], Train Loss: 0.48413\n","Epoch: 00 [ 9884/14322 ( 69%)], Train Loss: 0.48323\n","Epoch: 00 [ 9924/14322 ( 69%)], Train Loss: 0.48340\n","Epoch: 00 [ 9964/14322 ( 70%)], Train Loss: 0.48300\n","Epoch: 00 [10004/14322 ( 70%)], Train Loss: 0.48225\n","Epoch: 00 [10044/14322 ( 70%)], Train Loss: 0.48115\n","Epoch: 00 [10084/14322 ( 70%)], Train Loss: 0.48026\n","Epoch: 00 [10124/14322 ( 71%)], Train Loss: 0.47984\n","Epoch: 00 [10164/14322 ( 71%)], Train Loss: 0.47949\n","Epoch: 00 [10204/14322 ( 71%)], Train Loss: 0.47927\n","Epoch: 00 [10244/14322 ( 72%)], Train Loss: 0.47890\n","Epoch: 00 [10284/14322 ( 72%)], Train Loss: 0.47869\n","Epoch: 00 [10324/14322 ( 72%)], Train Loss: 0.47832\n","Epoch: 00 [10364/14322 ( 72%)], Train Loss: 0.47729\n","Epoch: 00 [10404/14322 ( 73%)], Train Loss: 0.47656\n","Epoch: 00 [10444/14322 ( 73%)], Train Loss: 0.47649\n","Epoch: 00 [10484/14322 ( 73%)], Train Loss: 0.47560\n","Epoch: 00 [10524/14322 ( 73%)], Train Loss: 0.47516\n","Epoch: 00 [10564/14322 ( 74%)], Train Loss: 0.47413\n","Epoch: 00 [10604/14322 ( 74%)], Train Loss: 0.47397\n","Epoch: 00 [10644/14322 ( 74%)], Train Loss: 0.47339\n","Epoch: 00 [10684/14322 ( 75%)], Train Loss: 0.47306\n","Epoch: 00 [10724/14322 ( 75%)], Train Loss: 0.47252\n","Epoch: 00 [10764/14322 ( 75%)], Train Loss: 0.47229\n","Epoch: 00 [10804/14322 ( 75%)], Train Loss: 0.47245\n","Epoch: 00 [10844/14322 ( 76%)], Train Loss: 0.47145\n","Epoch: 00 [10884/14322 ( 76%)], Train Loss: 0.47130\n","Epoch: 00 [10924/14322 ( 76%)], Train Loss: 0.47163\n","Epoch: 00 [10964/14322 ( 77%)], Train Loss: 0.47108\n","Epoch: 00 [11004/14322 ( 77%)], Train Loss: 0.47039\n","Epoch: 00 [11044/14322 ( 77%)], Train Loss: 0.46994\n","Epoch: 00 [11084/14322 ( 77%)], Train Loss: 0.46971\n","Epoch: 00 [11124/14322 ( 78%)], Train Loss: 0.46933\n","Epoch: 00 [11164/14322 ( 78%)], Train Loss: 0.46913\n","Epoch: 00 [11204/14322 ( 78%)], Train Loss: 0.46853\n","Epoch: 00 [11244/14322 ( 79%)], Train Loss: 0.46761\n","Epoch: 00 [11284/14322 ( 79%)], Train Loss: 0.46744\n","Epoch: 00 [11324/14322 ( 79%)], Train Loss: 0.46670\n","Epoch: 00 [11364/14322 ( 79%)], Train Loss: 0.46562\n","Epoch: 00 [11404/14322 ( 80%)], Train Loss: 0.46537\n","Epoch: 00 [11444/14322 ( 80%)], Train Loss: 0.46502\n","Epoch: 00 [11484/14322 ( 80%)], Train Loss: 0.46398\n","Epoch: 00 [11524/14322 ( 80%)], Train Loss: 0.46331\n","Epoch: 00 [11564/14322 ( 81%)], Train Loss: 0.46295\n","Epoch: 00 [11604/14322 ( 81%)], Train Loss: 0.46225\n","Epoch: 00 [11644/14322 ( 81%)], Train Loss: 0.46278\n","Epoch: 00 [11684/14322 ( 82%)], Train Loss: 0.46256\n","Epoch: 00 [11724/14322 ( 82%)], Train Loss: 0.46217\n","Epoch: 00 [11764/14322 ( 82%)], Train Loss: 0.46170\n","Epoch: 00 [11804/14322 ( 82%)], Train Loss: 0.46105\n","Epoch: 00 [11844/14322 ( 83%)], Train Loss: 0.46095\n","Epoch: 00 [11884/14322 ( 83%)], Train Loss: 0.46025\n","Epoch: 00 [11924/14322 ( 83%)], Train Loss: 0.46013\n","Epoch: 00 [11964/14322 ( 84%)], Train Loss: 0.45991\n","Epoch: 00 [12004/14322 ( 84%)], Train Loss: 0.46015\n","Epoch: 00 [12044/14322 ( 84%)], Train Loss: 0.46031\n","Epoch: 00 [12084/14322 ( 84%)], Train Loss: 0.46012\n","Epoch: 00 [12124/14322 ( 85%)], Train Loss: 0.45973\n","Epoch: 00 [12164/14322 ( 85%)], Train Loss: 0.45968\n","Epoch: 00 [12204/14322 ( 85%)], Train Loss: 0.45920\n","Epoch: 00 [12244/14322 ( 85%)], Train Loss: 0.45883\n","Epoch: 00 [12284/14322 ( 86%)], Train Loss: 0.45835\n","Epoch: 00 [12324/14322 ( 86%)], Train Loss: 0.45820\n","Epoch: 00 [12364/14322 ( 86%)], Train Loss: 0.45764\n","Epoch: 00 [12404/14322 ( 87%)], Train Loss: 0.45775\n","Epoch: 00 [12444/14322 ( 87%)], Train Loss: 0.45755\n","Epoch: 00 [12484/14322 ( 87%)], Train Loss: 0.45718\n","Epoch: 00 [12524/14322 ( 87%)], Train Loss: 0.45663\n","Epoch: 00 [12564/14322 ( 88%)], Train Loss: 0.45618\n","Epoch: 00 [12604/14322 ( 88%)], Train Loss: 0.45545\n","Epoch: 00 [12644/14322 ( 88%)], Train Loss: 0.45487\n","Epoch: 00 [12684/14322 ( 89%)], Train Loss: 0.45479\n","Epoch: 00 [12724/14322 ( 89%)], Train Loss: 0.45498\n","Epoch: 00 [12764/14322 ( 89%)], Train Loss: 0.45467\n","Epoch: 00 [12804/14322 ( 89%)], Train Loss: 0.45431\n","Epoch: 00 [12844/14322 ( 90%)], Train Loss: 0.45388\n","Epoch: 00 [12884/14322 ( 90%)], Train Loss: 0.45346\n","Epoch: 00 [12924/14322 ( 90%)], Train Loss: 0.45270\n","Epoch: 00 [12964/14322 ( 91%)], Train Loss: 0.45298\n","Epoch: 00 [13004/14322 ( 91%)], Train Loss: 0.45272\n","Epoch: 00 [13044/14322 ( 91%)], Train Loss: 0.45205\n","Epoch: 00 [13084/14322 ( 91%)], Train Loss: 0.45155\n","Epoch: 00 [13124/14322 ( 92%)], Train Loss: 0.45139\n","Epoch: 00 [13164/14322 ( 92%)], Train Loss: 0.45181\n","Epoch: 00 [13204/14322 ( 92%)], Train Loss: 0.45113\n","Epoch: 00 [13244/14322 ( 92%)], Train Loss: 0.45051\n","Epoch: 00 [13284/14322 ( 93%)], Train Loss: 0.44964\n","Epoch: 00 [13324/14322 ( 93%)], Train Loss: 0.44929\n","Epoch: 00 [13364/14322 ( 93%)], Train Loss: 0.44908\n","Epoch: 00 [13404/14322 ( 94%)], Train Loss: 0.44857\n","Epoch: 00 [13444/14322 ( 94%)], Train Loss: 0.44831\n","Epoch: 00 [13484/14322 ( 94%)], Train Loss: 0.44867\n","Epoch: 00 [13524/14322 ( 94%)], Train Loss: 0.44832\n","Epoch: 00 [13564/14322 ( 95%)], Train Loss: 0.44808\n","Epoch: 00 [13604/14322 ( 95%)], Train Loss: 0.44807\n","Epoch: 00 [13644/14322 ( 95%)], Train Loss: 0.44833\n","Epoch: 00 [13684/14322 ( 96%)], Train Loss: 0.44810\n","Epoch: 00 [13724/14322 ( 96%)], Train Loss: 0.44827\n","Epoch: 00 [13764/14322 ( 96%)], Train Loss: 0.44740\n","Epoch: 00 [13804/14322 ( 96%)], Train Loss: 0.44701\n","Epoch: 00 [13844/14322 ( 97%)], Train Loss: 0.44727\n","Epoch: 00 [13884/14322 ( 97%)], Train Loss: 0.44741\n","Epoch: 00 [13924/14322 ( 97%)], Train Loss: 0.44742\n","Epoch: 00 [13964/14322 ( 98%)], Train Loss: 0.44689\n","Epoch: 00 [14004/14322 ( 98%)], Train Loss: 0.44611\n","Epoch: 00 [14044/14322 ( 98%)], Train Loss: 0.44575\n","Epoch: 00 [14084/14322 ( 98%)], Train Loss: 0.44515\n","Epoch: 00 [14124/14322 ( 99%)], Train Loss: 0.44462\n","Epoch: 00 [14164/14322 ( 99%)], Train Loss: 0.44400\n","Epoch: 00 [14204/14322 ( 99%)], Train Loss: 0.44385\n","Epoch: 00 [14244/14322 ( 99%)], Train Loss: 0.44351\n","Epoch: 00 [14284/14322 (100%)], Train Loss: 0.44306\n","Epoch: 00 [14322/14322 (100%)], Train Loss: 0.44265\n","----Validation Results Summary----\n","Epoch: [0] Valid Loss: 0.75112\n","0 Epoch, Best epoch was updated! Valid Loss: 0.75112\n","Saving model checkpoint to chaii-qa-5-fold-xlmroberta-torch-fit/checkpoint-fold-3.\n","\n","Total Training Time: 1990.8134412765503secs, Average Training Time per Epoch: 1990.8134412765503secs.\n","Total Validation Time: 136.2303590774536secs, Average Validation Time per Epoch: 136.2303590774536secs.\n","\n","\n","--------------------------------------------------\n","FOLD: 4\n","--------------------------------------------------\n","Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n","Num examples Train= 14178, Num examples Valid=3380\n","Total Training Steps: 1773, Total Warmup Steps: 177\n","Epoch: 00 [    4/14178 (  0%)], Train Loss: 2.83606\n","Epoch: 00 [   44/14178 (  0%)], Train Loss: 2.83053\n","Epoch: 00 [   84/14178 (  1%)], Train Loss: 2.83716\n","Epoch: 00 [  124/14178 (  1%)], Train Loss: 2.78941\n","Epoch: 00 [  164/14178 (  1%)], Train Loss: 2.73446\n","Epoch: 00 [  204/14178 (  1%)], Train Loss: 2.67155\n","Epoch: 00 [  244/14178 (  2%)], Train Loss: 2.58588\n","Epoch: 00 [  284/14178 (  2%)], Train Loss: 2.46482\n","Epoch: 00 [  324/14178 (  2%)], Train Loss: 2.34665\n","Epoch: 00 [  364/14178 (  3%)], Train Loss: 2.19179\n","Epoch: 00 [  404/14178 (  3%)], Train Loss: 2.05446\n","Epoch: 00 [  444/14178 (  3%)], Train Loss: 1.92657\n","Epoch: 00 [  484/14178 (  3%)], Train Loss: 1.82661\n","Epoch: 00 [  524/14178 (  4%)], Train Loss: 1.74047\n","Epoch: 00 [  564/14178 (  4%)], Train Loss: 1.65479\n","Epoch: 00 [  604/14178 (  4%)], Train Loss: 1.58695\n","Epoch: 00 [  644/14178 (  5%)], Train Loss: 1.51163\n","Epoch: 00 [  684/14178 (  5%)], Train Loss: 1.43639\n","Epoch: 00 [  724/14178 (  5%)], Train Loss: 1.38739\n","Epoch: 00 [  764/14178 (  5%)], Train Loss: 1.33604\n","Epoch: 00 [  804/14178 (  6%)], Train Loss: 1.29232\n","Epoch: 00 [  844/14178 (  6%)], Train Loss: 1.25543\n","Epoch: 00 [  884/14178 (  6%)], Train Loss: 1.22166\n","Epoch: 00 [  924/14178 (  7%)], Train Loss: 1.19194\n","Epoch: 00 [  964/14178 (  7%)], Train Loss: 1.16753\n","Epoch: 00 [ 1004/14178 (  7%)], Train Loss: 1.13535\n","Epoch: 00 [ 1044/14178 (  7%)], Train Loss: 1.11277\n","Epoch: 00 [ 1084/14178 (  8%)], Train Loss: 1.09074\n","Epoch: 00 [ 1124/14178 (  8%)], Train Loss: 1.06728\n","Epoch: 00 [ 1164/14178 (  8%)], Train Loss: 1.04648\n","Epoch: 00 [ 1204/14178 (  8%)], Train Loss: 1.02827\n","Epoch: 00 [ 1244/14178 (  9%)], Train Loss: 1.00372\n","Epoch: 00 [ 1284/14178 (  9%)], Train Loss: 1.00260\n","Epoch: 00 [ 1324/14178 (  9%)], Train Loss: 0.98419\n","Epoch: 00 [ 1364/14178 ( 10%)], Train Loss: 0.97183\n","Epoch: 00 [ 1404/14178 ( 10%)], Train Loss: 0.95320\n","Epoch: 00 [ 1444/14178 ( 10%)], Train Loss: 0.94594\n","Epoch: 00 [ 1484/14178 ( 10%)], Train Loss: 0.93048\n","Epoch: 00 [ 1524/14178 ( 11%)], Train Loss: 0.91892\n","Epoch: 00 [ 1564/14178 ( 11%)], Train Loss: 0.90493\n","Epoch: 00 [ 1604/14178 ( 11%)], Train Loss: 0.88952\n","Epoch: 00 [ 1644/14178 ( 12%)], Train Loss: 0.88725\n","Epoch: 00 [ 1684/14178 ( 12%)], Train Loss: 0.87800\n","Epoch: 00 [ 1724/14178 ( 12%)], Train Loss: 0.87106\n","Epoch: 00 [ 1764/14178 ( 12%)], Train Loss: 0.86162\n","Epoch: 00 [ 1804/14178 ( 13%)], Train Loss: 0.85071\n","Epoch: 00 [ 1844/14178 ( 13%)], Train Loss: 0.84463\n","Epoch: 00 [ 1884/14178 ( 13%)], Train Loss: 0.83511\n","Epoch: 00 [ 1924/14178 ( 14%)], Train Loss: 0.82968\n","Epoch: 00 [ 1964/14178 ( 14%)], Train Loss: 0.82161\n","Epoch: 00 [ 2004/14178 ( 14%)], Train Loss: 0.81496\n","Epoch: 00 [ 2044/14178 ( 14%)], Train Loss: 0.80389\n","Epoch: 00 [ 2084/14178 ( 15%)], Train Loss: 0.79844\n","Epoch: 00 [ 2124/14178 ( 15%)], Train Loss: 0.79108\n","Epoch: 00 [ 2164/14178 ( 15%)], Train Loss: 0.78533\n","Epoch: 00 [ 2204/14178 ( 16%)], Train Loss: 0.78013\n","Epoch: 00 [ 2244/14178 ( 16%)], Train Loss: 0.77113\n","Epoch: 00 [ 2284/14178 ( 16%)], Train Loss: 0.76305\n","Epoch: 00 [ 2324/14178 ( 16%)], Train Loss: 0.75820\n","Epoch: 00 [ 2364/14178 ( 17%)], Train Loss: 0.75025\n","Epoch: 00 [ 2404/14178 ( 17%)], Train Loss: 0.74282\n","Epoch: 00 [ 2444/14178 ( 17%)], Train Loss: 0.73683\n","Epoch: 00 [ 2484/14178 ( 18%)], Train Loss: 0.73016\n","Epoch: 00 [ 2524/14178 ( 18%)], Train Loss: 0.72981\n","Epoch: 00 [ 2564/14178 ( 18%)], Train Loss: 0.72261\n","Epoch: 00 [ 2604/14178 ( 18%)], Train Loss: 0.71625\n","Epoch: 00 [ 2644/14178 ( 19%)], Train Loss: 0.71193\n","Epoch: 00 [ 2684/14178 ( 19%)], Train Loss: 0.70788\n","Epoch: 00 [ 2724/14178 ( 19%)], Train Loss: 0.70614\n","Epoch: 00 [ 2764/14178 ( 19%)], Train Loss: 0.70321\n","Epoch: 00 [ 2804/14178 ( 20%)], Train Loss: 0.69961\n","Epoch: 00 [ 2844/14178 ( 20%)], Train Loss: 0.69403\n","Epoch: 00 [ 2884/14178 ( 20%)], Train Loss: 0.69307\n","Epoch: 00 [ 2924/14178 ( 21%)], Train Loss: 0.68891\n","Epoch: 00 [ 2964/14178 ( 21%)], Train Loss: 0.68490\n","Epoch: 00 [ 3004/14178 ( 21%)], Train Loss: 0.67949\n","Epoch: 00 [ 3044/14178 ( 21%)], Train Loss: 0.67763\n","Epoch: 00 [ 3084/14178 ( 22%)], Train Loss: 0.67374\n","Epoch: 00 [ 3124/14178 ( 22%)], Train Loss: 0.67103\n","Epoch: 00 [ 3164/14178 ( 22%)], Train Loss: 0.66878\n","Epoch: 00 [ 3204/14178 ( 23%)], Train Loss: 0.66243\n","Epoch: 00 [ 3244/14178 ( 23%)], Train Loss: 0.65791\n","Epoch: 00 [ 3284/14178 ( 23%)], Train Loss: 0.65356\n","Epoch: 00 [ 3324/14178 ( 23%)], Train Loss: 0.65214\n","Epoch: 00 [ 3364/14178 ( 24%)], Train Loss: 0.64791\n","Epoch: 00 [ 3404/14178 ( 24%)], Train Loss: 0.64454\n","Epoch: 00 [ 3444/14178 ( 24%)], Train Loss: 0.64336\n","Epoch: 00 [ 3484/14178 ( 25%)], Train Loss: 0.64059\n","Epoch: 00 [ 3524/14178 ( 25%)], Train Loss: 0.64163\n","Epoch: 00 [ 3564/14178 ( 25%)], Train Loss: 0.63893\n","Epoch: 00 [ 3604/14178 ( 25%)], Train Loss: 0.63538\n","Epoch: 00 [ 3644/14178 ( 26%)], Train Loss: 0.63336\n","Epoch: 00 [ 3684/14178 ( 26%)], Train Loss: 0.63150\n","Epoch: 00 [ 3724/14178 ( 26%)], Train Loss: 0.62909\n","Epoch: 00 [ 3764/14178 ( 27%)], Train Loss: 0.62651\n","Epoch: 00 [ 3804/14178 ( 27%)], Train Loss: 0.62516\n","Epoch: 00 [ 3844/14178 ( 27%)], Train Loss: 0.62257\n","Epoch: 00 [ 3884/14178 ( 27%)], Train Loss: 0.62117\n","Epoch: 00 [ 3924/14178 ( 28%)], Train Loss: 0.61995\n","Epoch: 00 [ 3964/14178 ( 28%)], Train Loss: 0.61633\n","Epoch: 00 [ 4004/14178 ( 28%)], Train Loss: 0.61319\n","Epoch: 00 [ 4044/14178 ( 29%)], Train Loss: 0.60989\n","Epoch: 00 [ 4084/14178 ( 29%)], Train Loss: 0.60613\n","Epoch: 00 [ 4124/14178 ( 29%)], Train Loss: 0.60404\n","Epoch: 00 [ 4164/14178 ( 29%)], Train Loss: 0.60131\n","Epoch: 00 [ 4204/14178 ( 30%)], Train Loss: 0.59899\n","Epoch: 00 [ 4244/14178 ( 30%)], Train Loss: 0.59717\n","Epoch: 00 [ 4284/14178 ( 30%)], Train Loss: 0.59452\n","Epoch: 00 [ 4324/14178 ( 30%)], Train Loss: 0.59192\n","Epoch: 00 [ 4364/14178 ( 31%)], Train Loss: 0.59115\n","Epoch: 00 [ 4404/14178 ( 31%)], Train Loss: 0.59025\n","Epoch: 00 [ 4444/14178 ( 31%)], Train Loss: 0.59031\n","Epoch: 00 [ 4484/14178 ( 32%)], Train Loss: 0.58798\n","Epoch: 00 [ 4524/14178 ( 32%)], Train Loss: 0.58593\n","Epoch: 00 [ 4564/14178 ( 32%)], Train Loss: 0.58561\n","Epoch: 00 [ 4604/14178 ( 32%)], Train Loss: 0.58495\n","Epoch: 00 [ 4644/14178 ( 33%)], Train Loss: 0.58435\n","Epoch: 00 [ 4684/14178 ( 33%)], Train Loss: 0.58254\n","Epoch: 00 [ 4724/14178 ( 33%)], Train Loss: 0.58137\n","Epoch: 00 [ 4764/14178 ( 34%)], Train Loss: 0.57911\n","Epoch: 00 [ 4804/14178 ( 34%)], Train Loss: 0.57662\n","Epoch: 00 [ 4844/14178 ( 34%)], Train Loss: 0.57515\n","Epoch: 00 [ 4884/14178 ( 34%)], Train Loss: 0.57424\n","Epoch: 00 [ 4924/14178 ( 35%)], Train Loss: 0.57323\n","Epoch: 00 [ 4964/14178 ( 35%)], Train Loss: 0.57118\n","Epoch: 00 [ 5004/14178 ( 35%)], Train Loss: 0.57132\n","Epoch: 00 [ 5044/14178 ( 36%)], Train Loss: 0.56941\n","Epoch: 00 [ 5084/14178 ( 36%)], Train Loss: 0.56980\n","Epoch: 00 [ 5124/14178 ( 36%)], Train Loss: 0.56832\n","Epoch: 00 [ 5164/14178 ( 36%)], Train Loss: 0.56733\n","Epoch: 00 [ 5204/14178 ( 37%)], Train Loss: 0.56593\n","Epoch: 00 [ 5244/14178 ( 37%)], Train Loss: 0.56533\n","Epoch: 00 [ 5284/14178 ( 37%)], Train Loss: 0.56527\n","Epoch: 00 [ 5324/14178 ( 38%)], Train Loss: 0.56500\n","Epoch: 00 [ 5364/14178 ( 38%)], Train Loss: 0.56271\n","Epoch: 00 [ 5404/14178 ( 38%)], Train Loss: 0.56149\n","Epoch: 00 [ 5444/14178 ( 38%)], Train Loss: 0.56159\n","Epoch: 00 [ 5484/14178 ( 39%)], Train Loss: 0.55984\n","Epoch: 00 [ 5524/14178 ( 39%)], Train Loss: 0.55821\n","Epoch: 00 [ 5564/14178 ( 39%)], Train Loss: 0.55804\n","Epoch: 00 [ 5604/14178 ( 40%)], Train Loss: 0.55777\n","Epoch: 00 [ 5644/14178 ( 40%)], Train Loss: 0.55605\n","Epoch: 00 [ 5684/14178 ( 40%)], Train Loss: 0.55388\n","Epoch: 00 [ 5724/14178 ( 40%)], Train Loss: 0.55259\n","Epoch: 00 [ 5764/14178 ( 41%)], Train Loss: 0.55101\n","Epoch: 00 [ 5804/14178 ( 41%)], Train Loss: 0.55024\n","Epoch: 00 [ 5844/14178 ( 41%)], Train Loss: 0.54892\n","Epoch: 00 [ 5884/14178 ( 42%)], Train Loss: 0.54741\n","Epoch: 00 [ 5924/14178 ( 42%)], Train Loss: 0.54702\n","Epoch: 00 [ 5964/14178 ( 42%)], Train Loss: 0.54721\n","Epoch: 00 [ 6004/14178 ( 42%)], Train Loss: 0.54600\n","Epoch: 00 [ 6044/14178 ( 43%)], Train Loss: 0.54439\n","Epoch: 00 [ 6084/14178 ( 43%)], Train Loss: 0.54323\n","Epoch: 00 [ 6124/14178 ( 43%)], Train Loss: 0.54117\n","Epoch: 00 [ 6164/14178 ( 43%)], Train Loss: 0.54013\n","Epoch: 00 [ 6204/14178 ( 44%)], Train Loss: 0.53930\n","Epoch: 00 [ 6244/14178 ( 44%)], Train Loss: 0.53879\n","Epoch: 00 [ 6284/14178 ( 44%)], Train Loss: 0.53760\n","Epoch: 00 [ 6324/14178 ( 45%)], Train Loss: 0.53766\n","Epoch: 00 [ 6364/14178 ( 45%)], Train Loss: 0.53623\n","Epoch: 00 [ 6404/14178 ( 45%)], Train Loss: 0.53619\n","Epoch: 00 [ 6444/14178 ( 45%)], Train Loss: 0.53495\n","Epoch: 00 [ 6484/14178 ( 46%)], Train Loss: 0.53388\n","Epoch: 00 [ 6524/14178 ( 46%)], Train Loss: 0.53312\n","Epoch: 00 [ 6564/14178 ( 46%)], Train Loss: 0.53110\n","Epoch: 00 [ 6604/14178 ( 47%)], Train Loss: 0.52987\n","Epoch: 00 [ 6644/14178 ( 47%)], Train Loss: 0.52966\n","Epoch: 00 [ 6684/14178 ( 47%)], Train Loss: 0.52900\n","Epoch: 00 [ 6724/14178 ( 47%)], Train Loss: 0.52822\n","Epoch: 00 [ 6764/14178 ( 48%)], Train Loss: 0.52723\n","Epoch: 00 [ 6804/14178 ( 48%)], Train Loss: 0.52552\n","Epoch: 00 [ 6844/14178 ( 48%)], Train Loss: 0.52474\n","Epoch: 00 [ 6884/14178 ( 49%)], Train Loss: 0.52399\n","Epoch: 00 [ 6924/14178 ( 49%)], Train Loss: 0.52322\n","Epoch: 00 [ 6964/14178 ( 49%)], Train Loss: 0.52340\n","Epoch: 00 [ 7004/14178 ( 49%)], Train Loss: 0.52160\n","Epoch: 00 [ 7044/14178 ( 50%)], Train Loss: 0.52127\n","Epoch: 00 [ 7084/14178 ( 50%)], Train Loss: 0.52022\n","Epoch: 00 [ 7124/14178 ( 50%)], Train Loss: 0.51950\n","Epoch: 00 [ 7164/14178 ( 51%)], Train Loss: 0.52045\n","Epoch: 00 [ 7204/14178 ( 51%)], Train Loss: 0.51987\n","Epoch: 00 [ 7244/14178 ( 51%)], Train Loss: 0.51836\n","Epoch: 00 [ 7284/14178 ( 51%)], Train Loss: 0.51769\n","Epoch: 00 [ 7324/14178 ( 52%)], Train Loss: 0.51698\n","Epoch: 00 [ 7364/14178 ( 52%)], Train Loss: 0.51604\n","Epoch: 00 [ 7404/14178 ( 52%)], Train Loss: 0.51669\n","Epoch: 00 [ 7444/14178 ( 53%)], Train Loss: 0.51567\n","Epoch: 00 [ 7484/14178 ( 53%)], Train Loss: 0.51408\n","Epoch: 00 [ 7524/14178 ( 53%)], Train Loss: 0.51432\n","Epoch: 00 [ 7564/14178 ( 53%)], Train Loss: 0.51400\n","Epoch: 00 [ 7604/14178 ( 54%)], Train Loss: 0.51237\n","Epoch: 00 [ 7644/14178 ( 54%)], Train Loss: 0.51104\n","Epoch: 00 [ 7684/14178 ( 54%)], Train Loss: 0.51070\n","Epoch: 00 [ 7724/14178 ( 54%)], Train Loss: 0.50960\n","Epoch: 00 [ 7764/14178 ( 55%)], Train Loss: 0.50923\n","Epoch: 00 [ 7804/14178 ( 55%)], Train Loss: 0.50769\n","Epoch: 00 [ 7844/14178 ( 55%)], Train Loss: 0.50735\n","Epoch: 00 [ 7884/14178 ( 56%)], Train Loss: 0.50711\n","Epoch: 00 [ 7924/14178 ( 56%)], Train Loss: 0.50704\n","Epoch: 00 [ 7964/14178 ( 56%)], Train Loss: 0.50675\n","Epoch: 00 [ 8004/14178 ( 56%)], Train Loss: 0.50602\n","Epoch: 00 [ 8044/14178 ( 57%)], Train Loss: 0.50546\n","Epoch: 00 [ 8084/14178 ( 57%)], Train Loss: 0.50485\n","Epoch: 00 [ 8124/14178 ( 57%)], Train Loss: 0.50460\n","Epoch: 00 [ 8164/14178 ( 58%)], Train Loss: 0.50431\n","Epoch: 00 [ 8204/14178 ( 58%)], Train Loss: 0.50356\n","Epoch: 00 [ 8244/14178 ( 58%)], Train Loss: 0.50342\n","Epoch: 00 [ 8284/14178 ( 58%)], Train Loss: 0.50208\n","Epoch: 00 [ 8324/14178 ( 59%)], Train Loss: 0.50214\n","Epoch: 00 [ 8364/14178 ( 59%)], Train Loss: 0.50124\n","Epoch: 00 [ 8404/14178 ( 59%)], Train Loss: 0.50051\n","Epoch: 00 [ 8444/14178 ( 60%)], Train Loss: 0.50007\n","Epoch: 00 [ 8484/14178 ( 60%)], Train Loss: 0.49863\n","Epoch: 00 [ 8524/14178 ( 60%)], Train Loss: 0.49849\n","Epoch: 00 [ 8564/14178 ( 60%)], Train Loss: 0.49746\n","Epoch: 00 [ 8604/14178 ( 61%)], Train Loss: 0.49664\n","Epoch: 00 [ 8644/14178 ( 61%)], Train Loss: 0.49607\n","Epoch: 00 [ 8684/14178 ( 61%)], Train Loss: 0.49487\n","Epoch: 00 [ 8724/14178 ( 62%)], Train Loss: 0.49423\n","Epoch: 00 [ 8764/14178 ( 62%)], Train Loss: 0.49285\n","Epoch: 00 [ 8804/14178 ( 62%)], Train Loss: 0.49276\n","Epoch: 00 [ 8844/14178 ( 62%)], Train Loss: 0.49163\n","Epoch: 00 [ 8884/14178 ( 63%)], Train Loss: 0.49118\n","Epoch: 00 [ 8924/14178 ( 63%)], Train Loss: 0.49049\n","Epoch: 00 [ 8964/14178 ( 63%)], Train Loss: 0.48962\n","Epoch: 00 [ 9004/14178 ( 64%)], Train Loss: 0.48934\n","Epoch: 00 [ 9044/14178 ( 64%)], Train Loss: 0.48936\n","Epoch: 00 [ 9084/14178 ( 64%)], Train Loss: 0.48851\n","Epoch: 00 [ 9124/14178 ( 64%)], Train Loss: 0.48753\n","Epoch: 00 [ 9164/14178 ( 65%)], Train Loss: 0.48693\n","Epoch: 00 [ 9204/14178 ( 65%)], Train Loss: 0.48795\n","Epoch: 00 [ 9244/14178 ( 65%)], Train Loss: 0.48752\n","Epoch: 00 [ 9284/14178 ( 65%)], Train Loss: 0.48682\n","Epoch: 00 [ 9324/14178 ( 66%)], Train Loss: 0.48697\n","Epoch: 00 [ 9364/14178 ( 66%)], Train Loss: 0.48662\n","Epoch: 00 [ 9404/14178 ( 66%)], Train Loss: 0.48689\n","Epoch: 00 [ 9444/14178 ( 67%)], Train Loss: 0.48634\n","Epoch: 00 [ 9484/14178 ( 67%)], Train Loss: 0.48585\n","Epoch: 00 [ 9524/14178 ( 67%)], Train Loss: 0.48578\n","Epoch: 00 [ 9564/14178 ( 67%)], Train Loss: 0.48595\n","Epoch: 00 [ 9604/14178 ( 68%)], Train Loss: 0.48504\n","Epoch: 00 [ 9644/14178 ( 68%)], Train Loss: 0.48487\n","Epoch: 00 [ 9684/14178 ( 68%)], Train Loss: 0.48373\n","Epoch: 00 [ 9724/14178 ( 69%)], Train Loss: 0.48325\n","Epoch: 00 [ 9764/14178 ( 69%)], Train Loss: 0.48312\n","Epoch: 00 [ 9804/14178 ( 69%)], Train Loss: 0.48380\n","Epoch: 00 [ 9844/14178 ( 69%)], Train Loss: 0.48338\n","Epoch: 00 [ 9884/14178 ( 70%)], Train Loss: 0.48267\n","Epoch: 00 [ 9924/14178 ( 70%)], Train Loss: 0.48287\n","Epoch: 00 [ 9964/14178 ( 70%)], Train Loss: 0.48300\n","Epoch: 00 [10004/14178 ( 71%)], Train Loss: 0.48267\n","Epoch: 00 [10044/14178 ( 71%)], Train Loss: 0.48181\n","Epoch: 00 [10084/14178 ( 71%)], Train Loss: 0.48051\n","Epoch: 00 [10124/14178 ( 71%)], Train Loss: 0.48049\n","Epoch: 00 [10164/14178 ( 72%)], Train Loss: 0.48003\n","Epoch: 00 [10204/14178 ( 72%)], Train Loss: 0.47897\n","Epoch: 00 [10244/14178 ( 72%)], Train Loss: 0.47785\n","Epoch: 00 [10284/14178 ( 73%)], Train Loss: 0.47709\n","Epoch: 00 [10324/14178 ( 73%)], Train Loss: 0.47635\n","Epoch: 00 [10364/14178 ( 73%)], Train Loss: 0.47692\n","Epoch: 00 [10404/14178 ( 73%)], Train Loss: 0.47686\n","Epoch: 00 [10444/14178 ( 74%)], Train Loss: 0.47665\n","Epoch: 00 [10484/14178 ( 74%)], Train Loss: 0.47671\n","Epoch: 00 [10524/14178 ( 74%)], Train Loss: 0.47607\n","Epoch: 00 [10564/14178 ( 75%)], Train Loss: 0.47576\n","Epoch: 00 [10604/14178 ( 75%)], Train Loss: 0.47568\n","Epoch: 00 [10644/14178 ( 75%)], Train Loss: 0.47544\n","Epoch: 00 [10684/14178 ( 75%)], Train Loss: 0.47504\n","Epoch: 00 [10724/14178 ( 76%)], Train Loss: 0.47510\n","Epoch: 00 [10764/14178 ( 76%)], Train Loss: 0.47516\n","Epoch: 00 [10804/14178 ( 76%)], Train Loss: 0.47518\n","Epoch: 00 [10844/14178 ( 76%)], Train Loss: 0.47502\n","Epoch: 00 [10884/14178 ( 77%)], Train Loss: 0.47484\n","Epoch: 00 [10924/14178 ( 77%)], Train Loss: 0.47480\n","Epoch: 00 [10964/14178 ( 77%)], Train Loss: 0.47409\n","Epoch: 00 [11004/14178 ( 78%)], Train Loss: 0.47357\n","Epoch: 00 [11044/14178 ( 78%)], Train Loss: 0.47329\n","Epoch: 00 [11084/14178 ( 78%)], Train Loss: 0.47263\n","Epoch: 00 [11124/14178 ( 78%)], Train Loss: 0.47256\n","Epoch: 00 [11164/14178 ( 79%)], Train Loss: 0.47255\n","Epoch: 00 [11204/14178 ( 79%)], Train Loss: 0.47148\n","Epoch: 00 [11244/14178 ( 79%)], Train Loss: 0.47080\n","Epoch: 00 [11284/14178 ( 80%)], Train Loss: 0.47070\n","Epoch: 00 [11324/14178 ( 80%)], Train Loss: 0.47013\n","Epoch: 00 [11364/14178 ( 80%)], Train Loss: 0.46985\n","Epoch: 00 [11404/14178 ( 80%)], Train Loss: 0.46960\n","Epoch: 00 [11444/14178 ( 81%)], Train Loss: 0.46885\n","Epoch: 00 [11484/14178 ( 81%)], Train Loss: 0.46857\n","Epoch: 00 [11524/14178 ( 81%)], Train Loss: 0.46760\n","Epoch: 00 [11564/14178 ( 82%)], Train Loss: 0.46716\n","Epoch: 00 [11604/14178 ( 82%)], Train Loss: 0.46709\n","Epoch: 00 [11644/14178 ( 82%)], Train Loss: 0.46645\n","Epoch: 00 [11684/14178 ( 82%)], Train Loss: 0.46642\n","Epoch: 00 [11724/14178 ( 83%)], Train Loss: 0.46554\n","Epoch: 00 [11764/14178 ( 83%)], Train Loss: 0.46504\n","Epoch: 00 [11804/14178 ( 83%)], Train Loss: 0.46458\n","Epoch: 00 [11844/14178 ( 84%)], Train Loss: 0.46423\n","Epoch: 00 [11884/14178 ( 84%)], Train Loss: 0.46400\n","Epoch: 00 [11924/14178 ( 84%)], Train Loss: 0.46316\n","Epoch: 00 [11964/14178 ( 84%)], Train Loss: 0.46255\n","Epoch: 00 [12004/14178 ( 85%)], Train Loss: 0.46220\n","Epoch: 00 [12044/14178 ( 85%)], Train Loss: 0.46166\n","Epoch: 00 [12084/14178 ( 85%)], Train Loss: 0.46129\n","Epoch: 00 [12124/14178 ( 86%)], Train Loss: 0.46087\n","Epoch: 00 [12164/14178 ( 86%)], Train Loss: 0.46011\n","Epoch: 00 [12204/14178 ( 86%)], Train Loss: 0.45984\n","Epoch: 00 [12244/14178 ( 86%)], Train Loss: 0.45928\n","Epoch: 00 [12284/14178 ( 87%)], Train Loss: 0.45848\n","Epoch: 00 [12324/14178 ( 87%)], Train Loss: 0.45829\n","Epoch: 00 [12364/14178 ( 87%)], Train Loss: 0.45799\n","Epoch: 00 [12404/14178 ( 87%)], Train Loss: 0.45810\n","Epoch: 00 [12444/14178 ( 88%)], Train Loss: 0.45765\n","Epoch: 00 [12484/14178 ( 88%)], Train Loss: 0.45752\n","Epoch: 00 [12524/14178 ( 88%)], Train Loss: 0.45758\n","Epoch: 00 [12564/14178 ( 89%)], Train Loss: 0.45779\n","Epoch: 00 [12604/14178 ( 89%)], Train Loss: 0.45712\n","Epoch: 00 [12644/14178 ( 89%)], Train Loss: 0.45648\n","Epoch: 00 [12684/14178 ( 89%)], Train Loss: 0.45572\n","Epoch: 00 [12724/14178 ( 90%)], Train Loss: 0.45499\n","Epoch: 00 [12764/14178 ( 90%)], Train Loss: 0.45480\n","Epoch: 00 [12804/14178 ( 90%)], Train Loss: 0.45480\n","Epoch: 00 [12844/14178 ( 91%)], Train Loss: 0.45445\n","Epoch: 00 [12884/14178 ( 91%)], Train Loss: 0.45423\n","Epoch: 00 [12924/14178 ( 91%)], Train Loss: 0.45407\n","Epoch: 00 [12964/14178 ( 91%)], Train Loss: 0.45402\n","Epoch: 00 [13004/14178 ( 92%)], Train Loss: 0.45360\n","Epoch: 00 [13044/14178 ( 92%)], Train Loss: 0.45313\n","Epoch: 00 [13084/14178 ( 92%)], Train Loss: 0.45271\n","Epoch: 00 [13124/14178 ( 93%)], Train Loss: 0.45245\n","Epoch: 00 [13164/14178 ( 93%)], Train Loss: 0.45175\n","Epoch: 00 [13204/14178 ( 93%)], Train Loss: 0.45130\n","Epoch: 00 [13244/14178 ( 93%)], Train Loss: 0.45117\n","Epoch: 00 [13284/14178 ( 94%)], Train Loss: 0.45078\n","Epoch: 00 [13324/14178 ( 94%)], Train Loss: 0.45063\n","Epoch: 00 [13364/14178 ( 94%)], Train Loss: 0.45011\n","Epoch: 00 [13404/14178 ( 95%)], Train Loss: 0.44974\n","Epoch: 00 [13444/14178 ( 95%)], Train Loss: 0.44921\n","Epoch: 00 [13484/14178 ( 95%)], Train Loss: 0.44940\n","Epoch: 00 [13524/14178 ( 95%)], Train Loss: 0.44910\n","Epoch: 00 [13564/14178 ( 96%)], Train Loss: 0.44874\n","Epoch: 00 [13604/14178 ( 96%)], Train Loss: 0.44836\n","Epoch: 00 [13644/14178 ( 96%)], Train Loss: 0.44865\n","Epoch: 00 [13684/14178 ( 97%)], Train Loss: 0.44840\n","Epoch: 00 [13724/14178 ( 97%)], Train Loss: 0.44790\n","Epoch: 00 [13764/14178 ( 97%)], Train Loss: 0.44785\n","Epoch: 00 [13804/14178 ( 97%)], Train Loss: 0.44767\n","Epoch: 00 [13844/14178 ( 98%)], Train Loss: 0.44717\n","Epoch: 00 [13884/14178 ( 98%)], Train Loss: 0.44733\n","Epoch: 00 [13924/14178 ( 98%)], Train Loss: 0.44784\n","Epoch: 00 [13964/14178 ( 98%)], Train Loss: 0.44719\n","Epoch: 00 [14004/14178 ( 99%)], Train Loss: 0.44698\n","Epoch: 00 [14044/14178 ( 99%)], Train Loss: 0.44640\n","Epoch: 00 [14084/14178 ( 99%)], Train Loss: 0.44633\n","Epoch: 00 [14124/14178 (100%)], Train Loss: 0.44587\n","Epoch: 00 [14164/14178 (100%)], Train Loss: 0.44541\n","Epoch: 00 [14178/14178 (100%)], Train Loss: 0.44526\n","----Validation Results Summary----\n","Epoch: [0] Valid Loss: 0.74460\n","0 Epoch, Best epoch was updated! Valid Loss: 0.74460\n","Saving model checkpoint to chaii-qa-5-fold-xlmroberta-torch-fit/checkpoint-fold-4.\n","\n","Total Training Time: 1970.9307487010956secs, Average Training Time per Epoch: 1970.9307487010956secs.\n","Total Validation Time: 142.26519894599915secs, Average Validation Time per Epoch: 142.26519894599915secs.\n","\n","\n","--------------------------------------------------\n","FOLD: 5\n","--------------------------------------------------\n","Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n","Num examples Train= 17558, Num examples Valid=0\n","Total Training Steps: 2195, Total Warmup Steps: 219\n","Epoch: 00 [    4/17558 (  0%)], Train Loss: 2.90503\n","Epoch: 00 [   44/17558 (  0%)], Train Loss: 2.90903\n","Epoch: 00 [   84/17558 (  0%)], Train Loss: 2.87755\n","Epoch: 00 [  124/17558 (  1%)], Train Loss: 2.82837\n","Epoch: 00 [  164/17558 (  1%)], Train Loss: 2.77801\n","Epoch: 00 [  204/17558 (  1%)], Train Loss: 2.70844\n","Epoch: 00 [  244/17558 (  1%)], Train Loss: 2.63425\n","Epoch: 00 [  284/17558 (  2%)], Train Loss: 2.55297\n","Epoch: 00 [  324/17558 (  2%)], Train Loss: 2.44818\n","Epoch: 00 [  364/17558 (  2%)], Train Loss: 2.32262\n","Epoch: 00 [  404/17558 (  2%)], Train Loss: 2.21615\n","Epoch: 00 [  444/17558 (  3%)], Train Loss: 2.08723\n","Epoch: 00 [  484/17558 (  3%)], Train Loss: 1.97749\n","Epoch: 00 [  524/17558 (  3%)], Train Loss: 1.86873\n","Epoch: 00 [  564/17558 (  3%)], Train Loss: 1.78360\n","Epoch: 00 [  604/17558 (  3%)], Train Loss: 1.69671\n","Epoch: 00 [  644/17558 (  4%)], Train Loss: 1.63230\n","Epoch: 00 [  684/17558 (  4%)], Train Loss: 1.58063\n","Epoch: 00 [  724/17558 (  4%)], Train Loss: 1.52012\n","Epoch: 00 [  764/17558 (  4%)], Train Loss: 1.47820\n","Epoch: 00 [  804/17558 (  5%)], Train Loss: 1.43180\n","Epoch: 00 [  844/17558 (  5%)], Train Loss: 1.38529\n","Epoch: 00 [  884/17558 (  5%)], Train Loss: 1.34599\n","Epoch: 00 [  924/17558 (  5%)], Train Loss: 1.30789\n","Epoch: 00 [  964/17558 (  5%)], Train Loss: 1.27267\n","Epoch: 00 [ 1004/17558 (  6%)], Train Loss: 1.23950\n","Epoch: 00 [ 1044/17558 (  6%)], Train Loss: 1.20645\n","Epoch: 00 [ 1084/17558 (  6%)], Train Loss: 1.18636\n","Epoch: 00 [ 1124/17558 (  6%)], Train Loss: 1.16339\n","Epoch: 00 [ 1164/17558 (  7%)], Train Loss: 1.13736\n","Epoch: 00 [ 1204/17558 (  7%)], Train Loss: 1.11366\n","Epoch: 00 [ 1244/17558 (  7%)], Train Loss: 1.09350\n","Epoch: 00 [ 1284/17558 (  7%)], Train Loss: 1.07522\n","Epoch: 00 [ 1324/17558 (  8%)], Train Loss: 1.05174\n","Epoch: 00 [ 1364/17558 (  8%)], Train Loss: 1.03223\n","Epoch: 00 [ 1404/17558 (  8%)], Train Loss: 1.01837\n","Epoch: 00 [ 1444/17558 (  8%)], Train Loss: 1.00000\n","Epoch: 00 [ 1484/17558 (  8%)], Train Loss: 0.98577\n","Epoch: 00 [ 1524/17558 (  9%)], Train Loss: 0.96981\n","Epoch: 00 [ 1564/17558 (  9%)], Train Loss: 0.96382\n","Epoch: 00 [ 1604/17558 (  9%)], Train Loss: 0.95422\n","Epoch: 00 [ 1644/17558 (  9%)], Train Loss: 0.94515\n","Epoch: 00 [ 1684/17558 ( 10%)], Train Loss: 0.93511\n","Epoch: 00 [ 1724/17558 ( 10%)], Train Loss: 0.92032\n","Epoch: 00 [ 1764/17558 ( 10%)], Train Loss: 0.90799\n","Epoch: 00 [ 1804/17558 ( 10%)], Train Loss: 0.89717\n","Epoch: 00 [ 1844/17558 ( 11%)], Train Loss: 0.89001\n","Epoch: 00 [ 1884/17558 ( 11%)], Train Loss: 0.87991\n","Epoch: 00 [ 1924/17558 ( 11%)], Train Loss: 0.87035\n","Epoch: 00 [ 1964/17558 ( 11%)], Train Loss: 0.86158\n","Epoch: 00 [ 2004/17558 ( 11%)], Train Loss: 0.85226\n","Epoch: 00 [ 2044/17558 ( 12%)], Train Loss: 0.84193\n","Epoch: 00 [ 2084/17558 ( 12%)], Train Loss: 0.83188\n","Epoch: 00 [ 2124/17558 ( 12%)], Train Loss: 0.82285\n","Epoch: 00 [ 2164/17558 ( 12%)], Train Loss: 0.81630\n","Epoch: 00 [ 2204/17558 ( 13%)], Train Loss: 0.81092\n","Epoch: 00 [ 2244/17558 ( 13%)], Train Loss: 0.80115\n","Epoch: 00 [ 2284/17558 ( 13%)], Train Loss: 0.79414\n","Epoch: 00 [ 2324/17558 ( 13%)], Train Loss: 0.78720\n","Epoch: 00 [ 2364/17558 ( 13%)], Train Loss: 0.78093\n","Epoch: 00 [ 2404/17558 ( 14%)], Train Loss: 0.77590\n","Epoch: 00 [ 2444/17558 ( 14%)], Train Loss: 0.76913\n","Epoch: 00 [ 2484/17558 ( 14%)], Train Loss: 0.76263\n","Epoch: 00 [ 2524/17558 ( 14%)], Train Loss: 0.75947\n","Epoch: 00 [ 2564/17558 ( 15%)], Train Loss: 0.75363\n","Epoch: 00 [ 2604/17558 ( 15%)], Train Loss: 0.74765\n","Epoch: 00 [ 2644/17558 ( 15%)], Train Loss: 0.74801\n","Epoch: 00 [ 2684/17558 ( 15%)], Train Loss: 0.74324\n","Epoch: 00 [ 2724/17558 ( 16%)], Train Loss: 0.73731\n","Epoch: 00 [ 2764/17558 ( 16%)], Train Loss: 0.73263\n","Epoch: 00 [ 2804/17558 ( 16%)], Train Loss: 0.72890\n","Epoch: 00 [ 2844/17558 ( 16%)], Train Loss: 0.72592\n","Epoch: 00 [ 2884/17558 ( 16%)], Train Loss: 0.72130\n","Epoch: 00 [ 2924/17558 ( 17%)], Train Loss: 0.71667\n","Epoch: 00 [ 2964/17558 ( 17%)], Train Loss: 0.71258\n","Epoch: 00 [ 3004/17558 ( 17%)], Train Loss: 0.70697\n","Epoch: 00 [ 3044/17558 ( 17%)], Train Loss: 0.70262\n","Epoch: 00 [ 3084/17558 ( 18%)], Train Loss: 0.70558\n","Epoch: 00 [ 3124/17558 ( 18%)], Train Loss: 0.70264\n","Epoch: 00 [ 3164/17558 ( 18%)], Train Loss: 0.69882\n","Epoch: 00 [ 3204/17558 ( 18%)], Train Loss: 0.69514\n","Epoch: 00 [ 3244/17558 ( 18%)], Train Loss: 0.69085\n","Epoch: 00 [ 3284/17558 ( 19%)], Train Loss: 0.68856\n","Epoch: 00 [ 3324/17558 ( 19%)], Train Loss: 0.68814\n","Epoch: 00 [ 3364/17558 ( 19%)], Train Loss: 0.68613\n","Epoch: 00 [ 3404/17558 ( 19%)], Train Loss: 0.68124\n","Epoch: 00 [ 3444/17558 ( 20%)], Train Loss: 0.68025\n","Epoch: 00 [ 3484/17558 ( 20%)], Train Loss: 0.67818\n","Epoch: 00 [ 3524/17558 ( 20%)], Train Loss: 0.67474\n","Epoch: 00 [ 3564/17558 ( 20%)], Train Loss: 0.67202\n","Epoch: 00 [ 3604/17558 ( 21%)], Train Loss: 0.66875\n","Epoch: 00 [ 3644/17558 ( 21%)], Train Loss: 0.66456\n","Epoch: 00 [ 3684/17558 ( 21%)], Train Loss: 0.66186\n","Epoch: 00 [ 3724/17558 ( 21%)], Train Loss: 0.65857\n","Epoch: 00 [ 3764/17558 ( 21%)], Train Loss: 0.65483\n","Epoch: 00 [ 3804/17558 ( 22%)], Train Loss: 0.65342\n","Epoch: 00 [ 3844/17558 ( 22%)], Train Loss: 0.64990\n","Epoch: 00 [ 3884/17558 ( 22%)], Train Loss: 0.64718\n","Epoch: 00 [ 3924/17558 ( 22%)], Train Loss: 0.64299\n","Epoch: 00 [ 3964/17558 ( 23%)], Train Loss: 0.64099\n","Epoch: 00 [ 4004/17558 ( 23%)], Train Loss: 0.63869\n","Epoch: 00 [ 4044/17558 ( 23%)], Train Loss: 0.63669\n","Epoch: 00 [ 4084/17558 ( 23%)], Train Loss: 0.63478\n","Epoch: 00 [ 4124/17558 ( 23%)], Train Loss: 0.63339\n","Epoch: 00 [ 4164/17558 ( 24%)], Train Loss: 0.63279\n","Epoch: 00 [ 4204/17558 ( 24%)], Train Loss: 0.63039\n","Epoch: 00 [ 4244/17558 ( 24%)], Train Loss: 0.62858\n","Epoch: 00 [ 4284/17558 ( 24%)], Train Loss: 0.62736\n","Epoch: 00 [ 4324/17558 ( 25%)], Train Loss: 0.62444\n","Epoch: 00 [ 4364/17558 ( 25%)], Train Loss: 0.62181\n","Epoch: 00 [ 4404/17558 ( 25%)], Train Loss: 0.62114\n","Epoch: 00 [ 4444/17558 ( 25%)], Train Loss: 0.61734\n","Epoch: 00 [ 4484/17558 ( 26%)], Train Loss: 0.61663\n","Epoch: 00 [ 4524/17558 ( 26%)], Train Loss: 0.61411\n","Epoch: 00 [ 4564/17558 ( 26%)], Train Loss: 0.61299\n","Epoch: 00 [ 4604/17558 ( 26%)], Train Loss: 0.61044\n","Epoch: 00 [ 4644/17558 ( 26%)], Train Loss: 0.60954\n","Epoch: 00 [ 4684/17558 ( 27%)], Train Loss: 0.60626\n","Epoch: 00 [ 4724/17558 ( 27%)], Train Loss: 0.60405\n","Epoch: 00 [ 4764/17558 ( 27%)], Train Loss: 0.60267\n","Epoch: 00 [ 4804/17558 ( 27%)], Train Loss: 0.60071\n","Epoch: 00 [ 4844/17558 ( 28%)], Train Loss: 0.59907\n","Epoch: 00 [ 4884/17558 ( 28%)], Train Loss: 0.59837\n","Epoch: 00 [ 4924/17558 ( 28%)], Train Loss: 0.59831\n","Epoch: 00 [ 4964/17558 ( 28%)], Train Loss: 0.59710\n","Epoch: 00 [ 5004/17558 ( 28%)], Train Loss: 0.59762\n","Epoch: 00 [ 5044/17558 ( 29%)], Train Loss: 0.59628\n","Epoch: 00 [ 5084/17558 ( 29%)], Train Loss: 0.59637\n","Epoch: 00 [ 5124/17558 ( 29%)], Train Loss: 0.59531\n","Epoch: 00 [ 5164/17558 ( 29%)], Train Loss: 0.59320\n","Epoch: 00 [ 5204/17558 ( 30%)], Train Loss: 0.59014\n","Epoch: 00 [ 5244/17558 ( 30%)], Train Loss: 0.58759\n","Epoch: 00 [ 5284/17558 ( 30%)], Train Loss: 0.58760\n","Epoch: 00 [ 5324/17558 ( 30%)], Train Loss: 0.58545\n","Epoch: 00 [ 5364/17558 ( 31%)], Train Loss: 0.58542\n","Epoch: 00 [ 5404/17558 ( 31%)], Train Loss: 0.58362\n","Epoch: 00 [ 5444/17558 ( 31%)], Train Loss: 0.58224\n","Epoch: 00 [ 5484/17558 ( 31%)], Train Loss: 0.58200\n","Epoch: 00 [ 5524/17558 ( 31%)], Train Loss: 0.58148\n","Epoch: 00 [ 5564/17558 ( 32%)], Train Loss: 0.58020\n","Epoch: 00 [ 5604/17558 ( 32%)], Train Loss: 0.57839\n","Epoch: 00 [ 5644/17558 ( 32%)], Train Loss: 0.57760\n","Epoch: 00 [ 5684/17558 ( 32%)], Train Loss: 0.57615\n","Epoch: 00 [ 5724/17558 ( 33%)], Train Loss: 0.57493\n","Epoch: 00 [ 5764/17558 ( 33%)], Train Loss: 0.57441\n","Epoch: 00 [ 5804/17558 ( 33%)], Train Loss: 0.57470\n","Epoch: 00 [ 5844/17558 ( 33%)], Train Loss: 0.57290\n","Epoch: 00 [ 5884/17558 ( 34%)], Train Loss: 0.57222\n","Epoch: 00 [ 5924/17558 ( 34%)], Train Loss: 0.57151\n","Epoch: 00 [ 5964/17558 ( 34%)], Train Loss: 0.57123\n","Epoch: 00 [ 6004/17558 ( 34%)], Train Loss: 0.56987\n","Epoch: 00 [ 6044/17558 ( 34%)], Train Loss: 0.56979\n","Epoch: 00 [ 6084/17558 ( 35%)], Train Loss: 0.56696\n","Epoch: 00 [ 6124/17558 ( 35%)], Train Loss: 0.56598\n","Epoch: 00 [ 6164/17558 ( 35%)], Train Loss: 0.56432\n","Epoch: 00 [ 6204/17558 ( 35%)], Train Loss: 0.56468\n","Epoch: 00 [ 6244/17558 ( 36%)], Train Loss: 0.56424\n","Epoch: 00 [ 6284/17558 ( 36%)], Train Loss: 0.56320\n","Epoch: 00 [ 6324/17558 ( 36%)], Train Loss: 0.56076\n","Epoch: 00 [ 6364/17558 ( 36%)], Train Loss: 0.55961\n","Epoch: 00 [ 6404/17558 ( 36%)], Train Loss: 0.55925\n","Epoch: 00 [ 6444/17558 ( 37%)], Train Loss: 0.55838\n","Epoch: 00 [ 6484/17558 ( 37%)], Train Loss: 0.55794\n","Epoch: 00 [ 6524/17558 ( 37%)], Train Loss: 0.55533\n","Epoch: 00 [ 6564/17558 ( 37%)], Train Loss: 0.55428\n","Epoch: 00 [ 6604/17558 ( 38%)], Train Loss: 0.55352\n","Epoch: 00 [ 6644/17558 ( 38%)], Train Loss: 0.55207\n","Epoch: 00 [ 6684/17558 ( 38%)], Train Loss: 0.55126\n","Epoch: 00 [ 6724/17558 ( 38%)], Train Loss: 0.55012\n","Epoch: 00 [ 6764/17558 ( 39%)], Train Loss: 0.54936\n","Epoch: 00 [ 6804/17558 ( 39%)], Train Loss: 0.54923\n","Epoch: 00 [ 6844/17558 ( 39%)], Train Loss: 0.54884\n","Epoch: 00 [ 6884/17558 ( 39%)], Train Loss: 0.54819\n","Epoch: 00 [ 6924/17558 ( 39%)], Train Loss: 0.54729\n","Epoch: 00 [ 6964/17558 ( 40%)], Train Loss: 0.54708\n","Epoch: 00 [ 7004/17558 ( 40%)], Train Loss: 0.54664\n","Epoch: 00 [ 7044/17558 ( 40%)], Train Loss: 0.54662\n","Epoch: 00 [ 7084/17558 ( 40%)], Train Loss: 0.54593\n","Epoch: 00 [ 7124/17558 ( 41%)], Train Loss: 0.54503\n","Epoch: 00 [ 7164/17558 ( 41%)], Train Loss: 0.54361\n","Epoch: 00 [ 7204/17558 ( 41%)], Train Loss: 0.54231\n","Epoch: 00 [ 7244/17558 ( 41%)], Train Loss: 0.54239\n","Epoch: 00 [ 7284/17558 ( 41%)], Train Loss: 0.54107\n","Epoch: 00 [ 7324/17558 ( 42%)], Train Loss: 0.53988\n","Epoch: 00 [ 7364/17558 ( 42%)], Train Loss: 0.53824\n","Epoch: 00 [ 7404/17558 ( 42%)], Train Loss: 0.53736\n","Epoch: 00 [ 7444/17558 ( 42%)], Train Loss: 0.53606\n","Epoch: 00 [ 7484/17558 ( 43%)], Train Loss: 0.53537\n","Epoch: 00 [ 7524/17558 ( 43%)], Train Loss: 0.53533\n","Epoch: 00 [ 7564/17558 ( 43%)], Train Loss: 0.53513\n","Epoch: 00 [ 7604/17558 ( 43%)], Train Loss: 0.53488\n","Epoch: 00 [ 7644/17558 ( 44%)], Train Loss: 0.53473\n","Epoch: 00 [ 7684/17558 ( 44%)], Train Loss: 0.53561\n","Epoch: 00 [ 7724/17558 ( 44%)], Train Loss: 0.53475\n","Epoch: 00 [ 7764/17558 ( 44%)], Train Loss: 0.53390\n","Epoch: 00 [ 7804/17558 ( 44%)], Train Loss: 0.53359\n","Epoch: 00 [ 7844/17558 ( 45%)], Train Loss: 0.53260\n","Epoch: 00 [ 7884/17558 ( 45%)], Train Loss: 0.53281\n","Epoch: 00 [ 7924/17558 ( 45%)], Train Loss: 0.53157\n","Epoch: 00 [ 7964/17558 ( 45%)], Train Loss: 0.53122\n","Epoch: 00 [ 8004/17558 ( 46%)], Train Loss: 0.53091\n","Epoch: 00 [ 8044/17558 ( 46%)], Train Loss: 0.52994\n","Epoch: 00 [ 8084/17558 ( 46%)], Train Loss: 0.52980\n","Epoch: 00 [ 8124/17558 ( 46%)], Train Loss: 0.52899\n","Epoch: 00 [ 8164/17558 ( 46%)], Train Loss: 0.52790\n","Epoch: 00 [ 8204/17558 ( 47%)], Train Loss: 0.52803\n","Epoch: 00 [ 8244/17558 ( 47%)], Train Loss: 0.52816\n","Epoch: 00 [ 8284/17558 ( 47%)], Train Loss: 0.52706\n","Epoch: 00 [ 8324/17558 ( 47%)], Train Loss: 0.52688\n","Epoch: 00 [ 8364/17558 ( 48%)], Train Loss: 0.52611\n","Epoch: 00 [ 8404/17558 ( 48%)], Train Loss: 0.52585\n","Epoch: 00 [ 8444/17558 ( 48%)], Train Loss: 0.52535\n","Epoch: 00 [ 8484/17558 ( 48%)], Train Loss: 0.52414\n","Epoch: 00 [ 8524/17558 ( 49%)], Train Loss: 0.52399\n","Epoch: 00 [ 8564/17558 ( 49%)], Train Loss: 0.52340\n","Epoch: 00 [ 8604/17558 ( 49%)], Train Loss: 0.52383\n","Epoch: 00 [ 8644/17558 ( 49%)], Train Loss: 0.52297\n","Epoch: 00 [ 8684/17558 ( 49%)], Train Loss: 0.52373\n","Epoch: 00 [ 8724/17558 ( 50%)], Train Loss: 0.52377\n","Epoch: 00 [ 8764/17558 ( 50%)], Train Loss: 0.52326\n","Epoch: 00 [ 8804/17558 ( 50%)], Train Loss: 0.52335\n","Epoch: 00 [ 8844/17558 ( 50%)], Train Loss: 0.52339\n","Epoch: 00 [ 8884/17558 ( 51%)], Train Loss: 0.52273\n","Epoch: 00 [ 8924/17558 ( 51%)], Train Loss: 0.52223\n","Epoch: 00 [ 8964/17558 ( 51%)], Train Loss: 0.52159\n","Epoch: 00 [ 9004/17558 ( 51%)], Train Loss: 0.52069\n","Epoch: 00 [ 9044/17558 ( 52%)], Train Loss: 0.51950\n","Epoch: 00 [ 9084/17558 ( 52%)], Train Loss: 0.51834\n","Epoch: 00 [ 9124/17558 ( 52%)], Train Loss: 0.51821\n","Epoch: 00 [ 9164/17558 ( 52%)], Train Loss: 0.51754\n","Epoch: 00 [ 9204/17558 ( 52%)], Train Loss: 0.51653\n","Epoch: 00 [ 9244/17558 ( 53%)], Train Loss: 0.51645\n","Epoch: 00 [ 9284/17558 ( 53%)], Train Loss: 0.51512\n","Epoch: 00 [ 9324/17558 ( 53%)], Train Loss: 0.51523\n","Epoch: 00 [ 9364/17558 ( 53%)], Train Loss: 0.51490\n","Epoch: 00 [ 9404/17558 ( 54%)], Train Loss: 0.51437\n","Epoch: 00 [ 9444/17558 ( 54%)], Train Loss: 0.51469\n","Epoch: 00 [ 9484/17558 ( 54%)], Train Loss: 0.51338\n","Epoch: 00 [ 9524/17558 ( 54%)], Train Loss: 0.51234\n","Epoch: 00 [ 9564/17558 ( 54%)], Train Loss: 0.51147\n","Epoch: 00 [ 9604/17558 ( 55%)], Train Loss: 0.51096\n","Epoch: 00 [ 9644/17558 ( 55%)], Train Loss: 0.51086\n","Epoch: 00 [ 9684/17558 ( 55%)], Train Loss: 0.51061\n","Epoch: 00 [ 9724/17558 ( 55%)], Train Loss: 0.50979\n","Epoch: 00 [ 9764/17558 ( 56%)], Train Loss: 0.50868\n","Epoch: 00 [ 9804/17558 ( 56%)], Train Loss: 0.50772\n","Epoch: 00 [ 9844/17558 ( 56%)], Train Loss: 0.50860\n","Epoch: 00 [ 9884/17558 ( 56%)], Train Loss: 0.50830\n","Epoch: 00 [ 9924/17558 ( 57%)], Train Loss: 0.50775\n","Epoch: 00 [ 9964/17558 ( 57%)], Train Loss: 0.50727\n","Epoch: 00 [10004/17558 ( 57%)], Train Loss: 0.50688\n","Epoch: 00 [10044/17558 ( 57%)], Train Loss: 0.50659\n","Epoch: 00 [10084/17558 ( 57%)], Train Loss: 0.50605\n","Epoch: 00 [10124/17558 ( 58%)], Train Loss: 0.50569\n","Epoch: 00 [10164/17558 ( 58%)], Train Loss: 0.50491\n","Epoch: 00 [10204/17558 ( 58%)], Train Loss: 0.50462\n","Epoch: 00 [10244/17558 ( 58%)], Train Loss: 0.50395\n","Epoch: 00 [10284/17558 ( 59%)], Train Loss: 0.50369\n","Epoch: 00 [10324/17558 ( 59%)], Train Loss: 0.50303\n","Epoch: 00 [10364/17558 ( 59%)], Train Loss: 0.50279\n","Epoch: 00 [10404/17558 ( 59%)], Train Loss: 0.50220\n","Epoch: 00 [10444/17558 ( 59%)], Train Loss: 0.50193\n","Epoch: 00 [10484/17558 ( 60%)], Train Loss: 0.50180\n","Epoch: 00 [10524/17558 ( 60%)], Train Loss: 0.50094\n","Epoch: 00 [10564/17558 ( 60%)], Train Loss: 0.50051\n","Epoch: 00 [10604/17558 ( 60%)], Train Loss: 0.50079\n","Epoch: 00 [10644/17558 ( 61%)], Train Loss: 0.50062\n","Epoch: 00 [10684/17558 ( 61%)], Train Loss: 0.49966\n","Epoch: 00 [10724/17558 ( 61%)], Train Loss: 0.49914\n","Epoch: 00 [10764/17558 ( 61%)], Train Loss: 0.49848\n","Epoch: 00 [10804/17558 ( 62%)], Train Loss: 0.49763\n","Epoch: 00 [10844/17558 ( 62%)], Train Loss: 0.49716\n","Epoch: 00 [10884/17558 ( 62%)], Train Loss: 0.49661\n","Epoch: 00 [10924/17558 ( 62%)], Train Loss: 0.49578\n","Epoch: 00 [10964/17558 ( 62%)], Train Loss: 0.49609\n","Epoch: 00 [11004/17558 ( 63%)], Train Loss: 0.49554\n","Epoch: 00 [11044/17558 ( 63%)], Train Loss: 0.49510\n","Epoch: 00 [11084/17558 ( 63%)], Train Loss: 0.49463\n","Epoch: 00 [11124/17558 ( 63%)], Train Loss: 0.49331\n","Epoch: 00 [11164/17558 ( 64%)], Train Loss: 0.49278\n","Epoch: 00 [11204/17558 ( 64%)], Train Loss: 0.49271\n","Epoch: 00 [11244/17558 ( 64%)], Train Loss: 0.49211\n","Epoch: 00 [11284/17558 ( 64%)], Train Loss: 0.49189\n","Epoch: 00 [11324/17558 ( 64%)], Train Loss: 0.49104\n","Epoch: 00 [11364/17558 ( 65%)], Train Loss: 0.49037\n","Epoch: 00 [11404/17558 ( 65%)], Train Loss: 0.48958\n","Epoch: 00 [11444/17558 ( 65%)], Train Loss: 0.48859\n","Epoch: 00 [11484/17558 ( 65%)], Train Loss: 0.48828\n","Epoch: 00 [11524/17558 ( 66%)], Train Loss: 0.48842\n","Epoch: 00 [11564/17558 ( 66%)], Train Loss: 0.48751\n","Epoch: 00 [11604/17558 ( 66%)], Train Loss: 0.48671\n","Epoch: 00 [11644/17558 ( 66%)], Train Loss: 0.48623\n","Epoch: 00 [11684/17558 ( 67%)], Train Loss: 0.48581\n","Epoch: 00 [11724/17558 ( 67%)], Train Loss: 0.48584\n","Epoch: 00 [11764/17558 ( 67%)], Train Loss: 0.48572\n","Epoch: 00 [11804/17558 ( 67%)], Train Loss: 0.48520\n","Epoch: 00 [11844/17558 ( 67%)], Train Loss: 0.48470\n","Epoch: 00 [11884/17558 ( 68%)], Train Loss: 0.48433\n","Epoch: 00 [11924/17558 ( 68%)], Train Loss: 0.48407\n","Epoch: 00 [11964/17558 ( 68%)], Train Loss: 0.48321\n","Epoch: 00 [12004/17558 ( 68%)], Train Loss: 0.48308\n","Epoch: 00 [12044/17558 ( 69%)], Train Loss: 0.48395\n","Epoch: 00 [12084/17558 ( 69%)], Train Loss: 0.48306\n","Epoch: 00 [12124/17558 ( 69%)], Train Loss: 0.48324\n","Epoch: 00 [12164/17558 ( 69%)], Train Loss: 0.48231\n","Epoch: 00 [12204/17558 ( 70%)], Train Loss: 0.48181\n","Epoch: 00 [12244/17558 ( 70%)], Train Loss: 0.48169\n","Epoch: 00 [12284/17558 ( 70%)], Train Loss: 0.48108\n","Epoch: 00 [12324/17558 ( 70%)], Train Loss: 0.48054\n","Epoch: 00 [12364/17558 ( 70%)], Train Loss: 0.47985\n","Epoch: 00 [12404/17558 ( 71%)], Train Loss: 0.47908\n","Epoch: 00 [12444/17558 ( 71%)], Train Loss: 0.47898\n","Epoch: 00 [12484/17558 ( 71%)], Train Loss: 0.47822\n","Epoch: 00 [12524/17558 ( 71%)], Train Loss: 0.47786\n","Epoch: 00 [12564/17558 ( 72%)], Train Loss: 0.47749\n","Epoch: 00 [12604/17558 ( 72%)], Train Loss: 0.47674\n","Epoch: 00 [12644/17558 ( 72%)], Train Loss: 0.47716\n","Epoch: 00 [12684/17558 ( 72%)], Train Loss: 0.47709\n","Epoch: 00 [12724/17558 ( 72%)], Train Loss: 0.47694\n","Epoch: 00 [12764/17558 ( 73%)], Train Loss: 0.47621\n","Epoch: 00 [12804/17558 ( 73%)], Train Loss: 0.47576\n","Epoch: 00 [12844/17558 ( 73%)], Train Loss: 0.47473\n","Epoch: 00 [12884/17558 ( 73%)], Train Loss: 0.47480\n","Epoch: 00 [12924/17558 ( 74%)], Train Loss: 0.47483\n","Epoch: 00 [12964/17558 ( 74%)], Train Loss: 0.47438\n","Epoch: 00 [13004/17558 ( 74%)], Train Loss: 0.47413\n","Epoch: 00 [13044/17558 ( 74%)], Train Loss: 0.47396\n","Epoch: 00 [13084/17558 ( 75%)], Train Loss: 0.47388\n","Epoch: 00 [13124/17558 ( 75%)], Train Loss: 0.47312\n","Epoch: 00 [13164/17558 ( 75%)], Train Loss: 0.47259\n","Epoch: 00 [13204/17558 ( 75%)], Train Loss: 0.47256\n","Epoch: 00 [13244/17558 ( 75%)], Train Loss: 0.47243\n","Epoch: 00 [13284/17558 ( 76%)], Train Loss: 0.47200\n","Epoch: 00 [13324/17558 ( 76%)], Train Loss: 0.47105\n","Epoch: 00 [13364/17558 ( 76%)], Train Loss: 0.47127\n","Epoch: 00 [13404/17558 ( 76%)], Train Loss: 0.47092\n","Epoch: 00 [13444/17558 ( 77%)], Train Loss: 0.47057\n","Epoch: 00 [13484/17558 ( 77%)], Train Loss: 0.47002\n","Epoch: 00 [13524/17558 ( 77%)], Train Loss: 0.47019\n","Epoch: 00 [13564/17558 ( 77%)], Train Loss: 0.46993\n","Epoch: 00 [13604/17558 ( 77%)], Train Loss: 0.47029\n","Epoch: 00 [13644/17558 ( 78%)], Train Loss: 0.47019\n","Epoch: 00 [13684/17558 ( 78%)], Train Loss: 0.46954\n","Epoch: 00 [13724/17558 ( 78%)], Train Loss: 0.47011\n","Epoch: 00 [13764/17558 ( 78%)], Train Loss: 0.46986\n","Epoch: 00 [13804/17558 ( 79%)], Train Loss: 0.46953\n","Epoch: 00 [13844/17558 ( 79%)], Train Loss: 0.46889\n","Epoch: 00 [13884/17558 ( 79%)], Train Loss: 0.46921\n","Epoch: 00 [13924/17558 ( 79%)], Train Loss: 0.46905\n","Epoch: 00 [13964/17558 ( 80%)], Train Loss: 0.46859\n","Epoch: 00 [14004/17558 ( 80%)], Train Loss: 0.46821\n","Epoch: 00 [14044/17558 ( 80%)], Train Loss: 0.46745\n","Epoch: 00 [14084/17558 ( 80%)], Train Loss: 0.46716\n","Epoch: 00 [14124/17558 ( 80%)], Train Loss: 0.46712\n","Epoch: 00 [14164/17558 ( 81%)], Train Loss: 0.46713\n","Epoch: 00 [14204/17558 ( 81%)], Train Loss: 0.46695\n","Epoch: 00 [14244/17558 ( 81%)], Train Loss: 0.46714\n","Epoch: 00 [14284/17558 ( 81%)], Train Loss: 0.46621\n","Epoch: 00 [14324/17558 ( 82%)], Train Loss: 0.46577\n","Epoch: 00 [14364/17558 ( 82%)], Train Loss: 0.46524\n","Epoch: 00 [14404/17558 ( 82%)], Train Loss: 0.46484\n","Epoch: 00 [14444/17558 ( 82%)], Train Loss: 0.46432\n","Epoch: 00 [14484/17558 ( 82%)], Train Loss: 0.46399\n","Epoch: 00 [14524/17558 ( 83%)], Train Loss: 0.46312\n","Epoch: 00 [14564/17558 ( 83%)], Train Loss: 0.46279\n","Epoch: 00 [14604/17558 ( 83%)], Train Loss: 0.46248\n","Epoch: 00 [14644/17558 ( 83%)], Train Loss: 0.46218\n","Epoch: 00 [14684/17558 ( 84%)], Train Loss: 0.46185\n","Epoch: 00 [14724/17558 ( 84%)], Train Loss: 0.46198\n","Epoch: 00 [14764/17558 ( 84%)], Train Loss: 0.46157\n","Epoch: 00 [14804/17558 ( 84%)], Train Loss: 0.46148\n","Epoch: 00 [14844/17558 ( 85%)], Train Loss: 0.46082\n","Epoch: 00 [14884/17558 ( 85%)], Train Loss: 0.46030\n","Epoch: 00 [14924/17558 ( 85%)], Train Loss: 0.45972\n","Epoch: 00 [14964/17558 ( 85%)], Train Loss: 0.46016\n","Epoch: 00 [15004/17558 ( 85%)], Train Loss: 0.45995\n","Epoch: 00 [15044/17558 ( 86%)], Train Loss: 0.45920\n","Epoch: 00 [15084/17558 ( 86%)], Train Loss: 0.45880\n","Epoch: 00 [15124/17558 ( 86%)], Train Loss: 0.45854\n","Epoch: 00 [15164/17558 ( 86%)], Train Loss: 0.45851\n","Epoch: 00 [15204/17558 ( 87%)], Train Loss: 0.45823\n","Epoch: 00 [15244/17558 ( 87%)], Train Loss: 0.45809\n","Epoch: 00 [15284/17558 ( 87%)], Train Loss: 0.45802\n","Epoch: 00 [15324/17558 ( 87%)], Train Loss: 0.45806\n","Epoch: 00 [15364/17558 ( 88%)], Train Loss: 0.45798\n","Epoch: 00 [15404/17558 ( 88%)], Train Loss: 0.45781\n","Epoch: 00 [15444/17558 ( 88%)], Train Loss: 0.45823\n","Epoch: 00 [15484/17558 ( 88%)], Train Loss: 0.45809\n","Epoch: 00 [15524/17558 ( 88%)], Train Loss: 0.45824\n","Epoch: 00 [15564/17558 ( 89%)], Train Loss: 0.45814\n","Epoch: 00 [15604/17558 ( 89%)], Train Loss: 0.45744\n","Epoch: 00 [15644/17558 ( 89%)], Train Loss: 0.45724\n","Epoch: 00 [15684/17558 ( 89%)], Train Loss: 0.45700\n","Epoch: 00 [15724/17558 ( 90%)], Train Loss: 0.45704\n","Epoch: 00 [15764/17558 ( 90%)], Train Loss: 0.45760\n","Epoch: 00 [15804/17558 ( 90%)], Train Loss: 0.45778\n","Epoch: 00 [15844/17558 ( 90%)], Train Loss: 0.45761\n","Epoch: 00 [15884/17558 ( 90%)], Train Loss: 0.45740\n","Epoch: 00 [15924/17558 ( 91%)], Train Loss: 0.45741\n","Epoch: 00 [15964/17558 ( 91%)], Train Loss: 0.45700\n","Epoch: 00 [16004/17558 ( 91%)], Train Loss: 0.45665\n","Epoch: 00 [16044/17558 ( 91%)], Train Loss: 0.45634\n","Epoch: 00 [16084/17558 ( 92%)], Train Loss: 0.45592\n","Epoch: 00 [16124/17558 ( 92%)], Train Loss: 0.45569\n","Epoch: 00 [16164/17558 ( 92%)], Train Loss: 0.45572\n","Epoch: 00 [16204/17558 ( 92%)], Train Loss: 0.45522\n","Epoch: 00 [16244/17558 ( 93%)], Train Loss: 0.45498\n","Epoch: 00 [16284/17558 ( 93%)], Train Loss: 0.45443\n","Epoch: 00 [16324/17558 ( 93%)], Train Loss: 0.45418\n","Epoch: 00 [16364/17558 ( 93%)], Train Loss: 0.45421\n","Epoch: 00 [16404/17558 ( 93%)], Train Loss: 0.45426\n","Epoch: 00 [16444/17558 ( 94%)], Train Loss: 0.45383\n","Epoch: 00 [16484/17558 ( 94%)], Train Loss: 0.45355\n","Epoch: 00 [16524/17558 ( 94%)], Train Loss: 0.45351\n","Epoch: 00 [16564/17558 ( 94%)], Train Loss: 0.45320\n","Epoch: 00 [16604/17558 ( 95%)], Train Loss: 0.45283\n","Epoch: 00 [16644/17558 ( 95%)], Train Loss: 0.45213\n","Epoch: 00 [16684/17558 ( 95%)], Train Loss: 0.45169\n","Epoch: 00 [16724/17558 ( 95%)], Train Loss: 0.45154\n","Epoch: 00 [16764/17558 ( 95%)], Train Loss: 0.45124\n","Epoch: 00 [16804/17558 ( 96%)], Train Loss: 0.45116\n","Epoch: 00 [16844/17558 ( 96%)], Train Loss: 0.45091\n","Epoch: 00 [16884/17558 ( 96%)], Train Loss: 0.45052\n","Epoch: 00 [16924/17558 ( 96%)], Train Loss: 0.45024\n","Epoch: 00 [16964/17558 ( 97%)], Train Loss: 0.44976\n","Epoch: 00 [17004/17558 ( 97%)], Train Loss: 0.44957\n","Epoch: 00 [17044/17558 ( 97%)], Train Loss: 0.44921\n","Epoch: 00 [17084/17558 ( 97%)], Train Loss: 0.44884\n","Epoch: 00 [17124/17558 ( 98%)], Train Loss: 0.44975\n","Epoch: 00 [17164/17558 ( 98%)], Train Loss: 0.44960\n","Epoch: 00 [17204/17558 ( 98%)], Train Loss: 0.44894\n","Epoch: 00 [17244/17558 ( 98%)], Train Loss: 0.44844\n","Epoch: 00 [17284/17558 ( 98%)], Train Loss: 0.44817\n","Epoch: 00 [17324/17558 ( 99%)], Train Loss: 0.44797\n","Epoch: 00 [17364/17558 ( 99%)], Train Loss: 0.44745\n","Epoch: 00 [17404/17558 ( 99%)], Train Loss: 0.44751\n","Epoch: 00 [17444/17558 ( 99%)], Train Loss: 0.44764\n","Epoch: 00 [17484/17558 (100%)], Train Loss: 0.44775\n","Epoch: 00 [17524/17558 (100%)], Train Loss: 0.44733\n","Epoch: 00 [17558/17558 (100%)], Train Loss: 0.44707\n","----Validation Results Summary----\n","Epoch: [0] Valid Loss: 0.00000\n","0 Epoch, Best epoch was updated! Valid Loss: 0.00000\n","Saving model checkpoint to chaii-qa-5-fold-xlmroberta-torch-fit/checkpoint-fold-5.\n","\n","Total Training Time: 2440.58789396286secs, Average Training Time per Epoch: 2440.58789396286secs.\n","Total Validation Time: 0.4730820655822754secs, Average Validation Time per Epoch: 0.4730820655822754secs.\n"]}]},{"cell_type":"code","metadata":{"id":"DkjRIhdbwjHx","execution":{"iopub.status.busy":"2021-08-17T20:36:49.722586Z","iopub.execute_input":"2021-08-17T20:36:49.723011Z","iopub.status.idle":"2021-08-17T20:36:49.727659Z","shell.execute_reply.started":"2021-08-17T20:36:49.722978Z","shell.execute_reply":"2021-08-17T20:36:49.726609Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1634385652985,"user_tz":-540,"elapsed":88,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["# example for training second fold\n","\n","# for fold in range(1, 2):\n","#     print();print()\n","#     print('-'*50)\n","#     print(f'FOLD: {fold}')\n","#     print('-'*50)\n","#     run(train, fold)"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7uIwhUECP4iP"},"source":["### Thanks and please do Upvote!"]}]}