{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"name":"chaii-qa-5-fold-xlmroberta-torch-fit.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"f7add7603b3b4cf594e26b781e55cf3d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_0dba04fa6db544318fcb642f7e76907f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_154de523a5104cccadcaa5074c0753c7","IPY_MODEL_7ca1ad84252b433d8a4e97b3620670a4","IPY_MODEL_313d7f68275443bbb2381d97b786ba6d"]}},"0dba04fa6db544318fcb642f7e76907f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"154de523a5104cccadcaa5074c0753c7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2c32d9499d9447779b0c546b7a0c6937","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0d4c17d3cba14ba189a19d7c7b7ad2cd"}},"7ca1ad84252b433d8a4e97b3620670a4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_17611a5436a94173b3d2efa3415bdac8","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":606,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":606,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f4dcf6a63bed475f9906302e0f0f664d"}},"313d7f68275443bbb2381d97b786ba6d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2fc55251e41d4c949301188572ac55e6","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 606/606 [00:00&lt;00:00, 17.0kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_41b0d71fb3794f71b3e0271094eb9820"}},"2c32d9499d9447779b0c546b7a0c6937":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0d4c17d3cba14ba189a19d7c7b7ad2cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"17611a5436a94173b3d2efa3415bdac8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f4dcf6a63bed475f9906302e0f0f664d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2fc55251e41d4c949301188572ac55e6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"41b0d71fb3794f71b3e0271094eb9820":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"74b7ab5dd949454ab70b4429ddd03bb0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_41d1b86842334c9783d7405310dbd726","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_2d1aef8068984609b916dbf0c603e478","IPY_MODEL_7e71c81838e4449f8072c26ccd322963","IPY_MODEL_f117b61c5df94a76af108eccefddc854"]}},"41d1b86842334c9783d7405310dbd726":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2d1aef8068984609b916dbf0c603e478":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_41c43ba898c64425892fa50074c91d68","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6cebfbfcb1f745268e5227d6ba6e3f42"}},"7e71c81838e4449f8072c26ccd322963":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_79e1b4e1c3bd4bcfae3811fd9ec27c73","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":179,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":179,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_04dcc7b9d9cb44749e16662c5a8cb760"}},"f117b61c5df94a76af108eccefddc854":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b5b2b9b66c8244dc9d3f08970019c47d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 179/179 [00:00&lt;00:00, 4.47kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_42f019d174ed477daee0ad641b790f85"}},"41c43ba898c64425892fa50074c91d68":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6cebfbfcb1f745268e5227d6ba6e3f42":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"79e1b4e1c3bd4bcfae3811fd9ec27c73":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"04dcc7b9d9cb44749e16662c5a8cb760":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b5b2b9b66c8244dc9d3f08970019c47d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"42f019d174ed477daee0ad641b790f85":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"97f29b39cbc34e9b94691e981890a771":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_1e92d001e55a4dc78e2a2539bfa10d63","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_3f71b120e2f14ccc875c2c8006e2c939","IPY_MODEL_a99b0e5041a14a909a707ce0e514a044","IPY_MODEL_a4ca538694ed46be90fd57287a055470"]}},"1e92d001e55a4dc78e2a2539bfa10d63":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3f71b120e2f14ccc875c2c8006e2c939":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8d9e98d253b343f7aec3a08d97008b69","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_786c29ea07ce4858aa20e0fe663c1f4f"}},"a99b0e5041a14a909a707ce0e514a044":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7dc5d33615694f47ad4062699e84687d","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":5069051,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":5069051,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a9a4127714184176aada1e4d5c99995f"}},"a4ca538694ed46be90fd57287a055470":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_132311f4e7934ecd8a635dc5ed2c3efd","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 4.83M/4.83M [00:00&lt;00:00, 24.8MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_30c75d2b75464d13a59263068609b998"}},"8d9e98d253b343f7aec3a08d97008b69":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"786c29ea07ce4858aa20e0fe663c1f4f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7dc5d33615694f47ad4062699e84687d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a9a4127714184176aada1e4d5c99995f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"132311f4e7934ecd8a635dc5ed2c3efd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"30c75d2b75464d13a59263068609b998":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cb1200447a524feeae324ba9f7b017f7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_94d96330da9a441da50be5c202bcbb1c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_bb994e569c04447e86aaf0eb7bd109de","IPY_MODEL_8d327017e5284e9d9dc855695d5b656b","IPY_MODEL_3cd0732b42f74b9384c18872d77ce735"]}},"94d96330da9a441da50be5c202bcbb1c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bb994e569c04447e86aaf0eb7bd109de":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c0b689ecb40643c5b4e4ec58c1211baf","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1650b772654c44fd8b211947955ca5ef"}},"8d327017e5284e9d9dc855695d5b656b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c09437d82e694787bce3f784c76e3342","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":150,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":150,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8715c597126c4af5825041e3dd173b4f"}},"3cd0732b42f74b9384c18872d77ce735":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_63b2837479604bc982381a0903ec37d3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 150/150 [00:00&lt;00:00, 4.80kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a458816189484799b027b2fa1cf63cc8"}},"c0b689ecb40643c5b4e4ec58c1211baf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1650b772654c44fd8b211947955ca5ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c09437d82e694787bce3f784c76e3342":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"8715c597126c4af5825041e3dd173b4f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"63b2837479604bc982381a0903ec37d3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a458816189484799b027b2fa1cf63cc8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7ad7571474974e1b86a247da0b2ee87b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_f2af91eddacf446982d229dcf4889ffc","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f5067ebd80ef45fe92ebc09c7c84c410","IPY_MODEL_46b7c97299aa49d084fbcd6fb51a416c","IPY_MODEL_010725bd0a324213938514274ff551a6"]}},"f2af91eddacf446982d229dcf4889ffc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f5067ebd80ef45fe92ebc09c7c84c410":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1ed22506645b416fb10a39a747cdf261","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_72307f12ddda453cb6fe138189732999"}},"46b7c97299aa49d084fbcd6fb51a416c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_a0cfd4213ddc4207b69d43ae584dad89","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":2239666418,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":2239666418,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b53ac1f3ffd644679b6c9d759d9fa443"}},"010725bd0a324213938514274ff551a6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9519c27f67ad44a6aab9f617c972f349","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 2.09G/2.09G [00:58&lt;00:00, 41.9MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d527e0e6ccf5471faaee1b6b9fedbe91"}},"1ed22506645b416fb10a39a747cdf261":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"72307f12ddda453cb6fe138189732999":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a0cfd4213ddc4207b69d43ae584dad89":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"b53ac1f3ffd644679b6c9d759d9fa443":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9519c27f67ad44a6aab9f617c972f349":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d527e0e6ccf5471faaee1b6b9fedbe91":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RkuY4GVbQ1k4","executionInfo":{"status":"ok","timestamp":1636447514636,"user_tz":-540,"elapsed":25621,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"77de570c-0545-4cfe-d017-6fe95cf1d53e"},"source":["# Kaggle or Colab\n","import sys\n","import os\n","if 'kaggle_web_client' in sys.modules:\n","    # Do something\n","    pass\n","elif 'google.colab' in sys.modules:\n","    # Do something\n","    from google.colab import drive\n","    drive.mount(\"/content/drive\")\n","\n","    comp_name_official = \"chaii-hindi-and-tamil-question-answering\"\n","    comp_name_local = \"Kaggle-chaii\"\n","\n","    !pip install --upgrade --force-reinstall --no-deps kaggle\n","    import json\n","    f = open(\"/content/drive/MyDrive/colab_notebooks/kaggle/kaggle.json\", \"r\")\n","    json_data = json.load(f)\n","    os.environ[\"KAGGLE_USERNAME\"] = json_data[\"username\"]\n","    os.environ[\"KAGGLE_KEY\"] = json_data[\"key\"]\n","\n","    %cd /content/drive/MyDrive/colab_notebooks/kaggle/{comp_name_local}/notebooks\n","\n","    dname = \"chaii-qa-5-fold-xlmroberta-torch-fit\"\n","    !mkdir {dname}\n","    #%cd /content/drive/MyDrive/colab_notebooks/kaggle/{comp_name_local}/notebooks/{dname}"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Collecting kaggle\n","  Downloading kaggle-1.5.12.tar.gz (58 kB)\n","\u001b[K     |████████████████████████████████| 58 kB 3.1 MB/s \n","\u001b[?25hBuilding wheels for collected packages: kaggle\n","  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73051 sha256=1542d0d4ae25c44dfbdf1345417792b7dd90a6a1432fc304f005ec61290a2152\n","  Stored in directory: /root/.cache/pip/wheels/62/d6/58/5853130f941e75b2177d281eb7e44b4a98ed46dd155f556dc5\n","Successfully built kaggle\n","Installing collected packages: kaggle\n","  Attempting uninstall: kaggle\n","    Found existing installation: kaggle 1.5.12\n","    Uninstalling kaggle-1.5.12:\n","      Successfully uninstalled kaggle-1.5.12\n","Successfully installed kaggle-1.5.12\n","/content/drive/MyDrive/colab_notebooks/kaggle/Kaggle-chaii/notebooks\n","mkdir: cannot create directory ‘chaii-qa-5-fold-xlmroberta-torch-fit’: File exists\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a0xZ62k7SB1g","executionInfo":{"status":"ok","timestamp":1636447522883,"user_tz":-540,"elapsed":8259,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"8252e106-c764-49c4-cffc-037c3475cdb3"},"source":["if 'kaggle_web_client' in sys.modules:\n","    # Do something\n","    pass\n","elif 'google.colab' in sys.modules:\n","    #!pip install transformers\n","    !pip install transformers[sentencepiece]\n","\n","    # import yaml\n","    # with open(f'./config_notebook/config.yaml') as file:\n","    #     cfg = yaml.load(file, Loader=yaml.FullLoader) # Loader is recommended\n","    # print(\"Config:\\n\", cfg)"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers[sentencepiece]\n","  Downloading transformers-4.12.3-py3-none-any.whl (3.1 MB)\n","\u001b[K     |████████████████████████████████| 3.1 MB 5.1 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n","\u001b[K     |████████████████████████████████| 59 kB 6.6 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (1.19.5)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 49.7 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (21.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2019.12.20)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (4.8.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.3.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (4.62.3)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 64.7 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 37.1 MB/s \n","\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.17.3)\n","Collecting sentencepiece!=0.1.92,>=0.1.91\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 57.6 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers[sentencepiece]) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers[sentencepiece]) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers[sentencepiece]) (3.6.0)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers[sentencepiece]) (1.15.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (2.10)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[sentencepiece]) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[sentencepiece]) (1.0.1)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers, sentencepiece\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.1.2 pyyaml-6.0 sacremoses-0.0.46 sentencepiece-0.1.96 tokenizers-0.10.3 transformers-4.12.3\n"]}]},{"cell_type":"markdown","metadata":{"id":"uQdNDkrBP4hj"},"source":["<h2>chaii QA - 5 Fold XLMRoberta Finetuning in Torch w/o Trainer API</h2>\n","    \n","<h3><span \"style: color=#444\">Introduction</span></h3>\n","\n","The kernel implements 5-Fold XLMRoberta QA Model without using the Trainer API from HuggingFace.\n","\n","This is a three part kernel,\n","\n","- [External Data - MLQA, XQUAD Preprocessing](https://www.kaggle.com/rhtsingh/external-data-mlqa-xquad-preprocessing) which preprocesses the Hindi Corpus of MLQA and XQUAD. I have used these data for training.\n","\n","- [chaii QA - 5 Fold XLMRoberta Torch | FIT](https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-fit/edit) This kernel is the current kernel and used for Finetuning (FIT) on competition + external data.\n","\n","- [chaii QA - 5 Fold XLMRoberta Torch | Infer](https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-infer) The Inference kernel where we ensemble our 5 Fold XLMRoberta Models and do the submission.\n","\n","<h3><span \"style: color=#444\">Techniques</span></h3>\n","\n","The kernel has implementation for below techniques, click on the links to learn more -\n","\n"," - [External Data Preprocessing + Training](https://www.kaggle.com/rhtsingh/external-data-mlqa-xquad-preprocessing)\n"," \n"," - [Mixed Precision Training using APEX](https://www.kaggle.com/rhtsingh/swa-apex-amp-interpreting-transformers-in-torch)\n"," \n"," - [Gradient Accumulation](https://www.kaggle.com/rhtsingh/speeding-up-transformer-w-optimization-strategies)\n"," \n"," - [Gradient Clipping](https://www.kaggle.com/rhtsingh/swa-apex-amp-interpreting-transformers-in-torch)\n"," \n"," - [Grouped Layerwise Learning Rate Decay](https://www.kaggle.com/rhtsingh/guide-to-huggingface-schedulers-differential-lrs)\n"," \n"," - [Utilizing Intermediate Transformer Representations](https://www.kaggle.com/rhtsingh/utilizing-transformer-representations-efficiently)\n"," \n"," - [Layer Initialization](https://www.kaggle.com/rhtsingh/on-stability-of-few-sample-transformer-fine-tuning)\n"," \n"," - [5-Fold Training](https://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-fit)\n"," \n"," - etc.\n"," \n","<h3><span \"style: color=#444\">References</span></h3>\n","I would like to acknowledge below kagglers work and resources without whom this kernel wouldn't have been possible,  \n","\n","- [roberta inference 5 folds by Abhishek](https://www.kaggle.com/abhishek/roberta-inference-5-folds)\n","\n","- [ChAII - EDA & Baseline by Darek](https://www.kaggle.com/thedrcat/chaii-eda-baseline) due to whom i found the great notebook below from HuggingFace,\n","\n","- [How to fine-tune a model on question answering](https://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb)\n","\n","- [HuggingFace QA Examples](https://github.com/huggingface/transformers/tree/master/examples/pytorch/question-answering)\n","\n","- [TensorFlow 2.0 Question Answering - Collections of gold solutions](https://www.kaggle.com/c/tensorflow2-question-answering/discussion/127409) these solutions are gold and highly recommend everyone to give a detailed read."]},{"cell_type":"markdown","metadata":{"id":"vI-eiJRPP4hw"},"source":["<h3><span style=\"color=#444\">Note</span></h3>\n","\n","The below points are worth noting,\n","\n"," - I haven't used FP16 because due to some reason this fails and model never starts training.\n"," - These are the original hyperparamters and setting that I have used for training my models.\n"," - I tried few pooling layers but none of them performed better than simple one.\n"," - Gradient clipping reduces model performance.\n"," - <span style=\"color:#2E86C1\">Training 5 Folds at once will give OOM issue on Kaggle and Colab. I have trained one fold at a time i.e. After 1st fold is completed I save the model as a dataset then train second fold. Repeat this process until all 5 folds are completed. Training each fold takes around 50 minuts on Kaggle.</span>\n"," - <span style=\"color:#2E86C1\">I have modified the preprocessing code a lot from original used in HuggingFace notebook since I am not using HuggingFace Datasets API as well. So I had to handle the sequence-ids specially since they contain None values which torch doesn't support.</span>"]},{"cell_type":"markdown","metadata":{"id":"8ykS_qCDP4hz"},"source":["### Install APEX"]},{"cell_type":"code","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","id":"aCY6yvR6ET3s","execution":{"iopub.status.busy":"2021-08-17T20:22:25.164246Z","iopub.execute_input":"2021-08-17T20:22:25.164729Z","iopub.status.idle":"2021-08-17T20:22:25.170948Z","shell.execute_reply.started":"2021-08-17T20:22:25.164627Z","shell.execute_reply":"2021-08-17T20:22:25.169352Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1636447522885,"user_tz":-540,"elapsed":11,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["# %%writefile setup.sh\n","# export CUDA_HOME=/usr/local/cuda-10.1\n","# git clone https://github.com/NVIDIA/apex\n","# cd apex\n","# pip install -v --disable-pip-version-check --no-cache-dir ./"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"-l2Jsav9ET3v","execution":{"iopub.status.busy":"2021-08-17T20:22:26.935257Z","iopub.execute_input":"2021-08-17T20:22:26.935919Z","iopub.status.idle":"2021-08-17T20:22:26.939334Z","shell.execute_reply.started":"2021-08-17T20:22:26.935881Z","shell.execute_reply":"2021-08-17T20:22:26.938488Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1636447522886,"user_tz":-540,"elapsed":10,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["# %%capture\n","# !sh setup.sh"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xFbbPlPgP4h7"},"source":["### Import Dependencies"]},{"cell_type":"code","metadata":{"id":"E4l6PirHET3x","execution":{"iopub.status.busy":"2021-08-17T20:23:50.232396Z","iopub.execute_input":"2021-08-17T20:23:50.232892Z","iopub.status.idle":"2021-08-17T20:24:01.076652Z","shell.execute_reply.started":"2021-08-17T20:23:50.232861Z","shell.execute_reply":"2021-08-17T20:24:01.075658Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636447538658,"user_tz":-540,"elapsed":15781,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"7e167d1e-31a0-4ff2-b613-24a6809e56fd"},"source":["#import os\n","import gc\n","gc.enable()\n","import math\n","#mport json\n","import time\n","import random\n","import multiprocessing\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=UserWarning)\n","\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm, trange\n","from sklearn import model_selection\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn import Parameter\n","import torch.optim as optim\n","from torch.utils.data import (\n","    Dataset, DataLoader,\n","    SequentialSampler, RandomSampler\n",")\n","from torch.utils.data.distributed import DistributedSampler\n","\n","try:\n","    from apex import amp\n","    APEX_INSTALLED = True\n","except ImportError:\n","    APEX_INSTALLED = False\n","\n","import transformers\n","from transformers import (\n","    WEIGHTS_NAME,\n","    AdamW,\n","    AutoConfig,\n","    AutoModel,\n","    AutoTokenizer,\n","    get_cosine_schedule_with_warmup,\n","    get_linear_schedule_with_warmup,\n","    logging,\n","    MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n",")\n","logging.set_verbosity_warning()\n","logging.set_verbosity_error()\n","\n","def fix_all_seeds(seed):\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","def optimal_num_of_loader_workers():\n","    num_cpus = multiprocessing.cpu_count()\n","    num_gpus = torch.cuda.device_count()\n","    optimal_value = min(num_cpus, num_gpus*4) if num_gpus else num_cpus - 1\n","    return optimal_value\n","\n","print(f\"Apex AMP Installed :: {APEX_INSTALLED}\")\n","MODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())\n","MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Apex AMP Installed :: False\n"]}]},{"cell_type":"markdown","metadata":{"id":"y0OvcnMaP4h9"},"source":["### Training Configuration"]},{"cell_type":"code","metadata":{"id":"LUx5XplNET3y","execution":{"iopub.status.busy":"2021-08-17T20:24:13.391322Z","iopub.execute_input":"2021-08-17T20:24:13.391696Z","iopub.status.idle":"2021-08-17T20:24:13.398397Z","shell.execute_reply.started":"2021-08-17T20:24:13.391662Z","shell.execute_reply":"2021-08-17T20:24:13.39732Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1636447539147,"user_tz":-540,"elapsed":517,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["class Config:\n","    # model\n","    model_type = 'xlm_roberta'\n","    model_name_or_path = \"deepset/xlm-roberta-large-squad2\"\n","    config_name = \"deepset/xlm-roberta-large-squad2\"\n","    fp16 = True if APEX_INSTALLED else False\n","    fp16_opt_level = \"O1\"\n","    gradient_accumulation_steps = 2\n","\n","    # tokenizer\n","    tokenizer_name = \"deepset/xlm-roberta-large-squad2\"\n","    max_seq_length = 32*13 #512 #384\n","    doc_stride = int(32*6) #80 #128\n","\n","    # train\n","    epochs = 1 #2 #7 #1\n","    train_batch_size = 4 #2 #4\n","    eval_batch_size = 8 #4 #8\n","\n","    # optimizer\n","    optimizer_type = 'AdamW'\n","    learning_rate = 1.5e-5\n","    weight_decay = 1e-2\n","    epsilon = 1e-8\n","    max_grad_norm = 1.0\n","\n","    # scheduler\n","    decay_name = 'linear-warmup'\n","    warmup_ratio = 0.1\n","\n","    # logging\n","    logging_steps = 10\n","\n","    # evaluate\n","    output_dir = dname #'output'\n","    seed = 2021"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HYno9o1OP4h-"},"source":["### Data Factory"]},{"cell_type":"code","metadata":{"id":"X_eRZQrzET3z","execution":{"iopub.status.busy":"2021-08-17T20:24:26.939105Z","iopub.execute_input":"2021-08-17T20:24:26.939437Z","iopub.status.idle":"2021-08-17T20:24:27.776039Z","shell.execute_reply.started":"2021-08-17T20:24:26.939409Z","shell.execute_reply":"2021-08-17T20:24:27.775204Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1636447541198,"user_tz":-540,"elapsed":2054,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["train = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/train.csv')\n","test = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/test.csv')\n","external_mlqa = pd.read_csv('../input/mlqa-hindi-processed/mlqa_hindi.csv')\n","external_xquad = pd.read_csv('../input/mlqa-hindi-processed/xquad.csv')\n","external_train = pd.concat([external_mlqa, external_xquad])\n","\n","def create_folds(data, num_splits):\n","    data[\"kfold\"] = -1\n","    kf = model_selection.StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=2021)\n","    for f, (t_, v_) in enumerate(kf.split(X=data, y=data['language'])):\n","        data.loc[v_, 'kfold'] = f\n","    return data\n","\n","#### RIOW\n","# train = create_folds(train, num_splits=5)\n","# external_train[\"kfold\"] = -1\n","# external_train['id'] = list(np.arange(1, len(external_train)+1))\n","# train = pd.concat([train, external_train]).reset_index(drop=True)\n","\n","external_train['id'] = list(np.arange(1, len(external_train)+1))\n","\n","# # Drop tamil\n","# train = train[train[\"language\"]!=\"tamil\"].reset_index(drop=True)\n","\n","train = pd.concat([train, external_train]).reset_index(drop=True)\n","train = create_folds(train, num_splits=5)\n","#### RIOWRIOW\n","\n","def convert_answers(row):\n","    return {'answer_start': [row[0]], 'text': [row[1]]}\n","\n","train['answers'] = train[['answer_start', 'answer_text']].apply(convert_answers, axis=1)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dAoAJUVaP4iA"},"source":["### Covert Examples to Features (Preprocess)"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-12T15:50:26.947214Z","iopub.execute_input":"2021-08-12T15:50:26.947589Z","iopub.status.idle":"2021-08-12T15:50:26.960916Z","shell.execute_reply.started":"2021-08-12T15:50:26.947551Z","shell.execute_reply":"2021-08-12T15:50:26.959064Z"},"id":"dxbZdct1ET3z","trusted":true,"executionInfo":{"status":"ok","timestamp":1636447541200,"user_tz":-540,"elapsed":17,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["def prepare_train_features(args, example, tokenizer):\n","    example[\"question\"] = example[\"question\"].lstrip()\n","    tokenized_example = tokenizer(\n","        example[\"question\"],\n","        example[\"context\"],\n","        truncation=\"only_second\",\n","        max_length=args.max_seq_length,\n","        stride=args.doc_stride,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True,\n","        padding=\"max_length\",\n","    )\n","\n","    sample_mapping = tokenized_example.pop(\"overflow_to_sample_mapping\")\n","    offset_mapping = tokenized_example.pop(\"offset_mapping\")\n","\n","    features = []\n","    for i, offsets in enumerate(offset_mapping):\n","        feature = {}\n","\n","        input_ids = tokenized_example[\"input_ids\"][i]\n","        attention_mask = tokenized_example[\"attention_mask\"][i]\n","\n","        feature['input_ids'] = input_ids\n","        feature['attention_mask'] = attention_mask\n","        feature['offset_mapping'] = offsets\n","\n","        cls_index = input_ids.index(tokenizer.cls_token_id)\n","        sequence_ids = tokenized_example.sequence_ids(i)\n","\n","        sample_index = sample_mapping[i]\n","        answers = example[\"answers\"]\n","\n","        if len(answers[\"answer_start\"]) == 0:\n","            feature[\"start_position\"] = cls_index\n","            feature[\"end_position\"] = cls_index\n","        else:\n","            start_char = answers[\"answer_start\"][0]\n","            end_char = start_char + len(answers[\"text\"][0])\n","\n","            token_start_index = 0\n","            while sequence_ids[token_start_index] != 1:\n","                token_start_index += 1\n","\n","            token_end_index = len(input_ids) - 1\n","            while sequence_ids[token_end_index] != 1:\n","                token_end_index -= 1\n","\n","            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n","                feature[\"start_position\"] = cls_index\n","                feature[\"end_position\"] = cls_index\n","            else:\n","                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n","                    token_start_index += 1\n","                feature[\"start_position\"] = token_start_index - 1\n","                while offsets[token_end_index][1] >= end_char:\n","                    token_end_index -= 1\n","                feature[\"end_position\"] = token_end_index + 1\n","\n","        features.append(feature)\n","    return features"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a3SIS_xAP4iC"},"source":["### Dataset Retriever"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-12T15:50:26.962738Z","iopub.execute_input":"2021-08-12T15:50:26.963118Z","iopub.status.idle":"2021-08-12T15:50:26.97542Z","shell.execute_reply.started":"2021-08-12T15:50:26.963075Z","shell.execute_reply":"2021-08-12T15:50:26.974431Z"},"id":"6TuzHdjmET30","trusted":true,"executionInfo":{"status":"ok","timestamp":1636447541201,"user_tz":-540,"elapsed":16,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["class DatasetRetriever(Dataset):\n","    def __init__(self, features, mode='train'):\n","        super(DatasetRetriever, self).__init__()\n","        self.features = features\n","        self.mode = mode\n","        \n","    def __len__(self):\n","        return len(self.features)\n","    \n","    def __getitem__(self, item):   \n","        feature = self.features[item]\n","        if self.mode == 'train':\n","            return {\n","                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n","                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n","                'offset_mapping':torch.tensor(feature['offset_mapping'], dtype=torch.long),\n","                'start_position':torch.tensor(feature['start_position'], dtype=torch.long),\n","                'end_position':torch.tensor(feature['end_position'], dtype=torch.long)\n","            }\n","        else:\n","            return {\n","                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n","                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n","                'offset_mapping':feature['offset_mapping'],\n","                'sequence_ids':feature['sequence_ids'],\n","                'id':feature['example_id'],\n","                'context': feature['context'],\n","                'question': feature['question']\n","            }"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tpH-2nPqP4iE"},"source":["### Model"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-12T15:50:26.976977Z","iopub.execute_input":"2021-08-12T15:50:26.977627Z","iopub.status.idle":"2021-08-12T15:50:26.990227Z","shell.execute_reply.started":"2021-08-12T15:50:26.97747Z","shell.execute_reply":"2021-08-12T15:50:26.989443Z"},"id":"9OxhKqxcET31","trusted":true,"executionInfo":{"status":"ok","timestamp":1636447541201,"user_tz":-540,"elapsed":16,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["class Model(nn.Module):\n","    def __init__(self, modelname_or_path, config):\n","        super(Model, self).__init__()\n","        self.config = config\n","        self.xlm_roberta = AutoModel.from_pretrained(modelname_or_path, config=config)\n","        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","        self._init_weights(self.qa_outputs)\n","        \n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","\n","    def forward(\n","        self, \n","        input_ids, \n","        attention_mask=None, \n","        # token_type_ids=None\n","    ):\n","        outputs = self.xlm_roberta(\n","            input_ids,\n","            attention_mask=attention_mask,\n","        )\n","\n","        sequence_output = outputs[0]\n","        pooled_output = outputs[1]\n","        \n","        # sequence_output = self.dropout(sequence_output)\n","        qa_logits = self.qa_outputs(sequence_output)\n","        \n","        start_logits, end_logits = qa_logits.split(1, dim=-1)\n","        start_logits = start_logits.squeeze(-1)\n","        end_logits = end_logits.squeeze(-1)\n","    \n","        return start_logits, end_logits"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xdMx6plqP4iG"},"source":["### Loss"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-12T15:50:26.991891Z","iopub.execute_input":"2021-08-12T15:50:26.99237Z","iopub.status.idle":"2021-08-12T15:50:27.000188Z","shell.execute_reply.started":"2021-08-12T15:50:26.992334Z","shell.execute_reply":"2021-08-12T15:50:26.999374Z"},"id":"SxuNrJqqET32","trusted":true,"executionInfo":{"status":"ok","timestamp":1636447541202,"user_tz":-540,"elapsed":16,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["def loss_fn(preds, labels):\n","    start_preds, end_preds = preds\n","    start_labels, end_labels = labels\n","    \n","    start_loss = nn.CrossEntropyLoss(ignore_index=-1)(start_preds, start_labels)\n","    end_loss = nn.CrossEntropyLoss(ignore_index=-1)(end_preds, end_labels)\n","    total_loss = (start_loss + end_loss) / 2\n","    return total_loss"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jUMtX08cP4iH"},"source":["### Grouped Layerwise Learning Rate Decay"]},{"cell_type":"code","metadata":{"id":"vf6HVcu2ET34","execution":{"iopub.status.busy":"2021-08-17T20:25:36.36033Z","iopub.execute_input":"2021-08-17T20:25:36.361058Z","iopub.status.idle":"2021-08-17T20:25:36.381524Z","shell.execute_reply.started":"2021-08-17T20:25:36.361009Z","shell.execute_reply":"2021-08-17T20:25:36.380328Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1636447541203,"user_tz":-540,"elapsed":17,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["def get_optimizer_grouped_parameters(args, model):\n","    no_decay = [\"bias\", \"LayerNorm.weight\"]\n","    group1=['layer.0.','layer.1.','layer.2.','layer.3.']\n","    group2=['layer.4.','layer.5.','layer.6.','layer.7.']    \n","    group3=['layer.8.','layer.9.','layer.10.','layer.11.']\n","    group_all=['layer.0.','layer.1.','layer.2.','layer.3.','layer.4.','layer.5.','layer.6.','layer.7.','layer.8.','layer.9.','layer.10.','layer.11.']\n","    optimizer_grouped_parameters = [\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': args.weight_decay},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': args.weight_decay, 'lr': args.learning_rate/2.6},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': args.weight_decay, 'lr': args.learning_rate},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': args.weight_decay, 'lr': args.learning_rate*2.6},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': 0.0},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': 0.0, 'lr': args.learning_rate/2.6},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': 0.0, 'lr': args.learning_rate},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': 0.0, 'lr': args.learning_rate*2.6},\n","        {'params': [p for n, p in model.named_parameters() if args.model_type not in n], 'lr':args.learning_rate*20, \"weight_decay\": 0.0},\n","    ]\n","    return optimizer_grouped_parameters"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oXz_0fIQP4iI"},"source":["### Metric Logger"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-12T15:50:27.021442Z","iopub.execute_input":"2021-08-12T15:50:27.021952Z","iopub.status.idle":"2021-08-12T15:50:27.03234Z","shell.execute_reply.started":"2021-08-12T15:50:27.021912Z","shell.execute_reply":"2021-08-12T15:50:27.03156Z"},"id":"bkFB-iMcET34","trusted":true,"executionInfo":{"status":"ok","timestamp":1636447541203,"user_tz":-540,"elapsed":16,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["class AverageMeter(object):\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","        self.max = 0\n","        self.min = 1e5\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","        if val > self.max:\n","            self.max = val\n","        if val < self.min:\n","            self.min = val"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y946gQxtP4iJ"},"source":["### Utilities"]},{"cell_type":"code","metadata":{"id":"spFRutV0ET34","execution":{"iopub.status.busy":"2021-08-17T20:26:30.014223Z","iopub.execute_input":"2021-08-17T20:26:30.014623Z","iopub.status.idle":"2021-08-17T20:26:30.030566Z","shell.execute_reply.started":"2021-08-17T20:26:30.01459Z","shell.execute_reply":"2021-08-17T20:26:30.029723Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1636447541204,"user_tz":-540,"elapsed":16,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["def make_model(args):\n","    config = AutoConfig.from_pretrained(args.config_name)\n","    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name)\n","    model = Model(args.model_name_or_path, config=config)\n","    return config, tokenizer, model\n","\n","def make_optimizer(args, model):\n","    # optimizer_grouped_parameters = get_optimizer_grouped_parameters(args, model)\n","    no_decay = [\"bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\n","            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","            \"weight_decay\": args.weight_decay,\n","        },\n","        {\n","            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n","            \"weight_decay\": 0.0,\n","        },\n","    ]\n","    if args.optimizer_type == \"AdamW\":\n","        optimizer = AdamW(\n","            optimizer_grouped_parameters,\n","            lr=args.learning_rate,\n","            eps=args.epsilon,\n","            correct_bias=True\n","        )\n","        return optimizer\n","\n","def make_scheduler(\n","    args, optimizer, \n","    num_warmup_steps, \n","    num_training_steps\n","):\n","    if args.decay_name == \"cosine-warmup\":\n","        scheduler = get_cosine_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=num_warmup_steps,\n","            num_training_steps=num_training_steps\n","        )\n","    else:\n","        scheduler = get_linear_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=num_warmup_steps,\n","            num_training_steps=num_training_steps\n","        )\n","    return scheduler    \n","\n","def make_loader(\n","    args, data, \n","    tokenizer, fold\n","):\n","    train_set, valid_set = data[data['kfold']!=fold], data[data['kfold']==fold]\n","    \n","    train_features, valid_features = [[] for _ in range(2)]\n","    for i, row in train_set.iterrows():\n","        train_features += prepare_train_features(args, row, tokenizer)\n","    for i, row in valid_set.iterrows():\n","        valid_features += prepare_train_features(args, row, tokenizer)\n","\n","    train_dataset = DatasetRetriever(train_features)\n","    valid_dataset = DatasetRetriever(valid_features)\n","    print(f\"Num examples Train= {len(train_dataset)}, Num examples Valid={len(valid_dataset)}\")\n","    \n","    train_sampler = RandomSampler(train_dataset)\n","    valid_sampler = SequentialSampler(valid_dataset)\n","\n","    train_dataloader = DataLoader(\n","        train_dataset,\n","        batch_size=args.train_batch_size,\n","        sampler=train_sampler,\n","        num_workers=optimal_num_of_loader_workers(),\n","        pin_memory=True,\n","        drop_last=False \n","    )\n","\n","    valid_dataloader = DataLoader(\n","        valid_dataset,\n","        batch_size=args.eval_batch_size, \n","        sampler=valid_sampler,\n","        num_workers=optimal_num_of_loader_workers(),\n","        pin_memory=True, \n","        drop_last=False\n","    )\n","\n","    return train_dataloader, valid_dataloader"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i-zfYJGxP4iK"},"source":["### Trainer"]},{"cell_type":"code","metadata":{"id":"iFLvh1VQET35","execution":{"iopub.status.busy":"2021-08-17T20:26:36.310455Z","iopub.execute_input":"2021-08-17T20:26:36.310827Z","iopub.status.idle":"2021-08-17T20:26:36.325575Z","shell.execute_reply.started":"2021-08-17T20:26:36.310796Z","shell.execute_reply":"2021-08-17T20:26:36.324417Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1636447541205,"user_tz":-540,"elapsed":16,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["class Trainer:\n","    def __init__(\n","        self, model, tokenizer, \n","        optimizer, scheduler\n","    ):\n","        self.model = model\n","        self.tokenizer = tokenizer\n","        self.optimizer = optimizer\n","        self.scheduler = scheduler\n","\n","    def train(\n","        self, args, \n","        train_dataloader, \n","        epoch, result_dict\n","    ):\n","        count = 0\n","        losses = AverageMeter()\n","        \n","        self.model.zero_grad()\n","        self.model.train()\n","        \n","        fix_all_seeds(args.seed)\n","        \n","        for batch_idx, batch_data in enumerate(train_dataloader):\n","            input_ids, attention_mask, targets_start, targets_end = \\\n","                batch_data['input_ids'], batch_data['attention_mask'], \\\n","                    batch_data['start_position'], batch_data['end_position']\n","            \n","            input_ids, attention_mask, targets_start, targets_end = \\\n","                input_ids.cuda(), attention_mask.cuda(), targets_start.cuda(), targets_end.cuda()\n","\n","            outputs_start, outputs_end = self.model(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","            )\n","            \n","            loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n","            loss = loss / args.gradient_accumulation_steps\n","\n","            if args.fp16:\n","                with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n","                    scaled_loss.backward()\n","            else:\n","                loss.backward()\n","\n","            count += input_ids.size(0)\n","            losses.update(loss.item(), input_ids.size(0))\n","\n","            # if args.fp16:\n","            #     torch.nn.utils.clip_grad_norm_(amp.master_params(self.optimizer), args.max_grad_norm)\n","            # else:\n","            #     torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.max_grad_norm)\n","\n","            if batch_idx % args.gradient_accumulation_steps == 0 or batch_idx == len(train_dataloader) - 1:\n","                self.optimizer.step()\n","                self.scheduler.step()\n","                self.optimizer.zero_grad()\n","\n","            if (batch_idx % args.logging_steps == 0) or (batch_idx+1)==len(train_dataloader):\n","                _s = str(len(str(len(train_dataloader.sampler))))\n","                ret = [\n","                    ('Epoch: {:0>2} [{: >' + _s + '}/{} ({: >3.0f}%)]').format(epoch, count, len(train_dataloader.sampler), 100 * count / len(train_dataloader.sampler)),\n","                    'Train Loss: {: >4.5f}'.format(losses.avg),\n","                ]\n","                print(', '.join(ret))\n","\n","        result_dict['train_loss'].append(losses.avg)\n","        return result_dict"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7pmOF0dxP4iL"},"source":["### Evaluator"]},{"cell_type":"code","metadata":{"id":"1a8kG2UYET36","execution":{"iopub.status.busy":"2021-08-17T20:26:39.517516Z","iopub.execute_input":"2021-08-17T20:26:39.517886Z","iopub.status.idle":"2021-08-17T20:26:39.528591Z","shell.execute_reply.started":"2021-08-17T20:26:39.517856Z","shell.execute_reply":"2021-08-17T20:26:39.527569Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1636447541745,"user_tz":-540,"elapsed":555,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["class Evaluator:\n","    def __init__(self, model):\n","        self.model = model\n","    \n","    def save(self, result, output_dir):\n","        with open(f'{output_dir}/result_dict.json', 'w') as f:\n","            f.write(json.dumps(result, sort_keys=True, indent=4, ensure_ascii=False))\n","\n","    def evaluate(self, valid_dataloader, epoch, result_dict):\n","        losses = AverageMeter()\n","        for batch_idx, batch_data in enumerate(valid_dataloader):\n","            self.model = self.model.eval()\n","            input_ids, attention_mask, targets_start, targets_end = \\\n","                batch_data['input_ids'], batch_data['attention_mask'], \\\n","                    batch_data['start_position'], batch_data['end_position']\n","            \n","            input_ids, attention_mask, targets_start, targets_end = \\\n","                input_ids.cuda(), attention_mask.cuda(), targets_start.cuda(), targets_end.cuda()\n","            \n","            with torch.no_grad():            \n","                outputs_start, outputs_end = self.model(\n","                    input_ids=input_ids,\n","                    attention_mask=attention_mask,\n","                )\n","                \n","                loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n","                losses.update(loss.item(), input_ids.size(0))\n","                \n","        print('----Validation Results Summary----')\n","        print('Epoch: [{}] Valid Loss: {: >4.5f}'.format(epoch, losses.avg))\n","        result_dict['val_loss'].append(losses.avg)        \n","        return result_dict"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cAZPoRJeP4iM"},"source":["### Initialize Training"]},{"cell_type":"code","metadata":{"id":"v-gUDyq2ET37","execution":{"iopub.status.busy":"2021-08-17T20:26:44.41837Z","iopub.execute_input":"2021-08-17T20:26:44.418722Z","iopub.status.idle":"2021-08-17T20:26:44.428918Z","shell.execute_reply.started":"2021-08-17T20:26:44.418691Z","shell.execute_reply":"2021-08-17T20:26:44.428086Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1636447541752,"user_tz":-540,"elapsed":40,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["def init_training(args, data, fold):\n","    fix_all_seeds(args.seed)\n","    \n","    if not os.path.exists(args.output_dir):\n","        os.makedirs(args.output_dir)\n","    \n","    # model\n","    model_config, tokenizer, model = make_model(args)\n","    if torch.cuda.device_count() >= 1:\n","        print('Model pushed to {} GPU(s), type {}.'.format(\n","            torch.cuda.device_count(), \n","            torch.cuda.get_device_name(0))\n","        )\n","        model = model.cuda() \n","    else:\n","        raise ValueError('CPU training is not supported')\n","    \n","    # data loaders\n","    train_dataloader, valid_dataloader = make_loader(args, data, tokenizer, fold)\n","\n","    # optimizer\n","    optimizer = make_optimizer(args, model)\n","\n","    # scheduler\n","    num_training_steps = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps) * args.epochs\n","    if args.warmup_ratio > 0:\n","        num_warmup_steps = int(args.warmup_ratio * num_training_steps)\n","    else:\n","        num_warmup_steps = 0\n","    print(f\"Total Training Steps: {num_training_steps}, Total Warmup Steps: {num_warmup_steps}\")\n","    scheduler = make_scheduler(args, optimizer, num_warmup_steps, num_training_steps)\n","\n","    # mixed precision training with NVIDIA Apex\n","    if args.fp16:\n","        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n","    \n","    result_dict = {\n","        'epoch':[], \n","        'train_loss': [], \n","        'val_loss' : [], \n","        'best_val_loss': np.inf\n","    }\n","\n","    return (\n","        model, model_config, tokenizer, optimizer, scheduler, \n","        train_dataloader, valid_dataloader, result_dict\n","    )"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g6ZBX-5aP4iN"},"source":["### Run"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-12T15:50:27.097353Z","iopub.execute_input":"2021-08-12T15:50:27.097724Z","iopub.status.idle":"2021-08-12T15:50:27.112113Z","shell.execute_reply.started":"2021-08-12T15:50:27.097688Z","shell.execute_reply":"2021-08-12T15:50:27.111224Z"},"id":"39ei5Bm5ET37","trusted":true,"executionInfo":{"status":"ok","timestamp":1636447541757,"user_tz":-540,"elapsed":41,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["def run(data, fold):\n","    args = Config()\n","    model, model_config, tokenizer, optimizer, scheduler, train_dataloader, \\\n","        valid_dataloader, result_dict = init_training(args, data, fold)\n","    \n","    trainer = Trainer(model, tokenizer, optimizer, scheduler)\n","    evaluator = Evaluator(model)\n","\n","    train_time_list = []\n","    valid_time_list = []\n","\n","    for epoch in range(args.epochs):\n","        result_dict['epoch'].append(epoch)\n","\n","        # Train\n","        torch.cuda.synchronize()\n","        tic1 = time.time()\n","        result_dict = trainer.train(\n","            args, train_dataloader, \n","            epoch, result_dict\n","        )\n","        torch.cuda.synchronize()\n","        tic2 = time.time() \n","        train_time_list.append(tic2 - tic1)\n","        \n","        # Evaluate\n","        torch.cuda.synchronize()\n","        tic3 = time.time()\n","        result_dict = evaluator.evaluate(\n","            valid_dataloader, epoch, result_dict\n","        )\n","        torch.cuda.synchronize()\n","        tic4 = time.time() \n","        valid_time_list.append(tic4 - tic3)\n","            \n","        output_dir = os.path.join(args.output_dir, f\"checkpoint-fold-{fold}\")\n","        if result_dict['val_loss'][-1] < result_dict['best_val_loss']:\n","            print(\"{} Epoch, Best epoch was updated! Valid Loss: {: >4.5f}\".format(epoch, result_dict['val_loss'][-1]))\n","            result_dict[\"best_val_loss\"] = result_dict['val_loss'][-1]        \n","            \n","            os.makedirs(output_dir, exist_ok=True)\n","            torch.save(model.state_dict(), f\"{output_dir}/pytorch_model.bin\")\n","            model_config.save_pretrained(output_dir)\n","            tokenizer.save_pretrained(output_dir)\n","            print(f\"Saving model checkpoint to {output_dir}.\")\n","            \n","        print()\n","\n","    evaluator.save(result_dict, output_dir)\n","    \n","    print(f\"Total Training Time: {np.sum(train_time_list)}secs, Average Training Time per Epoch: {np.mean(train_time_list)}secs.\")\n","    print(f\"Total Validation Time: {np.sum(valid_time_list)}secs, Average Validation Time per Epoch: {np.mean(valid_time_list)}secs.\")\n","    \n","    torch.cuda.empty_cache()\n","    del trainer, evaluator\n","    del model, model_config, tokenizer\n","    del optimizer, scheduler\n","    del train_dataloader, valid_dataloader, result_dict\n","    gc.collect()"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"mPaGnnCnbhWl","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["f7add7603b3b4cf594e26b781e55cf3d","0dba04fa6db544318fcb642f7e76907f","154de523a5104cccadcaa5074c0753c7","7ca1ad84252b433d8a4e97b3620670a4","313d7f68275443bbb2381d97b786ba6d","2c32d9499d9447779b0c546b7a0c6937","0d4c17d3cba14ba189a19d7c7b7ad2cd","17611a5436a94173b3d2efa3415bdac8","f4dcf6a63bed475f9906302e0f0f664d","2fc55251e41d4c949301188572ac55e6","41b0d71fb3794f71b3e0271094eb9820","74b7ab5dd949454ab70b4429ddd03bb0","41d1b86842334c9783d7405310dbd726","2d1aef8068984609b916dbf0c603e478","7e71c81838e4449f8072c26ccd322963","f117b61c5df94a76af108eccefddc854","41c43ba898c64425892fa50074c91d68","6cebfbfcb1f745268e5227d6ba6e3f42","79e1b4e1c3bd4bcfae3811fd9ec27c73","04dcc7b9d9cb44749e16662c5a8cb760","b5b2b9b66c8244dc9d3f08970019c47d","42f019d174ed477daee0ad641b790f85","97f29b39cbc34e9b94691e981890a771","1e92d001e55a4dc78e2a2539bfa10d63","3f71b120e2f14ccc875c2c8006e2c939","a99b0e5041a14a909a707ce0e514a044","a4ca538694ed46be90fd57287a055470","8d9e98d253b343f7aec3a08d97008b69","786c29ea07ce4858aa20e0fe663c1f4f","7dc5d33615694f47ad4062699e84687d","a9a4127714184176aada1e4d5c99995f","132311f4e7934ecd8a635dc5ed2c3efd","30c75d2b75464d13a59263068609b998","cb1200447a524feeae324ba9f7b017f7","94d96330da9a441da50be5c202bcbb1c","bb994e569c04447e86aaf0eb7bd109de","8d327017e5284e9d9dc855695d5b656b","3cd0732b42f74b9384c18872d77ce735","c0b689ecb40643c5b4e4ec58c1211baf","1650b772654c44fd8b211947955ca5ef","c09437d82e694787bce3f784c76e3342","8715c597126c4af5825041e3dd173b4f","63b2837479604bc982381a0903ec37d3","a458816189484799b027b2fa1cf63cc8","7ad7571474974e1b86a247da0b2ee87b","f2af91eddacf446982d229dcf4889ffc","f5067ebd80ef45fe92ebc09c7c84c410","46b7c97299aa49d084fbcd6fb51a416c","010725bd0a324213938514274ff551a6","1ed22506645b416fb10a39a747cdf261","72307f12ddda453cb6fe138189732999","a0cfd4213ddc4207b69d43ae584dad89","b53ac1f3ffd644679b6c9d759d9fa443","9519c27f67ad44a6aab9f617c972f349","d527e0e6ccf5471faaee1b6b9fedbe91"]},"executionInfo":{"status":"ok","timestamp":1636458385701,"user_tz":-540,"elapsed":10843984,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"62f7a8e8-4e61-4ed6-8d82-a0c311dd10fd"},"source":["#for fold in range(1,6):\n","for fold in range(3,6):\n","    print();print()\n","    print('-'*50)\n","    print(f'FOLD: {fold}')\n","    print('-'*50)\n","    run(train, fold)"],"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","--------------------------------------------------\n","FOLD: 3\n","--------------------------------------------------\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f7add7603b3b4cf594e26b781e55cf3d","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/606 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"74b7ab5dd949454ab70b4429ddd03bb0","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/179 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"97f29b39cbc34e9b94691e981890a771","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/4.83M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cb1200447a524feeae324ba9f7b017f7","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/150 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7ad7571474974e1b86a247da0b2ee87b","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/2.09G [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n","Num examples Train= 20182, Num examples Valid=4607\n","Total Training Steps: 2523, Total Warmup Steps: 252\n","Epoch: 00 [    4/20182 (  0%)], Train Loss: 3.02616\n","Epoch: 00 [   44/20182 (  0%)], Train Loss: 2.95303\n","Epoch: 00 [   84/20182 (  0%)], Train Loss: 2.92909\n","Epoch: 00 [  124/20182 (  1%)], Train Loss: 2.89914\n","Epoch: 00 [  164/20182 (  1%)], Train Loss: 2.85519\n","Epoch: 00 [  204/20182 (  1%)], Train Loss: 2.80010\n","Epoch: 00 [  244/20182 (  1%)], Train Loss: 2.72541\n","Epoch: 00 [  284/20182 (  1%)], Train Loss: 2.62705\n","Epoch: 00 [  324/20182 (  2%)], Train Loss: 2.53465\n","Epoch: 00 [  364/20182 (  2%)], Train Loss: 2.41538\n","Epoch: 00 [  404/20182 (  2%)], Train Loss: 2.28731\n","Epoch: 00 [  444/20182 (  2%)], Train Loss: 2.14973\n","Epoch: 00 [  484/20182 (  2%)], Train Loss: 2.02713\n","Epoch: 00 [  524/20182 (  3%)], Train Loss: 1.90117\n","Epoch: 00 [  564/20182 (  3%)], Train Loss: 1.80912\n","Epoch: 00 [  604/20182 (  3%)], Train Loss: 1.72929\n","Epoch: 00 [  644/20182 (  3%)], Train Loss: 1.66025\n","Epoch: 00 [  684/20182 (  3%)], Train Loss: 1.60598\n","Epoch: 00 [  724/20182 (  4%)], Train Loss: 1.54208\n","Epoch: 00 [  764/20182 (  4%)], Train Loss: 1.47343\n","Epoch: 00 [  804/20182 (  4%)], Train Loss: 1.41882\n","Epoch: 00 [  844/20182 (  4%)], Train Loss: 1.37033\n","Epoch: 00 [  884/20182 (  4%)], Train Loss: 1.32038\n","Epoch: 00 [  924/20182 (  5%)], Train Loss: 1.27574\n","Epoch: 00 [  964/20182 (  5%)], Train Loss: 1.24919\n","Epoch: 00 [ 1004/20182 (  5%)], Train Loss: 1.21223\n","Epoch: 00 [ 1044/20182 (  5%)], Train Loss: 1.17169\n","Epoch: 00 [ 1084/20182 (  5%)], Train Loss: 1.14539\n","Epoch: 00 [ 1124/20182 (  6%)], Train Loss: 1.12406\n","Epoch: 00 [ 1164/20182 (  6%)], Train Loss: 1.10324\n","Epoch: 00 [ 1204/20182 (  6%)], Train Loss: 1.07916\n","Epoch: 00 [ 1244/20182 (  6%)], Train Loss: 1.05565\n","Epoch: 00 [ 1284/20182 (  6%)], Train Loss: 1.03282\n","Epoch: 00 [ 1324/20182 (  7%)], Train Loss: 1.01995\n","Epoch: 00 [ 1364/20182 (  7%)], Train Loss: 0.99805\n","Epoch: 00 [ 1404/20182 (  7%)], Train Loss: 0.98292\n","Epoch: 00 [ 1444/20182 (  7%)], Train Loss: 0.96636\n","Epoch: 00 [ 1484/20182 (  7%)], Train Loss: 0.95195\n","Epoch: 00 [ 1524/20182 (  8%)], Train Loss: 0.93620\n","Epoch: 00 [ 1564/20182 (  8%)], Train Loss: 0.91893\n","Epoch: 00 [ 1604/20182 (  8%)], Train Loss: 0.90761\n","Epoch: 00 [ 1644/20182 (  8%)], Train Loss: 0.89531\n","Epoch: 00 [ 1684/20182 (  8%)], Train Loss: 0.88304\n","Epoch: 00 [ 1724/20182 (  9%)], Train Loss: 0.87372\n","Epoch: 00 [ 1764/20182 (  9%)], Train Loss: 0.86119\n","Epoch: 00 [ 1804/20182 (  9%)], Train Loss: 0.84988\n","Epoch: 00 [ 1844/20182 (  9%)], Train Loss: 0.84147\n","Epoch: 00 [ 1884/20182 (  9%)], Train Loss: 0.83089\n","Epoch: 00 [ 1924/20182 ( 10%)], Train Loss: 0.82403\n","Epoch: 00 [ 1964/20182 ( 10%)], Train Loss: 0.81238\n","Epoch: 00 [ 2004/20182 ( 10%)], Train Loss: 0.80127\n","Epoch: 00 [ 2044/20182 ( 10%)], Train Loss: 0.79213\n","Epoch: 00 [ 2084/20182 ( 10%)], Train Loss: 0.78243\n","Epoch: 00 [ 2124/20182 ( 11%)], Train Loss: 0.76972\n","Epoch: 00 [ 2164/20182 ( 11%)], Train Loss: 0.76032\n","Epoch: 00 [ 2204/20182 ( 11%)], Train Loss: 0.75484\n","Epoch: 00 [ 2244/20182 ( 11%)], Train Loss: 0.75074\n","Epoch: 00 [ 2284/20182 ( 11%)], Train Loss: 0.74691\n","Epoch: 00 [ 2324/20182 ( 12%)], Train Loss: 0.74207\n","Epoch: 00 [ 2364/20182 ( 12%)], Train Loss: 0.73560\n","Epoch: 00 [ 2404/20182 ( 12%)], Train Loss: 0.72773\n","Epoch: 00 [ 2444/20182 ( 12%)], Train Loss: 0.72064\n","Epoch: 00 [ 2484/20182 ( 12%)], Train Loss: 0.72024\n","Epoch: 00 [ 2524/20182 ( 13%)], Train Loss: 0.71799\n","Epoch: 00 [ 2564/20182 ( 13%)], Train Loss: 0.71150\n","Epoch: 00 [ 2604/20182 ( 13%)], Train Loss: 0.70460\n","Epoch: 00 [ 2644/20182 ( 13%)], Train Loss: 0.69623\n","Epoch: 00 [ 2684/20182 ( 13%)], Train Loss: 0.68973\n","Epoch: 00 [ 2724/20182 ( 13%)], Train Loss: 0.68670\n","Epoch: 00 [ 2764/20182 ( 14%)], Train Loss: 0.68407\n","Epoch: 00 [ 2804/20182 ( 14%)], Train Loss: 0.67854\n","Epoch: 00 [ 2844/20182 ( 14%)], Train Loss: 0.67404\n","Epoch: 00 [ 2884/20182 ( 14%)], Train Loss: 0.67094\n","Epoch: 00 [ 2924/20182 ( 14%)], Train Loss: 0.66783\n","Epoch: 00 [ 2964/20182 ( 15%)], Train Loss: 0.66325\n","Epoch: 00 [ 3004/20182 ( 15%)], Train Loss: 0.65957\n","Epoch: 00 [ 3044/20182 ( 15%)], Train Loss: 0.65843\n","Epoch: 00 [ 3084/20182 ( 15%)], Train Loss: 0.65535\n","Epoch: 00 [ 3124/20182 ( 15%)], Train Loss: 0.65149\n","Epoch: 00 [ 3164/20182 ( 16%)], Train Loss: 0.64686\n","Epoch: 00 [ 3204/20182 ( 16%)], Train Loss: 0.64266\n","Epoch: 00 [ 3244/20182 ( 16%)], Train Loss: 0.63868\n","Epoch: 00 [ 3284/20182 ( 16%)], Train Loss: 0.63655\n","Epoch: 00 [ 3324/20182 ( 16%)], Train Loss: 0.63358\n","Epoch: 00 [ 3364/20182 ( 17%)], Train Loss: 0.62972\n","Epoch: 00 [ 3404/20182 ( 17%)], Train Loss: 0.62647\n","Epoch: 00 [ 3444/20182 ( 17%)], Train Loss: 0.62352\n","Epoch: 00 [ 3484/20182 ( 17%)], Train Loss: 0.61948\n","Epoch: 00 [ 3524/20182 ( 17%)], Train Loss: 0.61671\n","Epoch: 00 [ 3564/20182 ( 18%)], Train Loss: 0.61499\n","Epoch: 00 [ 3604/20182 ( 18%)], Train Loss: 0.61338\n","Epoch: 00 [ 3644/20182 ( 18%)], Train Loss: 0.61236\n","Epoch: 00 [ 3684/20182 ( 18%)], Train Loss: 0.61239\n","Epoch: 00 [ 3724/20182 ( 18%)], Train Loss: 0.61080\n","Epoch: 00 [ 3764/20182 ( 19%)], Train Loss: 0.60843\n","Epoch: 00 [ 3804/20182 ( 19%)], Train Loss: 0.60643\n","Epoch: 00 [ 3844/20182 ( 19%)], Train Loss: 0.60335\n","Epoch: 00 [ 3884/20182 ( 19%)], Train Loss: 0.59999\n","Epoch: 00 [ 3924/20182 ( 19%)], Train Loss: 0.59665\n","Epoch: 00 [ 3964/20182 ( 20%)], Train Loss: 0.59247\n","Epoch: 00 [ 4004/20182 ( 20%)], Train Loss: 0.59050\n","Epoch: 00 [ 4044/20182 ( 20%)], Train Loss: 0.59007\n","Epoch: 00 [ 4084/20182 ( 20%)], Train Loss: 0.58893\n","Epoch: 00 [ 4124/20182 ( 20%)], Train Loss: 0.58692\n","Epoch: 00 [ 4164/20182 ( 21%)], Train Loss: 0.58713\n","Epoch: 00 [ 4204/20182 ( 21%)], Train Loss: 0.58481\n","Epoch: 00 [ 4244/20182 ( 21%)], Train Loss: 0.58410\n","Epoch: 00 [ 4284/20182 ( 21%)], Train Loss: 0.58106\n","Epoch: 00 [ 4324/20182 ( 21%)], Train Loss: 0.57867\n","Epoch: 00 [ 4364/20182 ( 22%)], Train Loss: 0.57536\n","Epoch: 00 [ 4404/20182 ( 22%)], Train Loss: 0.57265\n","Epoch: 00 [ 4444/20182 ( 22%)], Train Loss: 0.57013\n","Epoch: 00 [ 4484/20182 ( 22%)], Train Loss: 0.57003\n","Epoch: 00 [ 4524/20182 ( 22%)], Train Loss: 0.56716\n","Epoch: 00 [ 4564/20182 ( 23%)], Train Loss: 0.56342\n","Epoch: 00 [ 4604/20182 ( 23%)], Train Loss: 0.56115\n","Epoch: 00 [ 4644/20182 ( 23%)], Train Loss: 0.55864\n","Epoch: 00 [ 4684/20182 ( 23%)], Train Loss: 0.55760\n","Epoch: 00 [ 4724/20182 ( 23%)], Train Loss: 0.55801\n","Epoch: 00 [ 4764/20182 ( 24%)], Train Loss: 0.55465\n","Epoch: 00 [ 4804/20182 ( 24%)], Train Loss: 0.55476\n","Epoch: 00 [ 4844/20182 ( 24%)], Train Loss: 0.55406\n","Epoch: 00 [ 4884/20182 ( 24%)], Train Loss: 0.55257\n","Epoch: 00 [ 4924/20182 ( 24%)], Train Loss: 0.55267\n","Epoch: 00 [ 4964/20182 ( 25%)], Train Loss: 0.55280\n","Epoch: 00 [ 5004/20182 ( 25%)], Train Loss: 0.55261\n","Epoch: 00 [ 5044/20182 ( 25%)], Train Loss: 0.55019\n","Epoch: 00 [ 5084/20182 ( 25%)], Train Loss: 0.55081\n","Epoch: 00 [ 5124/20182 ( 25%)], Train Loss: 0.54957\n","Epoch: 00 [ 5164/20182 ( 26%)], Train Loss: 0.54896\n","Epoch: 00 [ 5204/20182 ( 26%)], Train Loss: 0.54691\n","Epoch: 00 [ 5244/20182 ( 26%)], Train Loss: 0.54637\n","Epoch: 00 [ 5284/20182 ( 26%)], Train Loss: 0.54332\n","Epoch: 00 [ 5324/20182 ( 26%)], Train Loss: 0.54153\n","Epoch: 00 [ 5364/20182 ( 27%)], Train Loss: 0.54041\n","Epoch: 00 [ 5404/20182 ( 27%)], Train Loss: 0.53814\n","Epoch: 00 [ 5444/20182 ( 27%)], Train Loss: 0.53695\n","Epoch: 00 [ 5484/20182 ( 27%)], Train Loss: 0.53419\n","Epoch: 00 [ 5524/20182 ( 27%)], Train Loss: 0.53217\n","Epoch: 00 [ 5564/20182 ( 28%)], Train Loss: 0.53100\n","Epoch: 00 [ 5604/20182 ( 28%)], Train Loss: 0.52973\n","Epoch: 00 [ 5644/20182 ( 28%)], Train Loss: 0.52932\n","Epoch: 00 [ 5684/20182 ( 28%)], Train Loss: 0.52796\n","Epoch: 00 [ 5724/20182 ( 28%)], Train Loss: 0.52779\n","Epoch: 00 [ 5764/20182 ( 29%)], Train Loss: 0.52757\n","Epoch: 00 [ 5804/20182 ( 29%)], Train Loss: 0.52643\n","Epoch: 00 [ 5844/20182 ( 29%)], Train Loss: 0.52468\n","Epoch: 00 [ 5884/20182 ( 29%)], Train Loss: 0.52331\n","Epoch: 00 [ 5924/20182 ( 29%)], Train Loss: 0.52212\n","Epoch: 00 [ 5964/20182 ( 30%)], Train Loss: 0.52003\n","Epoch: 00 [ 6004/20182 ( 30%)], Train Loss: 0.51825\n","Epoch: 00 [ 6044/20182 ( 30%)], Train Loss: 0.51719\n","Epoch: 00 [ 6084/20182 ( 30%)], Train Loss: 0.51706\n","Epoch: 00 [ 6124/20182 ( 30%)], Train Loss: 0.51503\n","Epoch: 00 [ 6164/20182 ( 31%)], Train Loss: 0.51510\n","Epoch: 00 [ 6204/20182 ( 31%)], Train Loss: 0.51408\n","Epoch: 00 [ 6244/20182 ( 31%)], Train Loss: 0.51283\n","Epoch: 00 [ 6284/20182 ( 31%)], Train Loss: 0.51212\n","Epoch: 00 [ 6324/20182 ( 31%)], Train Loss: 0.51027\n","Epoch: 00 [ 6364/20182 ( 32%)], Train Loss: 0.50845\n","Epoch: 00 [ 6404/20182 ( 32%)], Train Loss: 0.50645\n","Epoch: 00 [ 6444/20182 ( 32%)], Train Loss: 0.50505\n","Epoch: 00 [ 6484/20182 ( 32%)], Train Loss: 0.50319\n","Epoch: 00 [ 6524/20182 ( 32%)], Train Loss: 0.50197\n","Epoch: 00 [ 6564/20182 ( 33%)], Train Loss: 0.50088\n","Epoch: 00 [ 6604/20182 ( 33%)], Train Loss: 0.50069\n","Epoch: 00 [ 6644/20182 ( 33%)], Train Loss: 0.49918\n","Epoch: 00 [ 6684/20182 ( 33%)], Train Loss: 0.49768\n","Epoch: 00 [ 6724/20182 ( 33%)], Train Loss: 0.49812\n","Epoch: 00 [ 6764/20182 ( 34%)], Train Loss: 0.49625\n","Epoch: 00 [ 6804/20182 ( 34%)], Train Loss: 0.49596\n","Epoch: 00 [ 6844/20182 ( 34%)], Train Loss: 0.49517\n","Epoch: 00 [ 6884/20182 ( 34%)], Train Loss: 0.49408\n","Epoch: 00 [ 6924/20182 ( 34%)], Train Loss: 0.49398\n","Epoch: 00 [ 6964/20182 ( 35%)], Train Loss: 0.49266\n","Epoch: 00 [ 7004/20182 ( 35%)], Train Loss: 0.49158\n","Epoch: 00 [ 7044/20182 ( 35%)], Train Loss: 0.49080\n","Epoch: 00 [ 7084/20182 ( 35%)], Train Loss: 0.48949\n","Epoch: 00 [ 7124/20182 ( 35%)], Train Loss: 0.48798\n","Epoch: 00 [ 7164/20182 ( 35%)], Train Loss: 0.48713\n","Epoch: 00 [ 7204/20182 ( 36%)], Train Loss: 0.48699\n","Epoch: 00 [ 7244/20182 ( 36%)], Train Loss: 0.48709\n","Epoch: 00 [ 7284/20182 ( 36%)], Train Loss: 0.48623\n","Epoch: 00 [ 7324/20182 ( 36%)], Train Loss: 0.48572\n","Epoch: 00 [ 7364/20182 ( 36%)], Train Loss: 0.48482\n","Epoch: 00 [ 7404/20182 ( 37%)], Train Loss: 0.48383\n","Epoch: 00 [ 7444/20182 ( 37%)], Train Loss: 0.48228\n","Epoch: 00 [ 7484/20182 ( 37%)], Train Loss: 0.48152\n","Epoch: 00 [ 7524/20182 ( 37%)], Train Loss: 0.48227\n","Epoch: 00 [ 7564/20182 ( 37%)], Train Loss: 0.48086\n","Epoch: 00 [ 7604/20182 ( 38%)], Train Loss: 0.47939\n","Epoch: 00 [ 7644/20182 ( 38%)], Train Loss: 0.47831\n","Epoch: 00 [ 7684/20182 ( 38%)], Train Loss: 0.47726\n","Epoch: 00 [ 7724/20182 ( 38%)], Train Loss: 0.47617\n","Epoch: 00 [ 7764/20182 ( 38%)], Train Loss: 0.47482\n","Epoch: 00 [ 7804/20182 ( 39%)], Train Loss: 0.47402\n","Epoch: 00 [ 7844/20182 ( 39%)], Train Loss: 0.47226\n","Epoch: 00 [ 7884/20182 ( 39%)], Train Loss: 0.47186\n","Epoch: 00 [ 7924/20182 ( 39%)], Train Loss: 0.47102\n","Epoch: 00 [ 7964/20182 ( 39%)], Train Loss: 0.46984\n","Epoch: 00 [ 8004/20182 ( 40%)], Train Loss: 0.46905\n","Epoch: 00 [ 8044/20182 ( 40%)], Train Loss: 0.46909\n","Epoch: 00 [ 8084/20182 ( 40%)], Train Loss: 0.46770\n","Epoch: 00 [ 8124/20182 ( 40%)], Train Loss: 0.46702\n","Epoch: 00 [ 8164/20182 ( 40%)], Train Loss: 0.46792\n","Epoch: 00 [ 8204/20182 ( 41%)], Train Loss: 0.46752\n","Epoch: 00 [ 8244/20182 ( 41%)], Train Loss: 0.46710\n","Epoch: 00 [ 8284/20182 ( 41%)], Train Loss: 0.46619\n","Epoch: 00 [ 8324/20182 ( 41%)], Train Loss: 0.46618\n","Epoch: 00 [ 8364/20182 ( 41%)], Train Loss: 0.46595\n","Epoch: 00 [ 8404/20182 ( 42%)], Train Loss: 0.46543\n","Epoch: 00 [ 8444/20182 ( 42%)], Train Loss: 0.46544\n","Epoch: 00 [ 8484/20182 ( 42%)], Train Loss: 0.46395\n","Epoch: 00 [ 8524/20182 ( 42%)], Train Loss: 0.46332\n","Epoch: 00 [ 8564/20182 ( 42%)], Train Loss: 0.46203\n","Epoch: 00 [ 8604/20182 ( 43%)], Train Loss: 0.46246\n","Epoch: 00 [ 8644/20182 ( 43%)], Train Loss: 0.46268\n","Epoch: 00 [ 8684/20182 ( 43%)], Train Loss: 0.46188\n","Epoch: 00 [ 8724/20182 ( 43%)], Train Loss: 0.46114\n","Epoch: 00 [ 8764/20182 ( 43%)], Train Loss: 0.46008\n","Epoch: 00 [ 8804/20182 ( 44%)], Train Loss: 0.45926\n","Epoch: 00 [ 8844/20182 ( 44%)], Train Loss: 0.45771\n","Epoch: 00 [ 8884/20182 ( 44%)], Train Loss: 0.45755\n","Epoch: 00 [ 8924/20182 ( 44%)], Train Loss: 0.45737\n","Epoch: 00 [ 8964/20182 ( 44%)], Train Loss: 0.45660\n","Epoch: 00 [ 9004/20182 ( 45%)], Train Loss: 0.45580\n","Epoch: 00 [ 9044/20182 ( 45%)], Train Loss: 0.45500\n","Epoch: 00 [ 9084/20182 ( 45%)], Train Loss: 0.45407\n","Epoch: 00 [ 9124/20182 ( 45%)], Train Loss: 0.45415\n","Epoch: 00 [ 9164/20182 ( 45%)], Train Loss: 0.45345\n","Epoch: 00 [ 9204/20182 ( 46%)], Train Loss: 0.45252\n","Epoch: 00 [ 9244/20182 ( 46%)], Train Loss: 0.45135\n","Epoch: 00 [ 9284/20182 ( 46%)], Train Loss: 0.45196\n","Epoch: 00 [ 9324/20182 ( 46%)], Train Loss: 0.45246\n","Epoch: 00 [ 9364/20182 ( 46%)], Train Loss: 0.45158\n","Epoch: 00 [ 9404/20182 ( 47%)], Train Loss: 0.45194\n","Epoch: 00 [ 9444/20182 ( 47%)], Train Loss: 0.45182\n","Epoch: 00 [ 9484/20182 ( 47%)], Train Loss: 0.45117\n","Epoch: 00 [ 9524/20182 ( 47%)], Train Loss: 0.45145\n","Epoch: 00 [ 9564/20182 ( 47%)], Train Loss: 0.45094\n","Epoch: 00 [ 9604/20182 ( 48%)], Train Loss: 0.45058\n","Epoch: 00 [ 9644/20182 ( 48%)], Train Loss: 0.45053\n","Epoch: 00 [ 9684/20182 ( 48%)], Train Loss: 0.45073\n","Epoch: 00 [ 9724/20182 ( 48%)], Train Loss: 0.45033\n","Epoch: 00 [ 9764/20182 ( 48%)], Train Loss: 0.44900\n","Epoch: 00 [ 9804/20182 ( 49%)], Train Loss: 0.44776\n","Epoch: 00 [ 9844/20182 ( 49%)], Train Loss: 0.44741\n","Epoch: 00 [ 9884/20182 ( 49%)], Train Loss: 0.44831\n","Epoch: 00 [ 9924/20182 ( 49%)], Train Loss: 0.44769\n","Epoch: 00 [ 9964/20182 ( 49%)], Train Loss: 0.44714\n","Epoch: 00 [10004/20182 ( 50%)], Train Loss: 0.44643\n","Epoch: 00 [10044/20182 ( 50%)], Train Loss: 0.44514\n","Epoch: 00 [10084/20182 ( 50%)], Train Loss: 0.44466\n","Epoch: 00 [10124/20182 ( 50%)], Train Loss: 0.44369\n","Epoch: 00 [10164/20182 ( 50%)], Train Loss: 0.44395\n","Epoch: 00 [10204/20182 ( 51%)], Train Loss: 0.44317\n","Epoch: 00 [10244/20182 ( 51%)], Train Loss: 0.44254\n","Epoch: 00 [10284/20182 ( 51%)], Train Loss: 0.44186\n","Epoch: 00 [10324/20182 ( 51%)], Train Loss: 0.44162\n","Epoch: 00 [10364/20182 ( 51%)], Train Loss: 0.44125\n","Epoch: 00 [10404/20182 ( 52%)], Train Loss: 0.44041\n","Epoch: 00 [10444/20182 ( 52%)], Train Loss: 0.44019\n","Epoch: 00 [10484/20182 ( 52%)], Train Loss: 0.43955\n","Epoch: 00 [10524/20182 ( 52%)], Train Loss: 0.43878\n","Epoch: 00 [10564/20182 ( 52%)], Train Loss: 0.43876\n","Epoch: 00 [10604/20182 ( 53%)], Train Loss: 0.43830\n","Epoch: 00 [10644/20182 ( 53%)], Train Loss: 0.43775\n","Epoch: 00 [10684/20182 ( 53%)], Train Loss: 0.43756\n","Epoch: 00 [10724/20182 ( 53%)], Train Loss: 0.43742\n","Epoch: 00 [10764/20182 ( 53%)], Train Loss: 0.43747\n","Epoch: 00 [10804/20182 ( 54%)], Train Loss: 0.43664\n","Epoch: 00 [10844/20182 ( 54%)], Train Loss: 0.43603\n","Epoch: 00 [10884/20182 ( 54%)], Train Loss: 0.43599\n","Epoch: 00 [10924/20182 ( 54%)], Train Loss: 0.43545\n","Epoch: 00 [10964/20182 ( 54%)], Train Loss: 0.43509\n","Epoch: 00 [11004/20182 ( 55%)], Train Loss: 0.43427\n","Epoch: 00 [11044/20182 ( 55%)], Train Loss: 0.43425\n","Epoch: 00 [11084/20182 ( 55%)], Train Loss: 0.43336\n","Epoch: 00 [11124/20182 ( 55%)], Train Loss: 0.43284\n","Epoch: 00 [11164/20182 ( 55%)], Train Loss: 0.43264\n","Epoch: 00 [11204/20182 ( 56%)], Train Loss: 0.43229\n","Epoch: 00 [11244/20182 ( 56%)], Train Loss: 0.43167\n","Epoch: 00 [11284/20182 ( 56%)], Train Loss: 0.43107\n","Epoch: 00 [11324/20182 ( 56%)], Train Loss: 0.43107\n","Epoch: 00 [11364/20182 ( 56%)], Train Loss: 0.43063\n","Epoch: 00 [11404/20182 ( 57%)], Train Loss: 0.43005\n","Epoch: 00 [11444/20182 ( 57%)], Train Loss: 0.42969\n","Epoch: 00 [11484/20182 ( 57%)], Train Loss: 0.42899\n","Epoch: 00 [11524/20182 ( 57%)], Train Loss: 0.42812\n","Epoch: 00 [11564/20182 ( 57%)], Train Loss: 0.42809\n","Epoch: 00 [11604/20182 ( 57%)], Train Loss: 0.42722\n","Epoch: 00 [11644/20182 ( 58%)], Train Loss: 0.42655\n","Epoch: 00 [11684/20182 ( 58%)], Train Loss: 0.42652\n","Epoch: 00 [11724/20182 ( 58%)], Train Loss: 0.42551\n","Epoch: 00 [11764/20182 ( 58%)], Train Loss: 0.42475\n","Epoch: 00 [11804/20182 ( 58%)], Train Loss: 0.42403\n","Epoch: 00 [11844/20182 ( 59%)], Train Loss: 0.42416\n","Epoch: 00 [11884/20182 ( 59%)], Train Loss: 0.42394\n","Epoch: 00 [11924/20182 ( 59%)], Train Loss: 0.42356\n","Epoch: 00 [11964/20182 ( 59%)], Train Loss: 0.42314\n","Epoch: 00 [12004/20182 ( 59%)], Train Loss: 0.42249\n","Epoch: 00 [12044/20182 ( 60%)], Train Loss: 0.42196\n","Epoch: 00 [12084/20182 ( 60%)], Train Loss: 0.42121\n","Epoch: 00 [12124/20182 ( 60%)], Train Loss: 0.42018\n","Epoch: 00 [12164/20182 ( 60%)], Train Loss: 0.42013\n","Epoch: 00 [12204/20182 ( 60%)], Train Loss: 0.41918\n","Epoch: 00 [12244/20182 ( 61%)], Train Loss: 0.41865\n","Epoch: 00 [12284/20182 ( 61%)], Train Loss: 0.41851\n","Epoch: 00 [12324/20182 ( 61%)], Train Loss: 0.41819\n","Epoch: 00 [12364/20182 ( 61%)], Train Loss: 0.41784\n","Epoch: 00 [12404/20182 ( 61%)], Train Loss: 0.41735\n","Epoch: 00 [12444/20182 ( 62%)], Train Loss: 0.41736\n","Epoch: 00 [12484/20182 ( 62%)], Train Loss: 0.41719\n","Epoch: 00 [12524/20182 ( 62%)], Train Loss: 0.41729\n","Epoch: 00 [12564/20182 ( 62%)], Train Loss: 0.41677\n","Epoch: 00 [12604/20182 ( 62%)], Train Loss: 0.41625\n","Epoch: 00 [12644/20182 ( 63%)], Train Loss: 0.41608\n","Epoch: 00 [12684/20182 ( 63%)], Train Loss: 0.41567\n","Epoch: 00 [12724/20182 ( 63%)], Train Loss: 0.41548\n","Epoch: 00 [12764/20182 ( 63%)], Train Loss: 0.41453\n","Epoch: 00 [12804/20182 ( 63%)], Train Loss: 0.41506\n","Epoch: 00 [12844/20182 ( 64%)], Train Loss: 0.41480\n","Epoch: 00 [12884/20182 ( 64%)], Train Loss: 0.41452\n","Epoch: 00 [12924/20182 ( 64%)], Train Loss: 0.41430\n","Epoch: 00 [12964/20182 ( 64%)], Train Loss: 0.41442\n","Epoch: 00 [13004/20182 ( 64%)], Train Loss: 0.41406\n","Epoch: 00 [13044/20182 ( 65%)], Train Loss: 0.41342\n","Epoch: 00 [13084/20182 ( 65%)], Train Loss: 0.41322\n","Epoch: 00 [13124/20182 ( 65%)], Train Loss: 0.41272\n","Epoch: 00 [13164/20182 ( 65%)], Train Loss: 0.41306\n","Epoch: 00 [13204/20182 ( 65%)], Train Loss: 0.41231\n","Epoch: 00 [13244/20182 ( 66%)], Train Loss: 0.41205\n","Epoch: 00 [13284/20182 ( 66%)], Train Loss: 0.41207\n","Epoch: 00 [13324/20182 ( 66%)], Train Loss: 0.41199\n","Epoch: 00 [13364/20182 ( 66%)], Train Loss: 0.41206\n","Epoch: 00 [13404/20182 ( 66%)], Train Loss: 0.41191\n","Epoch: 00 [13444/20182 ( 67%)], Train Loss: 0.41134\n","Epoch: 00 [13484/20182 ( 67%)], Train Loss: 0.41121\n","Epoch: 00 [13524/20182 ( 67%)], Train Loss: 0.41143\n","Epoch: 00 [13564/20182 ( 67%)], Train Loss: 0.41134\n","Epoch: 00 [13604/20182 ( 67%)], Train Loss: 0.41056\n","Epoch: 00 [13644/20182 ( 68%)], Train Loss: 0.41022\n","Epoch: 00 [13684/20182 ( 68%)], Train Loss: 0.40968\n","Epoch: 00 [13724/20182 ( 68%)], Train Loss: 0.40978\n","Epoch: 00 [13764/20182 ( 68%)], Train Loss: 0.40905\n","Epoch: 00 [13804/20182 ( 68%)], Train Loss: 0.40884\n","Epoch: 00 [13844/20182 ( 69%)], Train Loss: 0.40833\n","Epoch: 00 [13884/20182 ( 69%)], Train Loss: 0.40845\n","Epoch: 00 [13924/20182 ( 69%)], Train Loss: 0.40857\n","Epoch: 00 [13964/20182 ( 69%)], Train Loss: 0.40871\n","Epoch: 00 [14004/20182 ( 69%)], Train Loss: 0.40872\n","Epoch: 00 [14044/20182 ( 70%)], Train Loss: 0.40872\n","Epoch: 00 [14084/20182 ( 70%)], Train Loss: 0.40837\n","Epoch: 00 [14124/20182 ( 70%)], Train Loss: 0.40760\n","Epoch: 00 [14164/20182 ( 70%)], Train Loss: 0.40750\n","Epoch: 00 [14204/20182 ( 70%)], Train Loss: 0.40735\n","Epoch: 00 [14244/20182 ( 71%)], Train Loss: 0.40745\n","Epoch: 00 [14284/20182 ( 71%)], Train Loss: 0.40702\n","Epoch: 00 [14324/20182 ( 71%)], Train Loss: 0.40690\n","Epoch: 00 [14364/20182 ( 71%)], Train Loss: 0.40678\n","Epoch: 00 [14404/20182 ( 71%)], Train Loss: 0.40631\n","Epoch: 00 [14444/20182 ( 72%)], Train Loss: 0.40587\n","Epoch: 00 [14484/20182 ( 72%)], Train Loss: 0.40509\n","Epoch: 00 [14524/20182 ( 72%)], Train Loss: 0.40468\n","Epoch: 00 [14564/20182 ( 72%)], Train Loss: 0.40432\n","Epoch: 00 [14604/20182 ( 72%)], Train Loss: 0.40348\n","Epoch: 00 [14644/20182 ( 73%)], Train Loss: 0.40306\n","Epoch: 00 [14684/20182 ( 73%)], Train Loss: 0.40249\n","Epoch: 00 [14724/20182 ( 73%)], Train Loss: 0.40196\n","Epoch: 00 [14764/20182 ( 73%)], Train Loss: 0.40164\n","Epoch: 00 [14804/20182 ( 73%)], Train Loss: 0.40168\n","Epoch: 00 [14844/20182 ( 74%)], Train Loss: 0.40156\n","Epoch: 00 [14884/20182 ( 74%)], Train Loss: 0.40143\n","Epoch: 00 [14924/20182 ( 74%)], Train Loss: 0.40112\n","Epoch: 00 [14964/20182 ( 74%)], Train Loss: 0.40083\n","Epoch: 00 [15004/20182 ( 74%)], Train Loss: 0.40059\n","Epoch: 00 [15044/20182 ( 75%)], Train Loss: 0.40017\n","Epoch: 00 [15084/20182 ( 75%)], Train Loss: 0.40026\n","Epoch: 00 [15124/20182 ( 75%)], Train Loss: 0.40043\n","Epoch: 00 [15164/20182 ( 75%)], Train Loss: 0.39997\n","Epoch: 00 [15204/20182 ( 75%)], Train Loss: 0.39965\n","Epoch: 00 [15244/20182 ( 76%)], Train Loss: 0.39949\n","Epoch: 00 [15284/20182 ( 76%)], Train Loss: 0.39904\n","Epoch: 00 [15324/20182 ( 76%)], Train Loss: 0.39890\n","Epoch: 00 [15364/20182 ( 76%)], Train Loss: 0.39855\n","Epoch: 00 [15404/20182 ( 76%)], Train Loss: 0.39834\n","Epoch: 00 [15444/20182 ( 77%)], Train Loss: 0.39774\n","Epoch: 00 [15484/20182 ( 77%)], Train Loss: 0.39772\n","Epoch: 00 [15524/20182 ( 77%)], Train Loss: 0.39701\n","Epoch: 00 [15564/20182 ( 77%)], Train Loss: 0.39704\n","Epoch: 00 [15604/20182 ( 77%)], Train Loss: 0.39666\n","Epoch: 00 [15644/20182 ( 78%)], Train Loss: 0.39645\n","Epoch: 00 [15684/20182 ( 78%)], Train Loss: 0.39604\n","Epoch: 00 [15724/20182 ( 78%)], Train Loss: 0.39594\n","Epoch: 00 [15764/20182 ( 78%)], Train Loss: 0.39556\n","Epoch: 00 [15804/20182 ( 78%)], Train Loss: 0.39556\n","Epoch: 00 [15844/20182 ( 79%)], Train Loss: 0.39537\n","Epoch: 00 [15884/20182 ( 79%)], Train Loss: 0.39506\n","Epoch: 00 [15924/20182 ( 79%)], Train Loss: 0.39500\n","Epoch: 00 [15964/20182 ( 79%)], Train Loss: 0.39480\n","Epoch: 00 [16004/20182 ( 79%)], Train Loss: 0.39452\n","Epoch: 00 [16044/20182 ( 79%)], Train Loss: 0.39420\n","Epoch: 00 [16084/20182 ( 80%)], Train Loss: 0.39379\n","Epoch: 00 [16124/20182 ( 80%)], Train Loss: 0.39340\n","Epoch: 00 [16164/20182 ( 80%)], Train Loss: 0.39333\n","Epoch: 00 [16204/20182 ( 80%)], Train Loss: 0.39288\n","Epoch: 00 [16244/20182 ( 80%)], Train Loss: 0.39238\n","Epoch: 00 [16284/20182 ( 81%)], Train Loss: 0.39228\n","Epoch: 00 [16324/20182 ( 81%)], Train Loss: 0.39163\n","Epoch: 00 [16364/20182 ( 81%)], Train Loss: 0.39205\n","Epoch: 00 [16404/20182 ( 81%)], Train Loss: 0.39168\n","Epoch: 00 [16444/20182 ( 81%)], Train Loss: 0.39143\n","Epoch: 00 [16484/20182 ( 82%)], Train Loss: 0.39120\n","Epoch: 00 [16524/20182 ( 82%)], Train Loss: 0.39118\n","Epoch: 00 [16564/20182 ( 82%)], Train Loss: 0.39082\n","Epoch: 00 [16604/20182 ( 82%)], Train Loss: 0.39073\n","Epoch: 00 [16644/20182 ( 82%)], Train Loss: 0.39032\n","Epoch: 00 [16684/20182 ( 83%)], Train Loss: 0.38985\n","Epoch: 00 [16724/20182 ( 83%)], Train Loss: 0.38985\n","Epoch: 00 [16764/20182 ( 83%)], Train Loss: 0.38979\n","Epoch: 00 [16804/20182 ( 83%)], Train Loss: 0.38968\n","Epoch: 00 [16844/20182 ( 83%)], Train Loss: 0.38929\n","Epoch: 00 [16884/20182 ( 84%)], Train Loss: 0.38912\n","Epoch: 00 [16924/20182 ( 84%)], Train Loss: 0.38916\n","Epoch: 00 [16964/20182 ( 84%)], Train Loss: 0.38857\n","Epoch: 00 [17004/20182 ( 84%)], Train Loss: 0.38835\n","Epoch: 00 [17044/20182 ( 84%)], Train Loss: 0.38779\n","Epoch: 00 [17084/20182 ( 85%)], Train Loss: 0.38730\n","Epoch: 00 [17124/20182 ( 85%)], Train Loss: 0.38688\n","Epoch: 00 [17164/20182 ( 85%)], Train Loss: 0.38641\n","Epoch: 00 [17204/20182 ( 85%)], Train Loss: 0.38617\n","Epoch: 00 [17244/20182 ( 85%)], Train Loss: 0.38570\n","Epoch: 00 [17284/20182 ( 86%)], Train Loss: 0.38533\n","Epoch: 00 [17324/20182 ( 86%)], Train Loss: 0.38567\n","Epoch: 00 [17364/20182 ( 86%)], Train Loss: 0.38537\n","Epoch: 00 [17404/20182 ( 86%)], Train Loss: 0.38498\n","Epoch: 00 [17444/20182 ( 86%)], Train Loss: 0.38529\n","Epoch: 00 [17484/20182 ( 87%)], Train Loss: 0.38475\n","Epoch: 00 [17524/20182 ( 87%)], Train Loss: 0.38462\n","Epoch: 00 [17564/20182 ( 87%)], Train Loss: 0.38439\n","Epoch: 00 [17604/20182 ( 87%)], Train Loss: 0.38400\n","Epoch: 00 [17644/20182 ( 87%)], Train Loss: 0.38397\n","Epoch: 00 [17684/20182 ( 88%)], Train Loss: 0.38353\n","Epoch: 00 [17724/20182 ( 88%)], Train Loss: 0.38299\n","Epoch: 00 [17764/20182 ( 88%)], Train Loss: 0.38271\n","Epoch: 00 [17804/20182 ( 88%)], Train Loss: 0.38274\n","Epoch: 00 [17844/20182 ( 88%)], Train Loss: 0.38233\n","Epoch: 00 [17884/20182 ( 89%)], Train Loss: 0.38204\n","Epoch: 00 [17924/20182 ( 89%)], Train Loss: 0.38199\n","Epoch: 00 [17964/20182 ( 89%)], Train Loss: 0.38196\n","Epoch: 00 [18004/20182 ( 89%)], Train Loss: 0.38152\n","Epoch: 00 [18044/20182 ( 89%)], Train Loss: 0.38117\n","Epoch: 00 [18084/20182 ( 90%)], Train Loss: 0.38114\n","Epoch: 00 [18124/20182 ( 90%)], Train Loss: 0.38116\n","Epoch: 00 [18164/20182 ( 90%)], Train Loss: 0.38059\n","Epoch: 00 [18204/20182 ( 90%)], Train Loss: 0.38015\n","Epoch: 00 [18244/20182 ( 90%)], Train Loss: 0.37990\n","Epoch: 00 [18284/20182 ( 91%)], Train Loss: 0.38003\n","Epoch: 00 [18324/20182 ( 91%)], Train Loss: 0.37989\n","Epoch: 00 [18364/20182 ( 91%)], Train Loss: 0.37956\n","Epoch: 00 [18404/20182 ( 91%)], Train Loss: 0.37944\n","Epoch: 00 [18444/20182 ( 91%)], Train Loss: 0.37968\n","Epoch: 00 [18484/20182 ( 92%)], Train Loss: 0.37919\n","Epoch: 00 [18524/20182 ( 92%)], Train Loss: 0.37870\n","Epoch: 00 [18564/20182 ( 92%)], Train Loss: 0.37879\n","Epoch: 00 [18604/20182 ( 92%)], Train Loss: 0.37853\n","Epoch: 00 [18644/20182 ( 92%)], Train Loss: 0.37834\n","Epoch: 00 [18684/20182 ( 93%)], Train Loss: 0.37818\n","Epoch: 00 [18724/20182 ( 93%)], Train Loss: 0.37768\n","Epoch: 00 [18764/20182 ( 93%)], Train Loss: 0.37730\n","Epoch: 00 [18804/20182 ( 93%)], Train Loss: 0.37695\n","Epoch: 00 [18844/20182 ( 93%)], Train Loss: 0.37738\n","Epoch: 00 [18884/20182 ( 94%)], Train Loss: 0.37742\n","Epoch: 00 [18924/20182 ( 94%)], Train Loss: 0.37715\n","Epoch: 00 [18964/20182 ( 94%)], Train Loss: 0.37707\n","Epoch: 00 [19004/20182 ( 94%)], Train Loss: 0.37663\n","Epoch: 00 [19044/20182 ( 94%)], Train Loss: 0.37646\n","Epoch: 00 [19084/20182 ( 95%)], Train Loss: 0.37634\n","Epoch: 00 [19124/20182 ( 95%)], Train Loss: 0.37646\n","Epoch: 00 [19164/20182 ( 95%)], Train Loss: 0.37619\n","Epoch: 00 [19204/20182 ( 95%)], Train Loss: 0.37620\n","Epoch: 00 [19244/20182 ( 95%)], Train Loss: 0.37595\n","Epoch: 00 [19284/20182 ( 96%)], Train Loss: 0.37566\n","Epoch: 00 [19324/20182 ( 96%)], Train Loss: 0.37523\n","Epoch: 00 [19364/20182 ( 96%)], Train Loss: 0.37523\n","Epoch: 00 [19404/20182 ( 96%)], Train Loss: 0.37516\n","Epoch: 00 [19444/20182 ( 96%)], Train Loss: 0.37490\n","Epoch: 00 [19484/20182 ( 97%)], Train Loss: 0.37470\n","Epoch: 00 [19524/20182 ( 97%)], Train Loss: 0.37492\n","Epoch: 00 [19564/20182 ( 97%)], Train Loss: 0.37444\n","Epoch: 00 [19604/20182 ( 97%)], Train Loss: 0.37443\n","Epoch: 00 [19644/20182 ( 97%)], Train Loss: 0.37435\n","Epoch: 00 [19684/20182 ( 98%)], Train Loss: 0.37391\n","Epoch: 00 [19724/20182 ( 98%)], Train Loss: 0.37356\n","Epoch: 00 [19764/20182 ( 98%)], Train Loss: 0.37321\n","Epoch: 00 [19804/20182 ( 98%)], Train Loss: 0.37286\n","Epoch: 00 [19844/20182 ( 98%)], Train Loss: 0.37271\n","Epoch: 00 [19884/20182 ( 99%)], Train Loss: 0.37275\n","Epoch: 00 [19924/20182 ( 99%)], Train Loss: 0.37249\n","Epoch: 00 [19964/20182 ( 99%)], Train Loss: 0.37222\n","Epoch: 00 [20004/20182 ( 99%)], Train Loss: 0.37201\n","Epoch: 00 [20044/20182 ( 99%)], Train Loss: 0.37167\n","Epoch: 00 [20084/20182 (100%)], Train Loss: 0.37145\n","Epoch: 00 [20124/20182 (100%)], Train Loss: 0.37133\n","Epoch: 00 [20164/20182 (100%)], Train Loss: 0.37127\n","Epoch: 00 [20182/20182 (100%)], Train Loss: 0.37115\n","----Validation Results Summary----\n","Epoch: [0] Valid Loss: 0.57748\n","0 Epoch, Best epoch was updated! Valid Loss: 0.57748\n","Saving model checkpoint to chaii-qa-5-fold-xlmroberta-torch-fit/checkpoint-fold-3.\n","\n","Total Training Time: 3022.342734336853secs, Average Training Time per Epoch: 3022.342734336853secs.\n","Total Validation Time: 208.44240736961365secs, Average Validation Time per Epoch: 208.44240736961365secs.\n","\n","\n","--------------------------------------------------\n","FOLD: 4\n","--------------------------------------------------\n","Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n","Num examples Train= 20121, Num examples Valid=4668\n","Total Training Steps: 2516, Total Warmup Steps: 251\n","Epoch: 00 [    4/20121 (  0%)], Train Loss: 3.03740\n","Epoch: 00 [   44/20121 (  0%)], Train Loss: 2.94681\n","Epoch: 00 [   84/20121 (  0%)], Train Loss: 2.91601\n","Epoch: 00 [  124/20121 (  1%)], Train Loss: 2.88056\n","Epoch: 00 [  164/20121 (  1%)], Train Loss: 2.82934\n","Epoch: 00 [  204/20121 (  1%)], Train Loss: 2.76679\n","Epoch: 00 [  244/20121 (  1%)], Train Loss: 2.69858\n","Epoch: 00 [  284/20121 (  1%)], Train Loss: 2.60490\n","Epoch: 00 [  324/20121 (  2%)], Train Loss: 2.49737\n","Epoch: 00 [  364/20121 (  2%)], Train Loss: 2.38762\n","Epoch: 00 [  404/20121 (  2%)], Train Loss: 2.28645\n","Epoch: 00 [  444/20121 (  2%)], Train Loss: 2.15238\n","Epoch: 00 [  484/20121 (  2%)], Train Loss: 2.02644\n","Epoch: 00 [  524/20121 (  3%)], Train Loss: 1.92134\n","Epoch: 00 [  564/20121 (  3%)], Train Loss: 1.82191\n","Epoch: 00 [  604/20121 (  3%)], Train Loss: 1.73480\n","Epoch: 00 [  644/20121 (  3%)], Train Loss: 1.65569\n","Epoch: 00 [  684/20121 (  3%)], Train Loss: 1.58272\n","Epoch: 00 [  724/20121 (  4%)], Train Loss: 1.52823\n","Epoch: 00 [  764/20121 (  4%)], Train Loss: 1.47451\n","Epoch: 00 [  804/20121 (  4%)], Train Loss: 1.42908\n","Epoch: 00 [  844/20121 (  4%)], Train Loss: 1.38239\n","Epoch: 00 [  884/20121 (  4%)], Train Loss: 1.33742\n","Epoch: 00 [  924/20121 (  5%)], Train Loss: 1.29475\n","Epoch: 00 [  964/20121 (  5%)], Train Loss: 1.26385\n","Epoch: 00 [ 1004/20121 (  5%)], Train Loss: 1.22325\n","Epoch: 00 [ 1044/20121 (  5%)], Train Loss: 1.19133\n","Epoch: 00 [ 1084/20121 (  5%)], Train Loss: 1.15773\n","Epoch: 00 [ 1124/20121 (  6%)], Train Loss: 1.12818\n","Epoch: 00 [ 1164/20121 (  6%)], Train Loss: 1.10468\n","Epoch: 00 [ 1204/20121 (  6%)], Train Loss: 1.08704\n","Epoch: 00 [ 1244/20121 (  6%)], Train Loss: 1.06628\n","Epoch: 00 [ 1284/20121 (  6%)], Train Loss: 1.04516\n","Epoch: 00 [ 1324/20121 (  7%)], Train Loss: 1.03282\n","Epoch: 00 [ 1364/20121 (  7%)], Train Loss: 1.01263\n","Epoch: 00 [ 1404/20121 (  7%)], Train Loss: 0.99677\n","Epoch: 00 [ 1444/20121 (  7%)], Train Loss: 0.98086\n","Epoch: 00 [ 1484/20121 (  7%)], Train Loss: 0.96337\n","Epoch: 00 [ 1524/20121 (  8%)], Train Loss: 0.94835\n","Epoch: 00 [ 1564/20121 (  8%)], Train Loss: 0.92980\n","Epoch: 00 [ 1604/20121 (  8%)], Train Loss: 0.91186\n","Epoch: 00 [ 1644/20121 (  8%)], Train Loss: 0.89528\n","Epoch: 00 [ 1684/20121 (  8%)], Train Loss: 0.88577\n","Epoch: 00 [ 1724/20121 (  9%)], Train Loss: 0.87172\n","Epoch: 00 [ 1764/20121 (  9%)], Train Loss: 0.86039\n","Epoch: 00 [ 1804/20121 (  9%)], Train Loss: 0.84866\n","Epoch: 00 [ 1844/20121 (  9%)], Train Loss: 0.83749\n","Epoch: 00 [ 1884/20121 (  9%)], Train Loss: 0.82498\n","Epoch: 00 [ 1924/20121 ( 10%)], Train Loss: 0.81441\n","Epoch: 00 [ 1964/20121 ( 10%)], Train Loss: 0.80295\n","Epoch: 00 [ 2004/20121 ( 10%)], Train Loss: 0.79116\n","Epoch: 00 [ 2044/20121 ( 10%)], Train Loss: 0.79047\n","Epoch: 00 [ 2084/20121 ( 10%)], Train Loss: 0.77960\n","Epoch: 00 [ 2124/20121 ( 11%)], Train Loss: 0.77040\n","Epoch: 00 [ 2164/20121 ( 11%)], Train Loss: 0.76682\n","Epoch: 00 [ 2204/20121 ( 11%)], Train Loss: 0.75863\n","Epoch: 00 [ 2244/20121 ( 11%)], Train Loss: 0.75085\n","Epoch: 00 [ 2284/20121 ( 11%)], Train Loss: 0.74576\n","Epoch: 00 [ 2324/20121 ( 12%)], Train Loss: 0.73922\n","Epoch: 00 [ 2364/20121 ( 12%)], Train Loss: 0.73595\n","Epoch: 00 [ 2404/20121 ( 12%)], Train Loss: 0.72901\n","Epoch: 00 [ 2444/20121 ( 12%)], Train Loss: 0.72204\n","Epoch: 00 [ 2484/20121 ( 12%)], Train Loss: 0.71448\n","Epoch: 00 [ 2524/20121 ( 13%)], Train Loss: 0.71178\n","Epoch: 00 [ 2564/20121 ( 13%)], Train Loss: 0.70613\n","Epoch: 00 [ 2604/20121 ( 13%)], Train Loss: 0.70325\n","Epoch: 00 [ 2644/20121 ( 13%)], Train Loss: 0.69652\n","Epoch: 00 [ 2684/20121 ( 13%)], Train Loss: 0.69367\n","Epoch: 00 [ 2724/20121 ( 14%)], Train Loss: 0.68873\n","Epoch: 00 [ 2764/20121 ( 14%)], Train Loss: 0.68221\n","Epoch: 00 [ 2804/20121 ( 14%)], Train Loss: 0.67823\n","Epoch: 00 [ 2844/20121 ( 14%)], Train Loss: 0.67278\n","Epoch: 00 [ 2884/20121 ( 14%)], Train Loss: 0.67122\n","Epoch: 00 [ 2924/20121 ( 15%)], Train Loss: 0.66821\n","Epoch: 00 [ 2964/20121 ( 15%)], Train Loss: 0.66330\n","Epoch: 00 [ 3004/20121 ( 15%)], Train Loss: 0.66036\n","Epoch: 00 [ 3044/20121 ( 15%)], Train Loss: 0.65702\n","Epoch: 00 [ 3084/20121 ( 15%)], Train Loss: 0.65476\n","Epoch: 00 [ 3124/20121 ( 16%)], Train Loss: 0.65238\n","Epoch: 00 [ 3164/20121 ( 16%)], Train Loss: 0.64947\n","Epoch: 00 [ 3204/20121 ( 16%)], Train Loss: 0.64744\n","Epoch: 00 [ 3244/20121 ( 16%)], Train Loss: 0.64274\n","Epoch: 00 [ 3284/20121 ( 16%)], Train Loss: 0.63826\n","Epoch: 00 [ 3324/20121 ( 17%)], Train Loss: 0.63518\n","Epoch: 00 [ 3364/20121 ( 17%)], Train Loss: 0.63343\n","Epoch: 00 [ 3404/20121 ( 17%)], Train Loss: 0.62866\n","Epoch: 00 [ 3444/20121 ( 17%)], Train Loss: 0.62336\n","Epoch: 00 [ 3484/20121 ( 17%)], Train Loss: 0.61985\n","Epoch: 00 [ 3524/20121 ( 18%)], Train Loss: 0.61899\n","Epoch: 00 [ 3564/20121 ( 18%)], Train Loss: 0.61924\n","Epoch: 00 [ 3604/20121 ( 18%)], Train Loss: 0.61722\n","Epoch: 00 [ 3644/20121 ( 18%)], Train Loss: 0.61367\n","Epoch: 00 [ 3684/20121 ( 18%)], Train Loss: 0.61425\n","Epoch: 00 [ 3724/20121 ( 19%)], Train Loss: 0.61254\n","Epoch: 00 [ 3764/20121 ( 19%)], Train Loss: 0.61054\n","Epoch: 00 [ 3804/20121 ( 19%)], Train Loss: 0.60729\n","Epoch: 00 [ 3844/20121 ( 19%)], Train Loss: 0.60552\n","Epoch: 00 [ 3884/20121 ( 19%)], Train Loss: 0.60229\n","Epoch: 00 [ 3924/20121 ( 20%)], Train Loss: 0.59940\n","Epoch: 00 [ 3964/20121 ( 20%)], Train Loss: 0.59854\n","Epoch: 00 [ 4004/20121 ( 20%)], Train Loss: 0.59655\n","Epoch: 00 [ 4044/20121 ( 20%)], Train Loss: 0.59253\n","Epoch: 00 [ 4084/20121 ( 20%)], Train Loss: 0.59048\n","Epoch: 00 [ 4124/20121 ( 20%)], Train Loss: 0.58753\n","Epoch: 00 [ 4164/20121 ( 21%)], Train Loss: 0.58578\n","Epoch: 00 [ 4204/20121 ( 21%)], Train Loss: 0.58346\n","Epoch: 00 [ 4244/20121 ( 21%)], Train Loss: 0.58125\n","Epoch: 00 [ 4284/20121 ( 21%)], Train Loss: 0.57721\n","Epoch: 00 [ 4324/20121 ( 21%)], Train Loss: 0.57474\n","Epoch: 00 [ 4364/20121 ( 22%)], Train Loss: 0.57616\n","Epoch: 00 [ 4404/20121 ( 22%)], Train Loss: 0.57443\n","Epoch: 00 [ 4444/20121 ( 22%)], Train Loss: 0.57315\n","Epoch: 00 [ 4484/20121 ( 22%)], Train Loss: 0.57176\n","Epoch: 00 [ 4524/20121 ( 22%)], Train Loss: 0.57052\n","Epoch: 00 [ 4564/20121 ( 23%)], Train Loss: 0.56878\n","Epoch: 00 [ 4604/20121 ( 23%)], Train Loss: 0.56641\n","Epoch: 00 [ 4644/20121 ( 23%)], Train Loss: 0.56427\n","Epoch: 00 [ 4684/20121 ( 23%)], Train Loss: 0.56308\n","Epoch: 00 [ 4724/20121 ( 23%)], Train Loss: 0.55924\n","Epoch: 00 [ 4764/20121 ( 24%)], Train Loss: 0.55691\n","Epoch: 00 [ 4804/20121 ( 24%)], Train Loss: 0.55513\n","Epoch: 00 [ 4844/20121 ( 24%)], Train Loss: 0.55328\n","Epoch: 00 [ 4884/20121 ( 24%)], Train Loss: 0.55035\n","Epoch: 00 [ 4924/20121 ( 24%)], Train Loss: 0.54821\n","Epoch: 00 [ 4964/20121 ( 25%)], Train Loss: 0.54815\n","Epoch: 00 [ 5004/20121 ( 25%)], Train Loss: 0.54729\n","Epoch: 00 [ 5044/20121 ( 25%)], Train Loss: 0.54434\n","Epoch: 00 [ 5084/20121 ( 25%)], Train Loss: 0.54324\n","Epoch: 00 [ 5124/20121 ( 25%)], Train Loss: 0.54249\n","Epoch: 00 [ 5164/20121 ( 26%)], Train Loss: 0.54239\n","Epoch: 00 [ 5204/20121 ( 26%)], Train Loss: 0.54117\n","Epoch: 00 [ 5244/20121 ( 26%)], Train Loss: 0.53827\n","Epoch: 00 [ 5284/20121 ( 26%)], Train Loss: 0.53845\n","Epoch: 00 [ 5324/20121 ( 26%)], Train Loss: 0.53734\n","Epoch: 00 [ 5364/20121 ( 27%)], Train Loss: 0.53564\n","Epoch: 00 [ 5404/20121 ( 27%)], Train Loss: 0.53426\n","Epoch: 00 [ 5444/20121 ( 27%)], Train Loss: 0.53208\n","Epoch: 00 [ 5484/20121 ( 27%)], Train Loss: 0.53099\n","Epoch: 00 [ 5524/20121 ( 27%)], Train Loss: 0.52932\n","Epoch: 00 [ 5564/20121 ( 28%)], Train Loss: 0.52746\n","Epoch: 00 [ 5604/20121 ( 28%)], Train Loss: 0.52624\n","Epoch: 00 [ 5644/20121 ( 28%)], Train Loss: 0.52526\n","Epoch: 00 [ 5684/20121 ( 28%)], Train Loss: 0.52299\n","Epoch: 00 [ 5724/20121 ( 28%)], Train Loss: 0.52295\n","Epoch: 00 [ 5764/20121 ( 29%)], Train Loss: 0.52214\n","Epoch: 00 [ 5804/20121 ( 29%)], Train Loss: 0.52077\n","Epoch: 00 [ 5844/20121 ( 29%)], Train Loss: 0.51874\n","Epoch: 00 [ 5884/20121 ( 29%)], Train Loss: 0.51808\n","Epoch: 00 [ 5924/20121 ( 29%)], Train Loss: 0.51813\n","Epoch: 00 [ 5964/20121 ( 30%)], Train Loss: 0.51856\n","Epoch: 00 [ 6004/20121 ( 30%)], Train Loss: 0.51707\n","Epoch: 00 [ 6044/20121 ( 30%)], Train Loss: 0.51597\n","Epoch: 00 [ 6084/20121 ( 30%)], Train Loss: 0.51603\n","Epoch: 00 [ 6124/20121 ( 30%)], Train Loss: 0.51492\n","Epoch: 00 [ 6164/20121 ( 31%)], Train Loss: 0.51278\n","Epoch: 00 [ 6204/20121 ( 31%)], Train Loss: 0.51223\n","Epoch: 00 [ 6244/20121 ( 31%)], Train Loss: 0.51145\n","Epoch: 00 [ 6284/20121 ( 31%)], Train Loss: 0.51129\n","Epoch: 00 [ 6324/20121 ( 31%)], Train Loss: 0.50976\n","Epoch: 00 [ 6364/20121 ( 32%)], Train Loss: 0.50946\n","Epoch: 00 [ 6404/20121 ( 32%)], Train Loss: 0.50833\n","Epoch: 00 [ 6444/20121 ( 32%)], Train Loss: 0.50664\n","Epoch: 00 [ 6484/20121 ( 32%)], Train Loss: 0.50519\n","Epoch: 00 [ 6524/20121 ( 32%)], Train Loss: 0.50316\n","Epoch: 00 [ 6564/20121 ( 33%)], Train Loss: 0.50222\n","Epoch: 00 [ 6604/20121 ( 33%)], Train Loss: 0.50175\n","Epoch: 00 [ 6644/20121 ( 33%)], Train Loss: 0.50155\n","Epoch: 00 [ 6684/20121 ( 33%)], Train Loss: 0.49998\n","Epoch: 00 [ 6724/20121 ( 33%)], Train Loss: 0.49751\n","Epoch: 00 [ 6764/20121 ( 34%)], Train Loss: 0.49672\n","Epoch: 00 [ 6804/20121 ( 34%)], Train Loss: 0.49643\n","Epoch: 00 [ 6844/20121 ( 34%)], Train Loss: 0.49449\n","Epoch: 00 [ 6884/20121 ( 34%)], Train Loss: 0.49315\n","Epoch: 00 [ 6924/20121 ( 34%)], Train Loss: 0.49153\n","Epoch: 00 [ 6964/20121 ( 35%)], Train Loss: 0.49058\n","Epoch: 00 [ 7004/20121 ( 35%)], Train Loss: 0.48951\n","Epoch: 00 [ 7044/20121 ( 35%)], Train Loss: 0.48808\n","Epoch: 00 [ 7084/20121 ( 35%)], Train Loss: 0.48663\n","Epoch: 00 [ 7124/20121 ( 35%)], Train Loss: 0.48523\n","Epoch: 00 [ 7164/20121 ( 36%)], Train Loss: 0.48437\n","Epoch: 00 [ 7204/20121 ( 36%)], Train Loss: 0.48403\n","Epoch: 00 [ 7244/20121 ( 36%)], Train Loss: 0.48253\n","Epoch: 00 [ 7284/20121 ( 36%)], Train Loss: 0.48206\n","Epoch: 00 [ 7324/20121 ( 36%)], Train Loss: 0.48158\n","Epoch: 00 [ 7364/20121 ( 37%)], Train Loss: 0.47983\n","Epoch: 00 [ 7404/20121 ( 37%)], Train Loss: 0.47999\n","Epoch: 00 [ 7444/20121 ( 37%)], Train Loss: 0.47943\n","Epoch: 00 [ 7484/20121 ( 37%)], Train Loss: 0.47878\n","Epoch: 00 [ 7524/20121 ( 37%)], Train Loss: 0.47777\n","Epoch: 00 [ 7564/20121 ( 38%)], Train Loss: 0.47667\n","Epoch: 00 [ 7604/20121 ( 38%)], Train Loss: 0.47615\n","Epoch: 00 [ 7644/20121 ( 38%)], Train Loss: 0.47552\n","Epoch: 00 [ 7684/20121 ( 38%)], Train Loss: 0.47402\n","Epoch: 00 [ 7724/20121 ( 38%)], Train Loss: 0.47303\n","Epoch: 00 [ 7764/20121 ( 39%)], Train Loss: 0.47160\n","Epoch: 00 [ 7804/20121 ( 39%)], Train Loss: 0.47147\n","Epoch: 00 [ 7844/20121 ( 39%)], Train Loss: 0.47002\n","Epoch: 00 [ 7884/20121 ( 39%)], Train Loss: 0.46927\n","Epoch: 00 [ 7924/20121 ( 39%)], Train Loss: 0.46845\n","Epoch: 00 [ 7964/20121 ( 40%)], Train Loss: 0.46737\n","Epoch: 00 [ 8004/20121 ( 40%)], Train Loss: 0.46729\n","Epoch: 00 [ 8044/20121 ( 40%)], Train Loss: 0.46694\n","Epoch: 00 [ 8084/20121 ( 40%)], Train Loss: 0.46702\n","Epoch: 00 [ 8124/20121 ( 40%)], Train Loss: 0.46651\n","Epoch: 00 [ 8164/20121 ( 41%)], Train Loss: 0.46513\n","Epoch: 00 [ 8204/20121 ( 41%)], Train Loss: 0.46477\n","Epoch: 00 [ 8244/20121 ( 41%)], Train Loss: 0.46369\n","Epoch: 00 [ 8284/20121 ( 41%)], Train Loss: 0.46323\n","Epoch: 00 [ 8324/20121 ( 41%)], Train Loss: 0.46288\n","Epoch: 00 [ 8364/20121 ( 42%)], Train Loss: 0.46281\n","Epoch: 00 [ 8404/20121 ( 42%)], Train Loss: 0.46184\n","Epoch: 00 [ 8444/20121 ( 42%)], Train Loss: 0.46219\n","Epoch: 00 [ 8484/20121 ( 42%)], Train Loss: 0.46149\n","Epoch: 00 [ 8524/20121 ( 42%)], Train Loss: 0.46058\n","Epoch: 00 [ 8564/20121 ( 43%)], Train Loss: 0.46005\n","Epoch: 00 [ 8604/20121 ( 43%)], Train Loss: 0.45869\n","Epoch: 00 [ 8644/20121 ( 43%)], Train Loss: 0.45744\n","Epoch: 00 [ 8684/20121 ( 43%)], Train Loss: 0.45649\n","Epoch: 00 [ 8724/20121 ( 43%)], Train Loss: 0.45515\n","Epoch: 00 [ 8764/20121 ( 44%)], Train Loss: 0.45498\n","Epoch: 00 [ 8804/20121 ( 44%)], Train Loss: 0.45441\n","Epoch: 00 [ 8844/20121 ( 44%)], Train Loss: 0.45400\n","Epoch: 00 [ 8884/20121 ( 44%)], Train Loss: 0.45349\n","Epoch: 00 [ 8924/20121 ( 44%)], Train Loss: 0.45308\n","Epoch: 00 [ 8964/20121 ( 45%)], Train Loss: 0.45284\n","Epoch: 00 [ 9004/20121 ( 45%)], Train Loss: 0.45153\n","Epoch: 00 [ 9044/20121 ( 45%)], Train Loss: 0.45181\n","Epoch: 00 [ 9084/20121 ( 45%)], Train Loss: 0.45102\n","Epoch: 00 [ 9124/20121 ( 45%)], Train Loss: 0.44991\n","Epoch: 00 [ 9164/20121 ( 46%)], Train Loss: 0.44921\n","Epoch: 00 [ 9204/20121 ( 46%)], Train Loss: 0.44881\n","Epoch: 00 [ 9244/20121 ( 46%)], Train Loss: 0.44774\n","Epoch: 00 [ 9284/20121 ( 46%)], Train Loss: 0.44692\n","Epoch: 00 [ 9324/20121 ( 46%)], Train Loss: 0.44619\n","Epoch: 00 [ 9364/20121 ( 47%)], Train Loss: 0.44569\n","Epoch: 00 [ 9404/20121 ( 47%)], Train Loss: 0.44551\n","Epoch: 00 [ 9444/20121 ( 47%)], Train Loss: 0.44510\n","Epoch: 00 [ 9484/20121 ( 47%)], Train Loss: 0.44526\n","Epoch: 00 [ 9524/20121 ( 47%)], Train Loss: 0.44467\n","Epoch: 00 [ 9564/20121 ( 48%)], Train Loss: 0.44441\n","Epoch: 00 [ 9604/20121 ( 48%)], Train Loss: 0.44398\n","Epoch: 00 [ 9644/20121 ( 48%)], Train Loss: 0.44373\n","Epoch: 00 [ 9684/20121 ( 48%)], Train Loss: 0.44369\n","Epoch: 00 [ 9724/20121 ( 48%)], Train Loss: 0.44374\n","Epoch: 00 [ 9764/20121 ( 49%)], Train Loss: 0.44376\n","Epoch: 00 [ 9804/20121 ( 49%)], Train Loss: 0.44321\n","Epoch: 00 [ 9844/20121 ( 49%)], Train Loss: 0.44279\n","Epoch: 00 [ 9884/20121 ( 49%)], Train Loss: 0.44245\n","Epoch: 00 [ 9924/20121 ( 49%)], Train Loss: 0.44233\n","Epoch: 00 [ 9964/20121 ( 50%)], Train Loss: 0.44209\n","Epoch: 00 [10004/20121 ( 50%)], Train Loss: 0.44122\n","Epoch: 00 [10044/20121 ( 50%)], Train Loss: 0.44053\n","Epoch: 00 [10084/20121 ( 50%)], Train Loss: 0.43989\n","Epoch: 00 [10124/20121 ( 50%)], Train Loss: 0.43915\n","Epoch: 00 [10164/20121 ( 51%)], Train Loss: 0.43823\n","Epoch: 00 [10204/20121 ( 51%)], Train Loss: 0.43739\n","Epoch: 00 [10244/20121 ( 51%)], Train Loss: 0.43662\n","Epoch: 00 [10284/20121 ( 51%)], Train Loss: 0.43575\n","Epoch: 00 [10324/20121 ( 51%)], Train Loss: 0.43626\n","Epoch: 00 [10364/20121 ( 52%)], Train Loss: 0.43605\n","Epoch: 00 [10404/20121 ( 52%)], Train Loss: 0.43523\n","Epoch: 00 [10444/20121 ( 52%)], Train Loss: 0.43411\n","Epoch: 00 [10484/20121 ( 52%)], Train Loss: 0.43309\n","Epoch: 00 [10524/20121 ( 52%)], Train Loss: 0.43294\n","Epoch: 00 [10564/20121 ( 53%)], Train Loss: 0.43303\n","Epoch: 00 [10604/20121 ( 53%)], Train Loss: 0.43240\n","Epoch: 00 [10644/20121 ( 53%)], Train Loss: 0.43174\n","Epoch: 00 [10684/20121 ( 53%)], Train Loss: 0.43106\n","Epoch: 00 [10724/20121 ( 53%)], Train Loss: 0.43069\n","Epoch: 00 [10764/20121 ( 53%)], Train Loss: 0.42989\n","Epoch: 00 [10804/20121 ( 54%)], Train Loss: 0.42960\n","Epoch: 00 [10844/20121 ( 54%)], Train Loss: 0.42925\n","Epoch: 00 [10884/20121 ( 54%)], Train Loss: 0.42879\n","Epoch: 00 [10924/20121 ( 54%)], Train Loss: 0.42814\n","Epoch: 00 [10964/20121 ( 54%)], Train Loss: 0.42770\n","Epoch: 00 [11004/20121 ( 55%)], Train Loss: 0.42701\n","Epoch: 00 [11044/20121 ( 55%)], Train Loss: 0.42623\n","Epoch: 00 [11084/20121 ( 55%)], Train Loss: 0.42535\n","Epoch: 00 [11124/20121 ( 55%)], Train Loss: 0.42470\n","Epoch: 00 [11164/20121 ( 55%)], Train Loss: 0.42395\n","Epoch: 00 [11204/20121 ( 56%)], Train Loss: 0.42351\n","Epoch: 00 [11244/20121 ( 56%)], Train Loss: 0.42315\n","Epoch: 00 [11284/20121 ( 56%)], Train Loss: 0.42303\n","Epoch: 00 [11324/20121 ( 56%)], Train Loss: 0.42257\n","Epoch: 00 [11364/20121 ( 56%)], Train Loss: 0.42155\n","Epoch: 00 [11404/20121 ( 57%)], Train Loss: 0.42127\n","Epoch: 00 [11444/20121 ( 57%)], Train Loss: 0.42114\n","Epoch: 00 [11484/20121 ( 57%)], Train Loss: 0.42146\n","Epoch: 00 [11524/20121 ( 57%)], Train Loss: 0.42142\n","Epoch: 00 [11564/20121 ( 57%)], Train Loss: 0.42092\n","Epoch: 00 [11604/20121 ( 58%)], Train Loss: 0.42090\n","Epoch: 00 [11644/20121 ( 58%)], Train Loss: 0.42053\n","Epoch: 00 [11684/20121 ( 58%)], Train Loss: 0.41978\n","Epoch: 00 [11724/20121 ( 58%)], Train Loss: 0.41919\n","Epoch: 00 [11764/20121 ( 58%)], Train Loss: 0.41907\n","Epoch: 00 [11804/20121 ( 59%)], Train Loss: 0.41850\n","Epoch: 00 [11844/20121 ( 59%)], Train Loss: 0.41838\n","Epoch: 00 [11884/20121 ( 59%)], Train Loss: 0.41849\n","Epoch: 00 [11924/20121 ( 59%)], Train Loss: 0.41863\n","Epoch: 00 [11964/20121 ( 59%)], Train Loss: 0.41826\n","Epoch: 00 [12004/20121 ( 60%)], Train Loss: 0.41809\n","Epoch: 00 [12044/20121 ( 60%)], Train Loss: 0.41780\n","Epoch: 00 [12084/20121 ( 60%)], Train Loss: 0.41739\n","Epoch: 00 [12124/20121 ( 60%)], Train Loss: 0.41660\n","Epoch: 00 [12164/20121 ( 60%)], Train Loss: 0.41621\n","Epoch: 00 [12204/20121 ( 61%)], Train Loss: 0.41613\n","Epoch: 00 [12244/20121 ( 61%)], Train Loss: 0.41649\n","Epoch: 00 [12284/20121 ( 61%)], Train Loss: 0.41604\n","Epoch: 00 [12324/20121 ( 61%)], Train Loss: 0.41606\n","Epoch: 00 [12364/20121 ( 61%)], Train Loss: 0.41604\n","Epoch: 00 [12404/20121 ( 62%)], Train Loss: 0.41602\n","Epoch: 00 [12444/20121 ( 62%)], Train Loss: 0.41553\n","Epoch: 00 [12484/20121 ( 62%)], Train Loss: 0.41512\n","Epoch: 00 [12524/20121 ( 62%)], Train Loss: 0.41463\n","Epoch: 00 [12564/20121 ( 62%)], Train Loss: 0.41412\n","Epoch: 00 [12604/20121 ( 63%)], Train Loss: 0.41384\n","Epoch: 00 [12644/20121 ( 63%)], Train Loss: 0.41285\n","Epoch: 00 [12684/20121 ( 63%)], Train Loss: 0.41326\n","Epoch: 00 [12724/20121 ( 63%)], Train Loss: 0.41300\n","Epoch: 00 [12764/20121 ( 63%)], Train Loss: 0.41251\n","Epoch: 00 [12804/20121 ( 64%)], Train Loss: 0.41206\n","Epoch: 00 [12844/20121 ( 64%)], Train Loss: 0.41175\n","Epoch: 00 [12884/20121 ( 64%)], Train Loss: 0.41144\n","Epoch: 00 [12924/20121 ( 64%)], Train Loss: 0.41117\n","Epoch: 00 [12964/20121 ( 64%)], Train Loss: 0.41074\n","Epoch: 00 [13004/20121 ( 65%)], Train Loss: 0.41038\n","Epoch: 00 [13044/20121 ( 65%)], Train Loss: 0.41023\n","Epoch: 00 [13084/20121 ( 65%)], Train Loss: 0.40970\n","Epoch: 00 [13124/20121 ( 65%)], Train Loss: 0.40976\n","Epoch: 00 [13164/20121 ( 65%)], Train Loss: 0.40931\n","Epoch: 00 [13204/20121 ( 66%)], Train Loss: 0.40850\n","Epoch: 00 [13244/20121 ( 66%)], Train Loss: 0.40862\n","Epoch: 00 [13284/20121 ( 66%)], Train Loss: 0.40812\n","Epoch: 00 [13324/20121 ( 66%)], Train Loss: 0.40776\n","Epoch: 00 [13364/20121 ( 66%)], Train Loss: 0.40692\n","Epoch: 00 [13404/20121 ( 67%)], Train Loss: 0.40643\n","Epoch: 00 [13444/20121 ( 67%)], Train Loss: 0.40583\n","Epoch: 00 [13484/20121 ( 67%)], Train Loss: 0.40497\n","Epoch: 00 [13524/20121 ( 67%)], Train Loss: 0.40460\n","Epoch: 00 [13564/20121 ( 67%)], Train Loss: 0.40421\n","Epoch: 00 [13604/20121 ( 68%)], Train Loss: 0.40385\n","Epoch: 00 [13644/20121 ( 68%)], Train Loss: 0.40419\n","Epoch: 00 [13684/20121 ( 68%)], Train Loss: 0.40389\n","Epoch: 00 [13724/20121 ( 68%)], Train Loss: 0.40356\n","Epoch: 00 [13764/20121 ( 68%)], Train Loss: 0.40287\n","Epoch: 00 [13804/20121 ( 69%)], Train Loss: 0.40225\n","Epoch: 00 [13844/20121 ( 69%)], Train Loss: 0.40204\n","Epoch: 00 [13884/20121 ( 69%)], Train Loss: 0.40145\n","Epoch: 00 [13924/20121 ( 69%)], Train Loss: 0.40110\n","Epoch: 00 [13964/20121 ( 69%)], Train Loss: 0.40090\n","Epoch: 00 [14004/20121 ( 70%)], Train Loss: 0.40035\n","Epoch: 00 [14044/20121 ( 70%)], Train Loss: 0.39975\n","Epoch: 00 [14084/20121 ( 70%)], Train Loss: 0.39980\n","Epoch: 00 [14124/20121 ( 70%)], Train Loss: 0.39924\n","Epoch: 00 [14164/20121 ( 70%)], Train Loss: 0.39913\n","Epoch: 00 [14204/20121 ( 71%)], Train Loss: 0.39858\n","Epoch: 00 [14244/20121 ( 71%)], Train Loss: 0.39837\n","Epoch: 00 [14284/20121 ( 71%)], Train Loss: 0.39826\n","Epoch: 00 [14324/20121 ( 71%)], Train Loss: 0.39809\n","Epoch: 00 [14364/20121 ( 71%)], Train Loss: 0.39790\n","Epoch: 00 [14404/20121 ( 72%)], Train Loss: 0.39769\n","Epoch: 00 [14444/20121 ( 72%)], Train Loss: 0.39724\n","Epoch: 00 [14484/20121 ( 72%)], Train Loss: 0.39678\n","Epoch: 00 [14524/20121 ( 72%)], Train Loss: 0.39643\n","Epoch: 00 [14564/20121 ( 72%)], Train Loss: 0.39597\n","Epoch: 00 [14604/20121 ( 73%)], Train Loss: 0.39556\n","Epoch: 00 [14644/20121 ( 73%)], Train Loss: 0.39563\n","Epoch: 00 [14684/20121 ( 73%)], Train Loss: 0.39580\n","Epoch: 00 [14724/20121 ( 73%)], Train Loss: 0.39521\n","Epoch: 00 [14764/20121 ( 73%)], Train Loss: 0.39490\n","Epoch: 00 [14804/20121 ( 74%)], Train Loss: 0.39485\n","Epoch: 00 [14844/20121 ( 74%)], Train Loss: 0.39459\n","Epoch: 00 [14884/20121 ( 74%)], Train Loss: 0.39419\n","Epoch: 00 [14924/20121 ( 74%)], Train Loss: 0.39397\n","Epoch: 00 [14964/20121 ( 74%)], Train Loss: 0.39425\n","Epoch: 00 [15004/20121 ( 75%)], Train Loss: 0.39408\n","Epoch: 00 [15044/20121 ( 75%)], Train Loss: 0.39377\n","Epoch: 00 [15084/20121 ( 75%)], Train Loss: 0.39352\n","Epoch: 00 [15124/20121 ( 75%)], Train Loss: 0.39290\n","Epoch: 00 [15164/20121 ( 75%)], Train Loss: 0.39203\n","Epoch: 00 [15204/20121 ( 76%)], Train Loss: 0.39149\n","Epoch: 00 [15244/20121 ( 76%)], Train Loss: 0.39129\n","Epoch: 00 [15284/20121 ( 76%)], Train Loss: 0.39063\n","Epoch: 00 [15324/20121 ( 76%)], Train Loss: 0.39051\n","Epoch: 00 [15364/20121 ( 76%)], Train Loss: 0.39023\n","Epoch: 00 [15404/20121 ( 77%)], Train Loss: 0.38961\n","Epoch: 00 [15444/20121 ( 77%)], Train Loss: 0.38913\n","Epoch: 00 [15484/20121 ( 77%)], Train Loss: 0.38856\n","Epoch: 00 [15524/20121 ( 77%)], Train Loss: 0.38803\n","Epoch: 00 [15564/20121 ( 77%)], Train Loss: 0.38735\n","Epoch: 00 [15604/20121 ( 78%)], Train Loss: 0.38697\n","Epoch: 00 [15644/20121 ( 78%)], Train Loss: 0.38661\n","Epoch: 00 [15684/20121 ( 78%)], Train Loss: 0.38615\n","Epoch: 00 [15724/20121 ( 78%)], Train Loss: 0.38613\n","Epoch: 00 [15764/20121 ( 78%)], Train Loss: 0.38543\n","Epoch: 00 [15804/20121 ( 79%)], Train Loss: 0.38498\n","Epoch: 00 [15844/20121 ( 79%)], Train Loss: 0.38511\n","Epoch: 00 [15884/20121 ( 79%)], Train Loss: 0.38500\n","Epoch: 00 [15924/20121 ( 79%)], Train Loss: 0.38467\n","Epoch: 00 [15964/20121 ( 79%)], Train Loss: 0.38412\n","Epoch: 00 [16004/20121 ( 80%)], Train Loss: 0.38408\n","Epoch: 00 [16044/20121 ( 80%)], Train Loss: 0.38413\n","Epoch: 00 [16084/20121 ( 80%)], Train Loss: 0.38344\n","Epoch: 00 [16124/20121 ( 80%)], Train Loss: 0.38289\n","Epoch: 00 [16164/20121 ( 80%)], Train Loss: 0.38283\n","Epoch: 00 [16204/20121 ( 81%)], Train Loss: 0.38229\n","Epoch: 00 [16244/20121 ( 81%)], Train Loss: 0.38173\n","Epoch: 00 [16284/20121 ( 81%)], Train Loss: 0.38161\n","Epoch: 00 [16324/20121 ( 81%)], Train Loss: 0.38126\n","Epoch: 00 [16364/20121 ( 81%)], Train Loss: 0.38120\n","Epoch: 00 [16404/20121 ( 82%)], Train Loss: 0.38066\n","Epoch: 00 [16444/20121 ( 82%)], Train Loss: 0.38005\n","Epoch: 00 [16484/20121 ( 82%)], Train Loss: 0.38003\n","Epoch: 00 [16524/20121 ( 82%)], Train Loss: 0.38001\n","Epoch: 00 [16564/20121 ( 82%)], Train Loss: 0.37993\n","Epoch: 00 [16604/20121 ( 83%)], Train Loss: 0.37978\n","Epoch: 00 [16644/20121 ( 83%)], Train Loss: 0.37980\n","Epoch: 00 [16684/20121 ( 83%)], Train Loss: 0.37948\n","Epoch: 00 [16724/20121 ( 83%)], Train Loss: 0.37919\n","Epoch: 00 [16764/20121 ( 83%)], Train Loss: 0.37893\n","Epoch: 00 [16804/20121 ( 84%)], Train Loss: 0.37848\n","Epoch: 00 [16844/20121 ( 84%)], Train Loss: 0.37823\n","Epoch: 00 [16884/20121 ( 84%)], Train Loss: 0.37770\n","Epoch: 00 [16924/20121 ( 84%)], Train Loss: 0.37716\n","Epoch: 00 [16964/20121 ( 84%)], Train Loss: 0.37687\n","Epoch: 00 [17004/20121 ( 85%)], Train Loss: 0.37675\n","Epoch: 00 [17044/20121 ( 85%)], Train Loss: 0.37645\n","Epoch: 00 [17084/20121 ( 85%)], Train Loss: 0.37682\n","Epoch: 00 [17124/20121 ( 85%)], Train Loss: 0.37696\n","Epoch: 00 [17164/20121 ( 85%)], Train Loss: 0.37677\n","Epoch: 00 [17204/20121 ( 86%)], Train Loss: 0.37658\n","Epoch: 00 [17244/20121 ( 86%)], Train Loss: 0.37634\n","Epoch: 00 [17284/20121 ( 86%)], Train Loss: 0.37589\n","Epoch: 00 [17324/20121 ( 86%)], Train Loss: 0.37564\n","Epoch: 00 [17364/20121 ( 86%)], Train Loss: 0.37576\n","Epoch: 00 [17404/20121 ( 86%)], Train Loss: 0.37557\n","Epoch: 00 [17444/20121 ( 87%)], Train Loss: 0.37514\n","Epoch: 00 [17484/20121 ( 87%)], Train Loss: 0.37481\n","Epoch: 00 [17524/20121 ( 87%)], Train Loss: 0.37474\n","Epoch: 00 [17564/20121 ( 87%)], Train Loss: 0.37440\n","Epoch: 00 [17604/20121 ( 87%)], Train Loss: 0.37414\n","Epoch: 00 [17644/20121 ( 88%)], Train Loss: 0.37414\n","Epoch: 00 [17684/20121 ( 88%)], Train Loss: 0.37399\n","Epoch: 00 [17724/20121 ( 88%)], Train Loss: 0.37388\n","Epoch: 00 [17764/20121 ( 88%)], Train Loss: 0.37358\n","Epoch: 00 [17804/20121 ( 88%)], Train Loss: 0.37354\n","Epoch: 00 [17844/20121 ( 89%)], Train Loss: 0.37306\n","Epoch: 00 [17884/20121 ( 89%)], Train Loss: 0.37287\n","Epoch: 00 [17924/20121 ( 89%)], Train Loss: 0.37254\n","Epoch: 00 [17964/20121 ( 89%)], Train Loss: 0.37251\n","Epoch: 00 [18004/20121 ( 89%)], Train Loss: 0.37233\n","Epoch: 00 [18044/20121 ( 90%)], Train Loss: 0.37233\n","Epoch: 00 [18084/20121 ( 90%)], Train Loss: 0.37229\n","Epoch: 00 [18124/20121 ( 90%)], Train Loss: 0.37208\n","Epoch: 00 [18164/20121 ( 90%)], Train Loss: 0.37218\n","Epoch: 00 [18204/20121 ( 90%)], Train Loss: 0.37213\n","Epoch: 00 [18244/20121 ( 91%)], Train Loss: 0.37218\n","Epoch: 00 [18284/20121 ( 91%)], Train Loss: 0.37258\n","Epoch: 00 [18324/20121 ( 91%)], Train Loss: 0.37234\n","Epoch: 00 [18364/20121 ( 91%)], Train Loss: 0.37216\n","Epoch: 00 [18404/20121 ( 91%)], Train Loss: 0.37187\n","Epoch: 00 [18444/20121 ( 92%)], Train Loss: 0.37172\n","Epoch: 00 [18484/20121 ( 92%)], Train Loss: 0.37128\n","Epoch: 00 [18524/20121 ( 92%)], Train Loss: 0.37119\n","Epoch: 00 [18564/20121 ( 92%)], Train Loss: 0.37109\n","Epoch: 00 [18604/20121 ( 92%)], Train Loss: 0.37109\n","Epoch: 00 [18644/20121 ( 93%)], Train Loss: 0.37119\n","Epoch: 00 [18684/20121 ( 93%)], Train Loss: 0.37093\n","Epoch: 00 [18724/20121 ( 93%)], Train Loss: 0.37067\n","Epoch: 00 [18764/20121 ( 93%)], Train Loss: 0.37040\n","Epoch: 00 [18804/20121 ( 93%)], Train Loss: 0.37038\n","Epoch: 00 [18844/20121 ( 94%)], Train Loss: 0.37025\n","Epoch: 00 [18884/20121 ( 94%)], Train Loss: 0.37010\n","Epoch: 00 [18924/20121 ( 94%)], Train Loss: 0.36978\n","Epoch: 00 [18964/20121 ( 94%)], Train Loss: 0.36950\n","Epoch: 00 [19004/20121 ( 94%)], Train Loss: 0.36943\n","Epoch: 00 [19044/20121 ( 95%)], Train Loss: 0.36957\n","Epoch: 00 [19084/20121 ( 95%)], Train Loss: 0.36933\n","Epoch: 00 [19124/20121 ( 95%)], Train Loss: 0.36918\n","Epoch: 00 [19164/20121 ( 95%)], Train Loss: 0.36931\n","Epoch: 00 [19204/20121 ( 95%)], Train Loss: 0.36926\n","Epoch: 00 [19244/20121 ( 96%)], Train Loss: 0.36919\n","Epoch: 00 [19284/20121 ( 96%)], Train Loss: 0.36915\n","Epoch: 00 [19324/20121 ( 96%)], Train Loss: 0.36896\n","Epoch: 00 [19364/20121 ( 96%)], Train Loss: 0.36890\n","Epoch: 00 [19404/20121 ( 96%)], Train Loss: 0.36904\n","Epoch: 00 [19444/20121 ( 97%)], Train Loss: 0.36885\n","Epoch: 00 [19484/20121 ( 97%)], Train Loss: 0.36881\n","Epoch: 00 [19524/20121 ( 97%)], Train Loss: 0.36847\n","Epoch: 00 [19564/20121 ( 97%)], Train Loss: 0.36839\n","Epoch: 00 [19604/20121 ( 97%)], Train Loss: 0.36858\n","Epoch: 00 [19644/20121 ( 98%)], Train Loss: 0.36866\n","Epoch: 00 [19684/20121 ( 98%)], Train Loss: 0.36877\n","Epoch: 00 [19724/20121 ( 98%)], Train Loss: 0.36862\n","Epoch: 00 [19764/20121 ( 98%)], Train Loss: 0.36850\n","Epoch: 00 [19804/20121 ( 98%)], Train Loss: 0.36842\n","Epoch: 00 [19844/20121 ( 99%)], Train Loss: 0.36847\n","Epoch: 00 [19884/20121 ( 99%)], Train Loss: 0.36858\n","Epoch: 00 [19924/20121 ( 99%)], Train Loss: 0.36846\n","Epoch: 00 [19964/20121 ( 99%)], Train Loss: 0.36797\n","Epoch: 00 [20004/20121 ( 99%)], Train Loss: 0.36780\n","Epoch: 00 [20044/20121 (100%)], Train Loss: 0.36750\n","Epoch: 00 [20084/20121 (100%)], Train Loss: 0.36726\n","Epoch: 00 [20121/20121 (100%)], Train Loss: 0.36745\n","----Validation Results Summary----\n","Epoch: [0] Valid Loss: 0.58417\n","0 Epoch, Best epoch was updated! Valid Loss: 0.58417\n","Saving model checkpoint to chaii-qa-5-fold-xlmroberta-torch-fit/checkpoint-fold-4.\n","\n","Total Training Time: 3014.1038529872894secs, Average Training Time per Epoch: 3014.1038529872894secs.\n","Total Validation Time: 211.68424081802368secs, Average Validation Time per Epoch: 211.68424081802368secs.\n","\n","\n","--------------------------------------------------\n","FOLD: 5\n","--------------------------------------------------\n","Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n","Num examples Train= 24789, Num examples Valid=0\n","Total Training Steps: 3099, Total Warmup Steps: 309\n","Epoch: 00 [    4/24789 (  0%)], Train Loss: 2.95156\n","Epoch: 00 [   44/24789 (  0%)], Train Loss: 2.94525\n","Epoch: 00 [   84/24789 (  0%)], Train Loss: 2.93362\n","Epoch: 00 [  124/24789 (  1%)], Train Loss: 2.89873\n","Epoch: 00 [  164/24789 (  1%)], Train Loss: 2.85800\n","Epoch: 00 [  204/24789 (  1%)], Train Loss: 2.80939\n","Epoch: 00 [  244/24789 (  1%)], Train Loss: 2.73746\n","Epoch: 00 [  284/24789 (  1%)], Train Loss: 2.66317\n","Epoch: 00 [  324/24789 (  1%)], Train Loss: 2.58545\n","Epoch: 00 [  364/24789 (  1%)], Train Loss: 2.49850\n","Epoch: 00 [  404/24789 (  2%)], Train Loss: 2.39091\n","Epoch: 00 [  444/24789 (  2%)], Train Loss: 2.27494\n","Epoch: 00 [  484/24789 (  2%)], Train Loss: 2.15510\n","Epoch: 00 [  524/24789 (  2%)], Train Loss: 2.05361\n","Epoch: 00 [  564/24789 (  2%)], Train Loss: 1.95123\n","Epoch: 00 [  604/24789 (  2%)], Train Loss: 1.87151\n","Epoch: 00 [  644/24789 (  3%)], Train Loss: 1.79249\n","Epoch: 00 [  684/24789 (  3%)], Train Loss: 1.72863\n","Epoch: 00 [  724/24789 (  3%)], Train Loss: 1.67805\n","Epoch: 00 [  764/24789 (  3%)], Train Loss: 1.62171\n","Epoch: 00 [  804/24789 (  3%)], Train Loss: 1.56451\n","Epoch: 00 [  844/24789 (  3%)], Train Loss: 1.51280\n","Epoch: 00 [  884/24789 (  4%)], Train Loss: 1.47541\n","Epoch: 00 [  924/24789 (  4%)], Train Loss: 1.42626\n","Epoch: 00 [  964/24789 (  4%)], Train Loss: 1.38477\n","Epoch: 00 [ 1004/24789 (  4%)], Train Loss: 1.34361\n","Epoch: 00 [ 1044/24789 (  4%)], Train Loss: 1.30802\n","Epoch: 00 [ 1084/24789 (  4%)], Train Loss: 1.27673\n","Epoch: 00 [ 1124/24789 (  5%)], Train Loss: 1.25217\n","Epoch: 00 [ 1164/24789 (  5%)], Train Loss: 1.21995\n","Epoch: 00 [ 1204/24789 (  5%)], Train Loss: 1.19350\n","Epoch: 00 [ 1244/24789 (  5%)], Train Loss: 1.16942\n","Epoch: 00 [ 1284/24789 (  5%)], Train Loss: 1.14857\n","Epoch: 00 [ 1324/24789 (  5%)], Train Loss: 1.12739\n","Epoch: 00 [ 1364/24789 (  6%)], Train Loss: 1.10644\n","Epoch: 00 [ 1404/24789 (  6%)], Train Loss: 1.08297\n","Epoch: 00 [ 1444/24789 (  6%)], Train Loss: 1.06769\n","Epoch: 00 [ 1484/24789 (  6%)], Train Loss: 1.05387\n","Epoch: 00 [ 1524/24789 (  6%)], Train Loss: 1.03431\n","Epoch: 00 [ 1564/24789 (  6%)], Train Loss: 1.01899\n","Epoch: 00 [ 1604/24789 (  6%)], Train Loss: 1.00477\n","Epoch: 00 [ 1644/24789 (  7%)], Train Loss: 0.99077\n","Epoch: 00 [ 1684/24789 (  7%)], Train Loss: 0.97675\n","Epoch: 00 [ 1724/24789 (  7%)], Train Loss: 0.96426\n","Epoch: 00 [ 1764/24789 (  7%)], Train Loss: 0.95447\n","Epoch: 00 [ 1804/24789 (  7%)], Train Loss: 0.93751\n","Epoch: 00 [ 1844/24789 (  7%)], Train Loss: 0.92960\n","Epoch: 00 [ 1884/24789 (  8%)], Train Loss: 0.91785\n","Epoch: 00 [ 1924/24789 (  8%)], Train Loss: 0.90601\n","Epoch: 00 [ 1964/24789 (  8%)], Train Loss: 0.89557\n","Epoch: 00 [ 2004/24789 (  8%)], Train Loss: 0.88566\n","Epoch: 00 [ 2044/24789 (  8%)], Train Loss: 0.87827\n","Epoch: 00 [ 2084/24789 (  8%)], Train Loss: 0.86673\n","Epoch: 00 [ 2124/24789 (  9%)], Train Loss: 0.85896\n","Epoch: 00 [ 2164/24789 (  9%)], Train Loss: 0.85073\n","Epoch: 00 [ 2204/24789 (  9%)], Train Loss: 0.83926\n","Epoch: 00 [ 2244/24789 (  9%)], Train Loss: 0.83646\n","Epoch: 00 [ 2284/24789 (  9%)], Train Loss: 0.83016\n","Epoch: 00 [ 2324/24789 (  9%)], Train Loss: 0.82102\n","Epoch: 00 [ 2364/24789 ( 10%)], Train Loss: 0.81312\n","Epoch: 00 [ 2404/24789 ( 10%)], Train Loss: 0.80563\n","Epoch: 00 [ 2444/24789 ( 10%)], Train Loss: 0.80266\n","Epoch: 00 [ 2484/24789 ( 10%)], Train Loss: 0.79688\n","Epoch: 00 [ 2524/24789 ( 10%)], Train Loss: 0.79229\n","Epoch: 00 [ 2564/24789 ( 10%)], Train Loss: 0.78548\n","Epoch: 00 [ 2604/24789 ( 11%)], Train Loss: 0.77824\n","Epoch: 00 [ 2644/24789 ( 11%)], Train Loss: 0.76958\n","Epoch: 00 [ 2684/24789 ( 11%)], Train Loss: 0.76232\n","Epoch: 00 [ 2724/24789 ( 11%)], Train Loss: 0.75871\n","Epoch: 00 [ 2764/24789 ( 11%)], Train Loss: 0.75167\n","Epoch: 00 [ 2804/24789 ( 11%)], Train Loss: 0.74728\n","Epoch: 00 [ 2844/24789 ( 11%)], Train Loss: 0.74134\n","Epoch: 00 [ 2884/24789 ( 12%)], Train Loss: 0.73965\n","Epoch: 00 [ 2924/24789 ( 12%)], Train Loss: 0.73645\n","Epoch: 00 [ 2964/24789 ( 12%)], Train Loss: 0.72934\n","Epoch: 00 [ 3004/24789 ( 12%)], Train Loss: 0.72302\n","Epoch: 00 [ 3044/24789 ( 12%)], Train Loss: 0.71770\n","Epoch: 00 [ 3084/24789 ( 12%)], Train Loss: 0.71252\n","Epoch: 00 [ 3124/24789 ( 13%)], Train Loss: 0.70790\n","Epoch: 00 [ 3164/24789 ( 13%)], Train Loss: 0.70339\n","Epoch: 00 [ 3204/24789 ( 13%)], Train Loss: 0.70067\n","Epoch: 00 [ 3244/24789 ( 13%)], Train Loss: 0.69631\n","Epoch: 00 [ 3284/24789 ( 13%)], Train Loss: 0.69254\n","Epoch: 00 [ 3324/24789 ( 13%)], Train Loss: 0.68994\n","Epoch: 00 [ 3364/24789 ( 14%)], Train Loss: 0.68350\n","Epoch: 00 [ 3404/24789 ( 14%)], Train Loss: 0.68036\n","Epoch: 00 [ 3444/24789 ( 14%)], Train Loss: 0.67667\n","Epoch: 00 [ 3484/24789 ( 14%)], Train Loss: 0.67250\n","Epoch: 00 [ 3524/24789 ( 14%)], Train Loss: 0.67071\n","Epoch: 00 [ 3564/24789 ( 14%)], Train Loss: 0.66651\n","Epoch: 00 [ 3604/24789 ( 15%)], Train Loss: 0.66319\n","Epoch: 00 [ 3644/24789 ( 15%)], Train Loss: 0.65942\n","Epoch: 00 [ 3684/24789 ( 15%)], Train Loss: 0.65741\n","Epoch: 00 [ 3724/24789 ( 15%)], Train Loss: 0.65360\n","Epoch: 00 [ 3764/24789 ( 15%)], Train Loss: 0.65034\n","Epoch: 00 [ 3804/24789 ( 15%)], Train Loss: 0.64480\n","Epoch: 00 [ 3844/24789 ( 16%)], Train Loss: 0.64072\n","Epoch: 00 [ 3884/24789 ( 16%)], Train Loss: 0.63693\n","Epoch: 00 [ 3924/24789 ( 16%)], Train Loss: 0.63379\n","Epoch: 00 [ 3964/24789 ( 16%)], Train Loss: 0.63163\n","Epoch: 00 [ 4004/24789 ( 16%)], Train Loss: 0.62769\n","Epoch: 00 [ 4044/24789 ( 16%)], Train Loss: 0.62267\n","Epoch: 00 [ 4084/24789 ( 16%)], Train Loss: 0.62044\n","Epoch: 00 [ 4124/24789 ( 17%)], Train Loss: 0.61658\n","Epoch: 00 [ 4164/24789 ( 17%)], Train Loss: 0.61856\n","Epoch: 00 [ 4204/24789 ( 17%)], Train Loss: 0.61606\n","Epoch: 00 [ 4244/24789 ( 17%)], Train Loss: 0.61293\n","Epoch: 00 [ 4284/24789 ( 17%)], Train Loss: 0.61278\n","Epoch: 00 [ 4324/24789 ( 17%)], Train Loss: 0.60933\n","Epoch: 00 [ 4364/24789 ( 18%)], Train Loss: 0.60533\n","Epoch: 00 [ 4404/24789 ( 18%)], Train Loss: 0.60355\n","Epoch: 00 [ 4444/24789 ( 18%)], Train Loss: 0.60138\n","Epoch: 00 [ 4484/24789 ( 18%)], Train Loss: 0.59952\n","Epoch: 00 [ 4524/24789 ( 18%)], Train Loss: 0.59962\n","Epoch: 00 [ 4564/24789 ( 18%)], Train Loss: 0.59682\n","Epoch: 00 [ 4604/24789 ( 19%)], Train Loss: 0.59593\n","Epoch: 00 [ 4644/24789 ( 19%)], Train Loss: 0.59512\n","Epoch: 00 [ 4684/24789 ( 19%)], Train Loss: 0.59381\n","Epoch: 00 [ 4724/24789 ( 19%)], Train Loss: 0.59313\n","Epoch: 00 [ 4764/24789 ( 19%)], Train Loss: 0.59172\n","Epoch: 00 [ 4804/24789 ( 19%)], Train Loss: 0.58985\n","Epoch: 00 [ 4844/24789 ( 20%)], Train Loss: 0.58770\n","Epoch: 00 [ 4884/24789 ( 20%)], Train Loss: 0.58516\n","Epoch: 00 [ 4924/24789 ( 20%)], Train Loss: 0.58256\n","Epoch: 00 [ 4964/24789 ( 20%)], Train Loss: 0.57939\n","Epoch: 00 [ 5004/24789 ( 20%)], Train Loss: 0.57918\n","Epoch: 00 [ 5044/24789 ( 20%)], Train Loss: 0.57719\n","Epoch: 00 [ 5084/24789 ( 21%)], Train Loss: 0.57556\n","Epoch: 00 [ 5124/24789 ( 21%)], Train Loss: 0.57457\n","Epoch: 00 [ 5164/24789 ( 21%)], Train Loss: 0.57317\n","Epoch: 00 [ 5204/24789 ( 21%)], Train Loss: 0.57092\n","Epoch: 00 [ 5244/24789 ( 21%)], Train Loss: 0.56885\n","Epoch: 00 [ 5284/24789 ( 21%)], Train Loss: 0.56843\n","Epoch: 00 [ 5324/24789 ( 21%)], Train Loss: 0.56670\n","Epoch: 00 [ 5364/24789 ( 22%)], Train Loss: 0.56367\n","Epoch: 00 [ 5404/24789 ( 22%)], Train Loss: 0.56246\n","Epoch: 00 [ 5444/24789 ( 22%)], Train Loss: 0.56176\n","Epoch: 00 [ 5484/24789 ( 22%)], Train Loss: 0.55969\n","Epoch: 00 [ 5524/24789 ( 22%)], Train Loss: 0.55866\n","Epoch: 00 [ 5564/24789 ( 22%)], Train Loss: 0.55709\n","Epoch: 00 [ 5604/24789 ( 23%)], Train Loss: 0.55413\n","Epoch: 00 [ 5644/24789 ( 23%)], Train Loss: 0.55316\n","Epoch: 00 [ 5684/24789 ( 23%)], Train Loss: 0.55239\n","Epoch: 00 [ 5724/24789 ( 23%)], Train Loss: 0.55010\n","Epoch: 00 [ 5764/24789 ( 23%)], Train Loss: 0.54914\n","Epoch: 00 [ 5804/24789 ( 23%)], Train Loss: 0.55043\n","Epoch: 00 [ 5844/24789 ( 24%)], Train Loss: 0.54848\n","Epoch: 00 [ 5884/24789 ( 24%)], Train Loss: 0.54778\n","Epoch: 00 [ 5924/24789 ( 24%)], Train Loss: 0.54806\n","Epoch: 00 [ 5964/24789 ( 24%)], Train Loss: 0.54584\n","Epoch: 00 [ 6004/24789 ( 24%)], Train Loss: 0.54428\n","Epoch: 00 [ 6044/24789 ( 24%)], Train Loss: 0.54261\n","Epoch: 00 [ 6084/24789 ( 25%)], Train Loss: 0.54059\n","Epoch: 00 [ 6124/24789 ( 25%)], Train Loss: 0.53815\n","Epoch: 00 [ 6164/24789 ( 25%)], Train Loss: 0.53605\n","Epoch: 00 [ 6204/24789 ( 25%)], Train Loss: 0.53403\n","Epoch: 00 [ 6244/24789 ( 25%)], Train Loss: 0.53407\n","Epoch: 00 [ 6284/24789 ( 25%)], Train Loss: 0.53295\n","Epoch: 00 [ 6324/24789 ( 26%)], Train Loss: 0.53237\n","Epoch: 00 [ 6364/24789 ( 26%)], Train Loss: 0.53220\n","Epoch: 00 [ 6404/24789 ( 26%)], Train Loss: 0.53057\n","Epoch: 00 [ 6444/24789 ( 26%)], Train Loss: 0.52968\n","Epoch: 00 [ 6484/24789 ( 26%)], Train Loss: 0.52867\n","Epoch: 00 [ 6524/24789 ( 26%)], Train Loss: 0.52799\n","Epoch: 00 [ 6564/24789 ( 26%)], Train Loss: 0.52685\n","Epoch: 00 [ 6604/24789 ( 27%)], Train Loss: 0.52515\n","Epoch: 00 [ 6644/24789 ( 27%)], Train Loss: 0.52459\n","Epoch: 00 [ 6684/24789 ( 27%)], Train Loss: 0.52365\n","Epoch: 00 [ 6724/24789 ( 27%)], Train Loss: 0.52184\n","Epoch: 00 [ 6764/24789 ( 27%)], Train Loss: 0.52125\n","Epoch: 00 [ 6804/24789 ( 27%)], Train Loss: 0.52072\n","Epoch: 00 [ 6844/24789 ( 28%)], Train Loss: 0.52015\n","Epoch: 00 [ 6884/24789 ( 28%)], Train Loss: 0.51907\n","Epoch: 00 [ 6924/24789 ( 28%)], Train Loss: 0.51692\n","Epoch: 00 [ 6964/24789 ( 28%)], Train Loss: 0.51635\n","Epoch: 00 [ 7004/24789 ( 28%)], Train Loss: 0.51608\n","Epoch: 00 [ 7044/24789 ( 28%)], Train Loss: 0.51527\n","Epoch: 00 [ 7084/24789 ( 29%)], Train Loss: 0.51481\n","Epoch: 00 [ 7124/24789 ( 29%)], Train Loss: 0.51346\n","Epoch: 00 [ 7164/24789 ( 29%)], Train Loss: 0.51282\n","Epoch: 00 [ 7204/24789 ( 29%)], Train Loss: 0.51190\n","Epoch: 00 [ 7244/24789 ( 29%)], Train Loss: 0.51089\n","Epoch: 00 [ 7284/24789 ( 29%)], Train Loss: 0.51052\n","Epoch: 00 [ 7324/24789 ( 30%)], Train Loss: 0.50917\n","Epoch: 00 [ 7364/24789 ( 30%)], Train Loss: 0.50742\n","Epoch: 00 [ 7404/24789 ( 30%)], Train Loss: 0.50577\n","Epoch: 00 [ 7444/24789 ( 30%)], Train Loss: 0.50549\n","Epoch: 00 [ 7484/24789 ( 30%)], Train Loss: 0.50477\n","Epoch: 00 [ 7524/24789 ( 30%)], Train Loss: 0.50418\n","Epoch: 00 [ 7564/24789 ( 31%)], Train Loss: 0.50329\n","Epoch: 00 [ 7604/24789 ( 31%)], Train Loss: 0.50260\n","Epoch: 00 [ 7644/24789 ( 31%)], Train Loss: 0.50203\n","Epoch: 00 [ 7684/24789 ( 31%)], Train Loss: 0.50034\n","Epoch: 00 [ 7724/24789 ( 31%)], Train Loss: 0.49974\n","Epoch: 00 [ 7764/24789 ( 31%)], Train Loss: 0.49890\n","Epoch: 00 [ 7804/24789 ( 31%)], Train Loss: 0.49749\n","Epoch: 00 [ 7844/24789 ( 32%)], Train Loss: 0.49757\n","Epoch: 00 [ 7884/24789 ( 32%)], Train Loss: 0.49709\n","Epoch: 00 [ 7924/24789 ( 32%)], Train Loss: 0.49668\n","Epoch: 00 [ 7964/24789 ( 32%)], Train Loss: 0.49521\n","Epoch: 00 [ 8004/24789 ( 32%)], Train Loss: 0.49431\n","Epoch: 00 [ 8044/24789 ( 32%)], Train Loss: 0.49365\n","Epoch: 00 [ 8084/24789 ( 33%)], Train Loss: 0.49324\n","Epoch: 00 [ 8124/24789 ( 33%)], Train Loss: 0.49292\n","Epoch: 00 [ 8164/24789 ( 33%)], Train Loss: 0.49200\n","Epoch: 00 [ 8204/24789 ( 33%)], Train Loss: 0.49108\n","Epoch: 00 [ 8244/24789 ( 33%)], Train Loss: 0.48987\n","Epoch: 00 [ 8284/24789 ( 33%)], Train Loss: 0.48881\n","Epoch: 00 [ 8324/24789 ( 34%)], Train Loss: 0.48797\n","Epoch: 00 [ 8364/24789 ( 34%)], Train Loss: 0.48722\n","Epoch: 00 [ 8404/24789 ( 34%)], Train Loss: 0.48708\n","Epoch: 00 [ 8444/24789 ( 34%)], Train Loss: 0.48574\n","Epoch: 00 [ 8484/24789 ( 34%)], Train Loss: 0.48475\n","Epoch: 00 [ 8524/24789 ( 34%)], Train Loss: 0.48427\n","Epoch: 00 [ 8564/24789 ( 35%)], Train Loss: 0.48340\n","Epoch: 00 [ 8604/24789 ( 35%)], Train Loss: 0.48354\n","Epoch: 00 [ 8644/24789 ( 35%)], Train Loss: 0.48258\n","Epoch: 00 [ 8684/24789 ( 35%)], Train Loss: 0.48273\n","Epoch: 00 [ 8724/24789 ( 35%)], Train Loss: 0.48173\n","Epoch: 00 [ 8764/24789 ( 35%)], Train Loss: 0.48092\n","Epoch: 00 [ 8804/24789 ( 36%)], Train Loss: 0.48006\n","Epoch: 00 [ 8844/24789 ( 36%)], Train Loss: 0.47980\n","Epoch: 00 [ 8884/24789 ( 36%)], Train Loss: 0.48027\n","Epoch: 00 [ 8924/24789 ( 36%)], Train Loss: 0.48014\n","Epoch: 00 [ 8964/24789 ( 36%)], Train Loss: 0.47907\n","Epoch: 00 [ 9004/24789 ( 36%)], Train Loss: 0.47853\n","Epoch: 00 [ 9044/24789 ( 36%)], Train Loss: 0.47750\n","Epoch: 00 [ 9084/24789 ( 37%)], Train Loss: 0.47656\n","Epoch: 00 [ 9124/24789 ( 37%)], Train Loss: 0.47691\n","Epoch: 00 [ 9164/24789 ( 37%)], Train Loss: 0.47606\n","Epoch: 00 [ 9204/24789 ( 37%)], Train Loss: 0.47637\n","Epoch: 00 [ 9244/24789 ( 37%)], Train Loss: 0.47578\n","Epoch: 00 [ 9284/24789 ( 37%)], Train Loss: 0.47489\n","Epoch: 00 [ 9324/24789 ( 38%)], Train Loss: 0.47505\n","Epoch: 00 [ 9364/24789 ( 38%)], Train Loss: 0.47416\n","Epoch: 00 [ 9404/24789 ( 38%)], Train Loss: 0.47365\n","Epoch: 00 [ 9444/24789 ( 38%)], Train Loss: 0.47278\n","Epoch: 00 [ 9484/24789 ( 38%)], Train Loss: 0.47190\n","Epoch: 00 [ 9524/24789 ( 38%)], Train Loss: 0.47188\n","Epoch: 00 [ 9564/24789 ( 39%)], Train Loss: 0.47138\n","Epoch: 00 [ 9604/24789 ( 39%)], Train Loss: 0.47040\n","Epoch: 00 [ 9644/24789 ( 39%)], Train Loss: 0.46936\n","Epoch: 00 [ 9684/24789 ( 39%)], Train Loss: 0.46796\n","Epoch: 00 [ 9724/24789 ( 39%)], Train Loss: 0.46789\n","Epoch: 00 [ 9764/24789 ( 39%)], Train Loss: 0.46805\n","Epoch: 00 [ 9804/24789 ( 40%)], Train Loss: 0.46767\n","Epoch: 00 [ 9844/24789 ( 40%)], Train Loss: 0.46727\n","Epoch: 00 [ 9884/24789 ( 40%)], Train Loss: 0.46721\n","Epoch: 00 [ 9924/24789 ( 40%)], Train Loss: 0.46639\n","Epoch: 00 [ 9964/24789 ( 40%)], Train Loss: 0.46584\n","Epoch: 00 [10004/24789 ( 40%)], Train Loss: 0.46494\n","Epoch: 00 [10044/24789 ( 41%)], Train Loss: 0.46434\n","Epoch: 00 [10084/24789 ( 41%)], Train Loss: 0.46358\n","Epoch: 00 [10124/24789 ( 41%)], Train Loss: 0.46287\n","Epoch: 00 [10164/24789 ( 41%)], Train Loss: 0.46354\n","Epoch: 00 [10204/24789 ( 41%)], Train Loss: 0.46356\n","Epoch: 00 [10244/24789 ( 41%)], Train Loss: 0.46270\n","Epoch: 00 [10284/24789 ( 41%)], Train Loss: 0.46181\n","Epoch: 00 [10324/24789 ( 42%)], Train Loss: 0.46220\n","Epoch: 00 [10364/24789 ( 42%)], Train Loss: 0.46216\n","Epoch: 00 [10404/24789 ( 42%)], Train Loss: 0.46185\n","Epoch: 00 [10444/24789 ( 42%)], Train Loss: 0.46106\n","Epoch: 00 [10484/24789 ( 42%)], Train Loss: 0.46076\n","Epoch: 00 [10524/24789 ( 42%)], Train Loss: 0.46022\n","Epoch: 00 [10564/24789 ( 43%)], Train Loss: 0.45958\n","Epoch: 00 [10604/24789 ( 43%)], Train Loss: 0.45883\n","Epoch: 00 [10644/24789 ( 43%)], Train Loss: 0.45818\n","Epoch: 00 [10684/24789 ( 43%)], Train Loss: 0.45808\n","Epoch: 00 [10724/24789 ( 43%)], Train Loss: 0.45773\n","Epoch: 00 [10764/24789 ( 43%)], Train Loss: 0.45672\n","Epoch: 00 [10804/24789 ( 44%)], Train Loss: 0.45704\n","Epoch: 00 [10844/24789 ( 44%)], Train Loss: 0.45650\n","Epoch: 00 [10884/24789 ( 44%)], Train Loss: 0.45665\n","Epoch: 00 [10924/24789 ( 44%)], Train Loss: 0.45620\n","Epoch: 00 [10964/24789 ( 44%)], Train Loss: 0.45617\n","Epoch: 00 [11004/24789 ( 44%)], Train Loss: 0.45591\n","Epoch: 00 [11044/24789 ( 45%)], Train Loss: 0.45589\n","Epoch: 00 [11084/24789 ( 45%)], Train Loss: 0.45504\n","Epoch: 00 [11124/24789 ( 45%)], Train Loss: 0.45515\n","Epoch: 00 [11164/24789 ( 45%)], Train Loss: 0.45469\n","Epoch: 00 [11204/24789 ( 45%)], Train Loss: 0.45423\n","Epoch: 00 [11244/24789 ( 45%)], Train Loss: 0.45311\n","Epoch: 00 [11284/24789 ( 46%)], Train Loss: 0.45244\n","Epoch: 00 [11324/24789 ( 46%)], Train Loss: 0.45227\n","Epoch: 00 [11364/24789 ( 46%)], Train Loss: 0.45221\n","Epoch: 00 [11404/24789 ( 46%)], Train Loss: 0.45122\n","Epoch: 00 [11444/24789 ( 46%)], Train Loss: 0.45118\n","Epoch: 00 [11484/24789 ( 46%)], Train Loss: 0.45132\n","Epoch: 00 [11524/24789 ( 46%)], Train Loss: 0.45105\n","Epoch: 00 [11564/24789 ( 47%)], Train Loss: 0.45042\n","Epoch: 00 [11604/24789 ( 47%)], Train Loss: 0.44982\n","Epoch: 00 [11644/24789 ( 47%)], Train Loss: 0.44896\n","Epoch: 00 [11684/24789 ( 47%)], Train Loss: 0.44872\n","Epoch: 00 [11724/24789 ( 47%)], Train Loss: 0.44802\n","Epoch: 00 [11764/24789 ( 47%)], Train Loss: 0.44767\n","Epoch: 00 [11804/24789 ( 48%)], Train Loss: 0.44794\n","Epoch: 00 [11844/24789 ( 48%)], Train Loss: 0.44730\n","Epoch: 00 [11884/24789 ( 48%)], Train Loss: 0.44685\n","Epoch: 00 [11924/24789 ( 48%)], Train Loss: 0.44597\n","Epoch: 00 [11964/24789 ( 48%)], Train Loss: 0.44544\n","Epoch: 00 [12004/24789 ( 48%)], Train Loss: 0.44500\n","Epoch: 00 [12044/24789 ( 49%)], Train Loss: 0.44482\n","Epoch: 00 [12084/24789 ( 49%)], Train Loss: 0.44407\n","Epoch: 00 [12124/24789 ( 49%)], Train Loss: 0.44353\n","Epoch: 00 [12164/24789 ( 49%)], Train Loss: 0.44326\n","Epoch: 00 [12204/24789 ( 49%)], Train Loss: 0.44216\n","Epoch: 00 [12244/24789 ( 49%)], Train Loss: 0.44219\n","Epoch: 00 [12284/24789 ( 50%)], Train Loss: 0.44145\n","Epoch: 00 [12324/24789 ( 50%)], Train Loss: 0.44046\n","Epoch: 00 [12364/24789 ( 50%)], Train Loss: 0.43999\n","Epoch: 00 [12404/24789 ( 50%)], Train Loss: 0.43961\n","Epoch: 00 [12444/24789 ( 50%)], Train Loss: 0.43959\n","Epoch: 00 [12484/24789 ( 50%)], Train Loss: 0.43880\n","Epoch: 00 [12524/24789 ( 51%)], Train Loss: 0.43829\n","Epoch: 00 [12564/24789 ( 51%)], Train Loss: 0.43800\n","Epoch: 00 [12604/24789 ( 51%)], Train Loss: 0.43731\n","Epoch: 00 [12644/24789 ( 51%)], Train Loss: 0.43778\n","Epoch: 00 [12684/24789 ( 51%)], Train Loss: 0.43712\n","Epoch: 00 [12724/24789 ( 51%)], Train Loss: 0.43635\n","Epoch: 00 [12764/24789 ( 51%)], Train Loss: 0.43592\n","Epoch: 00 [12804/24789 ( 52%)], Train Loss: 0.43576\n","Epoch: 00 [12844/24789 ( 52%)], Train Loss: 0.43560\n","Epoch: 00 [12884/24789 ( 52%)], Train Loss: 0.43469\n","Epoch: 00 [12924/24789 ( 52%)], Train Loss: 0.43485\n","Epoch: 00 [12964/24789 ( 52%)], Train Loss: 0.43418\n","Epoch: 00 [13004/24789 ( 52%)], Train Loss: 0.43395\n","Epoch: 00 [13044/24789 ( 53%)], Train Loss: 0.43370\n","Epoch: 00 [13084/24789 ( 53%)], Train Loss: 0.43369\n","Epoch: 00 [13124/24789 ( 53%)], Train Loss: 0.43328\n","Epoch: 00 [13164/24789 ( 53%)], Train Loss: 0.43267\n","Epoch: 00 [13204/24789 ( 53%)], Train Loss: 0.43192\n","Epoch: 00 [13244/24789 ( 53%)], Train Loss: 0.43136\n","Epoch: 00 [13284/24789 ( 54%)], Train Loss: 0.43085\n","Epoch: 00 [13324/24789 ( 54%)], Train Loss: 0.43024\n","Epoch: 00 [13364/24789 ( 54%)], Train Loss: 0.42975\n","Epoch: 00 [13404/24789 ( 54%)], Train Loss: 0.42916\n","Epoch: 00 [13444/24789 ( 54%)], Train Loss: 0.42866\n","Epoch: 00 [13484/24789 ( 54%)], Train Loss: 0.42810\n","Epoch: 00 [13524/24789 ( 55%)], Train Loss: 0.42741\n","Epoch: 00 [13564/24789 ( 55%)], Train Loss: 0.42696\n","Epoch: 00 [13604/24789 ( 55%)], Train Loss: 0.42698\n","Epoch: 00 [13644/24789 ( 55%)], Train Loss: 0.42718\n","Epoch: 00 [13684/24789 ( 55%)], Train Loss: 0.42717\n","Epoch: 00 [13724/24789 ( 55%)], Train Loss: 0.42682\n","Epoch: 00 [13764/24789 ( 56%)], Train Loss: 0.42627\n","Epoch: 00 [13804/24789 ( 56%)], Train Loss: 0.42589\n","Epoch: 00 [13844/24789 ( 56%)], Train Loss: 0.42625\n","Epoch: 00 [13884/24789 ( 56%)], Train Loss: 0.42634\n","Epoch: 00 [13924/24789 ( 56%)], Train Loss: 0.42592\n","Epoch: 00 [13964/24789 ( 56%)], Train Loss: 0.42512\n","Epoch: 00 [14004/24789 ( 56%)], Train Loss: 0.42453\n","Epoch: 00 [14044/24789 ( 57%)], Train Loss: 0.42409\n","Epoch: 00 [14084/24789 ( 57%)], Train Loss: 0.42369\n","Epoch: 00 [14124/24789 ( 57%)], Train Loss: 0.42325\n","Epoch: 00 [14164/24789 ( 57%)], Train Loss: 0.42258\n","Epoch: 00 [14204/24789 ( 57%)], Train Loss: 0.42231\n","Epoch: 00 [14244/24789 ( 57%)], Train Loss: 0.42240\n","Epoch: 00 [14284/24789 ( 58%)], Train Loss: 0.42190\n","Epoch: 00 [14324/24789 ( 58%)], Train Loss: 0.42188\n","Epoch: 00 [14364/24789 ( 58%)], Train Loss: 0.42157\n","Epoch: 00 [14404/24789 ( 58%)], Train Loss: 0.42136\n","Epoch: 00 [14444/24789 ( 58%)], Train Loss: 0.42100\n","Epoch: 00 [14484/24789 ( 58%)], Train Loss: 0.42078\n","Epoch: 00 [14524/24789 ( 59%)], Train Loss: 0.42037\n","Epoch: 00 [14564/24789 ( 59%)], Train Loss: 0.42018\n","Epoch: 00 [14604/24789 ( 59%)], Train Loss: 0.42000\n","Epoch: 00 [14644/24789 ( 59%)], Train Loss: 0.41919\n","Epoch: 00 [14684/24789 ( 59%)], Train Loss: 0.41856\n","Epoch: 00 [14724/24789 ( 59%)], Train Loss: 0.41856\n","Epoch: 00 [14764/24789 ( 60%)], Train Loss: 0.41839\n","Epoch: 00 [14804/24789 ( 60%)], Train Loss: 0.41796\n","Epoch: 00 [14844/24789 ( 60%)], Train Loss: 0.41760\n","Epoch: 00 [14884/24789 ( 60%)], Train Loss: 0.41702\n","Epoch: 00 [14924/24789 ( 60%)], Train Loss: 0.41702\n","Epoch: 00 [14964/24789 ( 60%)], Train Loss: 0.41689\n","Epoch: 00 [15004/24789 ( 61%)], Train Loss: 0.41619\n","Epoch: 00 [15044/24789 ( 61%)], Train Loss: 0.41563\n","Epoch: 00 [15084/24789 ( 61%)], Train Loss: 0.41492\n","Epoch: 00 [15124/24789 ( 61%)], Train Loss: 0.41482\n","Epoch: 00 [15164/24789 ( 61%)], Train Loss: 0.41480\n","Epoch: 00 [15204/24789 ( 61%)], Train Loss: 0.41447\n","Epoch: 00 [15244/24789 ( 61%)], Train Loss: 0.41455\n","Epoch: 00 [15284/24789 ( 62%)], Train Loss: 0.41402\n","Epoch: 00 [15324/24789 ( 62%)], Train Loss: 0.41374\n","Epoch: 00 [15364/24789 ( 62%)], Train Loss: 0.41360\n","Epoch: 00 [15404/24789 ( 62%)], Train Loss: 0.41299\n","Epoch: 00 [15444/24789 ( 62%)], Train Loss: 0.41254\n","Epoch: 00 [15484/24789 ( 62%)], Train Loss: 0.41262\n","Epoch: 00 [15524/24789 ( 63%)], Train Loss: 0.41231\n","Epoch: 00 [15564/24789 ( 63%)], Train Loss: 0.41183\n","Epoch: 00 [15604/24789 ( 63%)], Train Loss: 0.41154\n","Epoch: 00 [15644/24789 ( 63%)], Train Loss: 0.41140\n","Epoch: 00 [15684/24789 ( 63%)], Train Loss: 0.41110\n","Epoch: 00 [15724/24789 ( 63%)], Train Loss: 0.41058\n","Epoch: 00 [15764/24789 ( 64%)], Train Loss: 0.41018\n","Epoch: 00 [15804/24789 ( 64%)], Train Loss: 0.41023\n","Epoch: 00 [15844/24789 ( 64%)], Train Loss: 0.40973\n","Epoch: 00 [15884/24789 ( 64%)], Train Loss: 0.40951\n","Epoch: 00 [15924/24789 ( 64%)], Train Loss: 0.40945\n","Epoch: 00 [15964/24789 ( 64%)], Train Loss: 0.40924\n","Epoch: 00 [16004/24789 ( 65%)], Train Loss: 0.40895\n","Epoch: 00 [16044/24789 ( 65%)], Train Loss: 0.40836\n","Epoch: 00 [16084/24789 ( 65%)], Train Loss: 0.40786\n","Epoch: 00 [16124/24789 ( 65%)], Train Loss: 0.40766\n","Epoch: 00 [16164/24789 ( 65%)], Train Loss: 0.40770\n","Epoch: 00 [16204/24789 ( 65%)], Train Loss: 0.40717\n","Epoch: 00 [16244/24789 ( 66%)], Train Loss: 0.40646\n","Epoch: 00 [16284/24789 ( 66%)], Train Loss: 0.40613\n","Epoch: 00 [16324/24789 ( 66%)], Train Loss: 0.40609\n","Epoch: 00 [16364/24789 ( 66%)], Train Loss: 0.40529\n","Epoch: 00 [16404/24789 ( 66%)], Train Loss: 0.40529\n","Epoch: 00 [16444/24789 ( 66%)], Train Loss: 0.40541\n","Epoch: 00 [16484/24789 ( 66%)], Train Loss: 0.40537\n","Epoch: 00 [16524/24789 ( 67%)], Train Loss: 0.40534\n","Epoch: 00 [16564/24789 ( 67%)], Train Loss: 0.40495\n","Epoch: 00 [16604/24789 ( 67%)], Train Loss: 0.40486\n","Epoch: 00 [16644/24789 ( 67%)], Train Loss: 0.40497\n","Epoch: 00 [16684/24789 ( 67%)], Train Loss: 0.40446\n","Epoch: 00 [16724/24789 ( 67%)], Train Loss: 0.40423\n","Epoch: 00 [16764/24789 ( 68%)], Train Loss: 0.40373\n","Epoch: 00 [16804/24789 ( 68%)], Train Loss: 0.40364\n","Epoch: 00 [16844/24789 ( 68%)], Train Loss: 0.40350\n","Epoch: 00 [16884/24789 ( 68%)], Train Loss: 0.40311\n","Epoch: 00 [16924/24789 ( 68%)], Train Loss: 0.40274\n","Epoch: 00 [16964/24789 ( 68%)], Train Loss: 0.40230\n","Epoch: 00 [17004/24789 ( 69%)], Train Loss: 0.40204\n","Epoch: 00 [17044/24789 ( 69%)], Train Loss: 0.40141\n","Epoch: 00 [17084/24789 ( 69%)], Train Loss: 0.40117\n","Epoch: 00 [17124/24789 ( 69%)], Train Loss: 0.40098\n","Epoch: 00 [17164/24789 ( 69%)], Train Loss: 0.40066\n","Epoch: 00 [17204/24789 ( 69%)], Train Loss: 0.40058\n","Epoch: 00 [17244/24789 ( 70%)], Train Loss: 0.40028\n","Epoch: 00 [17284/24789 ( 70%)], Train Loss: 0.39989\n","Epoch: 00 [17324/24789 ( 70%)], Train Loss: 0.39958\n","Epoch: 00 [17364/24789 ( 70%)], Train Loss: 0.39980\n","Epoch: 00 [17404/24789 ( 70%)], Train Loss: 0.39933\n","Epoch: 00 [17444/24789 ( 70%)], Train Loss: 0.39920\n","Epoch: 00 [17484/24789 ( 71%)], Train Loss: 0.39935\n","Epoch: 00 [17524/24789 ( 71%)], Train Loss: 0.39892\n","Epoch: 00 [17564/24789 ( 71%)], Train Loss: 0.39875\n","Epoch: 00 [17604/24789 ( 71%)], Train Loss: 0.39884\n","Epoch: 00 [17644/24789 ( 71%)], Train Loss: 0.39862\n","Epoch: 00 [17684/24789 ( 71%)], Train Loss: 0.39859\n","Epoch: 00 [17724/24789 ( 71%)], Train Loss: 0.39789\n","Epoch: 00 [17764/24789 ( 72%)], Train Loss: 0.39795\n","Epoch: 00 [17804/24789 ( 72%)], Train Loss: 0.39787\n","Epoch: 00 [17844/24789 ( 72%)], Train Loss: 0.39814\n","Epoch: 00 [17884/24789 ( 72%)], Train Loss: 0.39800\n","Epoch: 00 [17924/24789 ( 72%)], Train Loss: 0.39751\n","Epoch: 00 [17964/24789 ( 72%)], Train Loss: 0.39750\n","Epoch: 00 [18004/24789 ( 73%)], Train Loss: 0.39706\n","Epoch: 00 [18044/24789 ( 73%)], Train Loss: 0.39715\n","Epoch: 00 [18084/24789 ( 73%)], Train Loss: 0.39669\n","Epoch: 00 [18124/24789 ( 73%)], Train Loss: 0.39617\n","Epoch: 00 [18164/24789 ( 73%)], Train Loss: 0.39590\n","Epoch: 00 [18204/24789 ( 73%)], Train Loss: 0.39558\n","Epoch: 00 [18244/24789 ( 74%)], Train Loss: 0.39557\n","Epoch: 00 [18284/24789 ( 74%)], Train Loss: 0.39521\n","Epoch: 00 [18324/24789 ( 74%)], Train Loss: 0.39470\n","Epoch: 00 [18364/24789 ( 74%)], Train Loss: 0.39435\n","Epoch: 00 [18404/24789 ( 74%)], Train Loss: 0.39416\n","Epoch: 00 [18444/24789 ( 74%)], Train Loss: 0.39382\n","Epoch: 00 [18484/24789 ( 75%)], Train Loss: 0.39359\n","Epoch: 00 [18524/24789 ( 75%)], Train Loss: 0.39333\n","Epoch: 00 [18564/24789 ( 75%)], Train Loss: 0.39321\n","Epoch: 00 [18604/24789 ( 75%)], Train Loss: 0.39285\n","Epoch: 00 [18644/24789 ( 75%)], Train Loss: 0.39265\n","Epoch: 00 [18684/24789 ( 75%)], Train Loss: 0.39287\n","Epoch: 00 [18724/24789 ( 76%)], Train Loss: 0.39295\n","Epoch: 00 [18764/24789 ( 76%)], Train Loss: 0.39259\n","Epoch: 00 [18804/24789 ( 76%)], Train Loss: 0.39244\n","Epoch: 00 [18844/24789 ( 76%)], Train Loss: 0.39201\n","Epoch: 00 [18884/24789 ( 76%)], Train Loss: 0.39171\n","Epoch: 00 [18924/24789 ( 76%)], Train Loss: 0.39169\n","Epoch: 00 [18964/24789 ( 77%)], Train Loss: 0.39132\n","Epoch: 00 [19004/24789 ( 77%)], Train Loss: 0.39136\n","Epoch: 00 [19044/24789 ( 77%)], Train Loss: 0.39136\n","Epoch: 00 [19084/24789 ( 77%)], Train Loss: 0.39113\n","Epoch: 00 [19124/24789 ( 77%)], Train Loss: 0.39102\n","Epoch: 00 [19164/24789 ( 77%)], Train Loss: 0.39091\n","Epoch: 00 [19204/24789 ( 77%)], Train Loss: 0.39053\n","Epoch: 00 [19244/24789 ( 78%)], Train Loss: 0.39099\n","Epoch: 00 [19284/24789 ( 78%)], Train Loss: 0.39088\n","Epoch: 00 [19324/24789 ( 78%)], Train Loss: 0.39078\n","Epoch: 00 [19364/24789 ( 78%)], Train Loss: 0.39051\n","Epoch: 00 [19404/24789 ( 78%)], Train Loss: 0.39032\n","Epoch: 00 [19444/24789 ( 78%)], Train Loss: 0.39025\n","Epoch: 00 [19484/24789 ( 79%)], Train Loss: 0.39006\n","Epoch: 00 [19524/24789 ( 79%)], Train Loss: 0.38997\n","Epoch: 00 [19564/24789 ( 79%)], Train Loss: 0.38970\n","Epoch: 00 [19604/24789 ( 79%)], Train Loss: 0.38981\n","Epoch: 00 [19644/24789 ( 79%)], Train Loss: 0.38977\n","Epoch: 00 [19684/24789 ( 79%)], Train Loss: 0.38958\n","Epoch: 00 [19724/24789 ( 80%)], Train Loss: 0.38920\n","Epoch: 00 [19764/24789 ( 80%)], Train Loss: 0.38925\n","Epoch: 00 [19804/24789 ( 80%)], Train Loss: 0.38909\n","Epoch: 00 [19844/24789 ( 80%)], Train Loss: 0.38887\n","Epoch: 00 [19884/24789 ( 80%)], Train Loss: 0.38903\n","Epoch: 00 [19924/24789 ( 80%)], Train Loss: 0.38904\n","Epoch: 00 [19964/24789 ( 81%)], Train Loss: 0.38891\n","Epoch: 00 [20004/24789 ( 81%)], Train Loss: 0.38850\n","Epoch: 00 [20044/24789 ( 81%)], Train Loss: 0.38827\n","Epoch: 00 [20084/24789 ( 81%)], Train Loss: 0.38807\n","Epoch: 00 [20124/24789 ( 81%)], Train Loss: 0.38765\n","Epoch: 00 [20164/24789 ( 81%)], Train Loss: 0.38740\n","Epoch: 00 [20204/24789 ( 82%)], Train Loss: 0.38696\n","Epoch: 00 [20244/24789 ( 82%)], Train Loss: 0.38722\n","Epoch: 00 [20284/24789 ( 82%)], Train Loss: 0.38718\n","Epoch: 00 [20324/24789 ( 82%)], Train Loss: 0.38706\n","Epoch: 00 [20364/24789 ( 82%)], Train Loss: 0.38700\n","Epoch: 00 [20404/24789 ( 82%)], Train Loss: 0.38704\n","Epoch: 00 [20444/24789 ( 82%)], Train Loss: 0.38689\n","Epoch: 00 [20484/24789 ( 83%)], Train Loss: 0.38687\n","Epoch: 00 [20524/24789 ( 83%)], Train Loss: 0.38657\n","Epoch: 00 [20564/24789 ( 83%)], Train Loss: 0.38669\n","Epoch: 00 [20604/24789 ( 83%)], Train Loss: 0.38649\n","Epoch: 00 [20644/24789 ( 83%)], Train Loss: 0.38629\n","Epoch: 00 [20684/24789 ( 83%)], Train Loss: 0.38638\n","Epoch: 00 [20724/24789 ( 84%)], Train Loss: 0.38638\n","Epoch: 00 [20764/24789 ( 84%)], Train Loss: 0.38623\n","Epoch: 00 [20804/24789 ( 84%)], Train Loss: 0.38618\n","Epoch: 00 [20844/24789 ( 84%)], Train Loss: 0.38617\n","Epoch: 00 [20884/24789 ( 84%)], Train Loss: 0.38599\n","Epoch: 00 [20924/24789 ( 84%)], Train Loss: 0.38598\n","Epoch: 00 [20964/24789 ( 85%)], Train Loss: 0.38569\n","Epoch: 00 [21004/24789 ( 85%)], Train Loss: 0.38551\n","Epoch: 00 [21044/24789 ( 85%)], Train Loss: 0.38504\n","Epoch: 00 [21084/24789 ( 85%)], Train Loss: 0.38491\n","Epoch: 00 [21124/24789 ( 85%)], Train Loss: 0.38470\n","Epoch: 00 [21164/24789 ( 85%)], Train Loss: 0.38431\n","Epoch: 00 [21204/24789 ( 86%)], Train Loss: 0.38399\n","Epoch: 00 [21244/24789 ( 86%)], Train Loss: 0.38356\n","Epoch: 00 [21284/24789 ( 86%)], Train Loss: 0.38322\n","Epoch: 00 [21324/24789 ( 86%)], Train Loss: 0.38305\n","Epoch: 00 [21364/24789 ( 86%)], Train Loss: 0.38311\n","Epoch: 00 [21404/24789 ( 86%)], Train Loss: 0.38269\n","Epoch: 00 [21444/24789 ( 87%)], Train Loss: 0.38280\n","Epoch: 00 [21484/24789 ( 87%)], Train Loss: 0.38248\n","Epoch: 00 [21524/24789 ( 87%)], Train Loss: 0.38216\n","Epoch: 00 [21564/24789 ( 87%)], Train Loss: 0.38192\n","Epoch: 00 [21604/24789 ( 87%)], Train Loss: 0.38176\n","Epoch: 00 [21644/24789 ( 87%)], Train Loss: 0.38158\n","Epoch: 00 [21684/24789 ( 87%)], Train Loss: 0.38126\n","Epoch: 00 [21724/24789 ( 88%)], Train Loss: 0.38105\n","Epoch: 00 [21764/24789 ( 88%)], Train Loss: 0.38072\n","Epoch: 00 [21804/24789 ( 88%)], Train Loss: 0.38053\n","Epoch: 00 [21844/24789 ( 88%)], Train Loss: 0.38023\n","Epoch: 00 [21884/24789 ( 88%)], Train Loss: 0.38004\n","Epoch: 00 [21924/24789 ( 88%)], Train Loss: 0.37981\n","Epoch: 00 [21964/24789 ( 89%)], Train Loss: 0.37976\n","Epoch: 00 [22004/24789 ( 89%)], Train Loss: 0.37971\n","Epoch: 00 [22044/24789 ( 89%)], Train Loss: 0.37923\n","Epoch: 00 [22084/24789 ( 89%)], Train Loss: 0.37906\n","Epoch: 00 [22124/24789 ( 89%)], Train Loss: 0.37888\n","Epoch: 00 [22164/24789 ( 89%)], Train Loss: 0.37860\n","Epoch: 00 [22204/24789 ( 90%)], Train Loss: 0.37813\n","Epoch: 00 [22244/24789 ( 90%)], Train Loss: 0.37796\n","Epoch: 00 [22284/24789 ( 90%)], Train Loss: 0.37815\n","Epoch: 00 [22324/24789 ( 90%)], Train Loss: 0.37775\n","Epoch: 00 [22364/24789 ( 90%)], Train Loss: 0.37739\n","Epoch: 00 [22404/24789 ( 90%)], Train Loss: 0.37718\n","Epoch: 00 [22444/24789 ( 91%)], Train Loss: 0.37688\n","Epoch: 00 [22484/24789 ( 91%)], Train Loss: 0.37694\n","Epoch: 00 [22524/24789 ( 91%)], Train Loss: 0.37660\n","Epoch: 00 [22564/24789 ( 91%)], Train Loss: 0.37637\n","Epoch: 00 [22604/24789 ( 91%)], Train Loss: 0.37619\n","Epoch: 00 [22644/24789 ( 91%)], Train Loss: 0.37592\n","Epoch: 00 [22684/24789 ( 92%)], Train Loss: 0.37570\n","Epoch: 00 [22724/24789 ( 92%)], Train Loss: 0.37530\n","Epoch: 00 [22764/24789 ( 92%)], Train Loss: 0.37519\n","Epoch: 00 [22804/24789 ( 92%)], Train Loss: 0.37488\n","Epoch: 00 [22844/24789 ( 92%)], Train Loss: 0.37463\n","Epoch: 00 [22884/24789 ( 92%)], Train Loss: 0.37437\n","Epoch: 00 [22924/24789 ( 92%)], Train Loss: 0.37423\n","Epoch: 00 [22964/24789 ( 93%)], Train Loss: 0.37389\n","Epoch: 00 [23004/24789 ( 93%)], Train Loss: 0.37366\n","Epoch: 00 [23044/24789 ( 93%)], Train Loss: 0.37357\n","Epoch: 00 [23084/24789 ( 93%)], Train Loss: 0.37362\n","Epoch: 00 [23124/24789 ( 93%)], Train Loss: 0.37330\n","Epoch: 00 [23164/24789 ( 93%)], Train Loss: 0.37298\n","Epoch: 00 [23204/24789 ( 94%)], Train Loss: 0.37291\n","Epoch: 00 [23244/24789 ( 94%)], Train Loss: 0.37242\n","Epoch: 00 [23284/24789 ( 94%)], Train Loss: 0.37248\n","Epoch: 00 [23324/24789 ( 94%)], Train Loss: 0.37247\n","Epoch: 00 [23364/24789 ( 94%)], Train Loss: 0.37241\n","Epoch: 00 [23404/24789 ( 94%)], Train Loss: 0.37229\n","Epoch: 00 [23444/24789 ( 95%)], Train Loss: 0.37216\n","Epoch: 00 [23484/24789 ( 95%)], Train Loss: 0.37202\n","Epoch: 00 [23524/24789 ( 95%)], Train Loss: 0.37223\n","Epoch: 00 [23564/24789 ( 95%)], Train Loss: 0.37220\n","Epoch: 00 [23604/24789 ( 95%)], Train Loss: 0.37185\n","Epoch: 00 [23644/24789 ( 95%)], Train Loss: 0.37154\n","Epoch: 00 [23684/24789 ( 96%)], Train Loss: 0.37144\n","Epoch: 00 [23724/24789 ( 96%)], Train Loss: 0.37122\n","Epoch: 00 [23764/24789 ( 96%)], Train Loss: 0.37139\n","Epoch: 00 [23804/24789 ( 96%)], Train Loss: 0.37129\n","Epoch: 00 [23844/24789 ( 96%)], Train Loss: 0.37110\n","Epoch: 00 [23884/24789 ( 96%)], Train Loss: 0.37094\n","Epoch: 00 [23924/24789 ( 97%)], Train Loss: 0.37107\n","Epoch: 00 [23964/24789 ( 97%)], Train Loss: 0.37086\n","Epoch: 00 [24004/24789 ( 97%)], Train Loss: 0.37087\n","Epoch: 00 [24044/24789 ( 97%)], Train Loss: 0.37062\n","Epoch: 00 [24084/24789 ( 97%)], Train Loss: 0.37063\n","Epoch: 00 [24124/24789 ( 97%)], Train Loss: 0.37032\n","Epoch: 00 [24164/24789 ( 97%)], Train Loss: 0.37032\n","Epoch: 00 [24204/24789 ( 98%)], Train Loss: 0.36998\n","Epoch: 00 [24244/24789 ( 98%)], Train Loss: 0.36972\n","Epoch: 00 [24284/24789 ( 98%)], Train Loss: 0.36963\n","Epoch: 00 [24324/24789 ( 98%)], Train Loss: 0.36951\n","Epoch: 00 [24364/24789 ( 98%)], Train Loss: 0.36912\n","Epoch: 00 [24404/24789 ( 98%)], Train Loss: 0.36928\n","Epoch: 00 [24444/24789 ( 99%)], Train Loss: 0.36903\n","Epoch: 00 [24484/24789 ( 99%)], Train Loss: 0.36908\n","Epoch: 00 [24524/24789 ( 99%)], Train Loss: 0.36918\n","Epoch: 00 [24564/24789 ( 99%)], Train Loss: 0.36890\n","Epoch: 00 [24604/24789 ( 99%)], Train Loss: 0.36879\n","Epoch: 00 [24644/24789 ( 99%)], Train Loss: 0.36881\n","Epoch: 00 [24684/24789 (100%)], Train Loss: 0.36870\n","Epoch: 00 [24724/24789 (100%)], Train Loss: 0.36836\n","Epoch: 00 [24764/24789 (100%)], Train Loss: 0.36806\n","Epoch: 00 [24789/24789 (100%)], Train Loss: 0.36791\n","----Validation Results Summary----\n","Epoch: [0] Valid Loss: 0.00000\n","0 Epoch, Best epoch was updated! Valid Loss: 0.00000\n","Saving model checkpoint to chaii-qa-5-fold-xlmroberta-torch-fit/checkpoint-fold-5.\n","\n","Total Training Time: 3713.3226940631866secs, Average Training Time per Epoch: 3713.3226940631866secs.\n","Total Validation Time: 0.560107946395874secs, Average Validation Time per Epoch: 0.560107946395874secs.\n"]}]},{"cell_type":"code","metadata":{"id":"DkjRIhdbwjHx","execution":{"iopub.status.busy":"2021-08-17T20:36:49.722586Z","iopub.execute_input":"2021-08-17T20:36:49.723011Z","iopub.status.idle":"2021-08-17T20:36:49.727659Z","shell.execute_reply.started":"2021-08-17T20:36:49.722978Z","shell.execute_reply":"2021-08-17T20:36:49.726609Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1636458385702,"user_tz":-540,"elapsed":27,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["# example for training second fold\n","\n","# for fold in range(1, 2):\n","#     print();print()\n","#     print('-'*50)\n","#     print(f'FOLD: {fold}')\n","#     print('-'*50)\n","#     run(train, fold)"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7uIwhUECP4iP"},"source":["### Thanks and please do Upvote!"]}]}