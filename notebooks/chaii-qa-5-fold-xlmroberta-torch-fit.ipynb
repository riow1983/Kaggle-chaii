{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"name":"chaii-qa-5-fold-xlmroberta-torch-fit.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"ba886f9fc6524981bed6628beced272d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_81ca36e618cc406cb6223333829a3a8c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_6fcc9be52a1343ebb24f488aa9ed2293","IPY_MODEL_64d6c79f402a4dc4a64b34c30413ed7c","IPY_MODEL_ca383419de924d24842fda34acb47f92"]}},"81ca36e618cc406cb6223333829a3a8c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6fcc9be52a1343ebb24f488aa9ed2293":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8d2f41a8d72a4873912117d10b997435","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_39cb5373d0b848e092de667993500759"}},"64d6c79f402a4dc4a64b34c30413ed7c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_9bab60a78e2448268f01323dad9563d1","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":606,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":606,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0f1eec3c98784e809eae131bbaf45ab5"}},"ca383419de924d24842fda34acb47f92":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2816377cee5247f6a6d97322cd24db01","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 606/606 [00:00&lt;00:00, 23.8kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ada8aa8462cf4f7f9daa022a27632000"}},"8d2f41a8d72a4873912117d10b997435":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"39cb5373d0b848e092de667993500759":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9bab60a78e2448268f01323dad9563d1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0f1eec3c98784e809eae131bbaf45ab5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2816377cee5247f6a6d97322cd24db01":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ada8aa8462cf4f7f9daa022a27632000":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RkuY4GVbQ1k4","executionInfo":{"status":"ok","timestamp":1633843936614,"user_tz":-540,"elapsed":34027,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"850666d1-4c72-41a3-a282-1132fd7d07c8"},"source":["# Kaggle or Colab\n","import sys\n","import os\n","if 'kaggle_web_client' in sys.modules:\n","    # Do something\n","    pass\n","elif 'google.colab' in sys.modules:\n","    # Do something\n","    from google.colab import drive\n","    drive.mount(\"/content/drive\")\n","\n","    comp_name_official = \"chaii-hindi-and-tamil-question-answering\"\n","    comp_name_local = \"Kaggle-chaii\"\n","\n","    !pip install --upgrade --force-reinstall --no-deps kaggle\n","    import json\n","    f = open(\"/content/drive/MyDrive/colab_notebooks/kaggle/kaggle.json\", \"r\")\n","    json_data = json.load(f)\n","    os.environ[\"KAGGLE_USERNAME\"] = json_data[\"username\"]\n","    os.environ[\"KAGGLE_KEY\"] = json_data[\"key\"]\n","\n","    %cd /content/drive/MyDrive/colab_notebooks/kaggle/{comp_name_local}/notebooks\n","\n","    dname = \"chaii-qa-5-fold-xlmroberta-torch-fit\"\n","    !mkdir {dname}\n","    #%cd /content/drive/MyDrive/colab_notebooks/kaggle/{comp_name_local}/notebooks/{dname}"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Collecting kaggle\n","  Downloading kaggle-1.5.12.tar.gz (58 kB)\n","\u001b[K     |████████████████████████████████| 58 kB 2.9 MB/s \n","\u001b[?25hBuilding wheels for collected packages: kaggle\n","  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73051 sha256=e655c79c73fdd905daf48c9491f099a1f68ac2829df44ae5eaa811088b13379a\n","  Stored in directory: /root/.cache/pip/wheels/62/d6/58/5853130f941e75b2177d281eb7e44b4a98ed46dd155f556dc5\n","Successfully built kaggle\n","Installing collected packages: kaggle\n","  Attempting uninstall: kaggle\n","    Found existing installation: kaggle 1.5.12\n","    Uninstalling kaggle-1.5.12:\n","      Successfully uninstalled kaggle-1.5.12\n","Successfully installed kaggle-1.5.12\n","/content/drive/MyDrive/colab_notebooks/kaggle/Kaggle-chaii/notebooks\n","mkdir: cannot create directory ‘chaii-qa-5-fold-xlmroberta-torch-fit’: File exists\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a0xZ62k7SB1g","executionInfo":{"status":"ok","timestamp":1633843942822,"user_tz":-540,"elapsed":6231,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"4dcab53c-bff2-44f2-ffcd-bc4ec57fa7d3"},"source":["if 'kaggle_web_client' in sys.modules:\n","    # Do something\n","    pass\n","elif 'google.colab' in sys.modules:\n","    #!pip install transformers\n","    !pip install transformers[sentencepiece]\n","\n","    # import yaml\n","    # with open(f'./config_notebook/config.yaml') as file:\n","    #     cfg = yaml.load(file, Loader=yaml.FullLoader) # Loader is recommended\n","    # print(\"Config:\\n\", cfg)"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers[sentencepiece]\n","  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n","\u001b[K     |████████████████████████████████| 2.9 MB 3.7 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (4.62.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.3.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (21.0)\n","Collecting huggingface-hub>=0.0.17\n","  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n","\u001b[K     |████████████████████████████████| 56 kB 6.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (1.19.5)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n","\u001b[K     |████████████████████████████████| 636 kB 65.1 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2019.12.20)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 84.6 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (4.8.1)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 62.7 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2.23.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.17.3)\n","Collecting sentencepiece!=0.1.92,>=0.1.91\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 74.8 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers[sentencepiece]) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers[sentencepiece]) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers[sentencepiece]) (3.6.0)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers[sentencepiece]) (1.15.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (3.0.4)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[sentencepiece]) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[sentencepiece]) (1.0.1)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers, sentencepiece\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.0.19 pyyaml-5.4.1 sacremoses-0.0.46 sentencepiece-0.1.96 tokenizers-0.10.3 transformers-4.11.3\n"]}]},{"cell_type":"markdown","metadata":{"id":"uQdNDkrBP4hj"},"source":["<h2>chaii QA - 5 Fold XLMRoberta Finetuning in Torch w/o Trainer API</h2>\n","    \n","<h3><span \"style: color=#444\">Introduction</span></h3>\n","\n","The kernel implements 5-Fold XLMRoberta QA Model without using the Trainer API from HuggingFace.\n","\n","This is a three part kernel,\n","\n","- [External Data - MLQA, XQUAD Preprocessing](https://www.kaggle.com/rhtsingh/external-data-mlqa-xquad-preprocessing) which preprocesses the Hindi Corpus of MLQA and XQUAD. I have used these data for training.\n","\n","- [chaii QA - 5 Fold XLMRoberta Torch | FIT](https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-fit/edit) This kernel is the current kernel and used for Finetuning (FIT) on competition + external data.\n","\n","- [chaii QA - 5 Fold XLMRoberta Torch | Infer](https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-infer) The Inference kernel where we ensemble our 5 Fold XLMRoberta Models and do the submission.\n","\n","<h3><span \"style: color=#444\">Techniques</span></h3>\n","\n","The kernel has implementation for below techniques, click on the links to learn more -\n","\n"," - [External Data Preprocessing + Training](https://www.kaggle.com/rhtsingh/external-data-mlqa-xquad-preprocessing)\n"," \n"," - [Mixed Precision Training using APEX](https://www.kaggle.com/rhtsingh/swa-apex-amp-interpreting-transformers-in-torch)\n"," \n"," - [Gradient Accumulation](https://www.kaggle.com/rhtsingh/speeding-up-transformer-w-optimization-strategies)\n"," \n"," - [Gradient Clipping](https://www.kaggle.com/rhtsingh/swa-apex-amp-interpreting-transformers-in-torch)\n"," \n"," - [Grouped Layerwise Learning Rate Decay](https://www.kaggle.com/rhtsingh/guide-to-huggingface-schedulers-differential-lrs)\n"," \n"," - [Utilizing Intermediate Transformer Representations](https://www.kaggle.com/rhtsingh/utilizing-transformer-representations-efficiently)\n"," \n"," - [Layer Initialization](https://www.kaggle.com/rhtsingh/on-stability-of-few-sample-transformer-fine-tuning)\n"," \n"," - [5-Fold Training](https://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-fit)\n"," \n"," - etc.\n"," \n","<h3><span \"style: color=#444\">References</span></h3>\n","I would like to acknowledge below kagglers work and resources without whom this kernel wouldn't have been possible,  \n","\n","- [roberta inference 5 folds by Abhishek](https://www.kaggle.com/abhishek/roberta-inference-5-folds)\n","\n","- [ChAII - EDA & Baseline by Darek](https://www.kaggle.com/thedrcat/chaii-eda-baseline) due to whom i found the great notebook below from HuggingFace,\n","\n","- [How to fine-tune a model on question answering](https://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb)\n","\n","- [HuggingFace QA Examples](https://github.com/huggingface/transformers/tree/master/examples/pytorch/question-answering)\n","\n","- [TensorFlow 2.0 Question Answering - Collections of gold solutions](https://www.kaggle.com/c/tensorflow2-question-answering/discussion/127409) these solutions are gold and highly recommend everyone to give a detailed read."]},{"cell_type":"markdown","metadata":{"id":"vI-eiJRPP4hw"},"source":["<h3><span style=\"color=#444\">Note</span></h3>\n","\n","The below points are worth noting,\n","\n"," - I haven't used FP16 because due to some reason this fails and model never starts training.\n"," - These are the original hyperparamters and setting that I have used for training my models.\n"," - I tried few pooling layers but none of them performed better than simple one.\n"," - Gradient clipping reduces model performance.\n"," - <span style=\"color:#2E86C1\">Training 5 Folds at once will give OOM issue on Kaggle and Colab. I have trained one fold at a time i.e. After 1st fold is completed I save the model as a dataset then train second fold. Repeat this process until all 5 folds are completed. Training each fold takes around 50 minuts on Kaggle.</span>\n"," - <span style=\"color:#2E86C1\">I have modified the preprocessing code a lot from original used in HuggingFace notebook since I am not using HuggingFace Datasets API as well. So I had to handle the sequence-ids specially since they contain None values which torch doesn't support.</span>"]},{"cell_type":"markdown","metadata":{"id":"8ykS_qCDP4hz"},"source":["### Install APEX"]},{"cell_type":"code","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","id":"aCY6yvR6ET3s","execution":{"iopub.status.busy":"2021-08-17T20:22:25.164246Z","iopub.execute_input":"2021-08-17T20:22:25.164729Z","iopub.status.idle":"2021-08-17T20:22:25.170948Z","shell.execute_reply.started":"2021-08-17T20:22:25.164627Z","shell.execute_reply":"2021-08-17T20:22:25.169352Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1633843942823,"user_tz":-540,"elapsed":58,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["# %%writefile setup.sh\n","# export CUDA_HOME=/usr/local/cuda-10.1\n","# git clone https://github.com/NVIDIA/apex\n","# cd apex\n","# pip install -v --disable-pip-version-check --no-cache-dir ./"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"-l2Jsav9ET3v","execution":{"iopub.status.busy":"2021-08-17T20:22:26.935257Z","iopub.execute_input":"2021-08-17T20:22:26.935919Z","iopub.status.idle":"2021-08-17T20:22:26.939334Z","shell.execute_reply.started":"2021-08-17T20:22:26.935881Z","shell.execute_reply":"2021-08-17T20:22:26.938488Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1633843942824,"user_tz":-540,"elapsed":55,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["# %%capture\n","# !sh setup.sh"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xFbbPlPgP4h7"},"source":["### Import Dependencies"]},{"cell_type":"code","metadata":{"id":"E4l6PirHET3x","execution":{"iopub.status.busy":"2021-08-17T20:23:50.232396Z","iopub.execute_input":"2021-08-17T20:23:50.232892Z","iopub.status.idle":"2021-08-17T20:24:01.076652Z","shell.execute_reply.started":"2021-08-17T20:23:50.232861Z","shell.execute_reply":"2021-08-17T20:24:01.075658Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633843957893,"user_tz":-540,"elapsed":15122,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"4a314f1e-9079-435d-efc1-3f63387563d3"},"source":["#import os\n","import gc\n","gc.enable()\n","import math\n","#mport json\n","import time\n","import random\n","import multiprocessing\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=UserWarning)\n","\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm, trange\n","from sklearn import model_selection\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn import Parameter\n","import torch.optim as optim\n","from torch.utils.data import (\n","    Dataset, DataLoader,\n","    SequentialSampler, RandomSampler\n",")\n","from torch.utils.data.distributed import DistributedSampler\n","\n","try:\n","    from apex import amp\n","    APEX_INSTALLED = True\n","except ImportError:\n","    APEX_INSTALLED = False\n","\n","import transformers\n","from transformers import (\n","    WEIGHTS_NAME,\n","    AdamW,\n","    AutoConfig,\n","    AutoModel,\n","    AutoTokenizer,\n","    get_cosine_schedule_with_warmup,\n","    get_linear_schedule_with_warmup,\n","    logging,\n","    MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n",")\n","logging.set_verbosity_warning()\n","logging.set_verbosity_error()\n","\n","def fix_all_seeds(seed):\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","def optimal_num_of_loader_workers():\n","    num_cpus = multiprocessing.cpu_count()\n","    num_gpus = torch.cuda.device_count()\n","    optimal_value = min(num_cpus, num_gpus*4) if num_gpus else num_cpus - 1\n","    return optimal_value\n","\n","print(f\"Apex AMP Installed :: {APEX_INSTALLED}\")\n","MODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())\n","MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Apex AMP Installed :: False\n"]}]},{"cell_type":"markdown","metadata":{"id":"y0OvcnMaP4h9"},"source":["### Training Configuration"]},{"cell_type":"code","metadata":{"id":"LUx5XplNET3y","execution":{"iopub.status.busy":"2021-08-17T20:24:13.391322Z","iopub.execute_input":"2021-08-17T20:24:13.391696Z","iopub.status.idle":"2021-08-17T20:24:13.398397Z","shell.execute_reply.started":"2021-08-17T20:24:13.391662Z","shell.execute_reply":"2021-08-17T20:24:13.39732Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1633843957895,"user_tz":-540,"elapsed":42,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["class Config:\n","    # model\n","    model_type = 'xlm_roberta'\n","    model_name_or_path = \"deepset/xlm-roberta-large-squad2\"\n","    config_name = \"deepset/xlm-roberta-large-squad2\"\n","    fp16 = True if APEX_INSTALLED else False\n","    fp16_opt_level = \"O1\"\n","    gradient_accumulation_steps = 2\n","\n","    # tokenizer\n","    tokenizer_name = \"deepset/xlm-roberta-large-squad2\"\n","    max_seq_length = 512 #384\n","    doc_stride = 80 #128\n","\n","    # train\n","    epochs = 7 #1\n","    train_batch_size = 2 #4\n","    eval_batch_size = 4 #8\n","\n","    # optimizer\n","    optimizer_type = 'AdamW'\n","    learning_rate = 1.5e-5\n","    weight_decay = 1e-2\n","    epsilon = 1e-8\n","    max_grad_norm = 1.0\n","\n","    # scheduler\n","    decay_name = 'linear-warmup'\n","    warmup_ratio = 0.1\n","\n","    # logging\n","    logging_steps = 10\n","\n","    # evaluate\n","    output_dir = dname #'output'\n","    seed = 2021"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HYno9o1OP4h-"},"source":["### Data Factory"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5GJG8wreSdQc","executionInfo":{"status":"ok","timestamp":1633843957895,"user_tz":-540,"elapsed":41,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"3b841db4-f3d8-4db6-bb0f-30bfdbc937f5"},"source":["!pwd"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/colab_notebooks/kaggle/Kaggle-chaii/notebooks\n"]}]},{"cell_type":"code","metadata":{"id":"X_eRZQrzET3z","execution":{"iopub.status.busy":"2021-08-17T20:24:26.939105Z","iopub.execute_input":"2021-08-17T20:24:26.939437Z","iopub.status.idle":"2021-08-17T20:24:27.776039Z","shell.execute_reply.started":"2021-08-17T20:24:26.939409Z","shell.execute_reply":"2021-08-17T20:24:27.775204Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1633843964537,"user_tz":-540,"elapsed":6675,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["train = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/train.csv')\n","test = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/test.csv')\n","external_mlqa = pd.read_csv('../input/mlqa-hindi-processed/mlqa_hindi.csv')\n","external_xquad = pd.read_csv('../input/mlqa-hindi-processed/xquad.csv')\n","external_train = pd.concat([external_mlqa, external_xquad])\n","\n","def create_folds(data, num_splits):\n","    data[\"kfold\"] = -1\n","    kf = model_selection.StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=2021)\n","    for f, (t_, v_) in enumerate(kf.split(X=data, y=data['language'])):\n","        data.loc[v_, 'kfold'] = f\n","    return data\n","\n","train = create_folds(train, num_splits=5)\n","external_train[\"kfold\"] = -1\n","external_train['id'] = list(np.arange(1, len(external_train)+1))\n","train = pd.concat([train, external_train]).reset_index(drop=True)\n","\n","def convert_answers(row):\n","    return {'answer_start': [row[0]], 'text': [row[1]]}\n","\n","train['answers'] = train[['answer_start', 'answer_text']].apply(convert_answers, axis=1)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dAoAJUVaP4iA"},"source":["### Covert Examples to Features (Preprocess)"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-12T15:50:26.947214Z","iopub.execute_input":"2021-08-12T15:50:26.947589Z","iopub.status.idle":"2021-08-12T15:50:26.960916Z","shell.execute_reply.started":"2021-08-12T15:50:26.947551Z","shell.execute_reply":"2021-08-12T15:50:26.959064Z"},"id":"dxbZdct1ET3z","trusted":true,"executionInfo":{"status":"ok","timestamp":1633843964538,"user_tz":-540,"elapsed":37,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["def prepare_train_features(args, example, tokenizer):\n","    example[\"question\"] = example[\"question\"].lstrip()\n","    tokenized_example = tokenizer(\n","        example[\"question\"],\n","        example[\"context\"],\n","        truncation=\"only_second\",\n","        max_length=args.max_seq_length,\n","        stride=args.doc_stride,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True,\n","        padding=\"max_length\",\n","    )\n","\n","    sample_mapping = tokenized_example.pop(\"overflow_to_sample_mapping\")\n","    offset_mapping = tokenized_example.pop(\"offset_mapping\")\n","\n","    features = []\n","    for i, offsets in enumerate(offset_mapping):\n","        feature = {}\n","\n","        input_ids = tokenized_example[\"input_ids\"][i]\n","        attention_mask = tokenized_example[\"attention_mask\"][i]\n","\n","        feature['input_ids'] = input_ids\n","        feature['attention_mask'] = attention_mask\n","        feature['offset_mapping'] = offsets\n","\n","        cls_index = input_ids.index(tokenizer.cls_token_id)\n","        sequence_ids = tokenized_example.sequence_ids(i)\n","\n","        sample_index = sample_mapping[i]\n","        answers = example[\"answers\"]\n","\n","        if len(answers[\"answer_start\"]) == 0:\n","            feature[\"start_position\"] = cls_index\n","            feature[\"end_position\"] = cls_index\n","        else:\n","            start_char = answers[\"answer_start\"][0]\n","            end_char = start_char + len(answers[\"text\"][0])\n","\n","            token_start_index = 0\n","            while sequence_ids[token_start_index] != 1:\n","                token_start_index += 1\n","\n","            token_end_index = len(input_ids) - 1\n","            while sequence_ids[token_end_index] != 1:\n","                token_end_index -= 1\n","\n","            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n","                feature[\"start_position\"] = cls_index\n","                feature[\"end_position\"] = cls_index\n","            else:\n","                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n","                    token_start_index += 1\n","                feature[\"start_position\"] = token_start_index - 1\n","                while offsets[token_end_index][1] >= end_char:\n","                    token_end_index -= 1\n","                feature[\"end_position\"] = token_end_index + 1\n","\n","        features.append(feature)\n","    return features"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a3SIS_xAP4iC"},"source":["### Dataset Retriever"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-12T15:50:26.962738Z","iopub.execute_input":"2021-08-12T15:50:26.963118Z","iopub.status.idle":"2021-08-12T15:50:26.97542Z","shell.execute_reply.started":"2021-08-12T15:50:26.963075Z","shell.execute_reply":"2021-08-12T15:50:26.974431Z"},"id":"6TuzHdjmET30","trusted":true,"executionInfo":{"status":"ok","timestamp":1633843964539,"user_tz":-540,"elapsed":37,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["class DatasetRetriever(Dataset):\n","    def __init__(self, features, mode='train'):\n","        super(DatasetRetriever, self).__init__()\n","        self.features = features\n","        self.mode = mode\n","        \n","    def __len__(self):\n","        return len(self.features)\n","    \n","    def __getitem__(self, item):   \n","        feature = self.features[item]\n","        if self.mode == 'train':\n","            return {\n","                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n","                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n","                'offset_mapping':torch.tensor(feature['offset_mapping'], dtype=torch.long),\n","                'start_position':torch.tensor(feature['start_position'], dtype=torch.long),\n","                'end_position':torch.tensor(feature['end_position'], dtype=torch.long)\n","            }\n","        else:\n","            return {\n","                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n","                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n","                'offset_mapping':feature['offset_mapping'],\n","                'sequence_ids':feature['sequence_ids'],\n","                'id':feature['example_id'],\n","                'context': feature['context'],\n","                'question': feature['question']\n","            }"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tpH-2nPqP4iE"},"source":["### Model"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-12T15:50:26.976977Z","iopub.execute_input":"2021-08-12T15:50:26.977627Z","iopub.status.idle":"2021-08-12T15:50:26.990227Z","shell.execute_reply.started":"2021-08-12T15:50:26.97747Z","shell.execute_reply":"2021-08-12T15:50:26.989443Z"},"id":"9OxhKqxcET31","trusted":true,"executionInfo":{"status":"ok","timestamp":1633843964540,"user_tz":-540,"elapsed":37,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["class Model(nn.Module):\n","    def __init__(self, modelname_or_path, config):\n","        super(Model, self).__init__()\n","        self.config = config\n","        self.xlm_roberta = AutoModel.from_pretrained(modelname_or_path, config=config)\n","        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","        self._init_weights(self.qa_outputs)\n","        \n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","\n","    def forward(\n","        self, \n","        input_ids, \n","        attention_mask=None, \n","        # token_type_ids=None\n","    ):\n","        outputs = self.xlm_roberta(\n","            input_ids,\n","            attention_mask=attention_mask,\n","        )\n","\n","        sequence_output = outputs[0]\n","        pooled_output = outputs[1]\n","        \n","        # sequence_output = self.dropout(sequence_output)\n","        qa_logits = self.qa_outputs(sequence_output)\n","        \n","        start_logits, end_logits = qa_logits.split(1, dim=-1)\n","        start_logits = start_logits.squeeze(-1)\n","        end_logits = end_logits.squeeze(-1)\n","    \n","        return start_logits, end_logits"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xdMx6plqP4iG"},"source":["### Loss"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-12T15:50:26.991891Z","iopub.execute_input":"2021-08-12T15:50:26.99237Z","iopub.status.idle":"2021-08-12T15:50:27.000188Z","shell.execute_reply.started":"2021-08-12T15:50:26.992334Z","shell.execute_reply":"2021-08-12T15:50:26.999374Z"},"id":"SxuNrJqqET32","trusted":true,"executionInfo":{"status":"ok","timestamp":1633843964540,"user_tz":-540,"elapsed":36,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["def loss_fn(preds, labels):\n","    start_preds, end_preds = preds\n","    start_labels, end_labels = labels\n","    \n","    start_loss = nn.CrossEntropyLoss(ignore_index=-1)(start_preds, start_labels)\n","    end_loss = nn.CrossEntropyLoss(ignore_index=-1)(end_preds, end_labels)\n","    total_loss = (start_loss + end_loss) / 2\n","    return total_loss"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jUMtX08cP4iH"},"source":["### Grouped Layerwise Learning Rate Decay"]},{"cell_type":"code","metadata":{"id":"vf6HVcu2ET34","execution":{"iopub.status.busy":"2021-08-17T20:25:36.36033Z","iopub.execute_input":"2021-08-17T20:25:36.361058Z","iopub.status.idle":"2021-08-17T20:25:36.381524Z","shell.execute_reply.started":"2021-08-17T20:25:36.361009Z","shell.execute_reply":"2021-08-17T20:25:36.380328Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1633843964541,"user_tz":-540,"elapsed":36,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["def get_optimizer_grouped_parameters(args, model):\n","    no_decay = [\"bias\", \"LayerNorm.weight\"]\n","    group1=['layer.0.','layer.1.','layer.2.','layer.3.']\n","    group2=['layer.4.','layer.5.','layer.6.','layer.7.']    \n","    group3=['layer.8.','layer.9.','layer.10.','layer.11.']\n","    group_all=['layer.0.','layer.1.','layer.2.','layer.3.','layer.4.','layer.5.','layer.6.','layer.7.','layer.8.','layer.9.','layer.10.','layer.11.']\n","    optimizer_grouped_parameters = [\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': args.weight_decay},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': args.weight_decay, 'lr': args.learning_rate/2.6},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': args.weight_decay, 'lr': args.learning_rate},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': args.weight_decay, 'lr': args.learning_rate*2.6},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': 0.0},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': 0.0, 'lr': args.learning_rate/2.6},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': 0.0, 'lr': args.learning_rate},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': 0.0, 'lr': args.learning_rate*2.6},\n","        {'params': [p for n, p in model.named_parameters() if args.model_type not in n], 'lr':args.learning_rate*20, \"weight_decay\": 0.0},\n","    ]\n","    return optimizer_grouped_parameters"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oXz_0fIQP4iI"},"source":["### Metric Logger"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-12T15:50:27.021442Z","iopub.execute_input":"2021-08-12T15:50:27.021952Z","iopub.status.idle":"2021-08-12T15:50:27.03234Z","shell.execute_reply.started":"2021-08-12T15:50:27.021912Z","shell.execute_reply":"2021-08-12T15:50:27.03156Z"},"id":"bkFB-iMcET34","trusted":true,"executionInfo":{"status":"ok","timestamp":1633843964542,"user_tz":-540,"elapsed":37,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["class AverageMeter(object):\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","        self.max = 0\n","        self.min = 1e5\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","        if val > self.max:\n","            self.max = val\n","        if val < self.min:\n","            self.min = val"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y946gQxtP4iJ"},"source":["### Utilities"]},{"cell_type":"code","metadata":{"id":"spFRutV0ET34","execution":{"iopub.status.busy":"2021-08-17T20:26:30.014223Z","iopub.execute_input":"2021-08-17T20:26:30.014623Z","iopub.status.idle":"2021-08-17T20:26:30.030566Z","shell.execute_reply.started":"2021-08-17T20:26:30.01459Z","shell.execute_reply":"2021-08-17T20:26:30.029723Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1633843964542,"user_tz":-540,"elapsed":36,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["def make_model(args):\n","    config = AutoConfig.from_pretrained(args.config_name)\n","    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name)\n","    model = Model(args.model_name_or_path, config=config)\n","    return config, tokenizer, model\n","\n","def make_optimizer(args, model):\n","    # optimizer_grouped_parameters = get_optimizer_grouped_parameters(args, model)\n","    no_decay = [\"bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\n","            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","            \"weight_decay\": args.weight_decay,\n","        },\n","        {\n","            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n","            \"weight_decay\": 0.0,\n","        },\n","    ]\n","    if args.optimizer_type == \"AdamW\":\n","        optimizer = AdamW(\n","            optimizer_grouped_parameters,\n","            lr=args.learning_rate,\n","            eps=args.epsilon,\n","            correct_bias=True\n","        )\n","        return optimizer\n","\n","def make_scheduler(\n","    args, optimizer, \n","    num_warmup_steps, \n","    num_training_steps\n","):\n","    if args.decay_name == \"cosine-warmup\":\n","        scheduler = get_cosine_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=num_warmup_steps,\n","            num_training_steps=num_training_steps\n","        )\n","    else:\n","        scheduler = get_linear_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=num_warmup_steps,\n","            num_training_steps=num_training_steps\n","        )\n","    return scheduler    \n","\n","def make_loader(\n","    args, data, \n","    tokenizer, fold\n","):\n","    train_set, valid_set = data[data['kfold']!=fold], data[data['kfold']==fold]\n","    \n","    train_features, valid_features = [[] for _ in range(2)]\n","    for i, row in train_set.iterrows():\n","        train_features += prepare_train_features(args, row, tokenizer)\n","    for i, row in valid_set.iterrows():\n","        valid_features += prepare_train_features(args, row, tokenizer)\n","\n","    train_dataset = DatasetRetriever(train_features)\n","    valid_dataset = DatasetRetriever(valid_features)\n","    print(f\"Num examples Train= {len(train_dataset)}, Num examples Valid={len(valid_dataset)}\")\n","    \n","    train_sampler = RandomSampler(train_dataset)\n","    valid_sampler = SequentialSampler(valid_dataset)\n","\n","    train_dataloader = DataLoader(\n","        train_dataset,\n","        batch_size=args.train_batch_size,\n","        sampler=train_sampler,\n","        num_workers=optimal_num_of_loader_workers(),\n","        pin_memory=True,\n","        drop_last=False \n","    )\n","\n","    valid_dataloader = DataLoader(\n","        valid_dataset,\n","        batch_size=args.eval_batch_size, \n","        sampler=valid_sampler,\n","        num_workers=optimal_num_of_loader_workers(),\n","        pin_memory=True, \n","        drop_last=False\n","    )\n","\n","    return train_dataloader, valid_dataloader"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i-zfYJGxP4iK"},"source":["### Trainer"]},{"cell_type":"code","metadata":{"id":"iFLvh1VQET35","execution":{"iopub.status.busy":"2021-08-17T20:26:36.310455Z","iopub.execute_input":"2021-08-17T20:26:36.310827Z","iopub.status.idle":"2021-08-17T20:26:36.325575Z","shell.execute_reply.started":"2021-08-17T20:26:36.310796Z","shell.execute_reply":"2021-08-17T20:26:36.324417Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1633843964543,"user_tz":-540,"elapsed":31,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["class Trainer:\n","    def __init__(\n","        self, model, tokenizer, \n","        optimizer, scheduler\n","    ):\n","        self.model = model\n","        self.tokenizer = tokenizer\n","        self.optimizer = optimizer\n","        self.scheduler = scheduler\n","\n","    def train(\n","        self, args, \n","        train_dataloader, \n","        epoch, result_dict\n","    ):\n","        count = 0\n","        losses = AverageMeter()\n","        \n","        self.model.zero_grad()\n","        self.model.train()\n","        \n","        fix_all_seeds(args.seed)\n","        \n","        for batch_idx, batch_data in enumerate(train_dataloader):\n","            input_ids, attention_mask, targets_start, targets_end = \\\n","                batch_data['input_ids'], batch_data['attention_mask'], \\\n","                    batch_data['start_position'], batch_data['end_position']\n","            \n","            input_ids, attention_mask, targets_start, targets_end = \\\n","                input_ids.cuda(), attention_mask.cuda(), targets_start.cuda(), targets_end.cuda()\n","\n","            outputs_start, outputs_end = self.model(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","            )\n","            \n","            loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n","            loss = loss / args.gradient_accumulation_steps\n","\n","            if args.fp16:\n","                with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n","                    scaled_loss.backward()\n","            else:\n","                loss.backward()\n","\n","            count += input_ids.size(0)\n","            losses.update(loss.item(), input_ids.size(0))\n","\n","            # if args.fp16:\n","            #     torch.nn.utils.clip_grad_norm_(amp.master_params(self.optimizer), args.max_grad_norm)\n","            # else:\n","            #     torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.max_grad_norm)\n","\n","            if batch_idx % args.gradient_accumulation_steps == 0 or batch_idx == len(train_dataloader) - 1:\n","                self.optimizer.step()\n","                self.scheduler.step()\n","                self.optimizer.zero_grad()\n","\n","            if (batch_idx % args.logging_steps == 0) or (batch_idx+1)==len(train_dataloader):\n","                _s = str(len(str(len(train_dataloader.sampler))))\n","                ret = [\n","                    ('Epoch: {:0>2} [{: >' + _s + '}/{} ({: >3.0f}%)]').format(epoch, count, len(train_dataloader.sampler), 100 * count / len(train_dataloader.sampler)),\n","                    'Train Loss: {: >4.5f}'.format(losses.avg),\n","                ]\n","                print(', '.join(ret))\n","\n","        result_dict['train_loss'].append(losses.avg)\n","        return result_dict"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7pmOF0dxP4iL"},"source":["### Evaluator"]},{"cell_type":"code","metadata":{"id":"1a8kG2UYET36","execution":{"iopub.status.busy":"2021-08-17T20:26:39.517516Z","iopub.execute_input":"2021-08-17T20:26:39.517886Z","iopub.status.idle":"2021-08-17T20:26:39.528591Z","shell.execute_reply.started":"2021-08-17T20:26:39.517856Z","shell.execute_reply":"2021-08-17T20:26:39.527569Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1633843964544,"user_tz":-540,"elapsed":31,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["class Evaluator:\n","    def __init__(self, model):\n","        self.model = model\n","    \n","    def save(self, result, output_dir):\n","        with open(f'{output_dir}/result_dict.json', 'w') as f:\n","            f.write(json.dumps(result, sort_keys=True, indent=4, ensure_ascii=False))\n","\n","    def evaluate(self, valid_dataloader, epoch, result_dict):\n","        losses = AverageMeter()\n","        for batch_idx, batch_data in enumerate(valid_dataloader):\n","            self.model = self.model.eval()\n","            input_ids, attention_mask, targets_start, targets_end = \\\n","                batch_data['input_ids'], batch_data['attention_mask'], \\\n","                    batch_data['start_position'], batch_data['end_position']\n","            \n","            input_ids, attention_mask, targets_start, targets_end = \\\n","                input_ids.cuda(), attention_mask.cuda(), targets_start.cuda(), targets_end.cuda()\n","            \n","            with torch.no_grad():            \n","                outputs_start, outputs_end = self.model(\n","                    input_ids=input_ids,\n","                    attention_mask=attention_mask,\n","                )\n","                \n","                loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n","                losses.update(loss.item(), input_ids.size(0))\n","                \n","        print('----Validation Results Summary----')\n","        print('Epoch: [{}] Valid Loss: {: >4.5f}'.format(epoch, losses.avg))\n","        result_dict['val_loss'].append(losses.avg)        \n","        return result_dict"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cAZPoRJeP4iM"},"source":["### Initialize Training"]},{"cell_type":"code","metadata":{"id":"v-gUDyq2ET37","execution":{"iopub.status.busy":"2021-08-17T20:26:44.41837Z","iopub.execute_input":"2021-08-17T20:26:44.418722Z","iopub.status.idle":"2021-08-17T20:26:44.428918Z","shell.execute_reply.started":"2021-08-17T20:26:44.418691Z","shell.execute_reply":"2021-08-17T20:26:44.428086Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1633843964545,"user_tz":-540,"elapsed":32,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["def init_training(args, data, fold):\n","    fix_all_seeds(args.seed)\n","    \n","    if not os.path.exists(args.output_dir):\n","        os.makedirs(args.output_dir)\n","    \n","    # model\n","    model_config, tokenizer, model = make_model(args)\n","    if torch.cuda.device_count() >= 1:\n","        print('Model pushed to {} GPU(s), type {}.'.format(\n","            torch.cuda.device_count(), \n","            torch.cuda.get_device_name(0))\n","        )\n","        model = model.cuda() \n","    else:\n","        raise ValueError('CPU training is not supported')\n","    \n","    # data loaders\n","    train_dataloader, valid_dataloader = make_loader(args, data, tokenizer, fold)\n","\n","    # optimizer\n","    optimizer = make_optimizer(args, model)\n","\n","    # scheduler\n","    num_training_steps = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps) * args.epochs\n","    if args.warmup_ratio > 0:\n","        num_warmup_steps = int(args.warmup_ratio * num_training_steps)\n","    else:\n","        num_warmup_steps = 0\n","    print(f\"Total Training Steps: {num_training_steps}, Total Warmup Steps: {num_warmup_steps}\")\n","    scheduler = make_scheduler(args, optimizer, num_warmup_steps, num_training_steps)\n","\n","    # mixed precision training with NVIDIA Apex\n","    if args.fp16:\n","        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n","    \n","    result_dict = {\n","        'epoch':[], \n","        'train_loss': [], \n","        'val_loss' : [], \n","        'best_val_loss': np.inf\n","    }\n","\n","    return (\n","        model, model_config, tokenizer, optimizer, scheduler, \n","        train_dataloader, valid_dataloader, result_dict\n","    )"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g6ZBX-5aP4iN"},"source":["### Run"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-12T15:50:27.097353Z","iopub.execute_input":"2021-08-12T15:50:27.097724Z","iopub.status.idle":"2021-08-12T15:50:27.112113Z","shell.execute_reply.started":"2021-08-12T15:50:27.097688Z","shell.execute_reply":"2021-08-12T15:50:27.111224Z"},"id":"39ei5Bm5ET37","trusted":true,"executionInfo":{"status":"ok","timestamp":1633843964546,"user_tz":-540,"elapsed":32,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["def run(data, fold):\n","    args = Config()\n","    model, model_config, tokenizer, optimizer, scheduler, train_dataloader, \\\n","        valid_dataloader, result_dict = init_training(args, data, fold)\n","    \n","    trainer = Trainer(model, tokenizer, optimizer, scheduler)\n","    evaluator = Evaluator(model)\n","\n","    train_time_list = []\n","    valid_time_list = []\n","\n","    for epoch in range(args.epochs):\n","        result_dict['epoch'].append(epoch)\n","\n","        # Train\n","        torch.cuda.synchronize()\n","        tic1 = time.time()\n","        result_dict = trainer.train(\n","            args, train_dataloader, \n","            epoch, result_dict\n","        )\n","        torch.cuda.synchronize()\n","        tic2 = time.time() \n","        train_time_list.append(tic2 - tic1)\n","        \n","        # Evaluate\n","        torch.cuda.synchronize()\n","        tic3 = time.time()\n","        result_dict = evaluator.evaluate(\n","            valid_dataloader, epoch, result_dict\n","        )\n","        torch.cuda.synchronize()\n","        tic4 = time.time() \n","        valid_time_list.append(tic4 - tic3)\n","            \n","        output_dir = os.path.join(args.output_dir, f\"checkpoint-fold-{fold}\")\n","        if result_dict['val_loss'][-1] < result_dict['best_val_loss']:\n","            print(\"{} Epoch, Best epoch was updated! Valid Loss: {: >4.5f}\".format(epoch, result_dict['val_loss'][-1]))\n","            result_dict[\"best_val_loss\"] = result_dict['val_loss'][-1]        \n","            \n","            os.makedirs(output_dir, exist_ok=True)\n","            torch.save(model.state_dict(), f\"{output_dir}/pytorch_model.bin\")\n","            model_config.save_pretrained(output_dir)\n","            tokenizer.save_pretrained(output_dir)\n","            print(f\"Saving model checkpoint to {output_dir}.\")\n","            \n","        print()\n","\n","    evaluator.save(result_dict, output_dir)\n","    \n","    print(f\"Total Training Time: {np.sum(train_time_list)}secs, Average Training Time per Epoch: {np.mean(train_time_list)}secs.\")\n","    print(f\"Total Validation Time: {np.sum(valid_time_list)}secs, Average Validation Time per Epoch: {np.mean(valid_time_list)}secs.\")\n","    \n","    torch.cuda.empty_cache()\n","    del trainer, evaluator\n","    del model, model_config, tokenizer\n","    del optimizer, scheduler\n","    del train_dataloader, valid_dataloader, result_dict\n","    gc.collect()"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"mPaGnnCnbhWl","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["ba886f9fc6524981bed6628beced272d","81ca36e618cc406cb6223333829a3a8c","6fcc9be52a1343ebb24f488aa9ed2293","64d6c79f402a4dc4a64b34c30413ed7c","ca383419de924d24842fda34acb47f92","8d2f41a8d72a4873912117d10b997435","39cb5373d0b848e092de667993500759","9bab60a78e2448268f01323dad9563d1","0f1eec3c98784e809eae131bbaf45ab5","2816377cee5247f6a6d97322cd24db01","ada8aa8462cf4f7f9daa022a27632000"]},"executionInfo":{"status":"ok","timestamp":1633891220224,"user_tz":-540,"elapsed":47255709,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"9575b160-c288-4579-b017-e846acdc79f4"},"source":["for fold in range(4,6):\n","    print();print()\n","    print('-'*50)\n","    print(f'FOLD: {fold}')\n","    print('-'*50)\n","    run(train, fold)"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","--------------------------------------------------\n","FOLD: 4\n","--------------------------------------------------\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ba886f9fc6524981bed6628beced272d","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/606 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e50c69bbd19f48dc8ecd76c29fac7edc","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/179 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6307fae4dc8d4f829a7b414c90574642","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/4.83M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2a46acdc30664a58a7cd814dc6d6500d","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/150 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"80825255993e4b6cac183929d8268115","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/2.09G [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Epoch: 00 [14182/16204 ( 88%)], Train Loss: 0.58515\n","Epoch: 00 [14202/16204 ( 88%)], Train Loss: 0.58535\n","Epoch: 00 [14222/16204 ( 88%)], Train Loss: 0.58516\n","Epoch: 00 [14242/16204 ( 88%)], Train Loss: 0.58476\n","Epoch: 00 [14262/16204 ( 88%)], Train Loss: 0.58467\n","Epoch: 00 [14282/16204 ( 88%)], Train Loss: 0.58416\n","Epoch: 00 [14302/16204 ( 88%)], Train Loss: 0.58387\n","Epoch: 00 [14322/16204 ( 88%)], Train Loss: 0.58380\n","Epoch: 00 [14342/16204 ( 89%)], Train Loss: 0.58351\n","Epoch: 00 [14362/16204 ( 89%)], Train Loss: 0.58305\n","Epoch: 00 [14382/16204 ( 89%)], Train Loss: 0.58279\n","Epoch: 00 [14402/16204 ( 89%)], Train Loss: 0.58246\n","Epoch: 00 [14422/16204 ( 89%)], Train Loss: 0.58209\n","Epoch: 00 [14442/16204 ( 89%)], Train Loss: 0.58216\n","Epoch: 00 [14462/16204 ( 89%)], Train Loss: 0.58223\n","Epoch: 00 [14482/16204 ( 89%)], Train Loss: 0.58240\n","Epoch: 00 [14502/16204 ( 89%)], Train Loss: 0.58207\n","Epoch: 00 [14522/16204 ( 90%)], Train Loss: 0.58186\n","Epoch: 00 [14542/16204 ( 90%)], Train Loss: 0.58177\n","Epoch: 00 [14562/16204 ( 90%)], Train Loss: 0.58179\n","Epoch: 00 [14582/16204 ( 90%)], Train Loss: 0.58147\n","Epoch: 00 [14602/16204 ( 90%)], Train Loss: 0.58135\n","Epoch: 00 [14622/16204 ( 90%)], Train Loss: 0.58156\n","Epoch: 00 [14642/16204 ( 90%)], Train Loss: 0.58153\n","Epoch: 00 [14662/16204 ( 90%)], Train Loss: 0.58120\n","Epoch: 00 [14682/16204 ( 91%)], Train Loss: 0.58089\n","Epoch: 00 [14702/16204 ( 91%)], Train Loss: 0.58048\n","Epoch: 00 [14722/16204 ( 91%)], Train Loss: 0.57991\n","Epoch: 00 [14742/16204 ( 91%)], Train Loss: 0.58011\n","Epoch: 00 [14762/16204 ( 91%)], Train Loss: 0.57990\n","Epoch: 00 [14782/16204 ( 91%)], Train Loss: 0.57959\n","Epoch: 00 [14802/16204 ( 91%)], Train Loss: 0.57906\n","Epoch: 00 [14822/16204 ( 91%)], Train Loss: 0.57881\n","Epoch: 00 [14842/16204 ( 92%)], Train Loss: 0.57863\n","Epoch: 00 [14862/16204 ( 92%)], Train Loss: 0.57849\n","Epoch: 00 [14882/16204 ( 92%)], Train Loss: 0.57807\n","Epoch: 00 [14902/16204 ( 92%)], Train Loss: 0.57802\n","Epoch: 00 [14922/16204 ( 92%)], Train Loss: 0.57799\n","Epoch: 00 [14942/16204 ( 92%)], Train Loss: 0.57776\n","Epoch: 00 [14962/16204 ( 92%)], Train Loss: 0.57774\n","Epoch: 00 [14982/16204 ( 92%)], Train Loss: 0.57782\n","Epoch: 00 [15002/16204 ( 93%)], Train Loss: 0.57744\n","Epoch: 00 [15022/16204 ( 93%)], Train Loss: 0.57730\n","Epoch: 00 [15042/16204 ( 93%)], Train Loss: 0.57714\n","Epoch: 00 [15062/16204 ( 93%)], Train Loss: 0.57725\n","Epoch: 00 [15082/16204 ( 93%)], Train Loss: 0.57703\n","Epoch: 00 [15102/16204 ( 93%)], Train Loss: 0.57710\n","Epoch: 00 [15122/16204 ( 93%)], Train Loss: 0.57687\n","Epoch: 00 [15142/16204 ( 93%)], Train Loss: 0.57685\n","Epoch: 00 [15162/16204 ( 94%)], Train Loss: 0.57677\n","Epoch: 00 [15182/16204 ( 94%)], Train Loss: 0.57681\n","Epoch: 00 [15202/16204 ( 94%)], Train Loss: 0.57649\n","Epoch: 00 [15222/16204 ( 94%)], Train Loss: 0.57627\n","Epoch: 00 [15242/16204 ( 94%)], Train Loss: 0.57622\n","Epoch: 00 [15262/16204 ( 94%)], Train Loss: 0.57626\n","Epoch: 00 [15282/16204 ( 94%)], Train Loss: 0.57580\n","Epoch: 00 [15302/16204 ( 94%)], Train Loss: 0.57566\n","Epoch: 00 [15322/16204 ( 95%)], Train Loss: 0.57544\n","Epoch: 00 [15342/16204 ( 95%)], Train Loss: 0.57547\n","Epoch: 00 [15362/16204 ( 95%)], Train Loss: 0.57611\n","Epoch: 00 [15382/16204 ( 95%)], Train Loss: 0.57593\n","Epoch: 00 [15402/16204 ( 95%)], Train Loss: 0.57573\n","Epoch: 00 [15422/16204 ( 95%)], Train Loss: 0.57577\n","Epoch: 00 [15442/16204 ( 95%)], Train Loss: 0.57541\n","Epoch: 00 [15462/16204 ( 95%)], Train Loss: 0.57515\n","Epoch: 00 [15482/16204 ( 96%)], Train Loss: 0.57526\n","Epoch: 00 [15502/16204 ( 96%)], Train Loss: 0.57489\n","Epoch: 00 [15522/16204 ( 96%)], Train Loss: 0.57524\n","Epoch: 00 [15542/16204 ( 96%)], Train Loss: 0.57528\n","Epoch: 00 [15562/16204 ( 96%)], Train Loss: 0.57512\n","Epoch: 00 [15582/16204 ( 96%)], Train Loss: 0.57480\n","Epoch: 00 [15602/16204 ( 96%)], Train Loss: 0.57450\n","Epoch: 00 [15622/16204 ( 96%)], Train Loss: 0.57470\n","Epoch: 00 [15642/16204 ( 97%)], Train Loss: 0.57420\n","Epoch: 00 [15662/16204 ( 97%)], Train Loss: 0.57385\n","Epoch: 00 [15682/16204 ( 97%)], Train Loss: 0.57371\n","Epoch: 00 [15702/16204 ( 97%)], Train Loss: 0.57368\n","Epoch: 00 [15722/16204 ( 97%)], Train Loss: 0.57341\n","Epoch: 00 [15742/16204 ( 97%)], Train Loss: 0.57285\n","Epoch: 00 [15762/16204 ( 97%)], Train Loss: 0.57301\n","Epoch: 00 [15782/16204 ( 97%)], Train Loss: 0.57279\n","Epoch: 00 [15802/16204 ( 98%)], Train Loss: 0.57276\n","Epoch: 00 [15822/16204 ( 98%)], Train Loss: 0.57263\n","Epoch: 00 [15842/16204 ( 98%)], Train Loss: 0.57246\n","Epoch: 00 [15862/16204 ( 98%)], Train Loss: 0.57262\n","Epoch: 00 [15882/16204 ( 98%)], Train Loss: 0.57262\n","Epoch: 00 [15902/16204 ( 98%)], Train Loss: 0.57243\n","Epoch: 00 [15922/16204 ( 98%)], Train Loss: 0.57206\n","Epoch: 00 [15942/16204 ( 98%)], Train Loss: 0.57156\n","Epoch: 00 [15962/16204 ( 99%)], Train Loss: 0.57152\n","Epoch: 00 [15982/16204 ( 99%)], Train Loss: 0.57124\n","Epoch: 00 [16002/16204 ( 99%)], Train Loss: 0.57140\n","Epoch: 00 [16022/16204 ( 99%)], Train Loss: 0.57115\n","Epoch: 00 [16042/16204 ( 99%)], Train Loss: 0.57095\n","Epoch: 00 [16062/16204 ( 99%)], Train Loss: 0.57064\n","Epoch: 00 [16082/16204 ( 99%)], Train Loss: 0.57046\n","Epoch: 00 [16102/16204 ( 99%)], Train Loss: 0.57005\n","Epoch: 00 [16122/16204 ( 99%)], Train Loss: 0.56971\n","Epoch: 00 [16142/16204 (100%)], Train Loss: 0.56963\n","Epoch: 00 [16162/16204 (100%)], Train Loss: 0.56932\n","Epoch: 00 [16182/16204 (100%)], Train Loss: 0.56928\n","Epoch: 00 [16202/16204 (100%)], Train Loss: 0.56943\n","Epoch: 00 [16204/16204 (100%)], Train Loss: 0.56936\n","----Validation Results Summary----\n","Epoch: [0] Valid Loss: 0.00000\n","0 Epoch, Best epoch was updated! Valid Loss: 0.00000\n","Saving model checkpoint to chaii-qa-5-fold-xlmroberta-torch-fit/checkpoint-fold-5.\n","\n","Epoch: 01 [    2/16204 (  0%)], Train Loss: 0.27105\n","Epoch: 01 [   22/16204 (  0%)], Train Loss: 0.43850\n","Epoch: 01 [   42/16204 (  0%)], Train Loss: 0.52025\n","Epoch: 01 [   62/16204 (  0%)], Train Loss: 0.50846\n","Epoch: 01 [   82/16204 (  1%)], Train Loss: 0.52033\n","Epoch: 01 [  102/16204 (  1%)], Train Loss: 0.49019\n","Epoch: 01 [  122/16204 (  1%)], Train Loss: 0.48131\n","Epoch: 01 [  142/16204 (  1%)], Train Loss: 0.44764\n","Epoch: 01 [  162/16204 (  1%)], Train Loss: 0.44628\n","Epoch: 01 [  182/16204 (  1%)], Train Loss: 0.46353\n","Epoch: 01 [  202/16204 (  1%)], Train Loss: 0.47237\n","Epoch: 01 [  222/16204 (  1%)], Train Loss: 0.46379\n","Epoch: 01 [  242/16204 (  1%)], Train Loss: 0.47001\n","Epoch: 01 [  262/16204 (  2%)], Train Loss: 0.48014\n","Epoch: 01 [  282/16204 (  2%)], Train Loss: 0.48483\n","Epoch: 01 [  302/16204 (  2%)], Train Loss: 0.47999\n","Epoch: 01 [  322/16204 (  2%)], Train Loss: 0.46973\n","Epoch: 01 [  342/16204 (  2%)], Train Loss: 0.45882\n","Epoch: 01 [  362/16204 (  2%)], Train Loss: 0.45292\n","Epoch: 01 [  382/16204 (  2%)], Train Loss: 0.46378\n","Epoch: 01 [  402/16204 (  2%)], Train Loss: 0.45536\n","Epoch: 01 [  422/16204 (  3%)], Train Loss: 0.44878\n","Epoch: 01 [  442/16204 (  3%)], Train Loss: 0.45459\n","Epoch: 01 [  462/16204 (  3%)], Train Loss: 0.44822\n","Epoch: 01 [  482/16204 (  3%)], Train Loss: 0.43675\n","Epoch: 01 [  502/16204 (  3%)], Train Loss: 0.43632\n","Epoch: 01 [  522/16204 (  3%)], Train Loss: 0.44099\n","Epoch: 01 [  542/16204 (  3%)], Train Loss: 0.45153\n","Epoch: 01 [  562/16204 (  3%)], Train Loss: 0.45001\n","Epoch: 01 [  582/16204 (  4%)], Train Loss: 0.44535\n","Epoch: 01 [  602/16204 (  4%)], Train Loss: 0.45194\n","Epoch: 01 [  622/16204 (  4%)], Train Loss: 0.44479\n","Epoch: 01 [  642/16204 (  4%)], Train Loss: 0.45482\n","Epoch: 01 [  662/16204 (  4%)], Train Loss: 0.46046\n","Epoch: 01 [  682/16204 (  4%)], Train Loss: 0.46614\n","Epoch: 01 [  702/16204 (  4%)], Train Loss: 0.45963\n","Epoch: 01 [  722/16204 (  4%)], Train Loss: 0.46081\n","Epoch: 01 [  742/16204 (  5%)], Train Loss: 0.45560\n","Epoch: 01 [  762/16204 (  5%)], Train Loss: 0.45225\n","Epoch: 01 [  782/16204 (  5%)], Train Loss: 0.44924\n","Epoch: 01 [  802/16204 (  5%)], Train Loss: 0.44710\n","Epoch: 01 [  822/16204 (  5%)], Train Loss: 0.45187\n","Epoch: 01 [  842/16204 (  5%)], Train Loss: 0.45430\n","Epoch: 01 [  862/16204 (  5%)], Train Loss: 0.44902\n","Epoch: 01 [  882/16204 (  5%)], Train Loss: 0.44616\n","Epoch: 01 [  902/16204 (  6%)], Train Loss: 0.44291\n","Epoch: 01 [  922/16204 (  6%)], Train Loss: 0.44638\n","Epoch: 01 [  942/16204 (  6%)], Train Loss: 0.44505\n","Epoch: 01 [  962/16204 (  6%)], Train Loss: 0.44389\n","Epoch: 01 [  982/16204 (  6%)], Train Loss: 0.44639\n","Epoch: 01 [ 1002/16204 (  6%)], Train Loss: 0.44585\n","Epoch: 01 [ 1022/16204 (  6%)], Train Loss: 0.44173\n","Epoch: 01 [ 1042/16204 (  6%)], Train Loss: 0.43817\n","Epoch: 01 [ 1062/16204 (  7%)], Train Loss: 0.43449\n","Epoch: 01 [ 1082/16204 (  7%)], Train Loss: 0.43735\n","Epoch: 01 [ 1102/16204 (  7%)], Train Loss: 0.44022\n","Epoch: 01 [ 1122/16204 (  7%)], Train Loss: 0.44121\n","Epoch: 01 [ 1142/16204 (  7%)], Train Loss: 0.44062\n","Epoch: 01 [ 1162/16204 (  7%)], Train Loss: 0.43759\n","Epoch: 01 [ 1182/16204 (  7%)], Train Loss: 0.43587\n","Epoch: 01 [ 1202/16204 (  7%)], Train Loss: 0.43690\n","Epoch: 01 [ 1222/16204 (  8%)], Train Loss: 0.43548\n","Epoch: 01 [ 1242/16204 (  8%)], Train Loss: 0.43372\n","Epoch: 01 [ 1262/16204 (  8%)], Train Loss: 0.43208\n","Epoch: 01 [ 1282/16204 (  8%)], Train Loss: 0.43276\n","Epoch: 01 [ 1302/16204 (  8%)], Train Loss: 0.43204\n","Epoch: 01 [ 1322/16204 (  8%)], Train Loss: 0.42981\n","Epoch: 01 [ 1342/16204 (  8%)], Train Loss: 0.43287\n","Epoch: 01 [ 1362/16204 (  8%)], Train Loss: 0.42975\n","Epoch: 01 [ 1382/16204 (  9%)], Train Loss: 0.42844\n","Epoch: 01 [ 1402/16204 (  9%)], Train Loss: 0.42775\n","Epoch: 01 [ 1422/16204 (  9%)], Train Loss: 0.42854\n","Epoch: 01 [ 1442/16204 (  9%)], Train Loss: 0.42771\n","Epoch: 01 [ 1462/16204 (  9%)], Train Loss: 0.42948\n","Epoch: 01 [ 1482/16204 (  9%)], Train Loss: 0.42859\n","Epoch: 01 [ 1502/16204 (  9%)], Train Loss: 0.42957\n","Epoch: 01 [ 1522/16204 (  9%)], Train Loss: 0.42697\n","Epoch: 01 [ 1542/16204 ( 10%)], Train Loss: 0.42655\n","Epoch: 01 [ 1562/16204 ( 10%)], Train Loss: 0.42504\n","Epoch: 01 [ 1582/16204 ( 10%)], Train Loss: 0.42648\n","Epoch: 01 [ 1602/16204 ( 10%)], Train Loss: 0.42619\n","Epoch: 01 [ 1622/16204 ( 10%)], Train Loss: 0.42815\n","Epoch: 01 [ 1642/16204 ( 10%)], Train Loss: 0.42511\n","Epoch: 01 [ 1662/16204 ( 10%)], Train Loss: 0.42904\n","Epoch: 01 [ 1682/16204 ( 10%)], Train Loss: 0.42655\n","Epoch: 01 [ 1702/16204 ( 11%)], Train Loss: 0.42683\n","Epoch: 01 [ 1722/16204 ( 11%)], Train Loss: 0.42639\n","Epoch: 01 [ 1742/16204 ( 11%)], Train Loss: 0.42665\n","Epoch: 01 [ 1762/16204 ( 11%)], Train Loss: 0.42750\n","Epoch: 01 [ 1782/16204 ( 11%)], Train Loss: 0.42732\n","Epoch: 01 [ 1802/16204 ( 11%)], Train Loss: 0.42630\n","Epoch: 01 [ 1822/16204 ( 11%)], Train Loss: 0.42438\n","Epoch: 01 [ 1842/16204 ( 11%)], Train Loss: 0.42229\n","Epoch: 01 [ 1862/16204 ( 11%)], Train Loss: 0.42481\n","Epoch: 01 [ 1882/16204 ( 12%)], Train Loss: 0.42654\n","Epoch: 01 [ 1902/16204 ( 12%)], Train Loss: 0.42526\n","Epoch: 01 [ 1922/16204 ( 12%)], Train Loss: 0.42721\n","Epoch: 01 [ 1942/16204 ( 12%)], Train Loss: 0.42814\n","Epoch: 01 [ 1962/16204 ( 12%)], Train Loss: 0.42868\n","Epoch: 01 [ 1982/16204 ( 12%)], Train Loss: 0.42615\n","Epoch: 01 [ 2002/16204 ( 12%)], Train Loss: 0.42592\n","Epoch: 01 [ 2022/16204 ( 12%)], Train Loss: 0.42736\n","Epoch: 01 [ 2042/16204 ( 13%)], Train Loss: 0.42894\n","Epoch: 01 [ 2062/16204 ( 13%)], Train Loss: 0.42721\n","Epoch: 01 [ 2082/16204 ( 13%)], Train Loss: 0.42696\n","Epoch: 01 [ 2102/16204 ( 13%)], Train Loss: 0.42822\n","Epoch: 01 [ 2122/16204 ( 13%)], Train Loss: 0.42625\n","Epoch: 01 [ 2142/16204 ( 13%)], Train Loss: 0.42559\n","Epoch: 01 [ 2162/16204 ( 13%)], Train Loss: 0.42519\n","Epoch: 01 [ 2182/16204 ( 13%)], Train Loss: 0.42345\n","Epoch: 01 [ 2202/16204 ( 14%)], Train Loss: 0.42505\n","Epoch: 01 [ 2222/16204 ( 14%)], Train Loss: 0.42343\n","Epoch: 01 [ 2242/16204 ( 14%)], Train Loss: 0.42513\n","Epoch: 01 [ 2262/16204 ( 14%)], Train Loss: 0.42553\n","Epoch: 01 [ 2282/16204 ( 14%)], Train Loss: 0.42621\n","Epoch: 01 [ 2302/16204 ( 14%)], Train Loss: 0.42617\n","Epoch: 01 [ 2322/16204 ( 14%)], Train Loss: 0.42514\n","Epoch: 01 [ 2342/16204 ( 14%)], Train Loss: 0.42446\n","Epoch: 01 [ 2362/16204 ( 15%)], Train Loss: 0.42952\n","Epoch: 01 [ 2382/16204 ( 15%)], Train Loss: 0.42839\n","Epoch: 01 [ 2402/16204 ( 15%)], Train Loss: 0.42841\n","Epoch: 01 [ 2422/16204 ( 15%)], Train Loss: 0.42746\n","Epoch: 01 [ 2442/16204 ( 15%)], Train Loss: 0.42947\n","Epoch: 01 [ 2462/16204 ( 15%)], Train Loss: 0.43162\n","Epoch: 01 [ 2482/16204 ( 15%)], Train Loss: 0.43069\n","Epoch: 01 [ 2502/16204 ( 15%)], Train Loss: 0.43051\n","Epoch: 01 [ 2522/16204 ( 16%)], Train Loss: 0.42957\n","Epoch: 01 [ 2542/16204 ( 16%)], Train Loss: 0.42927\n","Epoch: 01 [ 2562/16204 ( 16%)], Train Loss: 0.42943\n","Epoch: 01 [ 2582/16204 ( 16%)], Train Loss: 0.42942\n","Epoch: 01 [ 2602/16204 ( 16%)], Train Loss: 0.42983\n","Epoch: 01 [ 2622/16204 ( 16%)], Train Loss: 0.42887\n","Epoch: 01 [ 2642/16204 ( 16%)], Train Loss: 0.42866\n","Epoch: 01 [ 2662/16204 ( 16%)], Train Loss: 0.42712\n","Epoch: 01 [ 2682/16204 ( 17%)], Train Loss: 0.42586\n","Epoch: 01 [ 2702/16204 ( 17%)], Train Loss: 0.42418\n","Epoch: 01 [ 2722/16204 ( 17%)], Train Loss: 0.42316\n","Epoch: 01 [ 2742/16204 ( 17%)], Train Loss: 0.42118\n","Epoch: 01 [ 2762/16204 ( 17%)], Train Loss: 0.42256\n","Epoch: 01 [ 2782/16204 ( 17%)], Train Loss: 0.42179\n","Epoch: 01 [ 2802/16204 ( 17%)], Train Loss: 0.42094\n","Epoch: 01 [ 2822/16204 ( 17%)], Train Loss: 0.41927\n","Epoch: 01 [ 2842/16204 ( 18%)], Train Loss: 0.42043\n","Epoch: 01 [ 2862/16204 ( 18%)], Train Loss: 0.42057\n","Epoch: 01 [ 2882/16204 ( 18%)], Train Loss: 0.42013\n","Epoch: 01 [ 2902/16204 ( 18%)], Train Loss: 0.42037\n","Epoch: 01 [ 2922/16204 ( 18%)], Train Loss: 0.41881\n","Epoch: 01 [ 2942/16204 ( 18%)], Train Loss: 0.41709\n","Epoch: 01 [ 2962/16204 ( 18%)], Train Loss: 0.41516\n","Epoch: 01 [ 2982/16204 ( 18%)], Train Loss: 0.41414\n","Epoch: 01 [ 3002/16204 ( 19%)], Train Loss: 0.41348\n","Epoch: 01 [ 3022/16204 ( 19%)], Train Loss: 0.41195\n","Epoch: 01 [ 3042/16204 ( 19%)], Train Loss: 0.41259\n","Epoch: 01 [ 3062/16204 ( 19%)], Train Loss: 0.41151\n","Epoch: 01 [ 3082/16204 ( 19%)], Train Loss: 0.41156\n","Epoch: 01 [ 3102/16204 ( 19%)], Train Loss: 0.41057\n","Epoch: 01 [ 3122/16204 ( 19%)], Train Loss: 0.40913\n","Epoch: 01 [ 3142/16204 ( 19%)], Train Loss: 0.40812\n","Epoch: 01 [ 3162/16204 ( 20%)], Train Loss: 0.40819\n","Epoch: 01 [ 3182/16204 ( 20%)], Train Loss: 0.40688\n","Epoch: 01 [ 3202/16204 ( 20%)], Train Loss: 0.40547\n","Epoch: 01 [ 3222/16204 ( 20%)], Train Loss: 0.40458\n","Epoch: 01 [ 3242/16204 ( 20%)], Train Loss: 0.40296\n","Epoch: 01 [ 3262/16204 ( 20%)], Train Loss: 0.40537\n","Epoch: 01 [ 3282/16204 ( 20%)], Train Loss: 0.40481\n","Epoch: 01 [ 3302/16204 ( 20%)], Train Loss: 0.40471\n","Epoch: 01 [ 3322/16204 ( 21%)], Train Loss: 0.40488\n","Epoch: 01 [ 3342/16204 ( 21%)], Train Loss: 0.40323\n","Epoch: 01 [ 3362/16204 ( 21%)], Train Loss: 0.40355\n","Epoch: 01 [ 3382/16204 ( 21%)], Train Loss: 0.40279\n","Epoch: 01 [ 3402/16204 ( 21%)], Train Loss: 0.40253\n","Epoch: 01 [ 3422/16204 ( 21%)], Train Loss: 0.40233\n","Epoch: 01 [ 3442/16204 ( 21%)], Train Loss: 0.40168\n","Epoch: 01 [ 3462/16204 ( 21%)], Train Loss: 0.40197\n","Epoch: 01 [ 3482/16204 ( 21%)], Train Loss: 0.40094\n","Epoch: 01 [ 3502/16204 ( 22%)], Train Loss: 0.40075\n","Epoch: 01 [ 3522/16204 ( 22%)], Train Loss: 0.39987\n","Epoch: 01 [ 3542/16204 ( 22%)], Train Loss: 0.39995\n","Epoch: 01 [ 3562/16204 ( 22%)], Train Loss: 0.39891\n","Epoch: 01 [ 3582/16204 ( 22%)], Train Loss: 0.39982\n","Epoch: 01 [ 3602/16204 ( 22%)], Train Loss: 0.39904\n","Epoch: 01 [ 3622/16204 ( 22%)], Train Loss: 0.40002\n","Epoch: 01 [ 3642/16204 ( 22%)], Train Loss: 0.39962\n","Epoch: 01 [ 3662/16204 ( 23%)], Train Loss: 0.39859\n","Epoch: 01 [ 3682/16204 ( 23%)], Train Loss: 0.39815\n","Epoch: 01 [ 3702/16204 ( 23%)], Train Loss: 0.39797\n","Epoch: 01 [ 3722/16204 ( 23%)], Train Loss: 0.39712\n","Epoch: 01 [ 3742/16204 ( 23%)], Train Loss: 0.39711\n","Epoch: 01 [ 3762/16204 ( 23%)], Train Loss: 0.39691\n","Epoch: 01 [ 3782/16204 ( 23%)], Train Loss: 0.39665\n","Epoch: 01 [ 3802/16204 ( 23%)], Train Loss: 0.39621\n","Epoch: 01 [ 3822/16204 ( 24%)], Train Loss: 0.39717\n","Epoch: 01 [ 3842/16204 ( 24%)], Train Loss: 0.39739\n","Epoch: 01 [ 3862/16204 ( 24%)], Train Loss: 0.39712\n","Epoch: 01 [ 3882/16204 ( 24%)], Train Loss: 0.39640\n","Epoch: 01 [ 3902/16204 ( 24%)], Train Loss: 0.39553\n","Epoch: 01 [ 3922/16204 ( 24%)], Train Loss: 0.39513\n","Epoch: 01 [ 3942/16204 ( 24%)], Train Loss: 0.39544\n","Epoch: 01 [ 3962/16204 ( 24%)], Train Loss: 0.39478\n","Epoch: 01 [ 3982/16204 ( 25%)], Train Loss: 0.39394\n","Epoch: 01 [ 4002/16204 ( 25%)], Train Loss: 0.39342\n","Epoch: 01 [ 4022/16204 ( 25%)], Train Loss: 0.39274\n","Epoch: 01 [ 4042/16204 ( 25%)], Train Loss: 0.39178\n","Epoch: 01 [ 4062/16204 ( 25%)], Train Loss: 0.38995\n","Epoch: 01 [ 4082/16204 ( 25%)], Train Loss: 0.38913\n","Epoch: 01 [ 4102/16204 ( 25%)], Train Loss: 0.38859\n","Epoch: 01 [ 4122/16204 ( 25%)], Train Loss: 0.38805\n","Epoch: 01 [ 4142/16204 ( 26%)], Train Loss: 0.38723\n","Epoch: 01 [ 4162/16204 ( 26%)], Train Loss: 0.38698\n","Epoch: 01 [ 4182/16204 ( 26%)], Train Loss: 0.38746\n","Epoch: 01 [ 4202/16204 ( 26%)], Train Loss: 0.38795\n","Epoch: 01 [ 4222/16204 ( 26%)], Train Loss: 0.38867\n","Epoch: 01 [ 4242/16204 ( 26%)], Train Loss: 0.38815\n","Epoch: 01 [ 4262/16204 ( 26%)], Train Loss: 0.38783\n","Epoch: 01 [ 4282/16204 ( 26%)], Train Loss: 0.38675\n","Epoch: 01 [ 4302/16204 ( 27%)], Train Loss: 0.38652\n","Epoch: 01 [ 4322/16204 ( 27%)], Train Loss: 0.38693\n","Epoch: 01 [ 4342/16204 ( 27%)], Train Loss: 0.38552\n","Epoch: 01 [ 4362/16204 ( 27%)], Train Loss: 0.38564\n","Epoch: 01 [ 4382/16204 ( 27%)], Train Loss: 0.38541\n","Epoch: 01 [ 4402/16204 ( 27%)], Train Loss: 0.38520\n","Epoch: 01 [ 4422/16204 ( 27%)], Train Loss: 0.38517\n","Epoch: 01 [ 4442/16204 ( 27%)], Train Loss: 0.38605\n","Epoch: 01 [ 4462/16204 ( 28%)], Train Loss: 0.38723\n","Epoch: 01 [ 4482/16204 ( 28%)], Train Loss: 0.38675\n","Epoch: 01 [ 4502/16204 ( 28%)], Train Loss: 0.38638\n","Epoch: 01 [ 4522/16204 ( 28%)], Train Loss: 0.38632\n","Epoch: 01 [ 4542/16204 ( 28%)], Train Loss: 0.38602\n","Epoch: 01 [ 4562/16204 ( 28%)], Train Loss: 0.38563\n","Epoch: 01 [ 4582/16204 ( 28%)], Train Loss: 0.38662\n","Epoch: 01 [ 4602/16204 ( 28%)], Train Loss: 0.38682\n","Epoch: 01 [ 4622/16204 ( 29%)], Train Loss: 0.38773\n","Epoch: 01 [ 4642/16204 ( 29%)], Train Loss: 0.38761\n","Epoch: 01 [ 4662/16204 ( 29%)], Train Loss: 0.38798\n","Epoch: 01 [ 4682/16204 ( 29%)], Train Loss: 0.38838\n","Epoch: 01 [ 4702/16204 ( 29%)], Train Loss: 0.38763\n","Epoch: 01 [ 4722/16204 ( 29%)], Train Loss: 0.38698\n","Epoch: 01 [ 4742/16204 ( 29%)], Train Loss: 0.38670\n","Epoch: 01 [ 4762/16204 ( 29%)], Train Loss: 0.38639\n","Epoch: 01 [ 4782/16204 ( 30%)], Train Loss: 0.38595\n","Epoch: 01 [ 4802/16204 ( 30%)], Train Loss: 0.38634\n","Epoch: 01 [ 4822/16204 ( 30%)], Train Loss: 0.38676\n","Epoch: 01 [ 4842/16204 ( 30%)], Train Loss: 0.38604\n","Epoch: 01 [ 4862/16204 ( 30%)], Train Loss: 0.38628\n","Epoch: 01 [ 4882/16204 ( 30%)], Train Loss: 0.38659\n","Epoch: 01 [ 4902/16204 ( 30%)], Train Loss: 0.38572\n","Epoch: 01 [ 4922/16204 ( 30%)], Train Loss: 0.38484\n","Epoch: 01 [ 4942/16204 ( 30%)], Train Loss: 0.38392\n","Epoch: 01 [ 4962/16204 ( 31%)], Train Loss: 0.38363\n","Epoch: 01 [ 4982/16204 ( 31%)], Train Loss: 0.38287\n","Epoch: 01 [ 5002/16204 ( 31%)], Train Loss: 0.38284\n","Epoch: 01 [ 5022/16204 ( 31%)], Train Loss: 0.38267\n","Epoch: 01 [ 5042/16204 ( 31%)], Train Loss: 0.38320\n","Epoch: 01 [ 5062/16204 ( 31%)], Train Loss: 0.38389\n","Epoch: 01 [ 5082/16204 ( 31%)], Train Loss: 0.38296\n","Epoch: 01 [ 5102/16204 ( 31%)], Train Loss: 0.38254\n","Epoch: 01 [ 5122/16204 ( 32%)], Train Loss: 0.38192\n","Epoch: 01 [ 5142/16204 ( 32%)], Train Loss: 0.38171\n","Epoch: 01 [ 5162/16204 ( 32%)], Train Loss: 0.38283\n","Epoch: 01 [ 5182/16204 ( 32%)], Train Loss: 0.38253\n","Epoch: 01 [ 5202/16204 ( 32%)], Train Loss: 0.38204\n","Epoch: 01 [ 5222/16204 ( 32%)], Train Loss: 0.38137\n","Epoch: 01 [ 5242/16204 ( 32%)], Train Loss: 0.38073\n","Epoch: 01 [ 5262/16204 ( 32%)], Train Loss: 0.38062\n","Epoch: 01 [ 5282/16204 ( 33%)], Train Loss: 0.37995\n","Epoch: 01 [ 5302/16204 ( 33%)], Train Loss: 0.37936\n","Epoch: 01 [ 5322/16204 ( 33%)], Train Loss: 0.37908\n","Epoch: 01 [ 5342/16204 ( 33%)], Train Loss: 0.37876\n","Epoch: 01 [ 5362/16204 ( 33%)], Train Loss: 0.37799\n","Epoch: 01 [ 5382/16204 ( 33%)], Train Loss: 0.37947\n","Epoch: 01 [ 5402/16204 ( 33%)], Train Loss: 0.37861\n","Epoch: 01 [ 5422/16204 ( 33%)], Train Loss: 0.37909\n","Epoch: 01 [ 5442/16204 ( 34%)], Train Loss: 0.37850\n","Epoch: 01 [ 5462/16204 ( 34%)], Train Loss: 0.37792\n","Epoch: 01 [ 5482/16204 ( 34%)], Train Loss: 0.37802\n","Epoch: 01 [ 5502/16204 ( 34%)], Train Loss: 0.37699\n","Epoch: 01 [ 5522/16204 ( 34%)], Train Loss: 0.37628\n","Epoch: 01 [ 5542/16204 ( 34%)], Train Loss: 0.37549\n","Epoch: 01 [ 5562/16204 ( 34%)], Train Loss: 0.37506\n","Epoch: 01 [ 5582/16204 ( 34%)], Train Loss: 0.37541\n","Epoch: 01 [ 5602/16204 ( 35%)], Train Loss: 0.37455\n","Epoch: 01 [ 5622/16204 ( 35%)], Train Loss: 0.37368\n","Epoch: 01 [ 5642/16204 ( 35%)], Train Loss: 0.37320\n","Epoch: 01 [ 5662/16204 ( 35%)], Train Loss: 0.37220\n","Epoch: 01 [ 5682/16204 ( 35%)], Train Loss: 0.37191\n","Epoch: 01 [ 5702/16204 ( 35%)], Train Loss: 0.37096\n","Epoch: 01 [ 5722/16204 ( 35%)], Train Loss: 0.36998\n","Epoch: 01 [ 5742/16204 ( 35%)], Train Loss: 0.36920\n","Epoch: 01 [ 5762/16204 ( 36%)], Train Loss: 0.36854\n","Epoch: 01 [ 5782/16204 ( 36%)], Train Loss: 0.36799\n","Epoch: 01 [ 5802/16204 ( 36%)], Train Loss: 0.36805\n","Epoch: 01 [ 5822/16204 ( 36%)], Train Loss: 0.36858\n","Epoch: 01 [ 5842/16204 ( 36%)], Train Loss: 0.36831\n","Epoch: 01 [ 5862/16204 ( 36%)], Train Loss: 0.36754\n","Epoch: 01 [ 5882/16204 ( 36%)], Train Loss: 0.36731\n","Epoch: 01 [ 5902/16204 ( 36%)], Train Loss: 0.36701\n","Epoch: 01 [ 5922/16204 ( 37%)], Train Loss: 0.36661\n","Epoch: 01 [ 5942/16204 ( 37%)], Train Loss: 0.36577\n","Epoch: 01 [ 5962/16204 ( 37%)], Train Loss: 0.36525\n","Epoch: 01 [ 5982/16204 ( 37%)], Train Loss: 0.36534\n","Epoch: 01 [ 6002/16204 ( 37%)], Train Loss: 0.36527\n","Epoch: 01 [ 6022/16204 ( 37%)], Train Loss: 0.36522\n","Epoch: 01 [ 6042/16204 ( 37%)], Train Loss: 0.36455\n","Epoch: 01 [ 6062/16204 ( 37%)], Train Loss: 0.36399\n","Epoch: 01 [ 6082/16204 ( 38%)], Train Loss: 0.36335\n","Epoch: 01 [ 6102/16204 ( 38%)], Train Loss: 0.36263\n","Epoch: 01 [ 6122/16204 ( 38%)], Train Loss: 0.36222\n","Epoch: 01 [ 6142/16204 ( 38%)], Train Loss: 0.36211\n","Epoch: 01 [ 6162/16204 ( 38%)], Train Loss: 0.36155\n","Epoch: 01 [ 6182/16204 ( 38%)], Train Loss: 0.36076\n","Epoch: 01 [ 6202/16204 ( 38%)], Train Loss: 0.36087\n","Epoch: 01 [ 6222/16204 ( 38%)], Train Loss: 0.36060\n","Epoch: 01 [ 6242/16204 ( 39%)], Train Loss: 0.36041\n","Epoch: 01 [ 6262/16204 ( 39%)], Train Loss: 0.36049\n","Epoch: 01 [ 6282/16204 ( 39%)], Train Loss: 0.36003\n","Epoch: 01 [ 6302/16204 ( 39%)], Train Loss: 0.35963\n","Epoch: 01 [ 6322/16204 ( 39%)], Train Loss: 0.35913\n","Epoch: 01 [ 6342/16204 ( 39%)], Train Loss: 0.35897\n","Epoch: 01 [ 6362/16204 ( 39%)], Train Loss: 0.35847\n","Epoch: 01 [ 6382/16204 ( 39%)], Train Loss: 0.35869\n","Epoch: 01 [ 6402/16204 ( 40%)], Train Loss: 0.35856\n","Epoch: 01 [ 6422/16204 ( 40%)], Train Loss: 0.35845\n","Epoch: 01 [ 6442/16204 ( 40%)], Train Loss: 0.35818\n","Epoch: 01 [ 6462/16204 ( 40%)], Train Loss: 0.35778\n","Epoch: 01 [ 6482/16204 ( 40%)], Train Loss: 0.35746\n","Epoch: 01 [ 6502/16204 ( 40%)], Train Loss: 0.35663\n","Epoch: 01 [ 6522/16204 ( 40%)], Train Loss: 0.35596\n","Epoch: 01 [ 6542/16204 ( 40%)], Train Loss: 0.35557\n","Epoch: 01 [ 6562/16204 ( 40%)], Train Loss: 0.35500\n","Epoch: 01 [ 6582/16204 ( 41%)], Train Loss: 0.35410\n","Epoch: 01 [ 6602/16204 ( 41%)], Train Loss: 0.35354\n","Epoch: 01 [ 6622/16204 ( 41%)], Train Loss: 0.35315\n","Epoch: 01 [ 6642/16204 ( 41%)], Train Loss: 0.35311\n","Epoch: 01 [ 6662/16204 ( 41%)], Train Loss: 0.35377\n","Epoch: 01 [ 6682/16204 ( 41%)], Train Loss: 0.35318\n","Epoch: 01 [ 6702/16204 ( 41%)], Train Loss: 0.35331\n","Epoch: 01 [ 6722/16204 ( 41%)], Train Loss: 0.35313\n","Epoch: 01 [ 6742/16204 ( 42%)], Train Loss: 0.35232\n","Epoch: 01 [ 6762/16204 ( 42%)], Train Loss: 0.35194\n","Epoch: 01 [ 6782/16204 ( 42%)], Train Loss: 0.35191\n","Epoch: 01 [ 6802/16204 ( 42%)], Train Loss: 0.35166\n","Epoch: 01 [ 6822/16204 ( 42%)], Train Loss: 0.35127\n","Epoch: 01 [ 6842/16204 ( 42%)], Train Loss: 0.35086\n","Epoch: 01 [ 6862/16204 ( 42%)], Train Loss: 0.35088\n","Epoch: 01 [ 6882/16204 ( 42%)], Train Loss: 0.34992\n","Epoch: 01 [ 6902/16204 ( 43%)], Train Loss: 0.34956\n","Epoch: 01 [ 6922/16204 ( 43%)], Train Loss: 0.34884\n","Epoch: 01 [ 6942/16204 ( 43%)], Train Loss: 0.34844\n","Epoch: 01 [ 6962/16204 ( 43%)], Train Loss: 0.34903\n","Epoch: 01 [ 6982/16204 ( 43%)], Train Loss: 0.34895\n","Epoch: 01 [ 7002/16204 ( 43%)], Train Loss: 0.34866\n","Epoch: 01 [ 7022/16204 ( 43%)], Train Loss: 0.34833\n","Epoch: 01 [ 7042/16204 ( 43%)], Train Loss: 0.34786\n","Epoch: 01 [ 7062/16204 ( 44%)], Train Loss: 0.34748\n","Epoch: 01 [ 7082/16204 ( 44%)], Train Loss: 0.34689\n","Epoch: 01 [ 7102/16204 ( 44%)], Train Loss: 0.34655\n","Epoch: 01 [ 7122/16204 ( 44%)], Train Loss: 0.34622\n","Epoch: 01 [ 7142/16204 ( 44%)], Train Loss: 0.34589\n","Epoch: 01 [ 7162/16204 ( 44%)], Train Loss: 0.34682\n","Epoch: 01 [ 7182/16204 ( 44%)], Train Loss: 0.34644\n","Epoch: 01 [ 7202/16204 ( 44%)], Train Loss: 0.34633\n","Epoch: 01 [ 7222/16204 ( 45%)], Train Loss: 0.34589\n","Epoch: 01 [ 7242/16204 ( 45%)], Train Loss: 0.34545\n","Epoch: 01 [ 7262/16204 ( 45%)], Train Loss: 0.34531\n","Epoch: 01 [ 7282/16204 ( 45%)], Train Loss: 0.34477\n","Epoch: 01 [ 7302/16204 ( 45%)], Train Loss: 0.34511\n","Epoch: 01 [ 7322/16204 ( 45%)], Train Loss: 0.34483\n","Epoch: 01 [ 7342/16204 ( 45%)], Train Loss: 0.34440\n","Epoch: 01 [ 7362/16204 ( 45%)], Train Loss: 0.34418\n","Epoch: 01 [ 7382/16204 ( 46%)], Train Loss: 0.34379\n","Epoch: 01 [ 7402/16204 ( 46%)], Train Loss: 0.34322\n","Epoch: 01 [ 7422/16204 ( 46%)], Train Loss: 0.34267\n","Epoch: 01 [ 7442/16204 ( 46%)], Train Loss: 0.34212\n","Epoch: 01 [ 7462/16204 ( 46%)], Train Loss: 0.34146\n","Epoch: 01 [ 7482/16204 ( 46%)], Train Loss: 0.34085\n","Epoch: 01 [ 7502/16204 ( 46%)], Train Loss: 0.34022\n","Epoch: 01 [ 7522/16204 ( 46%)], Train Loss: 0.34036\n","Epoch: 01 [ 7542/16204 ( 47%)], Train Loss: 0.33996\n","Epoch: 01 [ 7562/16204 ( 47%)], Train Loss: 0.33974\n","Epoch: 01 [ 7582/16204 ( 47%)], Train Loss: 0.33913\n","Epoch: 01 [ 7602/16204 ( 47%)], Train Loss: 0.33932\n","Epoch: 01 [ 7622/16204 ( 47%)], Train Loss: 0.33939\n","Epoch: 01 [ 7642/16204 ( 47%)], Train Loss: 0.33935\n","Epoch: 01 [ 7662/16204 ( 47%)], Train Loss: 0.33914\n","Epoch: 01 [ 7682/16204 ( 47%)], Train Loss: 0.33953\n","Epoch: 01 [ 7702/16204 ( 48%)], Train Loss: 0.33941\n","Epoch: 01 [ 7722/16204 ( 48%)], Train Loss: 0.33908\n","Epoch: 01 [ 7742/16204 ( 48%)], Train Loss: 0.33899\n","Epoch: 01 [ 7762/16204 ( 48%)], Train Loss: 0.33838\n","Epoch: 01 [ 7782/16204 ( 48%)], Train Loss: 0.33768\n","Epoch: 01 [ 7802/16204 ( 48%)], Train Loss: 0.33715\n","Epoch: 01 [ 7822/16204 ( 48%)], Train Loss: 0.33700\n","Epoch: 01 [ 7842/16204 ( 48%)], Train Loss: 0.33701\n","Epoch: 01 [ 7862/16204 ( 49%)], Train Loss: 0.33667\n","Epoch: 01 [ 7882/16204 ( 49%)], Train Loss: 0.33694\n","Epoch: 01 [ 7902/16204 ( 49%)], Train Loss: 0.33640\n","Epoch: 01 [ 7922/16204 ( 49%)], Train Loss: 0.33680\n","Epoch: 01 [ 7942/16204 ( 49%)], Train Loss: 0.33666\n","Epoch: 01 [ 7962/16204 ( 49%)], Train Loss: 0.33668\n","Epoch: 01 [ 7982/16204 ( 49%)], Train Loss: 0.33627\n","Epoch: 01 [ 8002/16204 ( 49%)], Train Loss: 0.33579\n","Epoch: 01 [ 8022/16204 ( 50%)], Train Loss: 0.33584\n","Epoch: 01 [ 8042/16204 ( 50%)], Train Loss: 0.33515\n","Epoch: 01 [ 8062/16204 ( 50%)], Train Loss: 0.33502\n","Epoch: 01 [ 8082/16204 ( 50%)], Train Loss: 0.33461\n","Epoch: 01 [ 8102/16204 ( 50%)], Train Loss: 0.33392\n","Epoch: 01 [ 8122/16204 ( 50%)], Train Loss: 0.33344\n","Epoch: 01 [ 8142/16204 ( 50%)], Train Loss: 0.33282\n","Epoch: 01 [ 8162/16204 ( 50%)], Train Loss: 0.33239\n","Epoch: 01 [ 8182/16204 ( 50%)], Train Loss: 0.33210\n","Epoch: 01 [ 8202/16204 ( 51%)], Train Loss: 0.33269\n","Epoch: 01 [ 8222/16204 ( 51%)], Train Loss: 0.33221\n","Epoch: 01 [ 8242/16204 ( 51%)], Train Loss: 0.33177\n","Epoch: 01 [ 8262/16204 ( 51%)], Train Loss: 0.33159\n","Epoch: 01 [ 8282/16204 ( 51%)], Train Loss: 0.33122\n","Epoch: 01 [ 8302/16204 ( 51%)], Train Loss: 0.33070\n","Epoch: 01 [ 8322/16204 ( 51%)], Train Loss: 0.33017\n","Epoch: 01 [ 8342/16204 ( 51%)], Train Loss: 0.32983\n","Epoch: 01 [ 8362/16204 ( 52%)], Train Loss: 0.32927\n","Epoch: 01 [ 8382/16204 ( 52%)], Train Loss: 0.32872\n","Epoch: 01 [ 8402/16204 ( 52%)], Train Loss: 0.32888\n","Epoch: 01 [ 8422/16204 ( 52%)], Train Loss: 0.32854\n","Epoch: 01 [ 8442/16204 ( 52%)], Train Loss: 0.32814\n","Epoch: 01 [ 8462/16204 ( 52%)], Train Loss: 0.32772\n","Epoch: 01 [ 8482/16204 ( 52%)], Train Loss: 0.32724\n","Epoch: 01 [ 8502/16204 ( 52%)], Train Loss: 0.32716\n","Epoch: 01 [ 8522/16204 ( 53%)], Train Loss: 0.32680\n","Epoch: 01 [ 8542/16204 ( 53%)], Train Loss: 0.32658\n","Epoch: 01 [ 8562/16204 ( 53%)], Train Loss: 0.32637\n","Epoch: 01 [ 8582/16204 ( 53%)], Train Loss: 0.32673\n","Epoch: 01 [ 8602/16204 ( 53%)], Train Loss: 0.32621\n","Epoch: 01 [ 8622/16204 ( 53%)], Train Loss: 0.32589\n","Epoch: 01 [ 8642/16204 ( 53%)], Train Loss: 0.32540\n","Epoch: 01 [ 8662/16204 ( 53%)], Train Loss: 0.32490\n","Epoch: 01 [ 8682/16204 ( 54%)], Train Loss: 0.32442\n","Epoch: 01 [ 8702/16204 ( 54%)], Train Loss: 0.32444\n","Epoch: 01 [ 8722/16204 ( 54%)], Train Loss: 0.32435\n","Epoch: 01 [ 8742/16204 ( 54%)], Train Loss: 0.32388\n","Epoch: 01 [ 8762/16204 ( 54%)], Train Loss: 0.32336\n","Epoch: 01 [ 8782/16204 ( 54%)], Train Loss: 0.32274\n","Epoch: 01 [ 8802/16204 ( 54%)], Train Loss: 0.32247\n","Epoch: 01 [ 8822/16204 ( 54%)], Train Loss: 0.32209\n","Epoch: 01 [ 8842/16204 ( 55%)], Train Loss: 0.32201\n","Epoch: 01 [ 8862/16204 ( 55%)], Train Loss: 0.32173\n","Epoch: 01 [ 8882/16204 ( 55%)], Train Loss: 0.32138\n","Epoch: 01 [ 8902/16204 ( 55%)], Train Loss: 0.32078\n","Epoch: 01 [ 8922/16204 ( 55%)], Train Loss: 0.32060\n","Epoch: 01 [ 8942/16204 ( 55%)], Train Loss: 0.32055\n","Epoch: 01 [ 8962/16204 ( 55%)], Train Loss: 0.32041\n","Epoch: 01 [ 8982/16204 ( 55%)], Train Loss: 0.32093\n","Epoch: 01 [ 9002/16204 ( 56%)], Train Loss: 0.32070\n","Epoch: 01 [ 9022/16204 ( 56%)], Train Loss: 0.32053\n","Epoch: 01 [ 9042/16204 ( 56%)], Train Loss: 0.32057\n","Epoch: 01 [ 9062/16204 ( 56%)], Train Loss: 0.32007\n","Epoch: 01 [ 9082/16204 ( 56%)], Train Loss: 0.31967\n","Epoch: 01 [ 9102/16204 ( 56%)], Train Loss: 0.31959\n","Epoch: 01 [ 9122/16204 ( 56%)], Train Loss: 0.31913\n","Epoch: 01 [ 9142/16204 ( 56%)], Train Loss: 0.31876\n","Epoch: 01 [ 9162/16204 ( 57%)], Train Loss: 0.31832\n","Epoch: 01 [ 9182/16204 ( 57%)], Train Loss: 0.31811\n","Epoch: 01 [ 9202/16204 ( 57%)], Train Loss: 0.31749\n","Epoch: 01 [ 9222/16204 ( 57%)], Train Loss: 0.31757\n","Epoch: 01 [ 9242/16204 ( 57%)], Train Loss: 0.31749\n","Epoch: 01 [ 9262/16204 ( 57%)], Train Loss: 0.31742\n","Epoch: 01 [ 9282/16204 ( 57%)], Train Loss: 0.31696\n","Epoch: 01 [ 9302/16204 ( 57%)], Train Loss: 0.31644\n","Epoch: 01 [ 9322/16204 ( 58%)], Train Loss: 0.31633\n","Epoch: 01 [ 9342/16204 ( 58%)], Train Loss: 0.31615\n","Epoch: 01 [ 9362/16204 ( 58%)], Train Loss: 0.31648\n","Epoch: 01 [ 9382/16204 ( 58%)], Train Loss: 0.31605\n","Epoch: 01 [ 9402/16204 ( 58%)], Train Loss: 0.31594\n","Epoch: 01 [ 9422/16204 ( 58%)], Train Loss: 0.31578\n","Epoch: 01 [ 9442/16204 ( 58%)], Train Loss: 0.31606\n","Epoch: 01 [ 9462/16204 ( 58%)], Train Loss: 0.31594\n","Epoch: 01 [ 9482/16204 ( 59%)], Train Loss: 0.31582\n","Epoch: 01 [ 9502/16204 ( 59%)], Train Loss: 0.31540\n","Epoch: 01 [ 9522/16204 ( 59%)], Train Loss: 0.31488\n","Epoch: 01 [ 9542/16204 ( 59%)], Train Loss: 0.31465\n","Epoch: 01 [ 9562/16204 ( 59%)], Train Loss: 0.31422\n","Epoch: 01 [ 9582/16204 ( 59%)], Train Loss: 0.31381\n","Epoch: 01 [ 9602/16204 ( 59%)], Train Loss: 0.31341\n","Epoch: 01 [ 9622/16204 ( 59%)], Train Loss: 0.31288\n","Epoch: 01 [ 9642/16204 ( 60%)], Train Loss: 0.31247\n","Epoch: 01 [ 9662/16204 ( 60%)], Train Loss: 0.31189\n","Epoch: 01 [ 9682/16204 ( 60%)], Train Loss: 0.31145\n","Epoch: 01 [ 9702/16204 ( 60%)], Train Loss: 0.31148\n","Epoch: 01 [ 9722/16204 ( 60%)], Train Loss: 0.31147\n","Epoch: 01 [ 9742/16204 ( 60%)], Train Loss: 0.31101\n","Epoch: 01 [ 9762/16204 ( 60%)], Train Loss: 0.31045\n","Epoch: 01 [ 9782/16204 ( 60%)], Train Loss: 0.31082\n","Epoch: 01 [ 9802/16204 ( 60%)], Train Loss: 0.31072\n","Epoch: 01 [ 9822/16204 ( 61%)], Train Loss: 0.31087\n","Epoch: 01 [ 9842/16204 ( 61%)], Train Loss: 0.31054\n","Epoch: 01 [ 9862/16204 ( 61%)], Train Loss: 0.31013\n","Epoch: 01 [ 9882/16204 ( 61%)], Train Loss: 0.30967\n","Epoch: 01 [ 9902/16204 ( 61%)], Train Loss: 0.30940\n","Epoch: 01 [ 9922/16204 ( 61%)], Train Loss: 0.30911\n","Epoch: 01 [ 9942/16204 ( 61%)], Train Loss: 0.30872\n","Epoch: 01 [ 9962/16204 ( 61%)], Train Loss: 0.30862\n","Epoch: 01 [ 9982/16204 ( 62%)], Train Loss: 0.30847\n","Epoch: 01 [10002/16204 ( 62%)], Train Loss: 0.30857\n","Epoch: 01 [10022/16204 ( 62%)], Train Loss: 0.30884\n","Epoch: 01 [10042/16204 ( 62%)], Train Loss: 0.30836\n","Epoch: 01 [10062/16204 ( 62%)], Train Loss: 0.30807\n","Epoch: 01 [10082/16204 ( 62%)], Train Loss: 0.30770\n","Epoch: 01 [10102/16204 ( 62%)], Train Loss: 0.30752\n","Epoch: 01 [10122/16204 ( 62%)], Train Loss: 0.30736\n","Epoch: 01 [10142/16204 ( 63%)], Train Loss: 0.30694\n","Epoch: 01 [10162/16204 ( 63%)], Train Loss: 0.30656\n","Epoch: 01 [10182/16204 ( 63%)], Train Loss: 0.30626\n","Epoch: 01 [10202/16204 ( 63%)], Train Loss: 0.30584\n","Epoch: 01 [10222/16204 ( 63%)], Train Loss: 0.30581\n","Epoch: 01 [10242/16204 ( 63%)], Train Loss: 0.30540\n","Epoch: 01 [10262/16204 ( 63%)], Train Loss: 0.30493\n","Epoch: 01 [10282/16204 ( 63%)], Train Loss: 0.30472\n","Epoch: 01 [10302/16204 ( 64%)], Train Loss: 0.30422\n","Epoch: 01 [10322/16204 ( 64%)], Train Loss: 0.30426\n","Epoch: 01 [10342/16204 ( 64%)], Train Loss: 0.30395\n","Epoch: 01 [10362/16204 ( 64%)], Train Loss: 0.30368\n","Epoch: 01 [10382/16204 ( 64%)], Train Loss: 0.30335\n","Epoch: 01 [10402/16204 ( 64%)], Train Loss: 0.30293\n","Epoch: 01 [10422/16204 ( 64%)], Train Loss: 0.30248\n","Epoch: 01 [10442/16204 ( 64%)], Train Loss: 0.30211\n","Epoch: 01 [10462/16204 ( 65%)], Train Loss: 0.30207\n","Epoch: 01 [10482/16204 ( 65%)], Train Loss: 0.30201\n","Epoch: 01 [10502/16204 ( 65%)], Train Loss: 0.30167\n","Epoch: 01 [10522/16204 ( 65%)], Train Loss: 0.30120\n","Epoch: 01 [10542/16204 ( 65%)], Train Loss: 0.30069\n","Epoch: 01 [10562/16204 ( 65%)], Train Loss: 0.30025\n","Epoch: 01 [10582/16204 ( 65%)], Train Loss: 0.30002\n","Epoch: 01 [10602/16204 ( 65%)], Train Loss: 0.29995\n","Epoch: 01 [10622/16204 ( 66%)], Train Loss: 0.29960\n","Epoch: 01 [10642/16204 ( 66%)], Train Loss: 0.29924\n","Epoch: 01 [10662/16204 ( 66%)], Train Loss: 0.29893\n","Epoch: 01 [10682/16204 ( 66%)], Train Loss: 0.29878\n","Epoch: 01 [10702/16204 ( 66%)], Train Loss: 0.29851\n","Epoch: 01 [10722/16204 ( 66%)], Train Loss: 0.29821\n","Epoch: 01 [10742/16204 ( 66%)], Train Loss: 0.29806\n","Epoch: 01 [10762/16204 ( 66%)], Train Loss: 0.29772\n","Epoch: 01 [10782/16204 ( 67%)], Train Loss: 0.29769\n","Epoch: 01 [10802/16204 ( 67%)], Train Loss: 0.29744\n","Epoch: 01 [10822/16204 ( 67%)], Train Loss: 0.29726\n","Epoch: 01 [10842/16204 ( 67%)], Train Loss: 0.29698\n","Epoch: 01 [10862/16204 ( 67%)], Train Loss: 0.29682\n","Epoch: 01 [10882/16204 ( 67%)], Train Loss: 0.29673\n","Epoch: 01 [10902/16204 ( 67%)], Train Loss: 0.29652\n","Epoch: 01 [10922/16204 ( 67%)], Train Loss: 0.29642\n","Epoch: 01 [10942/16204 ( 68%)], Train Loss: 0.29656\n","Epoch: 01 [10962/16204 ( 68%)], Train Loss: 0.29613\n","Epoch: 01 [10982/16204 ( 68%)], Train Loss: 0.29600\n","Epoch: 01 [11002/16204 ( 68%)], Train Loss: 0.29605\n","Epoch: 01 [11022/16204 ( 68%)], Train Loss: 0.29611\n","Epoch: 01 [11042/16204 ( 68%)], Train Loss: 0.29572\n","Epoch: 01 [11062/16204 ( 68%)], Train Loss: 0.29559\n","Epoch: 01 [11082/16204 ( 68%)], Train Loss: 0.29558\n","Epoch: 01 [11102/16204 ( 69%)], Train Loss: 0.29545\n","Epoch: 01 [11122/16204 ( 69%)], Train Loss: 0.29505\n","Epoch: 01 [11142/16204 ( 69%)], Train Loss: 0.29530\n","Epoch: 01 [11162/16204 ( 69%)], Train Loss: 0.29497\n","Epoch: 01 [11182/16204 ( 69%)], Train Loss: 0.29469\n","Epoch: 01 [11202/16204 ( 69%)], Train Loss: 0.29470\n","Epoch: 01 [11222/16204 ( 69%)], Train Loss: 0.29445\n","Epoch: 01 [11242/16204 ( 69%)], Train Loss: 0.29445\n","Epoch: 01 [11262/16204 ( 70%)], Train Loss: 0.29439\n","Epoch: 01 [11282/16204 ( 70%)], Train Loss: 0.29412\n","Epoch: 01 [11302/16204 ( 70%)], Train Loss: 0.29395\n","Epoch: 01 [11322/16204 ( 70%)], Train Loss: 0.29387\n","Epoch: 01 [11342/16204 ( 70%)], Train Loss: 0.29417\n","Epoch: 01 [11362/16204 ( 70%)], Train Loss: 0.29392\n","Epoch: 01 [11382/16204 ( 70%)], Train Loss: 0.29398\n","Epoch: 01 [11402/16204 ( 70%)], Train Loss: 0.29389\n","Epoch: 01 [11422/16204 ( 70%)], Train Loss: 0.29402\n","Epoch: 01 [11442/16204 ( 71%)], Train Loss: 0.29365\n","Epoch: 01 [11462/16204 ( 71%)], Train Loss: 0.29338\n","Epoch: 01 [11482/16204 ( 71%)], Train Loss: 0.29321\n","Epoch: 01 [11502/16204 ( 71%)], Train Loss: 0.29288\n","Epoch: 01 [11522/16204 ( 71%)], Train Loss: 0.29254\n","Epoch: 01 [11542/16204 ( 71%)], Train Loss: 0.29213\n","Epoch: 01 [11562/16204 ( 71%)], Train Loss: 0.29195\n","Epoch: 01 [11582/16204 ( 71%)], Train Loss: 0.29173\n","Epoch: 01 [11602/16204 ( 72%)], Train Loss: 0.29189\n","Epoch: 01 [11622/16204 ( 72%)], Train Loss: 0.29166\n","Epoch: 01 [11642/16204 ( 72%)], Train Loss: 0.29138\n","Epoch: 01 [11662/16204 ( 72%)], Train Loss: 0.29199\n","Epoch: 01 [11682/16204 ( 72%)], Train Loss: 0.29184\n","Epoch: 01 [11702/16204 ( 72%)], Train Loss: 0.29152\n","Epoch: 01 [11722/16204 ( 72%)], Train Loss: 0.29138\n","Epoch: 01 [11742/16204 ( 72%)], Train Loss: 0.29123\n","Epoch: 01 [11762/16204 ( 73%)], Train Loss: 0.29121\n","Epoch: 01 [11782/16204 ( 73%)], Train Loss: 0.29105\n","Epoch: 01 [11802/16204 ( 73%)], Train Loss: 0.29071\n","Epoch: 01 [11822/16204 ( 73%)], Train Loss: 0.29059\n","Epoch: 01 [11842/16204 ( 73%)], Train Loss: 0.29072\n","Epoch: 01 [11862/16204 ( 73%)], Train Loss: 0.29067\n","Epoch: 01 [11882/16204 ( 73%)], Train Loss: 0.29035\n","Epoch: 01 [11902/16204 ( 73%)], Train Loss: 0.29013\n","Epoch: 01 [11922/16204 ( 74%)], Train Loss: 0.29007\n","Epoch: 01 [11942/16204 ( 74%)], Train Loss: 0.28978\n","Epoch: 01 [11962/16204 ( 74%)], Train Loss: 0.28949\n","Epoch: 01 [11982/16204 ( 74%)], Train Loss: 0.28963\n","Epoch: 01 [12002/16204 ( 74%)], Train Loss: 0.28944\n","Epoch: 01 [12022/16204 ( 74%)], Train Loss: 0.28929\n","Epoch: 01 [12042/16204 ( 74%)], Train Loss: 0.28918\n","Epoch: 01 [12062/16204 ( 74%)], Train Loss: 0.28881\n","Epoch: 01 [12082/16204 ( 75%)], Train Loss: 0.28877\n","Epoch: 01 [12102/16204 ( 75%)], Train Loss: 0.28840\n","Epoch: 01 [12122/16204 ( 75%)], Train Loss: 0.28804\n","Epoch: 01 [12142/16204 ( 75%)], Train Loss: 0.28787\n","Epoch: 01 [12162/16204 ( 75%)], Train Loss: 0.28776\n","Epoch: 01 [12182/16204 ( 75%)], Train Loss: 0.28760\n","Epoch: 01 [12202/16204 ( 75%)], Train Loss: 0.28764\n","Epoch: 01 [12222/16204 ( 75%)], Train Loss: 0.28753\n","Epoch: 01 [12242/16204 ( 76%)], Train Loss: 0.28715\n","Epoch: 01 [12262/16204 ( 76%)], Train Loss: 0.28676\n","Epoch: 01 [12282/16204 ( 76%)], Train Loss: 0.28649\n","Epoch: 01 [12302/16204 ( 76%)], Train Loss: 0.28617\n","Epoch: 01 [12322/16204 ( 76%)], Train Loss: 0.28581\n","Epoch: 01 [12342/16204 ( 76%)], Train Loss: 0.28543\n","Epoch: 01 [12362/16204 ( 76%)], Train Loss: 0.28512\n","Epoch: 01 [12382/16204 ( 76%)], Train Loss: 0.28485\n","Epoch: 01 [12402/16204 ( 77%)], Train Loss: 0.28466\n","Epoch: 01 [12422/16204 ( 77%)], Train Loss: 0.28455\n","Epoch: 01 [12442/16204 ( 77%)], Train Loss: 0.28426\n","Epoch: 01 [12462/16204 ( 77%)], Train Loss: 0.28387\n","Epoch: 01 [12482/16204 ( 77%)], Train Loss: 0.28382\n","Epoch: 01 [12502/16204 ( 77%)], Train Loss: 0.28360\n","Epoch: 01 [12522/16204 ( 77%)], Train Loss: 0.28330\n","Epoch: 01 [12542/16204 ( 77%)], Train Loss: 0.28307\n","Epoch: 01 [12562/16204 ( 78%)], Train Loss: 0.28273\n","Epoch: 01 [12582/16204 ( 78%)], Train Loss: 0.28248\n","Epoch: 01 [12602/16204 ( 78%)], Train Loss: 0.28210\n","Epoch: 01 [12622/16204 ( 78%)], Train Loss: 0.28199\n","Epoch: 01 [12642/16204 ( 78%)], Train Loss: 0.28175\n","Epoch: 01 [12662/16204 ( 78%)], Train Loss: 0.28167\n","Epoch: 01 [12682/16204 ( 78%)], Train Loss: 0.28188\n","Epoch: 01 [12702/16204 ( 78%)], Train Loss: 0.28180\n","Epoch: 01 [12722/16204 ( 79%)], Train Loss: 0.28186\n","Epoch: 01 [12742/16204 ( 79%)], Train Loss: 0.28173\n","Epoch: 01 [12762/16204 ( 79%)], Train Loss: 0.28147\n","Epoch: 01 [12782/16204 ( 79%)], Train Loss: 0.28144\n","Epoch: 01 [12802/16204 ( 79%)], Train Loss: 0.28128\n","Epoch: 01 [12822/16204 ( 79%)], Train Loss: 0.28112\n","Epoch: 01 [12842/16204 ( 79%)], Train Loss: 0.28092\n","Epoch: 01 [12862/16204 ( 79%)], Train Loss: 0.28076\n","Epoch: 01 [12882/16204 ( 79%)], Train Loss: 0.28041\n","Epoch: 01 [12902/16204 ( 80%)], Train Loss: 0.28048\n","Epoch: 01 [12922/16204 ( 80%)], Train Loss: 0.28030\n","Epoch: 01 [12942/16204 ( 80%)], Train Loss: 0.28039\n","Epoch: 01 [12962/16204 ( 80%)], Train Loss: 0.28034\n","Epoch: 01 [12982/16204 ( 80%)], Train Loss: 0.28013\n","Epoch: 01 [13002/16204 ( 80%)], Train Loss: 0.27997\n","Epoch: 01 [13022/16204 ( 80%)], Train Loss: 0.27988\n","Epoch: 01 [13042/16204 ( 80%)], Train Loss: 0.27968\n","Epoch: 01 [13062/16204 ( 81%)], Train Loss: 0.27966\n","Epoch: 01 [13082/16204 ( 81%)], Train Loss: 0.27962\n","Epoch: 01 [13102/16204 ( 81%)], Train Loss: 0.27941\n","Epoch: 01 [13122/16204 ( 81%)], Train Loss: 0.27938\n","Epoch: 01 [13142/16204 ( 81%)], Train Loss: 0.27909\n","Epoch: 01 [13162/16204 ( 81%)], Train Loss: 0.27907\n","Epoch: 01 [13182/16204 ( 81%)], Train Loss: 0.27935\n","Epoch: 01 [13202/16204 ( 81%)], Train Loss: 0.27944\n","Epoch: 01 [13222/16204 ( 82%)], Train Loss: 0.27913\n","Epoch: 01 [13242/16204 ( 82%)], Train Loss: 0.27930\n","Epoch: 01 [13262/16204 ( 82%)], Train Loss: 0.27958\n","Epoch: 01 [13282/16204 ( 82%)], Train Loss: 0.27961\n","Epoch: 01 [13302/16204 ( 82%)], Train Loss: 0.27949\n","Epoch: 01 [13322/16204 ( 82%)], Train Loss: 0.27950\n","Epoch: 01 [13342/16204 ( 82%)], Train Loss: 0.27953\n","Epoch: 01 [13362/16204 ( 82%)], Train Loss: 0.27983\n","Epoch: 01 [13382/16204 ( 83%)], Train Loss: 0.27958\n","Epoch: 01 [13402/16204 ( 83%)], Train Loss: 0.27945\n","Epoch: 01 [13422/16204 ( 83%)], Train Loss: 0.27925\n","Epoch: 01 [13442/16204 ( 83%)], Train Loss: 0.27916\n","Epoch: 01 [13462/16204 ( 83%)], Train Loss: 0.27902\n","Epoch: 01 [13482/16204 ( 83%)], Train Loss: 0.27874\n","Epoch: 01 [13502/16204 ( 83%)], Train Loss: 0.27853\n","Epoch: 01 [13522/16204 ( 83%)], Train Loss: 0.27835\n","Epoch: 01 [13542/16204 ( 84%)], Train Loss: 0.27809\n","Epoch: 01 [13562/16204 ( 84%)], Train Loss: 0.27811\n","Epoch: 01 [13582/16204 ( 84%)], Train Loss: 0.27816\n","Epoch: 01 [13602/16204 ( 84%)], Train Loss: 0.27816\n","Epoch: 01 [13622/16204 ( 84%)], Train Loss: 0.27813\n","Epoch: 01 [13642/16204 ( 84%)], Train Loss: 0.27776\n","Epoch: 01 [13662/16204 ( 84%)], Train Loss: 0.27756\n","Epoch: 01 [13682/16204 ( 84%)], Train Loss: 0.27737\n","Epoch: 01 [13702/16204 ( 85%)], Train Loss: 0.27756\n","Epoch: 01 [13722/16204 ( 85%)], Train Loss: 0.27793\n","Epoch: 01 [13742/16204 ( 85%)], Train Loss: 0.27788\n","Epoch: 01 [13762/16204 ( 85%)], Train Loss: 0.27792\n","Epoch: 01 [13782/16204 ( 85%)], Train Loss: 0.27787\n","Epoch: 01 [13802/16204 ( 85%)], Train Loss: 0.27837\n","Epoch: 01 [13822/16204 ( 85%)], Train Loss: 0.27880\n","Epoch: 01 [13842/16204 ( 85%)], Train Loss: 0.27881\n","Epoch: 01 [13862/16204 ( 86%)], Train Loss: 0.27883\n","Epoch: 01 [13882/16204 ( 86%)], Train Loss: 0.27876\n","Epoch: 01 [13902/16204 ( 86%)], Train Loss: 0.27920\n","Epoch: 01 [13922/16204 ( 86%)], Train Loss: 0.27909\n","Epoch: 01 [13942/16204 ( 86%)], Train Loss: 0.27905\n","Epoch: 01 [13962/16204 ( 86%)], Train Loss: 0.27873\n","Epoch: 01 [13982/16204 ( 86%)], Train Loss: 0.27855\n","Epoch: 01 [14002/16204 ( 86%)], Train Loss: 0.27841\n","Epoch: 01 [14022/16204 ( 87%)], Train Loss: 0.27835\n","Epoch: 01 [14042/16204 ( 87%)], Train Loss: 0.27821\n","Epoch: 01 [14062/16204 ( 87%)], Train Loss: 0.27819\n","Epoch: 01 [14082/16204 ( 87%)], Train Loss: 0.27823\n","Epoch: 01 [14102/16204 ( 87%)], Train Loss: 0.27805\n","Epoch: 01 [14122/16204 ( 87%)], Train Loss: 0.27801\n","Epoch: 01 [14142/16204 ( 87%)], Train Loss: 0.27798\n","Epoch: 01 [14162/16204 ( 87%)], Train Loss: 0.27807\n","Epoch: 01 [14182/16204 ( 88%)], Train Loss: 0.27780\n","Epoch: 01 [14202/16204 ( 88%)], Train Loss: 0.27772\n","Epoch: 01 [14222/16204 ( 88%)], Train Loss: 0.27764\n","Epoch: 01 [14242/16204 ( 88%)], Train Loss: 0.27740\n","Epoch: 01 [14262/16204 ( 88%)], Train Loss: 0.27730\n","Epoch: 01 [14282/16204 ( 88%)], Train Loss: 0.27708\n","Epoch: 01 [14302/16204 ( 88%)], Train Loss: 0.27686\n","Epoch: 01 [14322/16204 ( 88%)], Train Loss: 0.27684\n","Epoch: 01 [14342/16204 ( 89%)], Train Loss: 0.27650\n","Epoch: 01 [14362/16204 ( 89%)], Train Loss: 0.27633\n","Epoch: 01 [14382/16204 ( 89%)], Train Loss: 0.27605\n","Epoch: 01 [14402/16204 ( 89%)], Train Loss: 0.27589\n","Epoch: 01 [14422/16204 ( 89%)], Train Loss: 0.27561\n","Epoch: 01 [14442/16204 ( 89%)], Train Loss: 0.27543\n","Epoch: 01 [14462/16204 ( 89%)], Train Loss: 0.27544\n","Epoch: 01 [14482/16204 ( 89%)], Train Loss: 0.27531\n","Epoch: 01 [14502/16204 ( 89%)], Train Loss: 0.27509\n","Epoch: 01 [14522/16204 ( 90%)], Train Loss: 0.27514\n","Epoch: 01 [14542/16204 ( 90%)], Train Loss: 0.27531\n","Epoch: 01 [14562/16204 ( 90%)], Train Loss: 0.27513\n","Epoch: 01 [14582/16204 ( 90%)], Train Loss: 0.27491\n","Epoch: 01 [14602/16204 ( 90%)], Train Loss: 0.27474\n","Epoch: 01 [14622/16204 ( 90%)], Train Loss: 0.27474\n","Epoch: 01 [14642/16204 ( 90%)], Train Loss: 0.27458\n","Epoch: 01 [14662/16204 ( 90%)], Train Loss: 0.27438\n","Epoch: 01 [14682/16204 ( 91%)], Train Loss: 0.27430\n","Epoch: 01 [14702/16204 ( 91%)], Train Loss: 0.27417\n","Epoch: 01 [14722/16204 ( 91%)], Train Loss: 0.27388\n","Epoch: 01 [14742/16204 ( 91%)], Train Loss: 0.27410\n","Epoch: 01 [14762/16204 ( 91%)], Train Loss: 0.27390\n","Epoch: 01 [14782/16204 ( 91%)], Train Loss: 0.27369\n","Epoch: 01 [14802/16204 ( 91%)], Train Loss: 0.27344\n","Epoch: 01 [14822/16204 ( 91%)], Train Loss: 0.27320\n","Epoch: 01 [14842/16204 ( 92%)], Train Loss: 0.27299\n","Epoch: 01 [14862/16204 ( 92%)], Train Loss: 0.27276\n","Epoch: 01 [14882/16204 ( 92%)], Train Loss: 0.27259\n","Epoch: 01 [14902/16204 ( 92%)], Train Loss: 0.27237\n","Epoch: 01 [14922/16204 ( 92%)], Train Loss: 0.27218\n","Epoch: 01 [14942/16204 ( 92%)], Train Loss: 0.27209\n","Epoch: 01 [14962/16204 ( 92%)], Train Loss: 0.27193\n","Epoch: 01 [14982/16204 ( 92%)], Train Loss: 0.27213\n","Epoch: 01 [15002/16204 ( 93%)], Train Loss: 0.27199\n","Epoch: 01 [15022/16204 ( 93%)], Train Loss: 0.27190\n","Epoch: 01 [15042/16204 ( 93%)], Train Loss: 0.27181\n","Epoch: 01 [15062/16204 ( 93%)], Train Loss: 0.27184\n","Epoch: 01 [15082/16204 ( 93%)], Train Loss: 0.27174\n","Epoch: 01 [15102/16204 ( 93%)], Train Loss: 0.27168\n","Epoch: 01 [15122/16204 ( 93%)], Train Loss: 0.27144\n","Epoch: 01 [15142/16204 ( 93%)], Train Loss: 0.27160\n","Epoch: 01 [15162/16204 ( 94%)], Train Loss: 0.27151\n","Epoch: 01 [15182/16204 ( 94%)], Train Loss: 0.27137\n","Epoch: 01 [15202/16204 ( 94%)], Train Loss: 0.27108\n","Epoch: 01 [15222/16204 ( 94%)], Train Loss: 0.27082\n","Epoch: 01 [15242/16204 ( 94%)], Train Loss: 0.27072\n","Epoch: 01 [15262/16204 ( 94%)], Train Loss: 0.27076\n","Epoch: 01 [15282/16204 ( 94%)], Train Loss: 0.27051\n","Epoch: 01 [15302/16204 ( 94%)], Train Loss: 0.27043\n","Epoch: 01 [15322/16204 ( 95%)], Train Loss: 0.27016\n","Epoch: 01 [15342/16204 ( 95%)], Train Loss: 0.27017\n","Epoch: 01 [15362/16204 ( 95%)], Train Loss: 0.27021\n","Epoch: 01 [15382/16204 ( 95%)], Train Loss: 0.27003\n","Epoch: 01 [15402/16204 ( 95%)], Train Loss: 0.26999\n","Epoch: 01 [15422/16204 ( 95%)], Train Loss: 0.27001\n","Epoch: 01 [15442/16204 ( 95%)], Train Loss: 0.26977\n","Epoch: 01 [15462/16204 ( 95%)], Train Loss: 0.26953\n","Epoch: 01 [15482/16204 ( 96%)], Train Loss: 0.26938\n","Epoch: 01 [15502/16204 ( 96%)], Train Loss: 0.26912\n","Epoch: 01 [15522/16204 ( 96%)], Train Loss: 0.26896\n","Epoch: 01 [15542/16204 ( 96%)], Train Loss: 0.26912\n","Epoch: 01 [15562/16204 ( 96%)], Train Loss: 0.26904\n","Epoch: 01 [15582/16204 ( 96%)], Train Loss: 0.26888\n","Epoch: 01 [15602/16204 ( 96%)], Train Loss: 0.26864\n","Epoch: 01 [15622/16204 ( 96%)], Train Loss: 0.26884\n","Epoch: 01 [15642/16204 ( 97%)], Train Loss: 0.26860\n","Epoch: 01 [15662/16204 ( 97%)], Train Loss: 0.26840\n","Epoch: 01 [15682/16204 ( 97%)], Train Loss: 0.26829\n","Epoch: 01 [15702/16204 ( 97%)], Train Loss: 0.26822\n","Epoch: 01 [15722/16204 ( 97%)], Train Loss: 0.26814\n","Epoch: 01 [15742/16204 ( 97%)], Train Loss: 0.26795\n","Epoch: 01 [15762/16204 ( 97%)], Train Loss: 0.26786\n","Epoch: 01 [15782/16204 ( 97%)], Train Loss: 0.26783\n","Epoch: 01 [15802/16204 ( 98%)], Train Loss: 0.26772\n","Epoch: 01 [15822/16204 ( 98%)], Train Loss: 0.26758\n","Epoch: 01 [15842/16204 ( 98%)], Train Loss: 0.26737\n","Epoch: 01 [15862/16204 ( 98%)], Train Loss: 0.26738\n","Epoch: 01 [15882/16204 ( 98%)], Train Loss: 0.26725\n","Epoch: 01 [15902/16204 ( 98%)], Train Loss: 0.26714\n","Epoch: 01 [15922/16204 ( 98%)], Train Loss: 0.26694\n","Epoch: 01 [15942/16204 ( 98%)], Train Loss: 0.26663\n","Epoch: 01 [15962/16204 ( 99%)], Train Loss: 0.26657\n","Epoch: 01 [15982/16204 ( 99%)], Train Loss: 0.26634\n","Epoch: 01 [16002/16204 ( 99%)], Train Loss: 0.26636\n","Epoch: 01 [16022/16204 ( 99%)], Train Loss: 0.26631\n","Epoch: 01 [16042/16204 ( 99%)], Train Loss: 0.26624\n","Epoch: 01 [16062/16204 ( 99%)], Train Loss: 0.26621\n","Epoch: 01 [16082/16204 ( 99%)], Train Loss: 0.26608\n","Epoch: 01 [16102/16204 ( 99%)], Train Loss: 0.26587\n","Epoch: 01 [16122/16204 ( 99%)], Train Loss: 0.26568\n","Epoch: 01 [16142/16204 (100%)], Train Loss: 0.26557\n","Epoch: 01 [16162/16204 (100%)], Train Loss: 0.26538\n","Epoch: 01 [16182/16204 (100%)], Train Loss: 0.26523\n","Epoch: 01 [16202/16204 (100%)], Train Loss: 0.26520\n","Epoch: 01 [16204/16204 (100%)], Train Loss: 0.26517\n","----Validation Results Summary----\n","Epoch: [1] Valid Loss: 0.00000\n","\n","Epoch: 02 [    2/16204 (  0%)], Train Loss: 0.21377\n","Epoch: 02 [   22/16204 (  0%)], Train Loss: 0.20678\n","Epoch: 02 [   42/16204 (  0%)], Train Loss: 0.21120\n","Epoch: 02 [   62/16204 (  0%)], Train Loss: 0.21269\n","Epoch: 02 [   82/16204 (  1%)], Train Loss: 0.22091\n","Epoch: 02 [  102/16204 (  1%)], Train Loss: 0.18855\n","Epoch: 02 [  122/16204 (  1%)], Train Loss: 0.18534\n","Epoch: 02 [  142/16204 (  1%)], Train Loss: 0.17349\n","Epoch: 02 [  162/16204 (  1%)], Train Loss: 0.16449\n","Epoch: 02 [  182/16204 (  1%)], Train Loss: 0.18677\n","Epoch: 02 [  202/16204 (  1%)], Train Loss: 0.18816\n","Epoch: 02 [  222/16204 (  1%)], Train Loss: 0.18720\n","Epoch: 02 [  242/16204 (  1%)], Train Loss: 0.18309\n","Epoch: 02 [  262/16204 (  2%)], Train Loss: 0.19051\n","Epoch: 02 [  282/16204 (  2%)], Train Loss: 0.18415\n","Epoch: 02 [  302/16204 (  2%)], Train Loss: 0.18446\n","Epoch: 02 [  322/16204 (  2%)], Train Loss: 0.17937\n","Epoch: 02 [  342/16204 (  2%)], Train Loss: 0.17928\n","Epoch: 02 [  362/16204 (  2%)], Train Loss: 0.17920\n","Epoch: 02 [  382/16204 (  2%)], Train Loss: 0.17693\n","Epoch: 02 [  402/16204 (  2%)], Train Loss: 0.17561\n","Epoch: 02 [  422/16204 (  3%)], Train Loss: 0.17087\n","Epoch: 02 [  442/16204 (  3%)], Train Loss: 0.16986\n","Epoch: 02 [  462/16204 (  3%)], Train Loss: 0.16732\n","Epoch: 02 [  482/16204 (  3%)], Train Loss: 0.16274\n","Epoch: 02 [  502/16204 (  3%)], Train Loss: 0.16504\n","Epoch: 02 [  522/16204 (  3%)], Train Loss: 0.16781\n","Epoch: 02 [  542/16204 (  3%)], Train Loss: 0.17139\n","Epoch: 02 [  562/16204 (  3%)], Train Loss: 0.16958\n","Epoch: 02 [  582/16204 (  4%)], Train Loss: 0.16571\n","Epoch: 02 [  602/16204 (  4%)], Train Loss: 0.16513\n","Epoch: 02 [  622/16204 (  4%)], Train Loss: 0.16267\n","Epoch: 02 [  642/16204 (  4%)], Train Loss: 0.16469\n","Epoch: 02 [  662/16204 (  4%)], Train Loss: 0.16820\n","Epoch: 02 [  682/16204 (  4%)], Train Loss: 0.16907\n","Epoch: 02 [  702/16204 (  4%)], Train Loss: 0.16493\n","Epoch: 02 [  722/16204 (  4%)], Train Loss: 0.16347\n","Epoch: 02 [  742/16204 (  5%)], Train Loss: 0.16245\n","Epoch: 02 [  762/16204 (  5%)], Train Loss: 0.16214\n","Epoch: 02 [  782/16204 (  5%)], Train Loss: 0.16034\n","Epoch: 02 [  802/16204 (  5%)], Train Loss: 0.15721\n","Epoch: 02 [  822/16204 (  5%)], Train Loss: 0.15847\n","Epoch: 02 [  842/16204 (  5%)], Train Loss: 0.15982\n","Epoch: 02 [  862/16204 (  5%)], Train Loss: 0.15814\n","Epoch: 02 [  882/16204 (  5%)], Train Loss: 0.15709\n","Epoch: 02 [  902/16204 (  6%)], Train Loss: 0.15492\n","Epoch: 02 [  922/16204 (  6%)], Train Loss: 0.15904\n","Epoch: 02 [  942/16204 (  6%)], Train Loss: 0.15857\n","Epoch: 02 [  962/16204 (  6%)], Train Loss: 0.16341\n","Epoch: 02 [  982/16204 (  6%)], Train Loss: 0.16625\n","Epoch: 02 [ 1002/16204 (  6%)], Train Loss: 0.16735\n","Epoch: 02 [ 1022/16204 (  6%)], Train Loss: 0.16521\n","Epoch: 02 [ 1042/16204 (  6%)], Train Loss: 0.16340\n","Epoch: 02 [ 1062/16204 (  7%)], Train Loss: 0.16146\n","Epoch: 02 [ 1082/16204 (  7%)], Train Loss: 0.15951\n","Epoch: 02 [ 1102/16204 (  7%)], Train Loss: 0.15889\n","Epoch: 02 [ 1122/16204 (  7%)], Train Loss: 0.16077\n","Epoch: 02 [ 1142/16204 (  7%)], Train Loss: 0.16054\n","Epoch: 02 [ 1162/16204 (  7%)], Train Loss: 0.15994\n","Epoch: 02 [ 1182/16204 (  7%)], Train Loss: 0.15797\n","Epoch: 02 [ 1202/16204 (  7%)], Train Loss: 0.15776\n","Epoch: 02 [ 1222/16204 (  8%)], Train Loss: 0.15718\n","Epoch: 02 [ 1242/16204 (  8%)], Train Loss: 0.15658\n","Epoch: 02 [ 1262/16204 (  8%)], Train Loss: 0.15453\n","Epoch: 02 [ 1282/16204 (  8%)], Train Loss: 0.15705\n","Epoch: 02 [ 1302/16204 (  8%)], Train Loss: 0.15587\n","Epoch: 02 [ 1322/16204 (  8%)], Train Loss: 0.15706\n","Epoch: 02 [ 1342/16204 (  8%)], Train Loss: 0.15750\n","Epoch: 02 [ 1362/16204 (  8%)], Train Loss: 0.15575\n","Epoch: 02 [ 1382/16204 (  9%)], Train Loss: 0.15487\n","Epoch: 02 [ 1402/16204 (  9%)], Train Loss: 0.15387\n","Epoch: 02 [ 1422/16204 (  9%)], Train Loss: 0.15360\n","Epoch: 02 [ 1442/16204 (  9%)], Train Loss: 0.15266\n","Epoch: 02 [ 1462/16204 (  9%)], Train Loss: 0.15513\n","Epoch: 02 [ 1482/16204 (  9%)], Train Loss: 0.15468\n","Epoch: 02 [ 1502/16204 (  9%)], Train Loss: 0.15537\n","Epoch: 02 [ 1522/16204 (  9%)], Train Loss: 0.15467\n","Epoch: 02 [ 1542/16204 ( 10%)], Train Loss: 0.15624\n","Epoch: 02 [ 1562/16204 ( 10%)], Train Loss: 0.15509\n","Epoch: 02 [ 1582/16204 ( 10%)], Train Loss: 0.15460\n","Epoch: 02 [ 1602/16204 ( 10%)], Train Loss: 0.15540\n","Epoch: 02 [ 1622/16204 ( 10%)], Train Loss: 0.15771\n","Epoch: 02 [ 1642/16204 ( 10%)], Train Loss: 0.15747\n","Epoch: 02 [ 1662/16204 ( 10%)], Train Loss: 0.16139\n","Epoch: 02 [ 1682/16204 ( 10%)], Train Loss: 0.16180\n","Epoch: 02 [ 1702/16204 ( 11%)], Train Loss: 0.16059\n","Epoch: 02 [ 1722/16204 ( 11%)], Train Loss: 0.15989\n","Epoch: 02 [ 1742/16204 ( 11%)], Train Loss: 0.16017\n","Epoch: 02 [ 1762/16204 ( 11%)], Train Loss: 0.16121\n","Epoch: 02 [ 1782/16204 ( 11%)], Train Loss: 0.16237\n","Epoch: 02 [ 1802/16204 ( 11%)], Train Loss: 0.16249\n","Epoch: 02 [ 1822/16204 ( 11%)], Train Loss: 0.16191\n","Epoch: 02 [ 1842/16204 ( 11%)], Train Loss: 0.16134\n","Epoch: 02 [ 1862/16204 ( 11%)], Train Loss: 0.16334\n","Epoch: 02 [ 1882/16204 ( 12%)], Train Loss: 0.16543\n","Epoch: 02 [ 1902/16204 ( 12%)], Train Loss: 0.16481\n","Epoch: 02 [ 1922/16204 ( 12%)], Train Loss: 0.16625\n","Epoch: 02 [ 1942/16204 ( 12%)], Train Loss: 0.16582\n","Epoch: 02 [ 1962/16204 ( 12%)], Train Loss: 0.16591\n","Epoch: 02 [ 1982/16204 ( 12%)], Train Loss: 0.16469\n","Epoch: 02 [ 2002/16204 ( 12%)], Train Loss: 0.16505\n","Epoch: 02 [ 2022/16204 ( 12%)], Train Loss: 0.16879\n","Epoch: 02 [ 2042/16204 ( 13%)], Train Loss: 0.17117\n","Epoch: 02 [ 2062/16204 ( 13%)], Train Loss: 0.16977\n","Epoch: 02 [ 2082/16204 ( 13%)], Train Loss: 0.16964\n","Epoch: 02 [ 2102/16204 ( 13%)], Train Loss: 0.17035\n","Epoch: 02 [ 2122/16204 ( 13%)], Train Loss: 0.17023\n","Epoch: 02 [ 2142/16204 ( 13%)], Train Loss: 0.16968\n","Epoch: 02 [ 2162/16204 ( 13%)], Train Loss: 0.16902\n","Epoch: 02 [ 2182/16204 ( 13%)], Train Loss: 0.16846\n","Epoch: 02 [ 2202/16204 ( 14%)], Train Loss: 0.16804\n","Epoch: 02 [ 2222/16204 ( 14%)], Train Loss: 0.16780\n","Epoch: 02 [ 2242/16204 ( 14%)], Train Loss: 0.16783\n","Epoch: 02 [ 2262/16204 ( 14%)], Train Loss: 0.16843\n","Epoch: 02 [ 2282/16204 ( 14%)], Train Loss: 0.16845\n","Epoch: 02 [ 2302/16204 ( 14%)], Train Loss: 0.16854\n","Epoch: 02 [ 2322/16204 ( 14%)], Train Loss: 0.16751\n","Epoch: 02 [ 2342/16204 ( 14%)], Train Loss: 0.16672\n","Epoch: 02 [ 2362/16204 ( 15%)], Train Loss: 0.16799\n","Epoch: 02 [ 2382/16204 ( 15%)], Train Loss: 0.16760\n","Epoch: 02 [ 2402/16204 ( 15%)], Train Loss: 0.16781\n","Epoch: 02 [ 2422/16204 ( 15%)], Train Loss: 0.16725\n","Epoch: 02 [ 2442/16204 ( 15%)], Train Loss: 0.16803\n","Epoch: 02 [ 2462/16204 ( 15%)], Train Loss: 0.16869\n","Epoch: 02 [ 2482/16204 ( 15%)], Train Loss: 0.16862\n","Epoch: 02 [ 2502/16204 ( 15%)], Train Loss: 0.16847\n","Epoch: 02 [ 2522/16204 ( 16%)], Train Loss: 0.16733\n","Epoch: 02 [ 2542/16204 ( 16%)], Train Loss: 0.16751\n","Epoch: 02 [ 2562/16204 ( 16%)], Train Loss: 0.16730\n","Epoch: 02 [ 2582/16204 ( 16%)], Train Loss: 0.16733\n","Epoch: 02 [ 2602/16204 ( 16%)], Train Loss: 0.16686\n","Epoch: 02 [ 2622/16204 ( 16%)], Train Loss: 0.16716\n","Epoch: 02 [ 2642/16204 ( 16%)], Train Loss: 0.16732\n","Epoch: 02 [ 2662/16204 ( 16%)], Train Loss: 0.16721\n","Epoch: 02 [ 2682/16204 ( 17%)], Train Loss: 0.16678\n","Epoch: 02 [ 2702/16204 ( 17%)], Train Loss: 0.16638\n","Epoch: 02 [ 2722/16204 ( 17%)], Train Loss: 0.16573\n","Epoch: 02 [ 2742/16204 ( 17%)], Train Loss: 0.16477\n","Epoch: 02 [ 2762/16204 ( 17%)], Train Loss: 0.16444\n","Epoch: 02 [ 2782/16204 ( 17%)], Train Loss: 0.16364\n","Epoch: 02 [ 2802/16204 ( 17%)], Train Loss: 0.16351\n","Epoch: 02 [ 2822/16204 ( 17%)], Train Loss: 0.16273\n","Epoch: 02 [ 2842/16204 ( 18%)], Train Loss: 0.16243\n","Epoch: 02 [ 2862/16204 ( 18%)], Train Loss: 0.16259\n","Epoch: 02 [ 2882/16204 ( 18%)], Train Loss: 0.16281\n","Epoch: 02 [ 2902/16204 ( 18%)], Train Loss: 0.16247\n","Epoch: 02 [ 2922/16204 ( 18%)], Train Loss: 0.16180\n","Epoch: 02 [ 2942/16204 ( 18%)], Train Loss: 0.16115\n","Epoch: 02 [ 2962/16204 ( 18%)], Train Loss: 0.16064\n","Epoch: 02 [ 2982/16204 ( 18%)], Train Loss: 0.15972\n","Epoch: 02 [ 3002/16204 ( 19%)], Train Loss: 0.15926\n","Epoch: 02 [ 3022/16204 ( 19%)], Train Loss: 0.15877\n","Epoch: 02 [ 3042/16204 ( 19%)], Train Loss: 0.15961\n","Epoch: 02 [ 3062/16204 ( 19%)], Train Loss: 0.15920\n","Epoch: 02 [ 3082/16204 ( 19%)], Train Loss: 0.15920\n","Epoch: 02 [ 3102/16204 ( 19%)], Train Loss: 0.15920\n","Epoch: 02 [ 3122/16204 ( 19%)], Train Loss: 0.15843\n","Epoch: 02 [ 3142/16204 ( 19%)], Train Loss: 0.15831\n","Epoch: 02 [ 3162/16204 ( 20%)], Train Loss: 0.15812\n","Epoch: 02 [ 3182/16204 ( 20%)], Train Loss: 0.15769\n","Epoch: 02 [ 3202/16204 ( 20%)], Train Loss: 0.15688\n","Epoch: 02 [ 3222/16204 ( 20%)], Train Loss: 0.15625\n","Epoch: 02 [ 3242/16204 ( 20%)], Train Loss: 0.15544\n","Epoch: 02 [ 3262/16204 ( 20%)], Train Loss: 0.15564\n","Epoch: 02 [ 3282/16204 ( 20%)], Train Loss: 0.15496\n","Epoch: 02 [ 3302/16204 ( 20%)], Train Loss: 0.15458\n","Epoch: 02 [ 3322/16204 ( 21%)], Train Loss: 0.15488\n","Epoch: 02 [ 3342/16204 ( 21%)], Train Loss: 0.15408\n","Epoch: 02 [ 3362/16204 ( 21%)], Train Loss: 0.15397\n","Epoch: 02 [ 3382/16204 ( 21%)], Train Loss: 0.15352\n","Epoch: 02 [ 3402/16204 ( 21%)], Train Loss: 0.15290\n","Epoch: 02 [ 3422/16204 ( 21%)], Train Loss: 0.15254\n","Epoch: 02 [ 3442/16204 ( 21%)], Train Loss: 0.15258\n","Epoch: 02 [ 3462/16204 ( 21%)], Train Loss: 0.15319\n","Epoch: 02 [ 3482/16204 ( 21%)], Train Loss: 0.15269\n","Epoch: 02 [ 3502/16204 ( 22%)], Train Loss: 0.15240\n","Epoch: 02 [ 3522/16204 ( 22%)], Train Loss: 0.15192\n","Epoch: 02 [ 3542/16204 ( 22%)], Train Loss: 0.15244\n","Epoch: 02 [ 3562/16204 ( 22%)], Train Loss: 0.15218\n","Epoch: 02 [ 3582/16204 ( 22%)], Train Loss: 0.15360\n","Epoch: 02 [ 3602/16204 ( 22%)], Train Loss: 0.15334\n","Epoch: 02 [ 3622/16204 ( 22%)], Train Loss: 0.15391\n","Epoch: 02 [ 3642/16204 ( 22%)], Train Loss: 0.15464\n","Epoch: 02 [ 3662/16204 ( 23%)], Train Loss: 0.15436\n","Epoch: 02 [ 3682/16204 ( 23%)], Train Loss: 0.15425\n","Epoch: 02 [ 3702/16204 ( 23%)], Train Loss: 0.15383\n","Epoch: 02 [ 3722/16204 ( 23%)], Train Loss: 0.15321\n","Epoch: 02 [ 3742/16204 ( 23%)], Train Loss: 0.15280\n","Epoch: 02 [ 3762/16204 ( 23%)], Train Loss: 0.15282\n","Epoch: 02 [ 3782/16204 ( 23%)], Train Loss: 0.15300\n","Epoch: 02 [ 3802/16204 ( 23%)], Train Loss: 0.15257\n","Epoch: 02 [ 3822/16204 ( 24%)], Train Loss: 0.15281\n","Epoch: 02 [ 3842/16204 ( 24%)], Train Loss: 0.15283\n","Epoch: 02 [ 3862/16204 ( 24%)], Train Loss: 0.15350\n","Epoch: 02 [ 3882/16204 ( 24%)], Train Loss: 0.15319\n","Epoch: 02 [ 3902/16204 ( 24%)], Train Loss: 0.15337\n","Epoch: 02 [ 3922/16204 ( 24%)], Train Loss: 0.15412\n","Epoch: 02 [ 3942/16204 ( 24%)], Train Loss: 0.15399\n","Epoch: 02 [ 3962/16204 ( 24%)], Train Loss: 0.15411\n","Epoch: 02 [ 3982/16204 ( 25%)], Train Loss: 0.15380\n","Epoch: 02 [ 4002/16204 ( 25%)], Train Loss: 0.15401\n","Epoch: 02 [ 4022/16204 ( 25%)], Train Loss: 0.15417\n","Epoch: 02 [ 4042/16204 ( 25%)], Train Loss: 0.15386\n","Epoch: 02 [ 4062/16204 ( 25%)], Train Loss: 0.15322\n","Epoch: 02 [ 4082/16204 ( 25%)], Train Loss: 0.15408\n","Epoch: 02 [ 4102/16204 ( 25%)], Train Loss: 0.15411\n","Epoch: 02 [ 4122/16204 ( 25%)], Train Loss: 0.15400\n","Epoch: 02 [ 4142/16204 ( 26%)], Train Loss: 0.15411\n","Epoch: 02 [ 4162/16204 ( 26%)], Train Loss: 0.15349\n","Epoch: 02 [ 4182/16204 ( 26%)], Train Loss: 0.15320\n","Epoch: 02 [ 4202/16204 ( 26%)], Train Loss: 0.15340\n","Epoch: 02 [ 4222/16204 ( 26%)], Train Loss: 0.15386\n","Epoch: 02 [ 4242/16204 ( 26%)], Train Loss: 0.15360\n","Epoch: 02 [ 4262/16204 ( 26%)], Train Loss: 0.15334\n","Epoch: 02 [ 4282/16204 ( 26%)], Train Loss: 0.15292\n","Epoch: 02 [ 4302/16204 ( 27%)], Train Loss: 0.15308\n","Epoch: 02 [ 4322/16204 ( 27%)], Train Loss: 0.15312\n","Epoch: 02 [ 4342/16204 ( 27%)], Train Loss: 0.15256\n","Epoch: 02 [ 4362/16204 ( 27%)], Train Loss: 0.15232\n","Epoch: 02 [ 4382/16204 ( 27%)], Train Loss: 0.15203\n","Epoch: 02 [ 4402/16204 ( 27%)], Train Loss: 0.15171\n","Epoch: 02 [ 4422/16204 ( 27%)], Train Loss: 0.15125\n","Epoch: 02 [ 4442/16204 ( 27%)], Train Loss: 0.15227\n","Epoch: 02 [ 4462/16204 ( 28%)], Train Loss: 0.15325\n","Epoch: 02 [ 4482/16204 ( 28%)], Train Loss: 0.15345\n","Epoch: 02 [ 4502/16204 ( 28%)], Train Loss: 0.15370\n","Epoch: 02 [ 4522/16204 ( 28%)], Train Loss: 0.15360\n","Epoch: 02 [ 4542/16204 ( 28%)], Train Loss: 0.15379\n","Epoch: 02 [ 4562/16204 ( 28%)], Train Loss: 0.15349\n","Epoch: 02 [ 4582/16204 ( 28%)], Train Loss: 0.15407\n","Epoch: 02 [ 4602/16204 ( 28%)], Train Loss: 0.15379\n","Epoch: 02 [ 4622/16204 ( 29%)], Train Loss: 0.15381\n","Epoch: 02 [ 4642/16204 ( 29%)], Train Loss: 0.15372\n","Epoch: 02 [ 4662/16204 ( 29%)], Train Loss: 0.15475\n","Epoch: 02 [ 4682/16204 ( 29%)], Train Loss: 0.15473\n","Epoch: 02 [ 4702/16204 ( 29%)], Train Loss: 0.15427\n","Epoch: 02 [ 4722/16204 ( 29%)], Train Loss: 0.15384\n","Epoch: 02 [ 4742/16204 ( 29%)], Train Loss: 0.15401\n","Epoch: 02 [ 4762/16204 ( 29%)], Train Loss: 0.15371\n","Epoch: 02 [ 4782/16204 ( 30%)], Train Loss: 0.15361\n","Epoch: 02 [ 4802/16204 ( 30%)], Train Loss: 0.15382\n","Epoch: 02 [ 4822/16204 ( 30%)], Train Loss: 0.15388\n","Epoch: 02 [ 4842/16204 ( 30%)], Train Loss: 0.15347\n","Epoch: 02 [ 4862/16204 ( 30%)], Train Loss: 0.15363\n","Epoch: 02 [ 4882/16204 ( 30%)], Train Loss: 0.15362\n","Epoch: 02 [ 4902/16204 ( 30%)], Train Loss: 0.15352\n","Epoch: 02 [ 4922/16204 ( 30%)], Train Loss: 0.15304\n","Epoch: 02 [ 4942/16204 ( 30%)], Train Loss: 0.15251\n","Epoch: 02 [ 4962/16204 ( 31%)], Train Loss: 0.15234\n","Epoch: 02 [ 4982/16204 ( 31%)], Train Loss: 0.15202\n","Epoch: 02 [ 5002/16204 ( 31%)], Train Loss: 0.15166\n","Epoch: 02 [ 5022/16204 ( 31%)], Train Loss: 0.15145\n","Epoch: 02 [ 5042/16204 ( 31%)], Train Loss: 0.15142\n","Epoch: 02 [ 5062/16204 ( 31%)], Train Loss: 0.15130\n","Epoch: 02 [ 5082/16204 ( 31%)], Train Loss: 0.15081\n","Epoch: 02 [ 5102/16204 ( 31%)], Train Loss: 0.15053\n","Epoch: 02 [ 5122/16204 ( 32%)], Train Loss: 0.15067\n","Epoch: 02 [ 5142/16204 ( 32%)], Train Loss: 0.15056\n","Epoch: 02 [ 5162/16204 ( 32%)], Train Loss: 0.15063\n","Epoch: 02 [ 5182/16204 ( 32%)], Train Loss: 0.15046\n","Epoch: 02 [ 5202/16204 ( 32%)], Train Loss: 0.15024\n","Epoch: 02 [ 5222/16204 ( 32%)], Train Loss: 0.15026\n","Epoch: 02 [ 5242/16204 ( 32%)], Train Loss: 0.15006\n","Epoch: 02 [ 5262/16204 ( 32%)], Train Loss: 0.15019\n","Epoch: 02 [ 5282/16204 ( 33%)], Train Loss: 0.14984\n","Epoch: 02 [ 5302/16204 ( 33%)], Train Loss: 0.14948\n","Epoch: 02 [ 5322/16204 ( 33%)], Train Loss: 0.14928\n","Epoch: 02 [ 5342/16204 ( 33%)], Train Loss: 0.14898\n","Epoch: 02 [ 5362/16204 ( 33%)], Train Loss: 0.14854\n","Epoch: 02 [ 5382/16204 ( 33%)], Train Loss: 0.14933\n","Epoch: 02 [ 5402/16204 ( 33%)], Train Loss: 0.14918\n","Epoch: 02 [ 5422/16204 ( 33%)], Train Loss: 0.14995\n","Epoch: 02 [ 5442/16204 ( 34%)], Train Loss: 0.15030\n","Epoch: 02 [ 5462/16204 ( 34%)], Train Loss: 0.15064\n","Epoch: 02 [ 5482/16204 ( 34%)], Train Loss: 0.15036\n","Epoch: 02 [ 5502/16204 ( 34%)], Train Loss: 0.15001\n","Epoch: 02 [ 5522/16204 ( 34%)], Train Loss: 0.14980\n","Epoch: 02 [ 5542/16204 ( 34%)], Train Loss: 0.14945\n","Epoch: 02 [ 5562/16204 ( 34%)], Train Loss: 0.14910\n","Epoch: 02 [ 5582/16204 ( 34%)], Train Loss: 0.14906\n","Epoch: 02 [ 5602/16204 ( 35%)], Train Loss: 0.14878\n","Epoch: 02 [ 5622/16204 ( 35%)], Train Loss: 0.14833\n","Epoch: 02 [ 5642/16204 ( 35%)], Train Loss: 0.14803\n","Epoch: 02 [ 5662/16204 ( 35%)], Train Loss: 0.14782\n","Epoch: 02 [ 5682/16204 ( 35%)], Train Loss: 0.14770\n","Epoch: 02 [ 5702/16204 ( 35%)], Train Loss: 0.14735\n","Epoch: 02 [ 5722/16204 ( 35%)], Train Loss: 0.14710\n","Epoch: 02 [ 5742/16204 ( 35%)], Train Loss: 0.14670\n","Epoch: 02 [ 5762/16204 ( 36%)], Train Loss: 0.14633\n","Epoch: 02 [ 5782/16204 ( 36%)], Train Loss: 0.14601\n","Epoch: 02 [ 5802/16204 ( 36%)], Train Loss: 0.14568\n","Epoch: 02 [ 5822/16204 ( 36%)], Train Loss: 0.14577\n","Epoch: 02 [ 5842/16204 ( 36%)], Train Loss: 0.14556\n","Epoch: 02 [ 5862/16204 ( 36%)], Train Loss: 0.14510\n","Epoch: 02 [ 5882/16204 ( 36%)], Train Loss: 0.14484\n","Epoch: 02 [ 5902/16204 ( 36%)], Train Loss: 0.14463\n","Epoch: 02 [ 5922/16204 ( 37%)], Train Loss: 0.14437\n","Epoch: 02 [ 5942/16204 ( 37%)], Train Loss: 0.14411\n","Epoch: 02 [ 5962/16204 ( 37%)], Train Loss: 0.14377\n","Epoch: 02 [ 5982/16204 ( 37%)], Train Loss: 0.14341\n","Epoch: 02 [ 6002/16204 ( 37%)], Train Loss: 0.14333\n","Epoch: 02 [ 6022/16204 ( 37%)], Train Loss: 0.14332\n","Epoch: 02 [ 6042/16204 ( 37%)], Train Loss: 0.14291\n","Epoch: 02 [ 6062/16204 ( 37%)], Train Loss: 0.14263\n","Epoch: 02 [ 6082/16204 ( 38%)], Train Loss: 0.14231\n","Epoch: 02 [ 6102/16204 ( 38%)], Train Loss: 0.14188\n","Epoch: 02 [ 6122/16204 ( 38%)], Train Loss: 0.14197\n","Epoch: 02 [ 6142/16204 ( 38%)], Train Loss: 0.14170\n","Epoch: 02 [ 6162/16204 ( 38%)], Train Loss: 0.14194\n","Epoch: 02 [ 6182/16204 ( 38%)], Train Loss: 0.14155\n","Epoch: 02 [ 6202/16204 ( 38%)], Train Loss: 0.14139\n","Epoch: 02 [ 6222/16204 ( 38%)], Train Loss: 0.14115\n","Epoch: 02 [ 6242/16204 ( 39%)], Train Loss: 0.14117\n","Epoch: 02 [ 6262/16204 ( 39%)], Train Loss: 0.14105\n","Epoch: 02 [ 6282/16204 ( 39%)], Train Loss: 0.14101\n","Epoch: 02 [ 6302/16204 ( 39%)], Train Loss: 0.14073\n","Epoch: 02 [ 6322/16204 ( 39%)], Train Loss: 0.14077\n","Epoch: 02 [ 6342/16204 ( 39%)], Train Loss: 0.14071\n","Epoch: 02 [ 6362/16204 ( 39%)], Train Loss: 0.14048\n","Epoch: 02 [ 6382/16204 ( 39%)], Train Loss: 0.14068\n","Epoch: 02 [ 6402/16204 ( 40%)], Train Loss: 0.14130\n","Epoch: 02 [ 6422/16204 ( 40%)], Train Loss: 0.14102\n","Epoch: 02 [ 6442/16204 ( 40%)], Train Loss: 0.14089\n","Epoch: 02 [ 6462/16204 ( 40%)], Train Loss: 0.14073\n","Epoch: 02 [ 6482/16204 ( 40%)], Train Loss: 0.14052\n","Epoch: 02 [ 6502/16204 ( 40%)], Train Loss: 0.14051\n","Epoch: 02 [ 6522/16204 ( 40%)], Train Loss: 0.14022\n","Epoch: 02 [ 6542/16204 ( 40%)], Train Loss: 0.14009\n","Epoch: 02 [ 6562/16204 ( 40%)], Train Loss: 0.13989\n","Epoch: 02 [ 6582/16204 ( 41%)], Train Loss: 0.13948\n","Epoch: 02 [ 6602/16204 ( 41%)], Train Loss: 0.13952\n","Epoch: 02 [ 6622/16204 ( 41%)], Train Loss: 0.13948\n","Epoch: 02 [ 6642/16204 ( 41%)], Train Loss: 0.13927\n","Epoch: 02 [ 6662/16204 ( 41%)], Train Loss: 0.13909\n","Epoch: 02 [ 6682/16204 ( 41%)], Train Loss: 0.13893\n","Epoch: 02 [ 6702/16204 ( 41%)], Train Loss: 0.13988\n","Epoch: 02 [ 6722/16204 ( 41%)], Train Loss: 0.13962\n","Epoch: 02 [ 6742/16204 ( 42%)], Train Loss: 0.13931\n","Epoch: 02 [ 6762/16204 ( 42%)], Train Loss: 0.13912\n","Epoch: 02 [ 6782/16204 ( 42%)], Train Loss: 0.13880\n","Epoch: 02 [ 6802/16204 ( 42%)], Train Loss: 0.13890\n","Epoch: 02 [ 6822/16204 ( 42%)], Train Loss: 0.13885\n","Epoch: 02 [ 6842/16204 ( 42%)], Train Loss: 0.13861\n","Epoch: 02 [ 6862/16204 ( 42%)], Train Loss: 0.13898\n","Epoch: 02 [ 6882/16204 ( 42%)], Train Loss: 0.13859\n","Epoch: 02 [ 6902/16204 ( 43%)], Train Loss: 0.13831\n","Epoch: 02 [ 6922/16204 ( 43%)], Train Loss: 0.13798\n","Epoch: 02 [ 6942/16204 ( 43%)], Train Loss: 0.13779\n","Epoch: 02 [ 6962/16204 ( 43%)], Train Loss: 0.13766\n","Epoch: 02 [ 6982/16204 ( 43%)], Train Loss: 0.13773\n","Epoch: 02 [ 7002/16204 ( 43%)], Train Loss: 0.13753\n","Epoch: 02 [ 7022/16204 ( 43%)], Train Loss: 0.13744\n","Epoch: 02 [ 7042/16204 ( 43%)], Train Loss: 0.13708\n","Epoch: 02 [ 7062/16204 ( 44%)], Train Loss: 0.13694\n","Epoch: 02 [ 7082/16204 ( 44%)], Train Loss: 0.13666\n","Epoch: 02 [ 7102/16204 ( 44%)], Train Loss: 0.13651\n","Epoch: 02 [ 7122/16204 ( 44%)], Train Loss: 0.13655\n","Epoch: 02 [ 7142/16204 ( 44%)], Train Loss: 0.13624\n","Epoch: 02 [ 7162/16204 ( 44%)], Train Loss: 0.13622\n","Epoch: 02 [ 7182/16204 ( 44%)], Train Loss: 0.13594\n","Epoch: 02 [ 7202/16204 ( 44%)], Train Loss: 0.13583\n","Epoch: 02 [ 7222/16204 ( 45%)], Train Loss: 0.13559\n","Epoch: 02 [ 7242/16204 ( 45%)], Train Loss: 0.13527\n","Epoch: 02 [ 7262/16204 ( 45%)], Train Loss: 0.13513\n","Epoch: 02 [ 7282/16204 ( 45%)], Train Loss: 0.13507\n","Epoch: 02 [ 7302/16204 ( 45%)], Train Loss: 0.13522\n","Epoch: 02 [ 7322/16204 ( 45%)], Train Loss: 0.13548\n","Epoch: 02 [ 7342/16204 ( 45%)], Train Loss: 0.13532\n","Epoch: 02 [ 7362/16204 ( 45%)], Train Loss: 0.13517\n","Epoch: 02 [ 7382/16204 ( 46%)], Train Loss: 0.13500\n","Epoch: 02 [ 7402/16204 ( 46%)], Train Loss: 0.13478\n","Epoch: 02 [ 7422/16204 ( 46%)], Train Loss: 0.13481\n","Epoch: 02 [ 7442/16204 ( 46%)], Train Loss: 0.13496\n","Epoch: 02 [ 7462/16204 ( 46%)], Train Loss: 0.13484\n","Epoch: 02 [ 7482/16204 ( 46%)], Train Loss: 0.13459\n","Epoch: 02 [ 7502/16204 ( 46%)], Train Loss: 0.13454\n","Epoch: 02 [ 7522/16204 ( 46%)], Train Loss: 0.13460\n","Epoch: 02 [ 7542/16204 ( 47%)], Train Loss: 0.13446\n","Epoch: 02 [ 7562/16204 ( 47%)], Train Loss: 0.13432\n","Epoch: 02 [ 7582/16204 ( 47%)], Train Loss: 0.13413\n","Epoch: 02 [ 7602/16204 ( 47%)], Train Loss: 0.13390\n","Epoch: 02 [ 7622/16204 ( 47%)], Train Loss: 0.13380\n","Epoch: 02 [ 7642/16204 ( 47%)], Train Loss: 0.13369\n","Epoch: 02 [ 7662/16204 ( 47%)], Train Loss: 0.13341\n","Epoch: 02 [ 7682/16204 ( 47%)], Train Loss: 0.13342\n","Epoch: 02 [ 7702/16204 ( 48%)], Train Loss: 0.13338\n","Epoch: 02 [ 7722/16204 ( 48%)], Train Loss: 0.13327\n","Epoch: 02 [ 7742/16204 ( 48%)], Train Loss: 0.13323\n","Epoch: 02 [ 7762/16204 ( 48%)], Train Loss: 0.13303\n","Epoch: 02 [ 7782/16204 ( 48%)], Train Loss: 0.13271\n","Epoch: 02 [ 7802/16204 ( 48%)], Train Loss: 0.13245\n","Epoch: 02 [ 7822/16204 ( 48%)], Train Loss: 0.13247\n","Epoch: 02 [ 7842/16204 ( 48%)], Train Loss: 0.13297\n","Epoch: 02 [ 7862/16204 ( 49%)], Train Loss: 0.13277\n","Epoch: 02 [ 7882/16204 ( 49%)], Train Loss: 0.13329\n","Epoch: 02 [ 7902/16204 ( 49%)], Train Loss: 0.13308\n","Epoch: 02 [ 7922/16204 ( 49%)], Train Loss: 0.13343\n","Epoch: 02 [ 7942/16204 ( 49%)], Train Loss: 0.13316\n","Epoch: 02 [ 7962/16204 ( 49%)], Train Loss: 0.13332\n","Epoch: 02 [ 7982/16204 ( 49%)], Train Loss: 0.13327\n","Epoch: 02 [ 8002/16204 ( 49%)], Train Loss: 0.13304\n","Epoch: 02 [ 8022/16204 ( 50%)], Train Loss: 0.13313\n","Epoch: 02 [ 8042/16204 ( 50%)], Train Loss: 0.13287\n","Epoch: 02 [ 8062/16204 ( 50%)], Train Loss: 0.13298\n","Epoch: 02 [ 8082/16204 ( 50%)], Train Loss: 0.13280\n","Epoch: 02 [ 8102/16204 ( 50%)], Train Loss: 0.13257\n","Epoch: 02 [ 8122/16204 ( 50%)], Train Loss: 0.13235\n","Epoch: 02 [ 8142/16204 ( 50%)], Train Loss: 0.13205\n","Epoch: 02 [ 8162/16204 ( 50%)], Train Loss: 0.13214\n","Epoch: 02 [ 8182/16204 ( 50%)], Train Loss: 0.13191\n","Epoch: 02 [ 8202/16204 ( 51%)], Train Loss: 0.13187\n","Epoch: 02 [ 8222/16204 ( 51%)], Train Loss: 0.13159\n","Epoch: 02 [ 8242/16204 ( 51%)], Train Loss: 0.13132\n","Epoch: 02 [ 8262/16204 ( 51%)], Train Loss: 0.13159\n","Epoch: 02 [ 8282/16204 ( 51%)], Train Loss: 0.13134\n","Epoch: 02 [ 8302/16204 ( 51%)], Train Loss: 0.13110\n","Epoch: 02 [ 8322/16204 ( 51%)], Train Loss: 0.13091\n","Epoch: 02 [ 8342/16204 ( 51%)], Train Loss: 0.13067\n","Epoch: 02 [ 8362/16204 ( 52%)], Train Loss: 0.13042\n","Epoch: 02 [ 8382/16204 ( 52%)], Train Loss: 0.13034\n","Epoch: 02 [ 8402/16204 ( 52%)], Train Loss: 0.13020\n","Epoch: 02 [ 8422/16204 ( 52%)], Train Loss: 0.13001\n","Epoch: 02 [ 8442/16204 ( 52%)], Train Loss: 0.12991\n","Epoch: 02 [ 8462/16204 ( 52%)], Train Loss: 0.12975\n","Epoch: 02 [ 8482/16204 ( 52%)], Train Loss: 0.12950\n","Epoch: 02 [ 8502/16204 ( 52%)], Train Loss: 0.12963\n","Epoch: 02 [ 8522/16204 ( 53%)], Train Loss: 0.12940\n","Epoch: 02 [ 8542/16204 ( 53%)], Train Loss: 0.12967\n","Epoch: 02 [ 8562/16204 ( 53%)], Train Loss: 0.12949\n","Epoch: 02 [ 8582/16204 ( 53%)], Train Loss: 0.12951\n","Epoch: 02 [ 8602/16204 ( 53%)], Train Loss: 0.12929\n","Epoch: 02 [ 8622/16204 ( 53%)], Train Loss: 0.12917\n","Epoch: 02 [ 8642/16204 ( 53%)], Train Loss: 0.12893\n","Epoch: 02 [ 8662/16204 ( 53%)], Train Loss: 0.12874\n","Epoch: 02 [ 8682/16204 ( 54%)], Train Loss: 0.12852\n","Epoch: 02 [ 8702/16204 ( 54%)], Train Loss: 0.12835\n","Epoch: 02 [ 8722/16204 ( 54%)], Train Loss: 0.12859\n","Epoch: 02 [ 8742/16204 ( 54%)], Train Loss: 0.12833\n","Epoch: 02 [ 8762/16204 ( 54%)], Train Loss: 0.12867\n","Epoch: 02 [ 8782/16204 ( 54%)], Train Loss: 0.12846\n","Epoch: 02 [ 8802/16204 ( 54%)], Train Loss: 0.12840\n","Epoch: 02 [ 8822/16204 ( 54%)], Train Loss: 0.12839\n","Epoch: 02 [ 8842/16204 ( 55%)], Train Loss: 0.12821\n","Epoch: 02 [ 8862/16204 ( 55%)], Train Loss: 0.12827\n","Epoch: 02 [ 8882/16204 ( 55%)], Train Loss: 0.12846\n","Epoch: 02 [ 8902/16204 ( 55%)], Train Loss: 0.12822\n","Epoch: 02 [ 8922/16204 ( 55%)], Train Loss: 0.12810\n","Epoch: 02 [ 8942/16204 ( 55%)], Train Loss: 0.12818\n","Epoch: 02 [ 8962/16204 ( 55%)], Train Loss: 0.12798\n","Epoch: 02 [ 8982/16204 ( 55%)], Train Loss: 0.12850\n","Epoch: 02 [ 9002/16204 ( 56%)], Train Loss: 0.12840\n","Epoch: 02 [ 9022/16204 ( 56%)], Train Loss: 0.12826\n","Epoch: 02 [ 9042/16204 ( 56%)], Train Loss: 0.12842\n","Epoch: 02 [ 9062/16204 ( 56%)], Train Loss: 0.12821\n","Epoch: 02 [ 9082/16204 ( 56%)], Train Loss: 0.12809\n","Epoch: 02 [ 9102/16204 ( 56%)], Train Loss: 0.12800\n","Epoch: 02 [ 9122/16204 ( 56%)], Train Loss: 0.12781\n","Epoch: 02 [ 9142/16204 ( 56%)], Train Loss: 0.12766\n","Epoch: 02 [ 9162/16204 ( 57%)], Train Loss: 0.12781\n","Epoch: 02 [ 9182/16204 ( 57%)], Train Loss: 0.12765\n","Epoch: 02 [ 9202/16204 ( 57%)], Train Loss: 0.12741\n","Epoch: 02 [ 9222/16204 ( 57%)], Train Loss: 0.12733\n","Epoch: 02 [ 9242/16204 ( 57%)], Train Loss: 0.12712\n","Epoch: 02 [ 9262/16204 ( 57%)], Train Loss: 0.12694\n","Epoch: 02 [ 9282/16204 ( 57%)], Train Loss: 0.12669\n","Epoch: 02 [ 9302/16204 ( 57%)], Train Loss: 0.12644\n","Epoch: 02 [ 9322/16204 ( 58%)], Train Loss: 0.12639\n","Epoch: 02 [ 9342/16204 ( 58%)], Train Loss: 0.12614\n","Epoch: 02 [ 9362/16204 ( 58%)], Train Loss: 0.12611\n","Epoch: 02 [ 9382/16204 ( 58%)], Train Loss: 0.12600\n","Epoch: 02 [ 9402/16204 ( 58%)], Train Loss: 0.12611\n","Epoch: 02 [ 9422/16204 ( 58%)], Train Loss: 0.12593\n","Epoch: 02 [ 9442/16204 ( 58%)], Train Loss: 0.12578\n","Epoch: 02 [ 9462/16204 ( 58%)], Train Loss: 0.12584\n","Epoch: 02 [ 9482/16204 ( 59%)], Train Loss: 0.12563\n","Epoch: 02 [ 9502/16204 ( 59%)], Train Loss: 0.12538\n","Epoch: 02 [ 9522/16204 ( 59%)], Train Loss: 0.12531\n","Epoch: 02 [ 9542/16204 ( 59%)], Train Loss: 0.12518\n","Epoch: 02 [ 9562/16204 ( 59%)], Train Loss: 0.12508\n","Epoch: 02 [ 9582/16204 ( 59%)], Train Loss: 0.12487\n","Epoch: 02 [ 9602/16204 ( 59%)], Train Loss: 0.12467\n","Epoch: 02 [ 9622/16204 ( 59%)], Train Loss: 0.12449\n","Epoch: 02 [ 9642/16204 ( 60%)], Train Loss: 0.12452\n","Epoch: 02 [ 9662/16204 ( 60%)], Train Loss: 0.12428\n","Epoch: 02 [ 9682/16204 ( 60%)], Train Loss: 0.12411\n","Epoch: 02 [ 9702/16204 ( 60%)], Train Loss: 0.12414\n","Epoch: 02 [ 9722/16204 ( 60%)], Train Loss: 0.12409\n","Epoch: 02 [ 9742/16204 ( 60%)], Train Loss: 0.12389\n","Epoch: 02 [ 9762/16204 ( 60%)], Train Loss: 0.12385\n","Epoch: 02 [ 9782/16204 ( 60%)], Train Loss: 0.12390\n","Epoch: 02 [ 9802/16204 ( 60%)], Train Loss: 0.12404\n","Epoch: 02 [ 9822/16204 ( 61%)], Train Loss: 0.12397\n","Epoch: 02 [ 9842/16204 ( 61%)], Train Loss: 0.12385\n","Epoch: 02 [ 9862/16204 ( 61%)], Train Loss: 0.12380\n","Epoch: 02 [ 9882/16204 ( 61%)], Train Loss: 0.12374\n","Epoch: 02 [ 9902/16204 ( 61%)], Train Loss: 0.12363\n","Epoch: 02 [ 9922/16204 ( 61%)], Train Loss: 0.12349\n","Epoch: 02 [ 9942/16204 ( 61%)], Train Loss: 0.12364\n","Epoch: 02 [ 9962/16204 ( 61%)], Train Loss: 0.12362\n","Epoch: 02 [ 9982/16204 ( 62%)], Train Loss: 0.12380\n","Epoch: 02 [10002/16204 ( 62%)], Train Loss: 0.12380\n","Epoch: 02 [10022/16204 ( 62%)], Train Loss: 0.12374\n","Epoch: 02 [10042/16204 ( 62%)], Train Loss: 0.12369\n","Epoch: 02 [10062/16204 ( 62%)], Train Loss: 0.12361\n","Epoch: 02 [10082/16204 ( 62%)], Train Loss: 0.12343\n","Epoch: 02 [10102/16204 ( 62%)], Train Loss: 0.12333\n","Epoch: 02 [10122/16204 ( 62%)], Train Loss: 0.12368\n","Epoch: 02 [10142/16204 ( 63%)], Train Loss: 0.12369\n","Epoch: 02 [10162/16204 ( 63%)], Train Loss: 0.12349\n","Epoch: 02 [10182/16204 ( 63%)], Train Loss: 0.12344\n","Epoch: 02 [10202/16204 ( 63%)], Train Loss: 0.12325\n","Epoch: 02 [10222/16204 ( 63%)], Train Loss: 0.12330\n","Epoch: 02 [10242/16204 ( 63%)], Train Loss: 0.12313\n","Epoch: 02 [10262/16204 ( 63%)], Train Loss: 0.12305\n","Epoch: 02 [10282/16204 ( 63%)], Train Loss: 0.12314\n","Epoch: 02 [10302/16204 ( 64%)], Train Loss: 0.12295\n","Epoch: 02 [10322/16204 ( 64%)], Train Loss: 0.12288\n","Epoch: 02 [10342/16204 ( 64%)], Train Loss: 0.12274\n","Epoch: 02 [10362/16204 ( 64%)], Train Loss: 0.12277\n","Epoch: 02 [10382/16204 ( 64%)], Train Loss: 0.12267\n","Epoch: 02 [10402/16204 ( 64%)], Train Loss: 0.12244\n","Epoch: 02 [10422/16204 ( 64%)], Train Loss: 0.12223\n","Epoch: 02 [10442/16204 ( 64%)], Train Loss: 0.12203\n","Epoch: 02 [10462/16204 ( 65%)], Train Loss: 0.12212\n","Epoch: 02 [10482/16204 ( 65%)], Train Loss: 0.12210\n","Epoch: 02 [10502/16204 ( 65%)], Train Loss: 0.12190\n","Epoch: 02 [10522/16204 ( 65%)], Train Loss: 0.12171\n","Epoch: 02 [10542/16204 ( 65%)], Train Loss: 0.12150\n","Epoch: 02 [10562/16204 ( 65%)], Train Loss: 0.12132\n","Epoch: 02 [10582/16204 ( 65%)], Train Loss: 0.12121\n","Epoch: 02 [10602/16204 ( 65%)], Train Loss: 0.12109\n","Epoch: 02 [10622/16204 ( 66%)], Train Loss: 0.12113\n","Epoch: 02 [10642/16204 ( 66%)], Train Loss: 0.12101\n","Epoch: 02 [10662/16204 ( 66%)], Train Loss: 0.12082\n","Epoch: 02 [10682/16204 ( 66%)], Train Loss: 0.12087\n","Epoch: 02 [10702/16204 ( 66%)], Train Loss: 0.12082\n","Epoch: 02 [10722/16204 ( 66%)], Train Loss: 0.12074\n","Epoch: 02 [10742/16204 ( 66%)], Train Loss: 0.12064\n","Epoch: 02 [10762/16204 ( 66%)], Train Loss: 0.12044\n","Epoch: 02 [10782/16204 ( 67%)], Train Loss: 0.12033\n","Epoch: 02 [10802/16204 ( 67%)], Train Loss: 0.12028\n","Epoch: 02 [10822/16204 ( 67%)], Train Loss: 0.12027\n","Epoch: 02 [10842/16204 ( 67%)], Train Loss: 0.12011\n","Epoch: 02 [10862/16204 ( 67%)], Train Loss: 0.12028\n","Epoch: 02 [10882/16204 ( 67%)], Train Loss: 0.12014\n","Epoch: 02 [10902/16204 ( 67%)], Train Loss: 0.12011\n","Epoch: 02 [10922/16204 ( 67%)], Train Loss: 0.12012\n","Epoch: 02 [10942/16204 ( 68%)], Train Loss: 0.12020\n","Epoch: 02 [10962/16204 ( 68%)], Train Loss: 0.12013\n","Epoch: 02 [10982/16204 ( 68%)], Train Loss: 0.12003\n","Epoch: 02 [11002/16204 ( 68%)], Train Loss: 0.12003\n","Epoch: 02 [11022/16204 ( 68%)], Train Loss: 0.12006\n","Epoch: 02 [11042/16204 ( 68%)], Train Loss: 0.11993\n","Epoch: 02 [11062/16204 ( 68%)], Train Loss: 0.11985\n","Epoch: 02 [11082/16204 ( 68%)], Train Loss: 0.11987\n","Epoch: 02 [11102/16204 ( 69%)], Train Loss: 0.11978\n","Epoch: 02 [11122/16204 ( 69%)], Train Loss: 0.11963\n","Epoch: 02 [11142/16204 ( 69%)], Train Loss: 0.11971\n","Epoch: 02 [11162/16204 ( 69%)], Train Loss: 0.11957\n","Epoch: 02 [11182/16204 ( 69%)], Train Loss: 0.11942\n","Epoch: 02 [11202/16204 ( 69%)], Train Loss: 0.11929\n","Epoch: 02 [11222/16204 ( 69%)], Train Loss: 0.11923\n","Epoch: 02 [11242/16204 ( 69%)], Train Loss: 0.11917\n","Epoch: 02 [11262/16204 ( 70%)], Train Loss: 0.11925\n","Epoch: 02 [11282/16204 ( 70%)], Train Loss: 0.11909\n","Epoch: 02 [11302/16204 ( 70%)], Train Loss: 0.11894\n","Epoch: 02 [11322/16204 ( 70%)], Train Loss: 0.11882\n","Epoch: 02 [11342/16204 ( 70%)], Train Loss: 0.11932\n","Epoch: 02 [11362/16204 ( 70%)], Train Loss: 0.11933\n","Epoch: 02 [11382/16204 ( 70%)], Train Loss: 0.11924\n","Epoch: 02 [11402/16204 ( 70%)], Train Loss: 0.11940\n","Epoch: 02 [11422/16204 ( 70%)], Train Loss: 0.11928\n","Epoch: 02 [11442/16204 ( 71%)], Train Loss: 0.11913\n","Epoch: 02 [11462/16204 ( 71%)], Train Loss: 0.11907\n","Epoch: 02 [11482/16204 ( 71%)], Train Loss: 0.11902\n","Epoch: 02 [11502/16204 ( 71%)], Train Loss: 0.11892\n","Epoch: 02 [11522/16204 ( 71%)], Train Loss: 0.11875\n","Epoch: 02 [11542/16204 ( 71%)], Train Loss: 0.11860\n","Epoch: 02 [11562/16204 ( 71%)], Train Loss: 0.11851\n","Epoch: 02 [11582/16204 ( 71%)], Train Loss: 0.11868\n","Epoch: 02 [11602/16204 ( 72%)], Train Loss: 0.11871\n","Epoch: 02 [11622/16204 ( 72%)], Train Loss: 0.11862\n","Epoch: 02 [11642/16204 ( 72%)], Train Loss: 0.11851\n","Epoch: 02 [11662/16204 ( 72%)], Train Loss: 0.11878\n","Epoch: 02 [11682/16204 ( 72%)], Train Loss: 0.11893\n","Epoch: 02 [11702/16204 ( 72%)], Train Loss: 0.11886\n","Epoch: 02 [11722/16204 ( 72%)], Train Loss: 0.11874\n","Epoch: 02 [11742/16204 ( 72%)], Train Loss: 0.11869\n","Epoch: 02 [11762/16204 ( 73%)], Train Loss: 0.11867\n","Epoch: 02 [11782/16204 ( 73%)], Train Loss: 0.11860\n","Epoch: 02 [11802/16204 ( 73%)], Train Loss: 0.11852\n","Epoch: 02 [11822/16204 ( 73%)], Train Loss: 0.11849\n","Epoch: 02 [11842/16204 ( 73%)], Train Loss: 0.11836\n","Epoch: 02 [11862/16204 ( 73%)], Train Loss: 0.11825\n","Epoch: 02 [11882/16204 ( 73%)], Train Loss: 0.11809\n","Epoch: 02 [11902/16204 ( 73%)], Train Loss: 0.11799\n","Epoch: 02 [11922/16204 ( 74%)], Train Loss: 0.11794\n","Epoch: 02 [11942/16204 ( 74%)], Train Loss: 0.11784\n","Epoch: 02 [11962/16204 ( 74%)], Train Loss: 0.11773\n","Epoch: 02 [11982/16204 ( 74%)], Train Loss: 0.11769\n","Epoch: 02 [12002/16204 ( 74%)], Train Loss: 0.11753\n","Epoch: 02 [12022/16204 ( 74%)], Train Loss: 0.11746\n","Epoch: 02 [12042/16204 ( 74%)], Train Loss: 0.11730\n","Epoch: 02 [12062/16204 ( 74%)], Train Loss: 0.11711\n","Epoch: 02 [12082/16204 ( 75%)], Train Loss: 0.11703\n","Epoch: 02 [12102/16204 ( 75%)], Train Loss: 0.11686\n","Epoch: 02 [12122/16204 ( 75%)], Train Loss: 0.11683\n","Epoch: 02 [12142/16204 ( 75%)], Train Loss: 0.11678\n","Epoch: 02 [12162/16204 ( 75%)], Train Loss: 0.11696\n","Epoch: 02 [12182/16204 ( 75%)], Train Loss: 0.11681\n","Epoch: 02 [12202/16204 ( 75%)], Train Loss: 0.11670\n","Epoch: 02 [12222/16204 ( 75%)], Train Loss: 0.11674\n","Epoch: 02 [12242/16204 ( 76%)], Train Loss: 0.11668\n","Epoch: 02 [12262/16204 ( 76%)], Train Loss: 0.11654\n","Epoch: 02 [12282/16204 ( 76%)], Train Loss: 0.11655\n","Epoch: 02 [12302/16204 ( 76%)], Train Loss: 0.11642\n","Epoch: 02 [12322/16204 ( 76%)], Train Loss: 0.11625\n","Epoch: 02 [12342/16204 ( 76%)], Train Loss: 0.11611\n","Epoch: 02 [12362/16204 ( 76%)], Train Loss: 0.11596\n","Epoch: 02 [12382/16204 ( 76%)], Train Loss: 0.11593\n","Epoch: 02 [12402/16204 ( 77%)], Train Loss: 0.11578\n","Epoch: 02 [12422/16204 ( 77%)], Train Loss: 0.11561\n","Epoch: 02 [12442/16204 ( 77%)], Train Loss: 0.11567\n","Epoch: 02 [12462/16204 ( 77%)], Train Loss: 0.11556\n","Epoch: 02 [12482/16204 ( 77%)], Train Loss: 0.11551\n","Epoch: 02 [12502/16204 ( 77%)], Train Loss: 0.11539\n","Epoch: 02 [12522/16204 ( 77%)], Train Loss: 0.11523\n","Epoch: 02 [12542/16204 ( 77%)], Train Loss: 0.11522\n","Epoch: 02 [12562/16204 ( 78%)], Train Loss: 0.11507\n","Epoch: 02 [12582/16204 ( 78%)], Train Loss: 0.11498\n","Epoch: 02 [12602/16204 ( 78%)], Train Loss: 0.11486\n","Epoch: 02 [12622/16204 ( 78%)], Train Loss: 0.11495\n","Epoch: 02 [12642/16204 ( 78%)], Train Loss: 0.11484\n","Epoch: 02 [12662/16204 ( 78%)], Train Loss: 0.11473\n","Epoch: 02 [12682/16204 ( 78%)], Train Loss: 0.11478\n","Epoch: 02 [12702/16204 ( 78%)], Train Loss: 0.11467\n","Epoch: 02 [12722/16204 ( 79%)], Train Loss: 0.11471\n","Epoch: 02 [12742/16204 ( 79%)], Train Loss: 0.11469\n","Epoch: 02 [12762/16204 ( 79%)], Train Loss: 0.11460\n","Epoch: 02 [12782/16204 ( 79%)], Train Loss: 0.11450\n","Epoch: 02 [12802/16204 ( 79%)], Train Loss: 0.11448\n","Epoch: 02 [12822/16204 ( 79%)], Train Loss: 0.11441\n","Epoch: 02 [12842/16204 ( 79%)], Train Loss: 0.11429\n","Epoch: 02 [12862/16204 ( 79%)], Train Loss: 0.11421\n","Epoch: 02 [12882/16204 ( 79%)], Train Loss: 0.11417\n","Epoch: 02 [12902/16204 ( 80%)], Train Loss: 0.11416\n","Epoch: 02 [12922/16204 ( 80%)], Train Loss: 0.11404\n","Epoch: 02 [12942/16204 ( 80%)], Train Loss: 0.11418\n","Epoch: 02 [12962/16204 ( 80%)], Train Loss: 0.11410\n","Epoch: 02 [12982/16204 ( 80%)], Train Loss: 0.11425\n","Epoch: 02 [13002/16204 ( 80%)], Train Loss: 0.11413\n","Epoch: 02 [13022/16204 ( 80%)], Train Loss: 0.11412\n","Epoch: 02 [13042/16204 ( 80%)], Train Loss: 0.11404\n","Epoch: 02 [13062/16204 ( 81%)], Train Loss: 0.11397\n","Epoch: 02 [13082/16204 ( 81%)], Train Loss: 0.11384\n","Epoch: 02 [13102/16204 ( 81%)], Train Loss: 0.11375\n","Epoch: 02 [13122/16204 ( 81%)], Train Loss: 0.11370\n","Epoch: 02 [13142/16204 ( 81%)], Train Loss: 0.11354\n","Epoch: 02 [13162/16204 ( 81%)], Train Loss: 0.11380\n","Epoch: 02 [13182/16204 ( 81%)], Train Loss: 0.11391\n","Epoch: 02 [13202/16204 ( 81%)], Train Loss: 0.11400\n","Epoch: 02 [13222/16204 ( 82%)], Train Loss: 0.11419\n","Epoch: 02 [13242/16204 ( 82%)], Train Loss: 0.11450\n","Epoch: 02 [13262/16204 ( 82%)], Train Loss: 0.11454\n","Epoch: 02 [13282/16204 ( 82%)], Train Loss: 0.11453\n","Epoch: 02 [13302/16204 ( 82%)], Train Loss: 0.11442\n","Epoch: 02 [13322/16204 ( 82%)], Train Loss: 0.11442\n","Epoch: 02 [13342/16204 ( 82%)], Train Loss: 0.11452\n","Epoch: 02 [13362/16204 ( 82%)], Train Loss: 0.11456\n","Epoch: 02 [13382/16204 ( 83%)], Train Loss: 0.11444\n","Epoch: 02 [13402/16204 ( 83%)], Train Loss: 0.11437\n","Epoch: 02 [13422/16204 ( 83%)], Train Loss: 0.11427\n","Epoch: 02 [13442/16204 ( 83%)], Train Loss: 0.11416\n","Epoch: 02 [13462/16204 ( 83%)], Train Loss: 0.11400\n","Epoch: 02 [13482/16204 ( 83%)], Train Loss: 0.11386\n","Epoch: 02 [13502/16204 ( 83%)], Train Loss: 0.11371\n","Epoch: 02 [13522/16204 ( 83%)], Train Loss: 0.11359\n","Epoch: 02 [13542/16204 ( 84%)], Train Loss: 0.11344\n","Epoch: 02 [13562/16204 ( 84%)], Train Loss: 0.11342\n","Epoch: 02 [13582/16204 ( 84%)], Train Loss: 0.11345\n","Epoch: 02 [13602/16204 ( 84%)], Train Loss: 0.11339\n","Epoch: 02 [13622/16204 ( 84%)], Train Loss: 0.11331\n","Epoch: 02 [13642/16204 ( 84%)], Train Loss: 0.11317\n","Epoch: 02 [13662/16204 ( 84%)], Train Loss: 0.11304\n","Epoch: 02 [13682/16204 ( 84%)], Train Loss: 0.11306\n","Epoch: 02 [13702/16204 ( 85%)], Train Loss: 0.11302\n","Epoch: 02 [13722/16204 ( 85%)], Train Loss: 0.11307\n","Epoch: 02 [13742/16204 ( 85%)], Train Loss: 0.11296\n","Epoch: 02 [13762/16204 ( 85%)], Train Loss: 0.11294\n","Epoch: 02 [13782/16204 ( 85%)], Train Loss: 0.11291\n","Epoch: 02 [13802/16204 ( 85%)], Train Loss: 0.11291\n","Epoch: 02 [13822/16204 ( 85%)], Train Loss: 0.11328\n","Epoch: 02 [13842/16204 ( 85%)], Train Loss: 0.11321\n","Epoch: 02 [13862/16204 ( 86%)], Train Loss: 0.11318\n","Epoch: 02 [13882/16204 ( 86%)], Train Loss: 0.11307\n","Epoch: 02 [13902/16204 ( 86%)], Train Loss: 0.11311\n","Epoch: 02 [13922/16204 ( 86%)], Train Loss: 0.11298\n","Epoch: 02 [13942/16204 ( 86%)], Train Loss: 0.11291\n","Epoch: 02 [13962/16204 ( 86%)], Train Loss: 0.11282\n","Epoch: 02 [13982/16204 ( 86%)], Train Loss: 0.11280\n","Epoch: 02 [14002/16204 ( 86%)], Train Loss: 0.11274\n","Epoch: 02 [14022/16204 ( 87%)], Train Loss: 0.11267\n","Epoch: 02 [14042/16204 ( 87%)], Train Loss: 0.11271\n","Epoch: 02 [14062/16204 ( 87%)], Train Loss: 0.11259\n","Epoch: 02 [14082/16204 ( 87%)], Train Loss: 0.11279\n","Epoch: 02 [14102/16204 ( 87%)], Train Loss: 0.11265\n","Epoch: 02 [14122/16204 ( 87%)], Train Loss: 0.11251\n","Epoch: 02 [14142/16204 ( 87%)], Train Loss: 0.11260\n","Epoch: 02 [14162/16204 ( 87%)], Train Loss: 0.11251\n","Epoch: 02 [14182/16204 ( 88%)], Train Loss: 0.11244\n","Epoch: 02 [14202/16204 ( 88%)], Train Loss: 0.11248\n","Epoch: 02 [14222/16204 ( 88%)], Train Loss: 0.11241\n","Epoch: 02 [14242/16204 ( 88%)], Train Loss: 0.11232\n","Epoch: 02 [14262/16204 ( 88%)], Train Loss: 0.11235\n","Epoch: 02 [14282/16204 ( 88%)], Train Loss: 0.11250\n","Epoch: 02 [14302/16204 ( 88%)], Train Loss: 0.11243\n","Epoch: 02 [14322/16204 ( 88%)], Train Loss: 0.11235\n","Epoch: 02 [14342/16204 ( 89%)], Train Loss: 0.11221\n","Epoch: 02 [14362/16204 ( 89%)], Train Loss: 0.11210\n","Epoch: 02 [14382/16204 ( 89%)], Train Loss: 0.11201\n","Epoch: 02 [14402/16204 ( 89%)], Train Loss: 0.11197\n","Epoch: 02 [14422/16204 ( 89%)], Train Loss: 0.11189\n","Epoch: 02 [14442/16204 ( 89%)], Train Loss: 0.11179\n","Epoch: 02 [14462/16204 ( 89%)], Train Loss: 0.11173\n","Epoch: 02 [14482/16204 ( 89%)], Train Loss: 0.11166\n","Epoch: 02 [14502/16204 ( 89%)], Train Loss: 0.11151\n","Epoch: 02 [14522/16204 ( 90%)], Train Loss: 0.11151\n","Epoch: 02 [14542/16204 ( 90%)], Train Loss: 0.11148\n","Epoch: 02 [14562/16204 ( 90%)], Train Loss: 0.11136\n","Epoch: 02 [14582/16204 ( 90%)], Train Loss: 0.11124\n","Epoch: 02 [14602/16204 ( 90%)], Train Loss: 0.11121\n","Epoch: 02 [14622/16204 ( 90%)], Train Loss: 0.11113\n","Epoch: 02 [14642/16204 ( 90%)], Train Loss: 0.11102\n","Epoch: 02 [14662/16204 ( 90%)], Train Loss: 0.11109\n","Epoch: 02 [14682/16204 ( 91%)], Train Loss: 0.11097\n","Epoch: 02 [14702/16204 ( 91%)], Train Loss: 0.11087\n","Epoch: 02 [14722/16204 ( 91%)], Train Loss: 0.11075\n","Epoch: 02 [14742/16204 ( 91%)], Train Loss: 0.11085\n","Epoch: 02 [14762/16204 ( 91%)], Train Loss: 0.11079\n","Epoch: 02 [14782/16204 ( 91%)], Train Loss: 0.11072\n","Epoch: 02 [14802/16204 ( 91%)], Train Loss: 0.11060\n","Epoch: 02 [14822/16204 ( 91%)], Train Loss: 0.11048\n","Epoch: 02 [14842/16204 ( 92%)], Train Loss: 0.11040\n","Epoch: 02 [14862/16204 ( 92%)], Train Loss: 0.11036\n","Epoch: 02 [14882/16204 ( 92%)], Train Loss: 0.11022\n","Epoch: 02 [14902/16204 ( 92%)], Train Loss: 0.11016\n","Epoch: 02 [14922/16204 ( 92%)], Train Loss: 0.11011\n","Epoch: 02 [14942/16204 ( 92%)], Train Loss: 0.10999\n","Epoch: 02 [14962/16204 ( 92%)], Train Loss: 0.10999\n","Epoch: 02 [14982/16204 ( 92%)], Train Loss: 0.11017\n","Epoch: 02 [15002/16204 ( 93%)], Train Loss: 0.11006\n","Epoch: 02 [15022/16204 ( 93%)], Train Loss: 0.10994\n","Epoch: 02 [15042/16204 ( 93%)], Train Loss: 0.10993\n","Epoch: 02 [15062/16204 ( 93%)], Train Loss: 0.10992\n","Epoch: 02 [15082/16204 ( 93%)], Train Loss: 0.10982\n","Epoch: 02 [15102/16204 ( 93%)], Train Loss: 0.10982\n","Epoch: 02 [15122/16204 ( 93%)], Train Loss: 0.10971\n","Epoch: 02 [15142/16204 ( 93%)], Train Loss: 0.10982\n","Epoch: 02 [15162/16204 ( 94%)], Train Loss: 0.10974\n","Epoch: 02 [15182/16204 ( 94%)], Train Loss: 0.10970\n","Epoch: 02 [15202/16204 ( 94%)], Train Loss: 0.10956\n","Epoch: 02 [15222/16204 ( 94%)], Train Loss: 0.10945\n","Epoch: 02 [15242/16204 ( 94%)], Train Loss: 0.10939\n","Epoch: 02 [15262/16204 ( 94%)], Train Loss: 0.10929\n","Epoch: 02 [15282/16204 ( 94%)], Train Loss: 0.10919\n","Epoch: 02 [15302/16204 ( 94%)], Train Loss: 0.10909\n","Epoch: 02 [15322/16204 ( 95%)], Train Loss: 0.10903\n","Epoch: 02 [15342/16204 ( 95%)], Train Loss: 0.10910\n","Epoch: 02 [15362/16204 ( 95%)], Train Loss: 0.10905\n","Epoch: 02 [15382/16204 ( 95%)], Train Loss: 0.10895\n","Epoch: 02 [15402/16204 ( 95%)], Train Loss: 0.10884\n","Epoch: 02 [15422/16204 ( 95%)], Train Loss: 0.10884\n","Epoch: 02 [15442/16204 ( 95%)], Train Loss: 0.10874\n","Epoch: 02 [15462/16204 ( 95%)], Train Loss: 0.10865\n","Epoch: 02 [15482/16204 ( 96%)], Train Loss: 0.10872\n","Epoch: 02 [15502/16204 ( 96%)], Train Loss: 0.10869\n","Epoch: 02 [15522/16204 ( 96%)], Train Loss: 0.10861\n","Epoch: 02 [15542/16204 ( 96%)], Train Loss: 0.10855\n","Epoch: 02 [15562/16204 ( 96%)], Train Loss: 0.10848\n","Epoch: 02 [15582/16204 ( 96%)], Train Loss: 0.10840\n","Epoch: 02 [15602/16204 ( 96%)], Train Loss: 0.10833\n","Epoch: 02 [15622/16204 ( 96%)], Train Loss: 0.10859\n","Epoch: 02 [15642/16204 ( 97%)], Train Loss: 0.10847\n","Epoch: 02 [15662/16204 ( 97%)], Train Loss: 0.10842\n","Epoch: 02 [15682/16204 ( 97%)], Train Loss: 0.10837\n","Epoch: 02 [15702/16204 ( 97%)], Train Loss: 0.10840\n","Epoch: 02 [15722/16204 ( 97%)], Train Loss: 0.10838\n","Epoch: 02 [15742/16204 ( 97%)], Train Loss: 0.10831\n","Epoch: 02 [15762/16204 ( 97%)], Train Loss: 0.10821\n","Epoch: 02 [15782/16204 ( 97%)], Train Loss: 0.10817\n","Epoch: 02 [15802/16204 ( 98%)], Train Loss: 0.10809\n","Epoch: 02 [15822/16204 ( 98%)], Train Loss: 0.10805\n","Epoch: 02 [15842/16204 ( 98%)], Train Loss: 0.10796\n","Epoch: 02 [15862/16204 ( 98%)], Train Loss: 0.10797\n","Epoch: 02 [15882/16204 ( 98%)], Train Loss: 0.10796\n","Epoch: 02 [15902/16204 ( 98%)], Train Loss: 0.10789\n","Epoch: 02 [15922/16204 ( 98%)], Train Loss: 0.10781\n","Epoch: 02 [15942/16204 ( 98%)], Train Loss: 0.10780\n","Epoch: 02 [15962/16204 ( 99%)], Train Loss: 0.10774\n","Epoch: 02 [15982/16204 ( 99%)], Train Loss: 0.10772\n","Epoch: 02 [16002/16204 ( 99%)], Train Loss: 0.10772\n","Epoch: 02 [16022/16204 ( 99%)], Train Loss: 0.10770\n","Epoch: 02 [16042/16204 ( 99%)], Train Loss: 0.10763\n","Epoch: 02 [16062/16204 ( 99%)], Train Loss: 0.10768\n","Epoch: 02 [16082/16204 ( 99%)], Train Loss: 0.10774\n","Epoch: 02 [16102/16204 ( 99%)], Train Loss: 0.10766\n","Epoch: 02 [16122/16204 ( 99%)], Train Loss: 0.10757\n","Epoch: 02 [16142/16204 (100%)], Train Loss: 0.10749\n","Epoch: 02 [16162/16204 (100%)], Train Loss: 0.10737\n","Epoch: 02 [16182/16204 (100%)], Train Loss: 0.10731\n","Epoch: 02 [16202/16204 (100%)], Train Loss: 0.10728\n","Epoch: 02 [16204/16204 (100%)], Train Loss: 0.10727\n","----Validation Results Summary----\n","Epoch: [2] Valid Loss: 0.00000\n","\n","Epoch: 03 [    2/16204 (  0%)], Train Loss: 0.02240\n","Epoch: 03 [   22/16204 (  0%)], Train Loss: 0.13042\n","Epoch: 03 [   42/16204 (  0%)], Train Loss: 0.10152\n","Epoch: 03 [   62/16204 (  0%)], Train Loss: 0.11487\n","Epoch: 03 [   82/16204 (  1%)], Train Loss: 0.13115\n","Epoch: 03 [  102/16204 (  1%)], Train Loss: 0.12928\n","Epoch: 03 [  122/16204 (  1%)], Train Loss: 0.11436\n","Epoch: 03 [  142/16204 (  1%)], Train Loss: 0.09970\n","Epoch: 03 [  162/16204 (  1%)], Train Loss: 0.09616\n","Epoch: 03 [  182/16204 (  1%)], Train Loss: 0.09556\n","Epoch: 03 [  202/16204 (  1%)], Train Loss: 0.08998\n","Epoch: 03 [  222/16204 (  1%)], Train Loss: 0.08401\n","Epoch: 03 [  242/16204 (  1%)], Train Loss: 0.08193\n","Epoch: 03 [  262/16204 (  2%)], Train Loss: 0.08730\n","Epoch: 03 [  282/16204 (  2%)], Train Loss: 0.08528\n","Epoch: 03 [  302/16204 (  2%)], Train Loss: 0.08249\n","Epoch: 03 [  322/16204 (  2%)], Train Loss: 0.08339\n","Epoch: 03 [  342/16204 (  2%)], Train Loss: 0.07987\n","Epoch: 03 [  362/16204 (  2%)], Train Loss: 0.08213\n","Epoch: 03 [  382/16204 (  2%)], Train Loss: 0.08088\n","Epoch: 03 [  402/16204 (  2%)], Train Loss: 0.07982\n","Epoch: 03 [  422/16204 (  3%)], Train Loss: 0.08205\n","Epoch: 03 [  442/16204 (  3%)], Train Loss: 0.08424\n","Epoch: 03 [  462/16204 (  3%)], Train Loss: 0.08246\n","Epoch: 03 [  482/16204 (  3%)], Train Loss: 0.07934\n","Epoch: 03 [  502/16204 (  3%)], Train Loss: 0.07781\n","Epoch: 03 [  522/16204 (  3%)], Train Loss: 0.07570\n","Epoch: 03 [  542/16204 (  3%)], Train Loss: 0.07671\n","Epoch: 03 [  562/16204 (  3%)], Train Loss: 0.07605\n","Epoch: 03 [  582/16204 (  4%)], Train Loss: 0.07407\n","Epoch: 03 [  602/16204 (  4%)], Train Loss: 0.07315\n","Epoch: 03 [  622/16204 (  4%)], Train Loss: 0.07106\n","Epoch: 03 [  642/16204 (  4%)], Train Loss: 0.07247\n","Epoch: 03 [  662/16204 (  4%)], Train Loss: 0.07169\n","Epoch: 03 [  682/16204 (  4%)], Train Loss: 0.07085\n","Epoch: 03 [  702/16204 (  4%)], Train Loss: 0.06905\n","Epoch: 03 [  722/16204 (  4%)], Train Loss: 0.06807\n","Epoch: 03 [  742/16204 (  5%)], Train Loss: 0.06671\n","Epoch: 03 [  762/16204 (  5%)], Train Loss: 0.06521\n","Epoch: 03 [  782/16204 (  5%)], Train Loss: 0.06441\n","Epoch: 03 [  802/16204 (  5%)], Train Loss: 0.06311\n","Epoch: 03 [  822/16204 (  5%)], Train Loss: 0.06272\n","Epoch: 03 [  842/16204 (  5%)], Train Loss: 0.06530\n","Epoch: 03 [  862/16204 (  5%)], Train Loss: 0.06395\n","Epoch: 03 [  882/16204 (  5%)], Train Loss: 0.06331\n","Epoch: 03 [  902/16204 (  6%)], Train Loss: 0.06346\n","Epoch: 03 [  922/16204 (  6%)], Train Loss: 0.06562\n","Epoch: 03 [  942/16204 (  6%)], Train Loss: 0.06448\n","Epoch: 03 [  962/16204 (  6%)], Train Loss: 0.06694\n","Epoch: 03 [  982/16204 (  6%)], Train Loss: 0.06715\n","Epoch: 03 [ 1002/16204 (  6%)], Train Loss: 0.06638\n","Epoch: 03 [ 1022/16204 (  6%)], Train Loss: 0.06562\n","Epoch: 03 [ 1042/16204 (  6%)], Train Loss: 0.06502\n","Epoch: 03 [ 1062/16204 (  7%)], Train Loss: 0.06487\n","Epoch: 03 [ 1082/16204 (  7%)], Train Loss: 0.06456\n","Epoch: 03 [ 1102/16204 (  7%)], Train Loss: 0.06445\n","Epoch: 03 [ 1122/16204 (  7%)], Train Loss: 0.06560\n","Epoch: 03 [ 1142/16204 (  7%)], Train Loss: 0.06581\n","Epoch: 03 [ 1162/16204 (  7%)], Train Loss: 0.06530\n","Epoch: 03 [ 1182/16204 (  7%)], Train Loss: 0.06460\n","Epoch: 03 [ 1202/16204 (  7%)], Train Loss: 0.06471\n","Epoch: 03 [ 1222/16204 (  8%)], Train Loss: 0.06445\n","Epoch: 03 [ 1242/16204 (  8%)], Train Loss: 0.06413\n","Epoch: 03 [ 1262/16204 (  8%)], Train Loss: 0.06336\n","Epoch: 03 [ 1282/16204 (  8%)], Train Loss: 0.06340\n","Epoch: 03 [ 1302/16204 (  8%)], Train Loss: 0.06330\n","Epoch: 03 [ 1322/16204 (  8%)], Train Loss: 0.06337\n","Epoch: 03 [ 1342/16204 (  8%)], Train Loss: 0.06291\n","Epoch: 03 [ 1362/16204 (  8%)], Train Loss: 0.06210\n","Epoch: 03 [ 1382/16204 (  9%)], Train Loss: 0.06298\n","Epoch: 03 [ 1402/16204 (  9%)], Train Loss: 0.06234\n","Epoch: 03 [ 1422/16204 (  9%)], Train Loss: 0.06211\n","Epoch: 03 [ 1442/16204 (  9%)], Train Loss: 0.06378\n","Epoch: 03 [ 1462/16204 (  9%)], Train Loss: 0.06409\n","Epoch: 03 [ 1482/16204 (  9%)], Train Loss: 0.06380\n","Epoch: 03 [ 1502/16204 (  9%)], Train Loss: 0.06407\n","Epoch: 03 [ 1522/16204 (  9%)], Train Loss: 0.06350\n","Epoch: 03 [ 1542/16204 ( 10%)], Train Loss: 0.06380\n","Epoch: 03 [ 1562/16204 ( 10%)], Train Loss: 0.06312\n","Epoch: 03 [ 1582/16204 ( 10%)], Train Loss: 0.06289\n","Epoch: 03 [ 1602/16204 ( 10%)], Train Loss: 0.06246\n","Epoch: 03 [ 1622/16204 ( 10%)], Train Loss: 0.06469\n","Epoch: 03 [ 1642/16204 ( 10%)], Train Loss: 0.06489\n","Epoch: 03 [ 1662/16204 ( 10%)], Train Loss: 0.06519\n","Epoch: 03 [ 1682/16204 ( 10%)], Train Loss: 0.06528\n","Epoch: 03 [ 1702/16204 ( 11%)], Train Loss: 0.06503\n","Epoch: 03 [ 1722/16204 ( 11%)], Train Loss: 0.06505\n","Epoch: 03 [ 1742/16204 ( 11%)], Train Loss: 0.06456\n","Epoch: 03 [ 1762/16204 ( 11%)], Train Loss: 0.06460\n","Epoch: 03 [ 1782/16204 ( 11%)], Train Loss: 0.06475\n","Epoch: 03 [ 1802/16204 ( 11%)], Train Loss: 0.06453\n","Epoch: 03 [ 1822/16204 ( 11%)], Train Loss: 0.06458\n","Epoch: 03 [ 1842/16204 ( 11%)], Train Loss: 0.06475\n","Epoch: 03 [ 1862/16204 ( 11%)], Train Loss: 0.06483\n","Epoch: 03 [ 1882/16204 ( 12%)], Train Loss: 0.06481\n","Epoch: 03 [ 1902/16204 ( 12%)], Train Loss: 0.06503\n","Epoch: 03 [ 1922/16204 ( 12%)], Train Loss: 0.06660\n","Epoch: 03 [ 1942/16204 ( 12%)], Train Loss: 0.06618\n","Epoch: 03 [ 1962/16204 ( 12%)], Train Loss: 0.06572\n","Epoch: 03 [ 1982/16204 ( 12%)], Train Loss: 0.06519\n","Epoch: 03 [ 2002/16204 ( 12%)], Train Loss: 0.06549\n","Epoch: 03 [ 2022/16204 ( 12%)], Train Loss: 0.06708\n","Epoch: 03 [ 2042/16204 ( 13%)], Train Loss: 0.06763\n","Epoch: 03 [ 2062/16204 ( 13%)], Train Loss: 0.06712\n","Epoch: 03 [ 2082/16204 ( 13%)], Train Loss: 0.06690\n","Epoch: 03 [ 2102/16204 ( 13%)], Train Loss: 0.06681\n","Epoch: 03 [ 2122/16204 ( 13%)], Train Loss: 0.06629\n","Epoch: 03 [ 2142/16204 ( 13%)], Train Loss: 0.06664\n","Epoch: 03 [ 2162/16204 ( 13%)], Train Loss: 0.06631\n","Epoch: 03 [ 2182/16204 ( 13%)], Train Loss: 0.06580\n","Epoch: 03 [ 2202/16204 ( 14%)], Train Loss: 0.06532\n","Epoch: 03 [ 2222/16204 ( 14%)], Train Loss: 0.06495\n","Epoch: 03 [ 2242/16204 ( 14%)], Train Loss: 0.06513\n","Epoch: 03 [ 2262/16204 ( 14%)], Train Loss: 0.06508\n","Epoch: 03 [ 2282/16204 ( 14%)], Train Loss: 0.06491\n","Epoch: 03 [ 2302/16204 ( 14%)], Train Loss: 0.06447\n","Epoch: 03 [ 2322/16204 ( 14%)], Train Loss: 0.06402\n","Epoch: 03 [ 2342/16204 ( 14%)], Train Loss: 0.06366\n","Epoch: 03 [ 2362/16204 ( 15%)], Train Loss: 0.06362\n","Epoch: 03 [ 2382/16204 ( 15%)], Train Loss: 0.06332\n","Epoch: 03 [ 2402/16204 ( 15%)], Train Loss: 0.06298\n","Epoch: 03 [ 2422/16204 ( 15%)], Train Loss: 0.06334\n","Epoch: 03 [ 2442/16204 ( 15%)], Train Loss: 0.06355\n","Epoch: 03 [ 2462/16204 ( 15%)], Train Loss: 0.06366\n","Epoch: 03 [ 2482/16204 ( 15%)], Train Loss: 0.06402\n","Epoch: 03 [ 2502/16204 ( 15%)], Train Loss: 0.06365\n","Epoch: 03 [ 2522/16204 ( 16%)], Train Loss: 0.06324\n","Epoch: 03 [ 2542/16204 ( 16%)], Train Loss: 0.06309\n","Epoch: 03 [ 2562/16204 ( 16%)], Train Loss: 0.06268\n","Epoch: 03 [ 2582/16204 ( 16%)], Train Loss: 0.06247\n","Epoch: 03 [ 2602/16204 ( 16%)], Train Loss: 0.06266\n","Epoch: 03 [ 2622/16204 ( 16%)], Train Loss: 0.06228\n","Epoch: 03 [ 2642/16204 ( 16%)], Train Loss: 0.06187\n","Epoch: 03 [ 2662/16204 ( 16%)], Train Loss: 0.06182\n","Epoch: 03 [ 2682/16204 ( 17%)], Train Loss: 0.06167\n","Epoch: 03 [ 2702/16204 ( 17%)], Train Loss: 0.06133\n","Epoch: 03 [ 2722/16204 ( 17%)], Train Loss: 0.06140\n","Epoch: 03 [ 2742/16204 ( 17%)], Train Loss: 0.06098\n","Epoch: 03 [ 2762/16204 ( 17%)], Train Loss: 0.06109\n","Epoch: 03 [ 2782/16204 ( 17%)], Train Loss: 0.06125\n","Epoch: 03 [ 2802/16204 ( 17%)], Train Loss: 0.06095\n","Epoch: 03 [ 2822/16204 ( 17%)], Train Loss: 0.06079\n","Epoch: 03 [ 2842/16204 ( 18%)], Train Loss: 0.06048\n","Epoch: 03 [ 2862/16204 ( 18%)], Train Loss: 0.06055\n","Epoch: 03 [ 2882/16204 ( 18%)], Train Loss: 0.06117\n","Epoch: 03 [ 2902/16204 ( 18%)], Train Loss: 0.06137\n","Epoch: 03 [ 2922/16204 ( 18%)], Train Loss: 0.06109\n","Epoch: 03 [ 2942/16204 ( 18%)], Train Loss: 0.06077\n","Epoch: 03 [ 2962/16204 ( 18%)], Train Loss: 0.06042\n","Epoch: 03 [ 2982/16204 ( 18%)], Train Loss: 0.06026\n","Epoch: 03 [ 3002/16204 ( 19%)], Train Loss: 0.05991\n","Epoch: 03 [ 3022/16204 ( 19%)], Train Loss: 0.05980\n","Epoch: 03 [ 3042/16204 ( 19%)], Train Loss: 0.05953\n","Epoch: 03 [ 3062/16204 ( 19%)], Train Loss: 0.05927\n","Epoch: 03 [ 3082/16204 ( 19%)], Train Loss: 0.05900\n","Epoch: 03 [ 3102/16204 ( 19%)], Train Loss: 0.05870\n","Epoch: 03 [ 3122/16204 ( 19%)], Train Loss: 0.05836\n","Epoch: 03 [ 3142/16204 ( 19%)], Train Loss: 0.05822\n","Epoch: 03 [ 3162/16204 ( 20%)], Train Loss: 0.05826\n","Epoch: 03 [ 3182/16204 ( 20%)], Train Loss: 0.05826\n","Epoch: 03 [ 3202/16204 ( 20%)], Train Loss: 0.05790\n","Epoch: 03 [ 3222/16204 ( 20%)], Train Loss: 0.05824\n","Epoch: 03 [ 3242/16204 ( 20%)], Train Loss: 0.05789\n","Epoch: 03 [ 3262/16204 ( 20%)], Train Loss: 0.05771\n","Epoch: 03 [ 3282/16204 ( 20%)], Train Loss: 0.05753\n","Epoch: 03 [ 3302/16204 ( 20%)], Train Loss: 0.05730\n","Epoch: 03 [ 3322/16204 ( 21%)], Train Loss: 0.05708\n","Epoch: 03 [ 3342/16204 ( 21%)], Train Loss: 0.05682\n","Epoch: 03 [ 3362/16204 ( 21%)], Train Loss: 0.05706\n","Epoch: 03 [ 3382/16204 ( 21%)], Train Loss: 0.05687\n","Epoch: 03 [ 3402/16204 ( 21%)], Train Loss: 0.05701\n","Epoch: 03 [ 3422/16204 ( 21%)], Train Loss: 0.05682\n","Epoch: 03 [ 3442/16204 ( 21%)], Train Loss: 0.05651\n","Epoch: 03 [ 3462/16204 ( 21%)], Train Loss: 0.05714\n","Epoch: 03 [ 3482/16204 ( 21%)], Train Loss: 0.05704\n","Epoch: 03 [ 3502/16204 ( 22%)], Train Loss: 0.05686\n","Epoch: 03 [ 3522/16204 ( 22%)], Train Loss: 0.05663\n","Epoch: 03 [ 3542/16204 ( 22%)], Train Loss: 0.05699\n","Epoch: 03 [ 3562/16204 ( 22%)], Train Loss: 0.05676\n","Epoch: 03 [ 3582/16204 ( 22%)], Train Loss: 0.05686\n","Epoch: 03 [ 3602/16204 ( 22%)], Train Loss: 0.05673\n","Epoch: 03 [ 3622/16204 ( 22%)], Train Loss: 0.05649\n","Epoch: 03 [ 3642/16204 ( 22%)], Train Loss: 0.05678\n","Epoch: 03 [ 3662/16204 ( 23%)], Train Loss: 0.05658\n","Epoch: 03 [ 3682/16204 ( 23%)], Train Loss: 0.05634\n","Epoch: 03 [ 3702/16204 ( 23%)], Train Loss: 0.05628\n","Epoch: 03 [ 3722/16204 ( 23%)], Train Loss: 0.05611\n","Epoch: 03 [ 3742/16204 ( 23%)], Train Loss: 0.05584\n","Epoch: 03 [ 3762/16204 ( 23%)], Train Loss: 0.05623\n","Epoch: 03 [ 3782/16204 ( 23%)], Train Loss: 0.05686\n","Epoch: 03 [ 3802/16204 ( 23%)], Train Loss: 0.05696\n","Epoch: 03 [ 3822/16204 ( 24%)], Train Loss: 0.05713\n","Epoch: 03 [ 3842/16204 ( 24%)], Train Loss: 0.05697\n","Epoch: 03 [ 3862/16204 ( 24%)], Train Loss: 0.05683\n","Epoch: 03 [ 3882/16204 ( 24%)], Train Loss: 0.05655\n","Epoch: 03 [ 3902/16204 ( 24%)], Train Loss: 0.05669\n","Epoch: 03 [ 3922/16204 ( 24%)], Train Loss: 0.05681\n","Epoch: 03 [ 3942/16204 ( 24%)], Train Loss: 0.05670\n","Epoch: 03 [ 3962/16204 ( 24%)], Train Loss: 0.05655\n","Epoch: 03 [ 3982/16204 ( 25%)], Train Loss: 0.05738\n","Epoch: 03 [ 4002/16204 ( 25%)], Train Loss: 0.05721\n","Epoch: 03 [ 4022/16204 ( 25%)], Train Loss: 0.05714\n","Epoch: 03 [ 4042/16204 ( 25%)], Train Loss: 0.05692\n","Epoch: 03 [ 4062/16204 ( 25%)], Train Loss: 0.05695\n","Epoch: 03 [ 4082/16204 ( 25%)], Train Loss: 0.05674\n","Epoch: 03 [ 4102/16204 ( 25%)], Train Loss: 0.05657\n","Epoch: 03 [ 4122/16204 ( 25%)], Train Loss: 0.05660\n","Epoch: 03 [ 4142/16204 ( 26%)], Train Loss: 0.05636\n","Epoch: 03 [ 4162/16204 ( 26%)], Train Loss: 0.05610\n","Epoch: 03 [ 4182/16204 ( 26%)], Train Loss: 0.05598\n","Epoch: 03 [ 4202/16204 ( 26%)], Train Loss: 0.05604\n","Epoch: 03 [ 4222/16204 ( 26%)], Train Loss: 0.05651\n","Epoch: 03 [ 4242/16204 ( 26%)], Train Loss: 0.05630\n","Epoch: 03 [ 4262/16204 ( 26%)], Train Loss: 0.05611\n","Epoch: 03 [ 4282/16204 ( 26%)], Train Loss: 0.05597\n","Epoch: 03 [ 4302/16204 ( 27%)], Train Loss: 0.05611\n","Epoch: 03 [ 4322/16204 ( 27%)], Train Loss: 0.05594\n","Epoch: 03 [ 4342/16204 ( 27%)], Train Loss: 0.05582\n","Epoch: 03 [ 4362/16204 ( 27%)], Train Loss: 0.05576\n","Epoch: 03 [ 4382/16204 ( 27%)], Train Loss: 0.05555\n","Epoch: 03 [ 4402/16204 ( 27%)], Train Loss: 0.05543\n","Epoch: 03 [ 4422/16204 ( 27%)], Train Loss: 0.05524\n","Epoch: 03 [ 4442/16204 ( 27%)], Train Loss: 0.05555\n","Epoch: 03 [ 4462/16204 ( 28%)], Train Loss: 0.05602\n","Epoch: 03 [ 4482/16204 ( 28%)], Train Loss: 0.05624\n","Epoch: 03 [ 4502/16204 ( 28%)], Train Loss: 0.05618\n","Epoch: 03 [ 4522/16204 ( 28%)], Train Loss: 0.05681\n","Epoch: 03 [ 4542/16204 ( 28%)], Train Loss: 0.05674\n","Epoch: 03 [ 4562/16204 ( 28%)], Train Loss: 0.05657\n","Epoch: 03 [ 4582/16204 ( 28%)], Train Loss: 0.05666\n","Epoch: 03 [ 4602/16204 ( 28%)], Train Loss: 0.05657\n","Epoch: 03 [ 4622/16204 ( 29%)], Train Loss: 0.05710\n","Epoch: 03 [ 4642/16204 ( 29%)], Train Loss: 0.05714\n","Epoch: 03 [ 4662/16204 ( 29%)], Train Loss: 0.05740\n","Epoch: 03 [ 4682/16204 ( 29%)], Train Loss: 0.05744\n","Epoch: 03 [ 4702/16204 ( 29%)], Train Loss: 0.05745\n","Epoch: 03 [ 4722/16204 ( 29%)], Train Loss: 0.05766\n","Epoch: 03 [ 4742/16204 ( 29%)], Train Loss: 0.05759\n","Epoch: 03 [ 4762/16204 ( 29%)], Train Loss: 0.05770\n","Epoch: 03 [ 4782/16204 ( 30%)], Train Loss: 0.05765\n","Epoch: 03 [ 4802/16204 ( 30%)], Train Loss: 0.05800\n","Epoch: 03 [ 4822/16204 ( 30%)], Train Loss: 0.05807\n","Epoch: 03 [ 4842/16204 ( 30%)], Train Loss: 0.05794\n","Epoch: 03 [ 4862/16204 ( 30%)], Train Loss: 0.05773\n","Epoch: 03 [ 4882/16204 ( 30%)], Train Loss: 0.05794\n","Epoch: 03 [ 4902/16204 ( 30%)], Train Loss: 0.05793\n","Epoch: 03 [ 4922/16204 ( 30%)], Train Loss: 0.05780\n","Epoch: 03 [ 4942/16204 ( 30%)], Train Loss: 0.05758\n","Epoch: 03 [ 4962/16204 ( 31%)], Train Loss: 0.05804\n","Epoch: 03 [ 4982/16204 ( 31%)], Train Loss: 0.05804\n","Epoch: 03 [ 5002/16204 ( 31%)], Train Loss: 0.05793\n","Epoch: 03 [ 5022/16204 ( 31%)], Train Loss: 0.05830\n","Epoch: 03 [ 5042/16204 ( 31%)], Train Loss: 0.05842\n","Epoch: 03 [ 5062/16204 ( 31%)], Train Loss: 0.05853\n","Epoch: 03 [ 5082/16204 ( 31%)], Train Loss: 0.05859\n","Epoch: 03 [ 5102/16204 ( 31%)], Train Loss: 0.05844\n","Epoch: 03 [ 5122/16204 ( 32%)], Train Loss: 0.05830\n","Epoch: 03 [ 5142/16204 ( 32%)], Train Loss: 0.05827\n","Epoch: 03 [ 5162/16204 ( 32%)], Train Loss: 0.05918\n","Epoch: 03 [ 5182/16204 ( 32%)], Train Loss: 0.05902\n","Epoch: 03 [ 5202/16204 ( 32%)], Train Loss: 0.05887\n","Epoch: 03 [ 5222/16204 ( 32%)], Train Loss: 0.05870\n","Epoch: 03 [ 5242/16204 ( 32%)], Train Loss: 0.05873\n","Epoch: 03 [ 5262/16204 ( 32%)], Train Loss: 0.05902\n","Epoch: 03 [ 5282/16204 ( 33%)], Train Loss: 0.05881\n","Epoch: 03 [ 5302/16204 ( 33%)], Train Loss: 0.05867\n","Epoch: 03 [ 5322/16204 ( 33%)], Train Loss: 0.05897\n","Epoch: 03 [ 5342/16204 ( 33%)], Train Loss: 0.05886\n","Epoch: 03 [ 5362/16204 ( 33%)], Train Loss: 0.05882\n","Epoch: 03 [ 5382/16204 ( 33%)], Train Loss: 0.05908\n","Epoch: 03 [ 5402/16204 ( 33%)], Train Loss: 0.05888\n","Epoch: 03 [ 5422/16204 ( 33%)], Train Loss: 0.05949\n","Epoch: 03 [ 5442/16204 ( 34%)], Train Loss: 0.05948\n","Epoch: 03 [ 5462/16204 ( 34%)], Train Loss: 0.05935\n","Epoch: 03 [ 5482/16204 ( 34%)], Train Loss: 0.05930\n","Epoch: 03 [ 5502/16204 ( 34%)], Train Loss: 0.05912\n","Epoch: 03 [ 5522/16204 ( 34%)], Train Loss: 0.05894\n","Epoch: 03 [ 5542/16204 ( 34%)], Train Loss: 0.05878\n","Epoch: 03 [ 5562/16204 ( 34%)], Train Loss: 0.05874\n","Epoch: 03 [ 5582/16204 ( 34%)], Train Loss: 0.05856\n","Epoch: 03 [ 5602/16204 ( 35%)], Train Loss: 0.05844\n","Epoch: 03 [ 5622/16204 ( 35%)], Train Loss: 0.05827\n","Epoch: 03 [ 5642/16204 ( 35%)], Train Loss: 0.05846\n","Epoch: 03 [ 5662/16204 ( 35%)], Train Loss: 0.05826\n","Epoch: 03 [ 5682/16204 ( 35%)], Train Loss: 0.05828\n","Epoch: 03 [ 5702/16204 ( 35%)], Train Loss: 0.05814\n","Epoch: 03 [ 5722/16204 ( 35%)], Train Loss: 0.05795\n","Epoch: 03 [ 5742/16204 ( 35%)], Train Loss: 0.05806\n","Epoch: 03 [ 5762/16204 ( 36%)], Train Loss: 0.05792\n","Epoch: 03 [ 5782/16204 ( 36%)], Train Loss: 0.05772\n","Epoch: 03 [ 5802/16204 ( 36%)], Train Loss: 0.05755\n","Epoch: 03 [ 5822/16204 ( 36%)], Train Loss: 0.05760\n","Epoch: 03 [ 5842/16204 ( 36%)], Train Loss: 0.05742\n","Epoch: 03 [ 5862/16204 ( 36%)], Train Loss: 0.05725\n","Epoch: 03 [ 5882/16204 ( 36%)], Train Loss: 0.05709\n","Epoch: 03 [ 5902/16204 ( 36%)], Train Loss: 0.05696\n","Epoch: 03 [ 5922/16204 ( 37%)], Train Loss: 0.05680\n","Epoch: 03 [ 5942/16204 ( 37%)], Train Loss: 0.05690\n","Epoch: 03 [ 5962/16204 ( 37%)], Train Loss: 0.05674\n","Epoch: 03 [ 5982/16204 ( 37%)], Train Loss: 0.05681\n","Epoch: 03 [ 6002/16204 ( 37%)], Train Loss: 0.05664\n","Epoch: 03 [ 6022/16204 ( 37%)], Train Loss: 0.05659\n","Epoch: 03 [ 6042/16204 ( 37%)], Train Loss: 0.05643\n","Epoch: 03 [ 6062/16204 ( 37%)], Train Loss: 0.05631\n","Epoch: 03 [ 6082/16204 ( 38%)], Train Loss: 0.05615\n","Epoch: 03 [ 6102/16204 ( 38%)], Train Loss: 0.05598\n","Epoch: 03 [ 6122/16204 ( 38%)], Train Loss: 0.05588\n","Epoch: 03 [ 6142/16204 ( 38%)], Train Loss: 0.05629\n","Epoch: 03 [ 6162/16204 ( 38%)], Train Loss: 0.05622\n","Epoch: 03 [ 6182/16204 ( 38%)], Train Loss: 0.05611\n","Epoch: 03 [ 6202/16204 ( 38%)], Train Loss: 0.05619\n","Epoch: 03 [ 6222/16204 ( 38%)], Train Loss: 0.05604\n","Epoch: 03 [ 6242/16204 ( 39%)], Train Loss: 0.05628\n","Epoch: 03 [ 6262/16204 ( 39%)], Train Loss: 0.05616\n","Epoch: 03 [ 6282/16204 ( 39%)], Train Loss: 0.05608\n","Epoch: 03 [ 6302/16204 ( 39%)], Train Loss: 0.05600\n","Epoch: 03 [ 6322/16204 ( 39%)], Train Loss: 0.05585\n","Epoch: 03 [ 6342/16204 ( 39%)], Train Loss: 0.05574\n","Epoch: 03 [ 6362/16204 ( 39%)], Train Loss: 0.05573\n","Epoch: 03 [ 6382/16204 ( 39%)], Train Loss: 0.05557\n","Epoch: 03 [ 6402/16204 ( 40%)], Train Loss: 0.05564\n","Epoch: 03 [ 6422/16204 ( 40%)], Train Loss: 0.05550\n","Epoch: 03 [ 6442/16204 ( 40%)], Train Loss: 0.05544\n","Epoch: 03 [ 6462/16204 ( 40%)], Train Loss: 0.05545\n","Epoch: 03 [ 6482/16204 ( 40%)], Train Loss: 0.05531\n","Epoch: 03 [ 6502/16204 ( 40%)], Train Loss: 0.05517\n","Epoch: 03 [ 6522/16204 ( 40%)], Train Loss: 0.05502\n","Epoch: 03 [ 6542/16204 ( 40%)], Train Loss: 0.05506\n","Epoch: 03 [ 6562/16204 ( 40%)], Train Loss: 0.05490\n","Epoch: 03 [ 6582/16204 ( 41%)], Train Loss: 0.05475\n","Epoch: 03 [ 6602/16204 ( 41%)], Train Loss: 0.05464\n","Epoch: 03 [ 6622/16204 ( 41%)], Train Loss: 0.05453\n","Epoch: 03 [ 6642/16204 ( 41%)], Train Loss: 0.05448\n","Epoch: 03 [ 6662/16204 ( 41%)], Train Loss: 0.05443\n","Epoch: 03 [ 6682/16204 ( 41%)], Train Loss: 0.05432\n","Epoch: 03 [ 6702/16204 ( 41%)], Train Loss: 0.05450\n","Epoch: 03 [ 6722/16204 ( 41%)], Train Loss: 0.05434\n","Epoch: 03 [ 6742/16204 ( 42%)], Train Loss: 0.05419\n","Epoch: 03 [ 6762/16204 ( 42%)], Train Loss: 0.05411\n","Epoch: 03 [ 6782/16204 ( 42%)], Train Loss: 0.05403\n","Epoch: 03 [ 6802/16204 ( 42%)], Train Loss: 0.05422\n","Epoch: 03 [ 6822/16204 ( 42%)], Train Loss: 0.05441\n","Epoch: 03 [ 6842/16204 ( 42%)], Train Loss: 0.05477\n","Epoch: 03 [ 6862/16204 ( 42%)], Train Loss: 0.05472\n","Epoch: 03 [ 6882/16204 ( 42%)], Train Loss: 0.05457\n","Epoch: 03 [ 6902/16204 ( 43%)], Train Loss: 0.05454\n","Epoch: 03 [ 6922/16204 ( 43%)], Train Loss: 0.05451\n","Epoch: 03 [ 6942/16204 ( 43%)], Train Loss: 0.05441\n","Epoch: 03 [ 6962/16204 ( 43%)], Train Loss: 0.05437\n","Epoch: 03 [ 6982/16204 ( 43%)], Train Loss: 0.05447\n","Epoch: 03 [ 7002/16204 ( 43%)], Train Loss: 0.05446\n","Epoch: 03 [ 7022/16204 ( 43%)], Train Loss: 0.05431\n","Epoch: 03 [ 7042/16204 ( 43%)], Train Loss: 0.05417\n","Epoch: 03 [ 7062/16204 ( 44%)], Train Loss: 0.05405\n","Epoch: 03 [ 7082/16204 ( 44%)], Train Loss: 0.05392\n","Epoch: 03 [ 7102/16204 ( 44%)], Train Loss: 0.05384\n","Epoch: 03 [ 7122/16204 ( 44%)], Train Loss: 0.05387\n","Epoch: 03 [ 7142/16204 ( 44%)], Train Loss: 0.05374\n","Epoch: 03 [ 7162/16204 ( 44%)], Train Loss: 0.05410\n","Epoch: 03 [ 7182/16204 ( 44%)], Train Loss: 0.05398\n","Epoch: 03 [ 7202/16204 ( 44%)], Train Loss: 0.05398\n","Epoch: 03 [ 7222/16204 ( 45%)], Train Loss: 0.05398\n","Epoch: 03 [ 7242/16204 ( 45%)], Train Loss: 0.05393\n","Epoch: 03 [ 7262/16204 ( 45%)], Train Loss: 0.05386\n","Epoch: 03 [ 7282/16204 ( 45%)], Train Loss: 0.05374\n","Epoch: 03 [ 7302/16204 ( 45%)], Train Loss: 0.05377\n","Epoch: 03 [ 7322/16204 ( 45%)], Train Loss: 0.05368\n","Epoch: 03 [ 7342/16204 ( 45%)], Train Loss: 0.05362\n","Epoch: 03 [ 7362/16204 ( 45%)], Train Loss: 0.05369\n","Epoch: 03 [ 7382/16204 ( 46%)], Train Loss: 0.05359\n","Epoch: 03 [ 7402/16204 ( 46%)], Train Loss: 0.05347\n","Epoch: 03 [ 7422/16204 ( 46%)], Train Loss: 0.05337\n","Epoch: 03 [ 7442/16204 ( 46%)], Train Loss: 0.05326\n","Epoch: 03 [ 7462/16204 ( 46%)], Train Loss: 0.05317\n","Epoch: 03 [ 7482/16204 ( 46%)], Train Loss: 0.05303\n","Epoch: 03 [ 7502/16204 ( 46%)], Train Loss: 0.05291\n","Epoch: 03 [ 7522/16204 ( 46%)], Train Loss: 0.05279\n","Epoch: 03 [ 7542/16204 ( 47%)], Train Loss: 0.05267\n","Epoch: 03 [ 7562/16204 ( 47%)], Train Loss: 0.05255\n","Epoch: 03 [ 7582/16204 ( 47%)], Train Loss: 0.05243\n","Epoch: 03 [ 7602/16204 ( 47%)], Train Loss: 0.05236\n","Epoch: 03 [ 7622/16204 ( 47%)], Train Loss: 0.05224\n","Epoch: 03 [ 7642/16204 ( 47%)], Train Loss: 0.05214\n","Epoch: 03 [ 7662/16204 ( 47%)], Train Loss: 0.05222\n","Epoch: 03 [ 7682/16204 ( 47%)], Train Loss: 0.05220\n","Epoch: 03 [ 7702/16204 ( 48%)], Train Loss: 0.05211\n","Epoch: 03 [ 7722/16204 ( 48%)], Train Loss: 0.05201\n","Epoch: 03 [ 7742/16204 ( 48%)], Train Loss: 0.05190\n","Epoch: 03 [ 7762/16204 ( 48%)], Train Loss: 0.05192\n","Epoch: 03 [ 7782/16204 ( 48%)], Train Loss: 0.05186\n","Epoch: 03 [ 7802/16204 ( 48%)], Train Loss: 0.05176\n","Epoch: 03 [ 7822/16204 ( 48%)], Train Loss: 0.05169\n","Epoch: 03 [ 7842/16204 ( 48%)], Train Loss: 0.05173\n","Epoch: 03 [ 7862/16204 ( 49%)], Train Loss: 0.05161\n","Epoch: 03 [ 7882/16204 ( 49%)], Train Loss: 0.05209\n","Epoch: 03 [ 7902/16204 ( 49%)], Train Loss: 0.05214\n","Epoch: 03 [ 7922/16204 ( 49%)], Train Loss: 0.05213\n","Epoch: 03 [ 7942/16204 ( 49%)], Train Loss: 0.05204\n","Epoch: 03 [ 7962/16204 ( 49%)], Train Loss: 0.05197\n","Epoch: 03 [ 7982/16204 ( 49%)], Train Loss: 0.05185\n","Epoch: 03 [ 8002/16204 ( 49%)], Train Loss: 0.05173\n","Epoch: 03 [ 8022/16204 ( 50%)], Train Loss: 0.05165\n","Epoch: 03 [ 8042/16204 ( 50%)], Train Loss: 0.05153\n","Epoch: 03 [ 8062/16204 ( 50%)], Train Loss: 0.05142\n","Epoch: 03 [ 8082/16204 ( 50%)], Train Loss: 0.05133\n","Epoch: 03 [ 8102/16204 ( 50%)], Train Loss: 0.05130\n","Epoch: 03 [ 8122/16204 ( 50%)], Train Loss: 0.05120\n","Epoch: 03 [ 8142/16204 ( 50%)], Train Loss: 0.05111\n","Epoch: 03 [ 8162/16204 ( 50%)], Train Loss: 0.05100\n","Epoch: 03 [ 8182/16204 ( 50%)], Train Loss: 0.05113\n","Epoch: 03 [ 8202/16204 ( 51%)], Train Loss: 0.05121\n","Epoch: 03 [ 8222/16204 ( 51%)], Train Loss: 0.05110\n","Epoch: 03 [ 8242/16204 ( 51%)], Train Loss: 0.05111\n","Epoch: 03 [ 8262/16204 ( 51%)], Train Loss: 0.05104\n","Epoch: 03 [ 8282/16204 ( 51%)], Train Loss: 0.05098\n","Epoch: 03 [ 8302/16204 ( 51%)], Train Loss: 0.05088\n","Epoch: 03 [ 8322/16204 ( 51%)], Train Loss: 0.05084\n","Epoch: 03 [ 8342/16204 ( 51%)], Train Loss: 0.05077\n","Epoch: 03 [ 8362/16204 ( 52%)], Train Loss: 0.05073\n","Epoch: 03 [ 8382/16204 ( 52%)], Train Loss: 0.05065\n","Epoch: 03 [ 8402/16204 ( 52%)], Train Loss: 0.05058\n","Epoch: 03 [ 8422/16204 ( 52%)], Train Loss: 0.05047\n","Epoch: 03 [ 8442/16204 ( 52%)], Train Loss: 0.05039\n","Epoch: 03 [ 8462/16204 ( 52%)], Train Loss: 0.05031\n","Epoch: 03 [ 8482/16204 ( 52%)], Train Loss: 0.05021\n","Epoch: 03 [ 8502/16204 ( 52%)], Train Loss: 0.05036\n","Epoch: 03 [ 8522/16204 ( 53%)], Train Loss: 0.05055\n","Epoch: 03 [ 8542/16204 ( 53%)], Train Loss: 0.05046\n","Epoch: 03 [ 8562/16204 ( 53%)], Train Loss: 0.05039\n","Epoch: 03 [ 8582/16204 ( 53%)], Train Loss: 0.05052\n","Epoch: 03 [ 8602/16204 ( 53%)], Train Loss: 0.05049\n","Epoch: 03 [ 8622/16204 ( 53%)], Train Loss: 0.05044\n","Epoch: 03 [ 8642/16204 ( 53%)], Train Loss: 0.05042\n","Epoch: 03 [ 8662/16204 ( 53%)], Train Loss: 0.05033\n","Epoch: 03 [ 8682/16204 ( 54%)], Train Loss: 0.05023\n","Epoch: 03 [ 8702/16204 ( 54%)], Train Loss: 0.05019\n","Epoch: 03 [ 8722/16204 ( 54%)], Train Loss: 0.05032\n","Epoch: 03 [ 8742/16204 ( 54%)], Train Loss: 0.05020\n","Epoch: 03 [ 8762/16204 ( 54%)], Train Loss: 0.05010\n","Epoch: 03 [ 8782/16204 ( 54%)], Train Loss: 0.04998\n","Epoch: 03 [ 8802/16204 ( 54%)], Train Loss: 0.05015\n","Epoch: 03 [ 8822/16204 ( 54%)], Train Loss: 0.05006\n","Epoch: 03 [ 8842/16204 ( 55%)], Train Loss: 0.05004\n","Epoch: 03 [ 8862/16204 ( 55%)], Train Loss: 0.04996\n","Epoch: 03 [ 8882/16204 ( 55%)], Train Loss: 0.04988\n","Epoch: 03 [ 8902/16204 ( 55%)], Train Loss: 0.05021\n","Epoch: 03 [ 8922/16204 ( 55%)], Train Loss: 0.05017\n","Epoch: 03 [ 8942/16204 ( 55%)], Train Loss: 0.05008\n","Epoch: 03 [ 8962/16204 ( 55%)], Train Loss: 0.05014\n","Epoch: 03 [ 8982/16204 ( 55%)], Train Loss: 0.05034\n","Epoch: 03 [ 9002/16204 ( 56%)], Train Loss: 0.05026\n","Epoch: 03 [ 9022/16204 ( 56%)], Train Loss: 0.05019\n","Epoch: 03 [ 9042/16204 ( 56%)], Train Loss: 0.05020\n","Epoch: 03 [ 9062/16204 ( 56%)], Train Loss: 0.05010\n","Epoch: 03 [ 9082/16204 ( 56%)], Train Loss: 0.05006\n","Epoch: 03 [ 9102/16204 ( 56%)], Train Loss: 0.04999\n","Epoch: 03 [ 9122/16204 ( 56%)], Train Loss: 0.05001\n","Epoch: 03 [ 9142/16204 ( 56%)], Train Loss: 0.05003\n","Epoch: 03 [ 9162/16204 ( 57%)], Train Loss: 0.04995\n","Epoch: 03 [ 9182/16204 ( 57%)], Train Loss: 0.04994\n","Epoch: 03 [ 9202/16204 ( 57%)], Train Loss: 0.04990\n","Epoch: 03 [ 9222/16204 ( 57%)], Train Loss: 0.04990\n","Epoch: 03 [ 9242/16204 ( 57%)], Train Loss: 0.04997\n","Epoch: 03 [ 9262/16204 ( 57%)], Train Loss: 0.04998\n","Epoch: 03 [ 9282/16204 ( 57%)], Train Loss: 0.04988\n","Epoch: 03 [ 9302/16204 ( 57%)], Train Loss: 0.04978\n","Epoch: 03 [ 9322/16204 ( 58%)], Train Loss: 0.04969\n","Epoch: 03 [ 9342/16204 ( 58%)], Train Loss: 0.04962\n","Epoch: 03 [ 9362/16204 ( 58%)], Train Loss: 0.04987\n","Epoch: 03 [ 9382/16204 ( 58%)], Train Loss: 0.04990\n","Epoch: 03 [ 9402/16204 ( 58%)], Train Loss: 0.04982\n","Epoch: 03 [ 9422/16204 ( 58%)], Train Loss: 0.04975\n","Epoch: 03 [ 9442/16204 ( 58%)], Train Loss: 0.04970\n","Epoch: 03 [ 9462/16204 ( 58%)], Train Loss: 0.04969\n","Epoch: 03 [ 9482/16204 ( 59%)], Train Loss: 0.04972\n","Epoch: 03 [ 9502/16204 ( 59%)], Train Loss: 0.04962\n","Epoch: 03 [ 9522/16204 ( 59%)], Train Loss: 0.04972\n","Epoch: 03 [ 9542/16204 ( 59%)], Train Loss: 0.04964\n","Epoch: 03 [ 9562/16204 ( 59%)], Train Loss: 0.04954\n","Epoch: 03 [ 9582/16204 ( 59%)], Train Loss: 0.04947\n","Epoch: 03 [ 9602/16204 ( 59%)], Train Loss: 0.04940\n","Epoch: 03 [ 9622/16204 ( 59%)], Train Loss: 0.04939\n","Epoch: 03 [ 9642/16204 ( 60%)], Train Loss: 0.04930\n","Epoch: 03 [ 9662/16204 ( 60%)], Train Loss: 0.04920\n","Epoch: 03 [ 9682/16204 ( 60%)], Train Loss: 0.04916\n","Epoch: 03 [ 9702/16204 ( 60%)], Train Loss: 0.04908\n","Epoch: 03 [ 9722/16204 ( 60%)], Train Loss: 0.04900\n","Epoch: 03 [ 9742/16204 ( 60%)], Train Loss: 0.04899\n","Epoch: 03 [ 9762/16204 ( 60%)], Train Loss: 0.04889\n","Epoch: 03 [ 9782/16204 ( 60%)], Train Loss: 0.04898\n","Epoch: 03 [ 9802/16204 ( 60%)], Train Loss: 0.04893\n","Epoch: 03 [ 9822/16204 ( 61%)], Train Loss: 0.04889\n","Epoch: 03 [ 9842/16204 ( 61%)], Train Loss: 0.04898\n","Epoch: 03 [ 9862/16204 ( 61%)], Train Loss: 0.04897\n","Epoch: 03 [ 9882/16204 ( 61%)], Train Loss: 0.04892\n","Epoch: 03 [ 9902/16204 ( 61%)], Train Loss: 0.04887\n","Epoch: 03 [ 9922/16204 ( 61%)], Train Loss: 0.04879\n","Epoch: 03 [ 9942/16204 ( 61%)], Train Loss: 0.04871\n","Epoch: 03 [ 9962/16204 ( 61%)], Train Loss: 0.04868\n","Epoch: 03 [ 9982/16204 ( 62%)], Train Loss: 0.04878\n","Epoch: 03 [10002/16204 ( 62%)], Train Loss: 0.04871\n","Epoch: 03 [10022/16204 ( 62%)], Train Loss: 0.04864\n","Epoch: 03 [10042/16204 ( 62%)], Train Loss: 0.04856\n","Epoch: 03 [10062/16204 ( 62%)], Train Loss: 0.04848\n","Epoch: 03 [10082/16204 ( 62%)], Train Loss: 0.04847\n","Epoch: 03 [10102/16204 ( 62%)], Train Loss: 0.04850\n","Epoch: 03 [10122/16204 ( 62%)], Train Loss: 0.04856\n","Epoch: 03 [10142/16204 ( 63%)], Train Loss: 0.04853\n","Epoch: 03 [10162/16204 ( 63%)], Train Loss: 0.04845\n","Epoch: 03 [10182/16204 ( 63%)], Train Loss: 0.04853\n","Epoch: 03 [10202/16204 ( 63%)], Train Loss: 0.04844\n","Epoch: 03 [10222/16204 ( 63%)], Train Loss: 0.04849\n","Epoch: 03 [10242/16204 ( 63%)], Train Loss: 0.04843\n","Epoch: 03 [10262/16204 ( 63%)], Train Loss: 0.04835\n","Epoch: 03 [10282/16204 ( 63%)], Train Loss: 0.04837\n","Epoch: 03 [10302/16204 ( 64%)], Train Loss: 0.04828\n","Epoch: 03 [10322/16204 ( 64%)], Train Loss: 0.04821\n","Epoch: 03 [10342/16204 ( 64%)], Train Loss: 0.04815\n","Epoch: 03 [10362/16204 ( 64%)], Train Loss: 0.04816\n","Epoch: 03 [10382/16204 ( 64%)], Train Loss: 0.04814\n","Epoch: 03 [10402/16204 ( 64%)], Train Loss: 0.04810\n","Epoch: 03 [10422/16204 ( 64%)], Train Loss: 0.04803\n","Epoch: 03 [10442/16204 ( 64%)], Train Loss: 0.04797\n","Epoch: 03 [10462/16204 ( 65%)], Train Loss: 0.04788\n","Epoch: 03 [10482/16204 ( 65%)], Train Loss: 0.04804\n","Epoch: 03 [10502/16204 ( 65%)], Train Loss: 0.04795\n","Epoch: 03 [10522/16204 ( 65%)], Train Loss: 0.04792\n","Epoch: 03 [10542/16204 ( 65%)], Train Loss: 0.04784\n","Epoch: 03 [10562/16204 ( 65%)], Train Loss: 0.04775\n","Epoch: 03 [10582/16204 ( 65%)], Train Loss: 0.04774\n","Epoch: 03 [10602/16204 ( 65%)], Train Loss: 0.04773\n","Epoch: 03 [10622/16204 ( 66%)], Train Loss: 0.04776\n","Epoch: 03 [10642/16204 ( 66%)], Train Loss: 0.04770\n","Epoch: 03 [10662/16204 ( 66%)], Train Loss: 0.04761\n","Epoch: 03 [10682/16204 ( 66%)], Train Loss: 0.04755\n","Epoch: 03 [10702/16204 ( 66%)], Train Loss: 0.04749\n","Epoch: 03 [10722/16204 ( 66%)], Train Loss: 0.04745\n","Epoch: 03 [10742/16204 ( 66%)], Train Loss: 0.04748\n","Epoch: 03 [10762/16204 ( 66%)], Train Loss: 0.04752\n","Epoch: 03 [10782/16204 ( 67%)], Train Loss: 0.04745\n","Epoch: 03 [10802/16204 ( 67%)], Train Loss: 0.04737\n","Epoch: 03 [10822/16204 ( 67%)], Train Loss: 0.04731\n","Epoch: 03 [10842/16204 ( 67%)], Train Loss: 0.04724\n","Epoch: 03 [10862/16204 ( 67%)], Train Loss: 0.04716\n","Epoch: 03 [10882/16204 ( 67%)], Train Loss: 0.04713\n","Epoch: 03 [10902/16204 ( 67%)], Train Loss: 0.04709\n","Epoch: 03 [10922/16204 ( 67%)], Train Loss: 0.04703\n","Epoch: 03 [10942/16204 ( 68%)], Train Loss: 0.04699\n","Epoch: 03 [10962/16204 ( 68%)], Train Loss: 0.04691\n","Epoch: 03 [10982/16204 ( 68%)], Train Loss: 0.04687\n","Epoch: 03 [11002/16204 ( 68%)], Train Loss: 0.04687\n","Epoch: 03 [11022/16204 ( 68%)], Train Loss: 0.04680\n","Epoch: 03 [11042/16204 ( 68%)], Train Loss: 0.04673\n","Epoch: 03 [11062/16204 ( 68%)], Train Loss: 0.04669\n","Epoch: 03 [11082/16204 ( 68%)], Train Loss: 0.04670\n","Epoch: 03 [11102/16204 ( 69%)], Train Loss: 0.04664\n","Epoch: 03 [11122/16204 ( 69%)], Train Loss: 0.04662\n","Epoch: 03 [11142/16204 ( 69%)], Train Loss: 0.04654\n","Epoch: 03 [11162/16204 ( 69%)], Train Loss: 0.04647\n","Epoch: 03 [11182/16204 ( 69%)], Train Loss: 0.04656\n","Epoch: 03 [11202/16204 ( 69%)], Train Loss: 0.04677\n","Epoch: 03 [11222/16204 ( 69%)], Train Loss: 0.04670\n","Epoch: 03 [11242/16204 ( 69%)], Train Loss: 0.04677\n","Epoch: 03 [11262/16204 ( 70%)], Train Loss: 0.04678\n","Epoch: 03 [11282/16204 ( 70%)], Train Loss: 0.04670\n","Epoch: 03 [11302/16204 ( 70%)], Train Loss: 0.04663\n","Epoch: 03 [11322/16204 ( 70%)], Train Loss: 0.04663\n","Epoch: 03 [11342/16204 ( 70%)], Train Loss: 0.04663\n","Epoch: 03 [11362/16204 ( 70%)], Train Loss: 0.04665\n","Epoch: 03 [11382/16204 ( 70%)], Train Loss: 0.04661\n","Epoch: 03 [11402/16204 ( 70%)], Train Loss: 0.04654\n","Epoch: 03 [11422/16204 ( 70%)], Train Loss: 0.04661\n","Epoch: 03 [11442/16204 ( 71%)], Train Loss: 0.04660\n","Epoch: 03 [11462/16204 ( 71%)], Train Loss: 0.04659\n","Epoch: 03 [11482/16204 ( 71%)], Train Loss: 0.04655\n","Epoch: 03 [11502/16204 ( 71%)], Train Loss: 0.04651\n","Epoch: 03 [11522/16204 ( 71%)], Train Loss: 0.04651\n","Epoch: 03 [11542/16204 ( 71%)], Train Loss: 0.04645\n","Epoch: 03 [11562/16204 ( 71%)], Train Loss: 0.04641\n","Epoch: 03 [11582/16204 ( 71%)], Train Loss: 0.04640\n","Epoch: 03 [11602/16204 ( 72%)], Train Loss: 0.04638\n","Epoch: 03 [11622/16204 ( 72%)], Train Loss: 0.04657\n","Epoch: 03 [11642/16204 ( 72%)], Train Loss: 0.04660\n","Epoch: 03 [11662/16204 ( 72%)], Train Loss: 0.04663\n","Epoch: 03 [11682/16204 ( 72%)], Train Loss: 0.04662\n","Epoch: 03 [11702/16204 ( 72%)], Train Loss: 0.04654\n","Epoch: 03 [11722/16204 ( 72%)], Train Loss: 0.04649\n","Epoch: 03 [11742/16204 ( 72%)], Train Loss: 0.04646\n","Epoch: 03 [11762/16204 ( 73%)], Train Loss: 0.04649\n","Epoch: 03 [11782/16204 ( 73%)], Train Loss: 0.04654\n","Epoch: 03 [11802/16204 ( 73%)], Train Loss: 0.04646\n","Epoch: 03 [11822/16204 ( 73%)], Train Loss: 0.04647\n","Epoch: 03 [11842/16204 ( 73%)], Train Loss: 0.04647\n","Epoch: 03 [11862/16204 ( 73%)], Train Loss: 0.04658\n","Epoch: 03 [11882/16204 ( 73%)], Train Loss: 0.04650\n","Epoch: 03 [11902/16204 ( 73%)], Train Loss: 0.04644\n","Epoch: 03 [11922/16204 ( 74%)], Train Loss: 0.04637\n","Epoch: 03 [11942/16204 ( 74%)], Train Loss: 0.04630\n","Epoch: 03 [11962/16204 ( 74%)], Train Loss: 0.04623\n","Epoch: 03 [11982/16204 ( 74%)], Train Loss: 0.04617\n","Epoch: 03 [12002/16204 ( 74%)], Train Loss: 0.04615\n","Epoch: 03 [12022/16204 ( 74%)], Train Loss: 0.04614\n","Epoch: 03 [12042/16204 ( 74%)], Train Loss: 0.04617\n","Epoch: 03 [12062/16204 ( 74%)], Train Loss: 0.04615\n","Epoch: 03 [12082/16204 ( 75%)], Train Loss: 0.04608\n","Epoch: 03 [12102/16204 ( 75%)], Train Loss: 0.04603\n","Epoch: 03 [12122/16204 ( 75%)], Train Loss: 0.04599\n","Epoch: 03 [12142/16204 ( 75%)], Train Loss: 0.04600\n","Epoch: 03 [12162/16204 ( 75%)], Train Loss: 0.04593\n","Epoch: 03 [12182/16204 ( 75%)], Train Loss: 0.04590\n","Epoch: 03 [12202/16204 ( 75%)], Train Loss: 0.04582\n","Epoch: 03 [12222/16204 ( 75%)], Train Loss: 0.04596\n","Epoch: 03 [12242/16204 ( 76%)], Train Loss: 0.04590\n","Epoch: 03 [12262/16204 ( 76%)], Train Loss: 0.04583\n","Epoch: 03 [12282/16204 ( 76%)], Train Loss: 0.04577\n","Epoch: 03 [12302/16204 ( 76%)], Train Loss: 0.04575\n","Epoch: 03 [12322/16204 ( 76%)], Train Loss: 0.04571\n","Epoch: 03 [12342/16204 ( 76%)], Train Loss: 0.04573\n","Epoch: 03 [12362/16204 ( 76%)], Train Loss: 0.04566\n","Epoch: 03 [12382/16204 ( 76%)], Train Loss: 0.04560\n","Epoch: 03 [12402/16204 ( 77%)], Train Loss: 0.04553\n","Epoch: 03 [12422/16204 ( 77%)], Train Loss: 0.04558\n","Epoch: 03 [12442/16204 ( 77%)], Train Loss: 0.04553\n","Epoch: 03 [12462/16204 ( 77%)], Train Loss: 0.04556\n","Epoch: 03 [12482/16204 ( 77%)], Train Loss: 0.04553\n","Epoch: 03 [12502/16204 ( 77%)], Train Loss: 0.04570\n","Epoch: 03 [12522/16204 ( 77%)], Train Loss: 0.04564\n","Epoch: 03 [12542/16204 ( 77%)], Train Loss: 0.04563\n","Epoch: 03 [12562/16204 ( 78%)], Train Loss: 0.04557\n","Epoch: 03 [12582/16204 ( 78%)], Train Loss: 0.04554\n","Epoch: 03 [12602/16204 ( 78%)], Train Loss: 0.04552\n","Epoch: 03 [12622/16204 ( 78%)], Train Loss: 0.04550\n","Epoch: 03 [12642/16204 ( 78%)], Train Loss: 0.04558\n","Epoch: 03 [12662/16204 ( 78%)], Train Loss: 0.04552\n","Epoch: 03 [12682/16204 ( 78%)], Train Loss: 0.04556\n","Epoch: 03 [12702/16204 ( 78%)], Train Loss: 0.04551\n","Epoch: 03 [12722/16204 ( 79%)], Train Loss: 0.04551\n","Epoch: 03 [12742/16204 ( 79%)], Train Loss: 0.04549\n","Epoch: 03 [12762/16204 ( 79%)], Train Loss: 0.04550\n","Epoch: 03 [12782/16204 ( 79%)], Train Loss: 0.04544\n","Epoch: 03 [12802/16204 ( 79%)], Train Loss: 0.04541\n","Epoch: 03 [12822/16204 ( 79%)], Train Loss: 0.04542\n","Epoch: 03 [12842/16204 ( 79%)], Train Loss: 0.04544\n","Epoch: 03 [12862/16204 ( 79%)], Train Loss: 0.04543\n","Epoch: 03 [12882/16204 ( 79%)], Train Loss: 0.04538\n","Epoch: 03 [12902/16204 ( 80%)], Train Loss: 0.04534\n","Epoch: 03 [12922/16204 ( 80%)], Train Loss: 0.04529\n","Epoch: 03 [12942/16204 ( 80%)], Train Loss: 0.04539\n","Epoch: 03 [12962/16204 ( 80%)], Train Loss: 0.04533\n","Epoch: 03 [12982/16204 ( 80%)], Train Loss: 0.04529\n","Epoch: 03 [13002/16204 ( 80%)], Train Loss: 0.04523\n","Epoch: 03 [13022/16204 ( 80%)], Train Loss: 0.04526\n","Epoch: 03 [13042/16204 ( 80%)], Train Loss: 0.04523\n","Epoch: 03 [13062/16204 ( 81%)], Train Loss: 0.04534\n","Epoch: 03 [13082/16204 ( 81%)], Train Loss: 0.04536\n","Epoch: 03 [13102/16204 ( 81%)], Train Loss: 0.04532\n","Epoch: 03 [13122/16204 ( 81%)], Train Loss: 0.04530\n","Epoch: 03 [13142/16204 ( 81%)], Train Loss: 0.04524\n","Epoch: 03 [13162/16204 ( 81%)], Train Loss: 0.04537\n","Epoch: 03 [13182/16204 ( 81%)], Train Loss: 0.04537\n","Epoch: 03 [13202/16204 ( 81%)], Train Loss: 0.04534\n","Epoch: 03 [13222/16204 ( 82%)], Train Loss: 0.04529\n","Epoch: 03 [13242/16204 ( 82%)], Train Loss: 0.04553\n","Epoch: 03 [13262/16204 ( 82%)], Train Loss: 0.04558\n","Epoch: 03 [13282/16204 ( 82%)], Train Loss: 0.04552\n","Epoch: 03 [13302/16204 ( 82%)], Train Loss: 0.04562\n","Epoch: 03 [13322/16204 ( 82%)], Train Loss: 0.04556\n","Epoch: 03 [13342/16204 ( 82%)], Train Loss: 0.04552\n","Epoch: 03 [13362/16204 ( 82%)], Train Loss: 0.04549\n","Epoch: 03 [13382/16204 ( 83%)], Train Loss: 0.04543\n","Epoch: 03 [13402/16204 ( 83%)], Train Loss: 0.04539\n","Epoch: 03 [13422/16204 ( 83%)], Train Loss: 0.04539\n","Epoch: 03 [13442/16204 ( 83%)], Train Loss: 0.04532\n","Epoch: 03 [13462/16204 ( 83%)], Train Loss: 0.04528\n","Epoch: 03 [13482/16204 ( 83%)], Train Loss: 0.04522\n","Epoch: 03 [13502/16204 ( 83%)], Train Loss: 0.04516\n","Epoch: 03 [13522/16204 ( 83%)], Train Loss: 0.04522\n","Epoch: 03 [13542/16204 ( 84%)], Train Loss: 0.04516\n","Epoch: 03 [13562/16204 ( 84%)], Train Loss: 0.04511\n","Epoch: 03 [13582/16204 ( 84%)], Train Loss: 0.04505\n","Epoch: 03 [13602/16204 ( 84%)], Train Loss: 0.04512\n","Epoch: 03 [13622/16204 ( 84%)], Train Loss: 0.04508\n","Epoch: 03 [13642/16204 ( 84%)], Train Loss: 0.04503\n","Epoch: 03 [13662/16204 ( 84%)], Train Loss: 0.04497\n","Epoch: 03 [13682/16204 ( 84%)], Train Loss: 0.04492\n","Epoch: 03 [13702/16204 ( 85%)], Train Loss: 0.04494\n","Epoch: 03 [13722/16204 ( 85%)], Train Loss: 0.04512\n","Epoch: 03 [13742/16204 ( 85%)], Train Loss: 0.04507\n","Epoch: 03 [13762/16204 ( 85%)], Train Loss: 0.04512\n","Epoch: 03 [13782/16204 ( 85%)], Train Loss: 0.04513\n","Epoch: 03 [13802/16204 ( 85%)], Train Loss: 0.04508\n","Epoch: 03 [13822/16204 ( 85%)], Train Loss: 0.04510\n","Epoch: 03 [13842/16204 ( 85%)], Train Loss: 0.04506\n","Epoch: 03 [13862/16204 ( 86%)], Train Loss: 0.04507\n","Epoch: 03 [13882/16204 ( 86%)], Train Loss: 0.04503\n","Epoch: 03 [13902/16204 ( 86%)], Train Loss: 0.04498\n","Epoch: 03 [13922/16204 ( 86%)], Train Loss: 0.04499\n","Epoch: 03 [13942/16204 ( 86%)], Train Loss: 0.04501\n","Epoch: 03 [13962/16204 ( 86%)], Train Loss: 0.04495\n","Epoch: 03 [13982/16204 ( 86%)], Train Loss: 0.04493\n","Epoch: 03 [14002/16204 ( 86%)], Train Loss: 0.04489\n","Epoch: 03 [14022/16204 ( 87%)], Train Loss: 0.04485\n","Epoch: 03 [14042/16204 ( 87%)], Train Loss: 0.04488\n","Epoch: 03 [14062/16204 ( 87%)], Train Loss: 0.04483\n","Epoch: 03 [14082/16204 ( 87%)], Train Loss: 0.04492\n","Epoch: 03 [14102/16204 ( 87%)], Train Loss: 0.04489\n","Epoch: 03 [14122/16204 ( 87%)], Train Loss: 0.04483\n","Epoch: 03 [14142/16204 ( 87%)], Train Loss: 0.04492\n","Epoch: 03 [14162/16204 ( 87%)], Train Loss: 0.04492\n","Epoch: 03 [14182/16204 ( 88%)], Train Loss: 0.04486\n","Epoch: 03 [14202/16204 ( 88%)], Train Loss: 0.04486\n","Epoch: 03 [14222/16204 ( 88%)], Train Loss: 0.04484\n","Epoch: 03 [14242/16204 ( 88%)], Train Loss: 0.04484\n","Epoch: 03 [14262/16204 ( 88%)], Train Loss: 0.04483\n","Epoch: 03 [14282/16204 ( 88%)], Train Loss: 0.04479\n","Epoch: 03 [14302/16204 ( 88%)], Train Loss: 0.04479\n","Epoch: 03 [14322/16204 ( 88%)], Train Loss: 0.04475\n","Epoch: 03 [14342/16204 ( 89%)], Train Loss: 0.04471\n","Epoch: 03 [14362/16204 ( 89%)], Train Loss: 0.04466\n","Epoch: 03 [14382/16204 ( 89%)], Train Loss: 0.04460\n","Epoch: 03 [14402/16204 ( 89%)], Train Loss: 0.04459\n","Epoch: 03 [14422/16204 ( 89%)], Train Loss: 0.04454\n","Epoch: 03 [14442/16204 ( 89%)], Train Loss: 0.04449\n","Epoch: 03 [14462/16204 ( 89%)], Train Loss: 0.04444\n","Epoch: 03 [14482/16204 ( 89%)], Train Loss: 0.04439\n","Epoch: 03 [14502/16204 ( 89%)], Train Loss: 0.04438\n","Epoch: 03 [14522/16204 ( 90%)], Train Loss: 0.04450\n","Epoch: 03 [14542/16204 ( 90%)], Train Loss: 0.04449\n","Epoch: 03 [14562/16204 ( 90%)], Train Loss: 0.04443\n","Epoch: 03 [14582/16204 ( 90%)], Train Loss: 0.04438\n","Epoch: 03 [14602/16204 ( 90%)], Train Loss: 0.04439\n","Epoch: 03 [14622/16204 ( 90%)], Train Loss: 0.04441\n","Epoch: 03 [14642/16204 ( 90%)], Train Loss: 0.04437\n","Epoch: 03 [14662/16204 ( 90%)], Train Loss: 0.04435\n","Epoch: 03 [14682/16204 ( 91%)], Train Loss: 0.04429\n","Epoch: 03 [14702/16204 ( 91%)], Train Loss: 0.04442\n","Epoch: 03 [14722/16204 ( 91%)], Train Loss: 0.04437\n","Epoch: 03 [14742/16204 ( 91%)], Train Loss: 0.04448\n","Epoch: 03 [14762/16204 ( 91%)], Train Loss: 0.04446\n","Epoch: 03 [14782/16204 ( 91%)], Train Loss: 0.04445\n","Epoch: 03 [14802/16204 ( 91%)], Train Loss: 0.04439\n","Epoch: 03 [14822/16204 ( 91%)], Train Loss: 0.04435\n","Epoch: 03 [14842/16204 ( 92%)], Train Loss: 0.04430\n","Epoch: 03 [14862/16204 ( 92%)], Train Loss: 0.04424\n","Epoch: 03 [14882/16204 ( 92%)], Train Loss: 0.04420\n","Epoch: 03 [14902/16204 ( 92%)], Train Loss: 0.04419\n","Epoch: 03 [14922/16204 ( 92%)], Train Loss: 0.04417\n","Epoch: 03 [14942/16204 ( 92%)], Train Loss: 0.04412\n","Epoch: 03 [14962/16204 ( 92%)], Train Loss: 0.04407\n","Epoch: 03 [14982/16204 ( 92%)], Train Loss: 0.04421\n","Epoch: 03 [15002/16204 ( 93%)], Train Loss: 0.04416\n","Epoch: 03 [15022/16204 ( 93%)], Train Loss: 0.04411\n","Epoch: 03 [15042/16204 ( 93%)], Train Loss: 0.04409\n","Epoch: 03 [15062/16204 ( 93%)], Train Loss: 0.04404\n","Epoch: 03 [15082/16204 ( 93%)], Train Loss: 0.04401\n","Epoch: 03 [15102/16204 ( 93%)], Train Loss: 0.04401\n","Epoch: 03 [15122/16204 ( 93%)], Train Loss: 0.04395\n","Epoch: 03 [15142/16204 ( 93%)], Train Loss: 0.04392\n","Epoch: 03 [15162/16204 ( 94%)], Train Loss: 0.04391\n","Epoch: 03 [15182/16204 ( 94%)], Train Loss: 0.04385\n","Epoch: 03 [15202/16204 ( 94%)], Train Loss: 0.04380\n","Epoch: 03 [15222/16204 ( 94%)], Train Loss: 0.04375\n","Epoch: 03 [15242/16204 ( 94%)], Train Loss: 0.04372\n","Epoch: 03 [15262/16204 ( 94%)], Train Loss: 0.04368\n","Epoch: 03 [15282/16204 ( 94%)], Train Loss: 0.04362\n","Epoch: 03 [15302/16204 ( 94%)], Train Loss: 0.04358\n","Epoch: 03 [15322/16204 ( 95%)], Train Loss: 0.04357\n","Epoch: 03 [15342/16204 ( 95%)], Train Loss: 0.04353\n","Epoch: 03 [15362/16204 ( 95%)], Train Loss: 0.04348\n","Epoch: 03 [15382/16204 ( 95%)], Train Loss: 0.04345\n","Epoch: 03 [15402/16204 ( 95%)], Train Loss: 0.04344\n","Epoch: 03 [15422/16204 ( 95%)], Train Loss: 0.04342\n","Epoch: 03 [15442/16204 ( 95%)], Train Loss: 0.04341\n","Epoch: 03 [15462/16204 ( 95%)], Train Loss: 0.04337\n","Epoch: 03 [15482/16204 ( 96%)], Train Loss: 0.04332\n","Epoch: 03 [15502/16204 ( 96%)], Train Loss: 0.04328\n","Epoch: 03 [15522/16204 ( 96%)], Train Loss: 0.04326\n","Epoch: 03 [15542/16204 ( 96%)], Train Loss: 0.04323\n","Epoch: 03 [15562/16204 ( 96%)], Train Loss: 0.04318\n","Epoch: 03 [15582/16204 ( 96%)], Train Loss: 0.04312\n","Epoch: 03 [15602/16204 ( 96%)], Train Loss: 0.04307\n","Epoch: 03 [15622/16204 ( 96%)], Train Loss: 0.04303\n","Epoch: 03 [15642/16204 ( 97%)], Train Loss: 0.04298\n","Epoch: 03 [15662/16204 ( 97%)], Train Loss: 0.04296\n","Epoch: 03 [15682/16204 ( 97%)], Train Loss: 0.04299\n","Epoch: 03 [15702/16204 ( 97%)], Train Loss: 0.04301\n","Epoch: 03 [15722/16204 ( 97%)], Train Loss: 0.04309\n","Epoch: 03 [15742/16204 ( 97%)], Train Loss: 0.04319\n","Epoch: 03 [15762/16204 ( 97%)], Train Loss: 0.04326\n","Epoch: 03 [15782/16204 ( 97%)], Train Loss: 0.04324\n","Epoch: 03 [15802/16204 ( 98%)], Train Loss: 0.04324\n","Epoch: 03 [15822/16204 ( 98%)], Train Loss: 0.04320\n","Epoch: 03 [15842/16204 ( 98%)], Train Loss: 0.04315\n","Epoch: 03 [15862/16204 ( 98%)], Train Loss: 0.04315\n","Epoch: 03 [15882/16204 ( 98%)], Train Loss: 0.04319\n","Epoch: 03 [15902/16204 ( 98%)], Train Loss: 0.04314\n","Epoch: 03 [15922/16204 ( 98%)], Train Loss: 0.04309\n","Epoch: 03 [15942/16204 ( 98%)], Train Loss: 0.04311\n","Epoch: 03 [15962/16204 ( 99%)], Train Loss: 0.04309\n","Epoch: 03 [15982/16204 ( 99%)], Train Loss: 0.04309\n","Epoch: 03 [16002/16204 ( 99%)], Train Loss: 0.04323\n","Epoch: 03 [16022/16204 ( 99%)], Train Loss: 0.04319\n","Epoch: 03 [16042/16204 ( 99%)], Train Loss: 0.04321\n","Epoch: 03 [16062/16204 ( 99%)], Train Loss: 0.04325\n","Epoch: 03 [16082/16204 ( 99%)], Train Loss: 0.04323\n","Epoch: 03 [16102/16204 ( 99%)], Train Loss: 0.04321\n","Epoch: 03 [16122/16204 ( 99%)], Train Loss: 0.04324\n","Epoch: 03 [16142/16204 (100%)], Train Loss: 0.04319\n","Epoch: 03 [16162/16204 (100%)], Train Loss: 0.04314\n","Epoch: 03 [16182/16204 (100%)], Train Loss: 0.04319\n","Epoch: 03 [16202/16204 (100%)], Train Loss: 0.04316\n","Epoch: 03 [16204/16204 (100%)], Train Loss: 0.04316\n","----Validation Results Summary----\n","Epoch: [3] Valid Loss: 0.00000\n","\n","Epoch: 04 [    2/16204 (  0%)], Train Loss: 0.00624\n","Epoch: 04 [   22/16204 (  0%)], Train Loss: 0.04142\n","Epoch: 04 [   42/16204 (  0%)], Train Loss: 0.04132\n","Epoch: 04 [   62/16204 (  0%)], Train Loss: 0.05073\n","Epoch: 04 [   82/16204 (  1%)], Train Loss: 0.05917\n","Epoch: 04 [  102/16204 (  1%)], Train Loss: 0.04861\n","Epoch: 04 [  122/16204 (  1%)], Train Loss: 0.04751\n","Epoch: 04 [  142/16204 (  1%)], Train Loss: 0.04157\n","Epoch: 04 [  162/16204 (  1%)], Train Loss: 0.03690\n","Epoch: 04 [  182/16204 (  1%)], Train Loss: 0.04075\n","Epoch: 04 [  202/16204 (  1%)], Train Loss: 0.03696\n","Epoch: 04 [  222/16204 (  1%)], Train Loss: 0.03394\n","Epoch: 04 [  242/16204 (  1%)], Train Loss: 0.03145\n","Epoch: 04 [  262/16204 (  2%)], Train Loss: 0.03227\n","Epoch: 04 [  282/16204 (  2%)], Train Loss: 0.03177\n","Epoch: 04 [  302/16204 (  2%)], Train Loss: 0.03148\n","Epoch: 04 [  322/16204 (  2%)], Train Loss: 0.02999\n","Epoch: 04 [  342/16204 (  2%)], Train Loss: 0.02902\n","Epoch: 04 [  362/16204 (  2%)], Train Loss: 0.02907\n","Epoch: 04 [  382/16204 (  2%)], Train Loss: 0.02976\n","Epoch: 04 [  402/16204 (  2%)], Train Loss: 0.02900\n","Epoch: 04 [  422/16204 (  3%)], Train Loss: 0.03054\n","Epoch: 04 [  442/16204 (  3%)], Train Loss: 0.02918\n","Epoch: 04 [  462/16204 (  3%)], Train Loss: 0.02836\n","Epoch: 04 [  482/16204 (  3%)], Train Loss: 0.02763\n","Epoch: 04 [  502/16204 (  3%)], Train Loss: 0.02727\n","Epoch: 04 [  522/16204 (  3%)], Train Loss: 0.02691\n","Epoch: 04 [  542/16204 (  3%)], Train Loss: 0.02642\n","Epoch: 04 [  562/16204 (  3%)], Train Loss: 0.02554\n","Epoch: 04 [  582/16204 (  4%)], Train Loss: 0.02488\n","Epoch: 04 [  602/16204 (  4%)], Train Loss: 0.02661\n","Epoch: 04 [  622/16204 (  4%)], Train Loss: 0.02619\n","Epoch: 04 [  642/16204 (  4%)], Train Loss: 0.02757\n","Epoch: 04 [  662/16204 (  4%)], Train Loss: 0.02810\n","Epoch: 04 [  682/16204 (  4%)], Train Loss: 0.02753\n","Epoch: 04 [  702/16204 (  4%)], Train Loss: 0.02687\n","Epoch: 04 [  722/16204 (  4%)], Train Loss: 0.02688\n","Epoch: 04 [  742/16204 (  5%)], Train Loss: 0.02675\n","Epoch: 04 [  762/16204 (  5%)], Train Loss: 0.02621\n","Epoch: 04 [  782/16204 (  5%)], Train Loss: 0.02568\n","Epoch: 04 [  802/16204 (  5%)], Train Loss: 0.02538\n","Epoch: 04 [  822/16204 (  5%)], Train Loss: 0.02494\n","Epoch: 04 [  842/16204 (  5%)], Train Loss: 0.02637\n","Epoch: 04 [  862/16204 (  5%)], Train Loss: 0.03127\n","Epoch: 04 [  882/16204 (  5%)], Train Loss: 0.03074\n","Epoch: 04 [  902/16204 (  6%)], Train Loss: 0.03021\n","Epoch: 04 [  922/16204 (  6%)], Train Loss: 0.02977\n","Epoch: 04 [  942/16204 (  6%)], Train Loss: 0.02916\n","Epoch: 04 [  962/16204 (  6%)], Train Loss: 0.02980\n","Epoch: 04 [  982/16204 (  6%)], Train Loss: 0.02978\n","Epoch: 04 [ 1002/16204 (  6%)], Train Loss: 0.02950\n","Epoch: 04 [ 1022/16204 (  6%)], Train Loss: 0.02902\n","Epoch: 04 [ 1042/16204 (  6%)], Train Loss: 0.02881\n","Epoch: 04 [ 1062/16204 (  7%)], Train Loss: 0.02843\n","Epoch: 04 [ 1082/16204 (  7%)], Train Loss: 0.02869\n","Epoch: 04 [ 1102/16204 (  7%)], Train Loss: 0.03006\n","Epoch: 04 [ 1122/16204 (  7%)], Train Loss: 0.02974\n","Epoch: 04 [ 1142/16204 (  7%)], Train Loss: 0.02972\n","Epoch: 04 [ 1162/16204 (  7%)], Train Loss: 0.02922\n","Epoch: 04 [ 1182/16204 (  7%)], Train Loss: 0.02890\n","Epoch: 04 [ 1202/16204 (  7%)], Train Loss: 0.02885\n","Epoch: 04 [ 1222/16204 (  8%)], Train Loss: 0.02853\n","Epoch: 04 [ 1242/16204 (  8%)], Train Loss: 0.02840\n","Epoch: 04 [ 1262/16204 (  8%)], Train Loss: 0.02826\n","Epoch: 04 [ 1282/16204 (  8%)], Train Loss: 0.02794\n","Epoch: 04 [ 1302/16204 (  8%)], Train Loss: 0.02834\n","Epoch: 04 [ 1322/16204 (  8%)], Train Loss: 0.02832\n","Epoch: 04 [ 1342/16204 (  8%)], Train Loss: 0.02815\n","Epoch: 04 [ 1362/16204 (  8%)], Train Loss: 0.02782\n","Epoch: 04 [ 1382/16204 (  9%)], Train Loss: 0.02830\n","Epoch: 04 [ 1402/16204 (  9%)], Train Loss: 0.02794\n","Epoch: 04 [ 1422/16204 (  9%)], Train Loss: 0.02787\n","Epoch: 04 [ 1442/16204 (  9%)], Train Loss: 0.02753\n","Epoch: 04 [ 1462/16204 (  9%)], Train Loss: 0.02838\n","Epoch: 04 [ 1482/16204 (  9%)], Train Loss: 0.02803\n","Epoch: 04 [ 1502/16204 (  9%)], Train Loss: 0.02839\n","Epoch: 04 [ 1522/16204 (  9%)], Train Loss: 0.02828\n","Epoch: 04 [ 1542/16204 ( 10%)], Train Loss: 0.02798\n","Epoch: 04 [ 1562/16204 ( 10%)], Train Loss: 0.02770\n","Epoch: 04 [ 1582/16204 ( 10%)], Train Loss: 0.02755\n","Epoch: 04 [ 1602/16204 ( 10%)], Train Loss: 0.02727\n","Epoch: 04 [ 1622/16204 ( 10%)], Train Loss: 0.02852\n","Epoch: 04 [ 1642/16204 ( 10%)], Train Loss: 0.02830\n","Epoch: 04 [ 1662/16204 ( 10%)], Train Loss: 0.02804\n","Epoch: 04 [ 1682/16204 ( 10%)], Train Loss: 0.02772\n","Epoch: 04 [ 1702/16204 ( 11%)], Train Loss: 0.02779\n","Epoch: 04 [ 1722/16204 ( 11%)], Train Loss: 0.02752\n","Epoch: 04 [ 1742/16204 ( 11%)], Train Loss: 0.02726\n","Epoch: 04 [ 1762/16204 ( 11%)], Train Loss: 0.02715\n","Epoch: 04 [ 1782/16204 ( 11%)], Train Loss: 0.02710\n","Epoch: 04 [ 1802/16204 ( 11%)], Train Loss: 0.02697\n","Epoch: 04 [ 1822/16204 ( 11%)], Train Loss: 0.02741\n","Epoch: 04 [ 1842/16204 ( 11%)], Train Loss: 0.02735\n","Epoch: 04 [ 1862/16204 ( 11%)], Train Loss: 0.02735\n","Epoch: 04 [ 1882/16204 ( 12%)], Train Loss: 0.02710\n","Epoch: 04 [ 1902/16204 ( 12%)], Train Loss: 0.02684\n","Epoch: 04 [ 1922/16204 ( 12%)], Train Loss: 0.02709\n","Epoch: 04 [ 1942/16204 ( 12%)], Train Loss: 0.02691\n","Epoch: 04 [ 1962/16204 ( 12%)], Train Loss: 0.02675\n","Epoch: 04 [ 1982/16204 ( 12%)], Train Loss: 0.02650\n","Epoch: 04 [ 2002/16204 ( 12%)], Train Loss: 0.02634\n","Epoch: 04 [ 2022/16204 ( 12%)], Train Loss: 0.02634\n","Epoch: 04 [ 2042/16204 ( 13%)], Train Loss: 0.02616\n","Epoch: 04 [ 2062/16204 ( 13%)], Train Loss: 0.02593\n","Epoch: 04 [ 2082/16204 ( 13%)], Train Loss: 0.02586\n","Epoch: 04 [ 2102/16204 ( 13%)], Train Loss: 0.02602\n","Epoch: 04 [ 2122/16204 ( 13%)], Train Loss: 0.02578\n","Epoch: 04 [ 2142/16204 ( 13%)], Train Loss: 0.02557\n","Epoch: 04 [ 2162/16204 ( 13%)], Train Loss: 0.02537\n","Epoch: 04 [ 2182/16204 ( 13%)], Train Loss: 0.02526\n","Epoch: 04 [ 2202/16204 ( 14%)], Train Loss: 0.02521\n","Epoch: 04 [ 2222/16204 ( 14%)], Train Loss: 0.02500\n","Epoch: 04 [ 2242/16204 ( 14%)], Train Loss: 0.02547\n","Epoch: 04 [ 2262/16204 ( 14%)], Train Loss: 0.02526\n","Epoch: 04 [ 2282/16204 ( 14%)], Train Loss: 0.02518\n","Epoch: 04 [ 2302/16204 ( 14%)], Train Loss: 0.02499\n","Epoch: 04 [ 2322/16204 ( 14%)], Train Loss: 0.02520\n","Epoch: 04 [ 2342/16204 ( 14%)], Train Loss: 0.02499\n","Epoch: 04 [ 2362/16204 ( 15%)], Train Loss: 0.02508\n","Epoch: 04 [ 2382/16204 ( 15%)], Train Loss: 0.02488\n","Epoch: 04 [ 2402/16204 ( 15%)], Train Loss: 0.02471\n","Epoch: 04 [ 2422/16204 ( 15%)], Train Loss: 0.02481\n","Epoch: 04 [ 2442/16204 ( 15%)], Train Loss: 0.02477\n","Epoch: 04 [ 2462/16204 ( 15%)], Train Loss: 0.02458\n","Epoch: 04 [ 2482/16204 ( 15%)], Train Loss: 0.02456\n","Epoch: 04 [ 2502/16204 ( 15%)], Train Loss: 0.02439\n","Epoch: 04 [ 2522/16204 ( 16%)], Train Loss: 0.02422\n","Epoch: 04 [ 2542/16204 ( 16%)], Train Loss: 0.02404\n","Epoch: 04 [ 2562/16204 ( 16%)], Train Loss: 0.02386\n","Epoch: 04 [ 2582/16204 ( 16%)], Train Loss: 0.02396\n","Epoch: 04 [ 2602/16204 ( 16%)], Train Loss: 0.02390\n","Epoch: 04 [ 2622/16204 ( 16%)], Train Loss: 0.02372\n","Epoch: 04 [ 2642/16204 ( 16%)], Train Loss: 0.02364\n","Epoch: 04 [ 2662/16204 ( 16%)], Train Loss: 0.02354\n","Epoch: 04 [ 2682/16204 ( 17%)], Train Loss: 0.02340\n","Epoch: 04 [ 2702/16204 ( 17%)], Train Loss: 0.02323\n","Epoch: 04 [ 2722/16204 ( 17%)], Train Loss: 0.02348\n","Epoch: 04 [ 2742/16204 ( 17%)], Train Loss: 0.02331\n","Epoch: 04 [ 2762/16204 ( 17%)], Train Loss: 0.02317\n","Epoch: 04 [ 2782/16204 ( 17%)], Train Loss: 0.02303\n","Epoch: 04 [ 2802/16204 ( 17%)], Train Loss: 0.02292\n","Epoch: 04 [ 2822/16204 ( 17%)], Train Loss: 0.02277\n","Epoch: 04 [ 2842/16204 ( 18%)], Train Loss: 0.02365\n","Epoch: 04 [ 2862/16204 ( 18%)], Train Loss: 0.02426\n","Epoch: 04 [ 2882/16204 ( 18%)], Train Loss: 0.02423\n","Epoch: 04 [ 2902/16204 ( 18%)], Train Loss: 0.02408\n","Epoch: 04 [ 2922/16204 ( 18%)], Train Loss: 0.02396\n","Epoch: 04 [ 2942/16204 ( 18%)], Train Loss: 0.02385\n","Epoch: 04 [ 2962/16204 ( 18%)], Train Loss: 0.02386\n","Epoch: 04 [ 2982/16204 ( 18%)], Train Loss: 0.02403\n","Epoch: 04 [ 3002/16204 ( 19%)], Train Loss: 0.02395\n","Epoch: 04 [ 3022/16204 ( 19%)], Train Loss: 0.02382\n","Epoch: 04 [ 3042/16204 ( 19%)], Train Loss: 0.02369\n","Epoch: 04 [ 3062/16204 ( 19%)], Train Loss: 0.02358\n","Epoch: 04 [ 3082/16204 ( 19%)], Train Loss: 0.02347\n","Epoch: 04 [ 3102/16204 ( 19%)], Train Loss: 0.02335\n","Epoch: 04 [ 3122/16204 ( 19%)], Train Loss: 0.02326\n","Epoch: 04 [ 3142/16204 ( 19%)], Train Loss: 0.02312\n","Epoch: 04 [ 3162/16204 ( 20%)], Train Loss: 0.02306\n","Epoch: 04 [ 3182/16204 ( 20%)], Train Loss: 0.02293\n","Epoch: 04 [ 3202/16204 ( 20%)], Train Loss: 0.02280\n","Epoch: 04 [ 3222/16204 ( 20%)], Train Loss: 0.02273\n","Epoch: 04 [ 3242/16204 ( 20%)], Train Loss: 0.02260\n","Epoch: 04 [ 3262/16204 ( 20%)], Train Loss: 0.02259\n","Epoch: 04 [ 3282/16204 ( 20%)], Train Loss: 0.02284\n","Epoch: 04 [ 3302/16204 ( 20%)], Train Loss: 0.02272\n","Epoch: 04 [ 3322/16204 ( 21%)], Train Loss: 0.02260\n","Epoch: 04 [ 3342/16204 ( 21%)], Train Loss: 0.02247\n","Epoch: 04 [ 3362/16204 ( 21%)], Train Loss: 0.02251\n","Epoch: 04 [ 3382/16204 ( 21%)], Train Loss: 0.02254\n","Epoch: 04 [ 3402/16204 ( 21%)], Train Loss: 0.02252\n","Epoch: 04 [ 3422/16204 ( 21%)], Train Loss: 0.02242\n","Epoch: 04 [ 3442/16204 ( 21%)], Train Loss: 0.02251\n","Epoch: 04 [ 3462/16204 ( 21%)], Train Loss: 0.02361\n","Epoch: 04 [ 3482/16204 ( 21%)], Train Loss: 0.02423\n","Epoch: 04 [ 3502/16204 ( 22%)], Train Loss: 0.02415\n","Epoch: 04 [ 3522/16204 ( 22%)], Train Loss: 0.02406\n","Epoch: 04 [ 3542/16204 ( 22%)], Train Loss: 0.02418\n","Epoch: 04 [ 3562/16204 ( 22%)], Train Loss: 0.02419\n","Epoch: 04 [ 3582/16204 ( 22%)], Train Loss: 0.02416\n","Epoch: 04 [ 3602/16204 ( 22%)], Train Loss: 0.02424\n","Epoch: 04 [ 3622/16204 ( 22%)], Train Loss: 0.02419\n","Epoch: 04 [ 3642/16204 ( 22%)], Train Loss: 0.02422\n","Epoch: 04 [ 3662/16204 ( 23%)], Train Loss: 0.02411\n","Epoch: 04 [ 3682/16204 ( 23%)], Train Loss: 0.02406\n","Epoch: 04 [ 3702/16204 ( 23%)], Train Loss: 0.02395\n","Epoch: 04 [ 3722/16204 ( 23%)], Train Loss: 0.02397\n","Epoch: 04 [ 3742/16204 ( 23%)], Train Loss: 0.02387\n","Epoch: 04 [ 3762/16204 ( 23%)], Train Loss: 0.02404\n","Epoch: 04 [ 3782/16204 ( 23%)], Train Loss: 0.02422\n","Epoch: 04 [ 3802/16204 ( 23%)], Train Loss: 0.02409\n","Epoch: 04 [ 3822/16204 ( 24%)], Train Loss: 0.02435\n","Epoch: 04 [ 3842/16204 ( 24%)], Train Loss: 0.02423\n","Epoch: 04 [ 3862/16204 ( 24%)], Train Loss: 0.02413\n","Epoch: 04 [ 3882/16204 ( 24%)], Train Loss: 0.02401\n","Epoch: 04 [ 3902/16204 ( 24%)], Train Loss: 0.02402\n","Epoch: 04 [ 3922/16204 ( 24%)], Train Loss: 0.02393\n","Epoch: 04 [ 3942/16204 ( 24%)], Train Loss: 0.02382\n","Epoch: 04 [ 3962/16204 ( 24%)], Train Loss: 0.02389\n","Epoch: 04 [ 3982/16204 ( 25%)], Train Loss: 0.02378\n","Epoch: 04 [ 4002/16204 ( 25%)], Train Loss: 0.02367\n","Epoch: 04 [ 4022/16204 ( 25%)], Train Loss: 0.02358\n","Epoch: 04 [ 4042/16204 ( 25%)], Train Loss: 0.02352\n","Epoch: 04 [ 4062/16204 ( 25%)], Train Loss: 0.02340\n","Epoch: 04 [ 4082/16204 ( 25%)], Train Loss: 0.02337\n","Epoch: 04 [ 4102/16204 ( 25%)], Train Loss: 0.02327\n","Epoch: 04 [ 4122/16204 ( 25%)], Train Loss: 0.02317\n","Epoch: 04 [ 4142/16204 ( 26%)], Train Loss: 0.02374\n","Epoch: 04 [ 4162/16204 ( 26%)], Train Loss: 0.02363\n","Epoch: 04 [ 4182/16204 ( 26%)], Train Loss: 0.02355\n","Epoch: 04 [ 4202/16204 ( 26%)], Train Loss: 0.02344\n","Epoch: 04 [ 4222/16204 ( 26%)], Train Loss: 0.02339\n","Epoch: 04 [ 4242/16204 ( 26%)], Train Loss: 0.02354\n","Epoch: 04 [ 4262/16204 ( 26%)], Train Loss: 0.02344\n","Epoch: 04 [ 4282/16204 ( 26%)], Train Loss: 0.02339\n","Epoch: 04 [ 4302/16204 ( 27%)], Train Loss: 0.02329\n","Epoch: 04 [ 4322/16204 ( 27%)], Train Loss: 0.02322\n","Epoch: 04 [ 4342/16204 ( 27%)], Train Loss: 0.02312\n","Epoch: 04 [ 4362/16204 ( 27%)], Train Loss: 0.02302\n","Epoch: 04 [ 4382/16204 ( 27%)], Train Loss: 0.02297\n","Epoch: 04 [ 4402/16204 ( 27%)], Train Loss: 0.02287\n","Epoch: 04 [ 4422/16204 ( 27%)], Train Loss: 0.02277\n","Epoch: 04 [ 4442/16204 ( 27%)], Train Loss: 0.02270\n","Epoch: 04 [ 4462/16204 ( 28%)], Train Loss: 0.02329\n","Epoch: 04 [ 4482/16204 ( 28%)], Train Loss: 0.02358\n","Epoch: 04 [ 4502/16204 ( 28%)], Train Loss: 0.02364\n","Epoch: 04 [ 4522/16204 ( 28%)], Train Loss: 0.02359\n","Epoch: 04 [ 4542/16204 ( 28%)], Train Loss: 0.02353\n","Epoch: 04 [ 4562/16204 ( 28%)], Train Loss: 0.02345\n","Epoch: 04 [ 4582/16204 ( 28%)], Train Loss: 0.02381\n","Epoch: 04 [ 4602/16204 ( 28%)], Train Loss: 0.02372\n","Epoch: 04 [ 4622/16204 ( 29%)], Train Loss: 0.02406\n","Epoch: 04 [ 4642/16204 ( 29%)], Train Loss: 0.02405\n","Epoch: 04 [ 4662/16204 ( 29%)], Train Loss: 0.02414\n","Epoch: 04 [ 4682/16204 ( 29%)], Train Loss: 0.02414\n","Epoch: 04 [ 4702/16204 ( 29%)], Train Loss: 0.02405\n","Epoch: 04 [ 4722/16204 ( 29%)], Train Loss: 0.02420\n","Epoch: 04 [ 4742/16204 ( 29%)], Train Loss: 0.02439\n","Epoch: 04 [ 4762/16204 ( 29%)], Train Loss: 0.02430\n","Epoch: 04 [ 4782/16204 ( 30%)], Train Loss: 0.02425\n","Epoch: 04 [ 4802/16204 ( 30%)], Train Loss: 0.02446\n","Epoch: 04 [ 4822/16204 ( 30%)], Train Loss: 0.02441\n","Epoch: 04 [ 4842/16204 ( 30%)], Train Loss: 0.02445\n","Epoch: 04 [ 4862/16204 ( 30%)], Train Loss: 0.02437\n","Epoch: 04 [ 4882/16204 ( 30%)], Train Loss: 0.02428\n","Epoch: 04 [ 4902/16204 ( 30%)], Train Loss: 0.02418\n","Epoch: 04 [ 4922/16204 ( 30%)], Train Loss: 0.02409\n","Epoch: 04 [ 4942/16204 ( 30%)], Train Loss: 0.02399\n","Epoch: 04 [ 4962/16204 ( 31%)], Train Loss: 0.02402\n","Epoch: 04 [ 4982/16204 ( 31%)], Train Loss: 0.02397\n","Epoch: 04 [ 5002/16204 ( 31%)], Train Loss: 0.02388\n","Epoch: 04 [ 5022/16204 ( 31%)], Train Loss: 0.02388\n","Epoch: 04 [ 5042/16204 ( 31%)], Train Loss: 0.02405\n","Epoch: 04 [ 5062/16204 ( 31%)], Train Loss: 0.02415\n","Epoch: 04 [ 5082/16204 ( 31%)], Train Loss: 0.02407\n","Epoch: 04 [ 5102/16204 ( 31%)], Train Loss: 0.02398\n","Epoch: 04 [ 5122/16204 ( 32%)], Train Loss: 0.02392\n","Epoch: 04 [ 5142/16204 ( 32%)], Train Loss: 0.02383\n","Epoch: 04 [ 5162/16204 ( 32%)], Train Loss: 0.02393\n","Epoch: 04 [ 5182/16204 ( 32%)], Train Loss: 0.02389\n","Epoch: 04 [ 5202/16204 ( 32%)], Train Loss: 0.02382\n","Epoch: 04 [ 5222/16204 ( 32%)], Train Loss: 0.02374\n","Epoch: 04 [ 5242/16204 ( 32%)], Train Loss: 0.02369\n","Epoch: 04 [ 5262/16204 ( 32%)], Train Loss: 0.02369\n","Epoch: 04 [ 5282/16204 ( 33%)], Train Loss: 0.02374\n","Epoch: 04 [ 5302/16204 ( 33%)], Train Loss: 0.02369\n","Epoch: 04 [ 5322/16204 ( 33%)], Train Loss: 0.02383\n","Epoch: 04 [ 5342/16204 ( 33%)], Train Loss: 0.02382\n","Epoch: 04 [ 5362/16204 ( 33%)], Train Loss: 0.02376\n","Epoch: 04 [ 5382/16204 ( 33%)], Train Loss: 0.02386\n","Epoch: 04 [ 5402/16204 ( 33%)], Train Loss: 0.02378\n","Epoch: 04 [ 5422/16204 ( 33%)], Train Loss: 0.02409\n","Epoch: 04 [ 5442/16204 ( 34%)], Train Loss: 0.02402\n","Epoch: 04 [ 5462/16204 ( 34%)], Train Loss: 0.02394\n","Epoch: 04 [ 5482/16204 ( 34%)], Train Loss: 0.02386\n","Epoch: 04 [ 5502/16204 ( 34%)], Train Loss: 0.02378\n","Epoch: 04 [ 5522/16204 ( 34%)], Train Loss: 0.02378\n","Epoch: 04 [ 5542/16204 ( 34%)], Train Loss: 0.02370\n","Epoch: 04 [ 5562/16204 ( 34%)], Train Loss: 0.02364\n","Epoch: 04 [ 5582/16204 ( 34%)], Train Loss: 0.02359\n","Epoch: 04 [ 5602/16204 ( 35%)], Train Loss: 0.02358\n","Epoch: 04 [ 5622/16204 ( 35%)], Train Loss: 0.02349\n","Epoch: 04 [ 5642/16204 ( 35%)], Train Loss: 0.02380\n","Epoch: 04 [ 5662/16204 ( 35%)], Train Loss: 0.02372\n","Epoch: 04 [ 5682/16204 ( 35%)], Train Loss: 0.02366\n","Epoch: 04 [ 5702/16204 ( 35%)], Train Loss: 0.02359\n","Epoch: 04 [ 5722/16204 ( 35%)], Train Loss: 0.02353\n","Epoch: 04 [ 5742/16204 ( 35%)], Train Loss: 0.02346\n","Epoch: 04 [ 5762/16204 ( 36%)], Train Loss: 0.02339\n","Epoch: 04 [ 5782/16204 ( 36%)], Train Loss: 0.02333\n","Epoch: 04 [ 5802/16204 ( 36%)], Train Loss: 0.02326\n","Epoch: 04 [ 5822/16204 ( 36%)], Train Loss: 0.02326\n","Epoch: 04 [ 5842/16204 ( 36%)], Train Loss: 0.02319\n","Epoch: 04 [ 5862/16204 ( 36%)], Train Loss: 0.02312\n","Epoch: 04 [ 5882/16204 ( 36%)], Train Loss: 0.02306\n","Epoch: 04 [ 5902/16204 ( 36%)], Train Loss: 0.02301\n","Epoch: 04 [ 5922/16204 ( 37%)], Train Loss: 0.02329\n","Epoch: 04 [ 5942/16204 ( 37%)], Train Loss: 0.02322\n","Epoch: 04 [ 5962/16204 ( 37%)], Train Loss: 0.02314\n","Epoch: 04 [ 5982/16204 ( 37%)], Train Loss: 0.02313\n","Epoch: 04 [ 6002/16204 ( 37%)], Train Loss: 0.02308\n","Epoch: 04 [ 6022/16204 ( 37%)], Train Loss: 0.02305\n","Epoch: 04 [ 6042/16204 ( 37%)], Train Loss: 0.02302\n","Epoch: 04 [ 6062/16204 ( 37%)], Train Loss: 0.02299\n","Epoch: 04 [ 6082/16204 ( 38%)], Train Loss: 0.02294\n","Epoch: 04 [ 6102/16204 ( 38%)], Train Loss: 0.02293\n","Epoch: 04 [ 6122/16204 ( 38%)], Train Loss: 0.02297\n","Epoch: 04 [ 6142/16204 ( 38%)], Train Loss: 0.02294\n","Epoch: 04 [ 6162/16204 ( 38%)], Train Loss: 0.02288\n","Epoch: 04 [ 6182/16204 ( 38%)], Train Loss: 0.02282\n","Epoch: 04 [ 6202/16204 ( 38%)], Train Loss: 0.02288\n","Epoch: 04 [ 6222/16204 ( 38%)], Train Loss: 0.02292\n","Epoch: 04 [ 6242/16204 ( 39%)], Train Loss: 0.02304\n","Epoch: 04 [ 6262/16204 ( 39%)], Train Loss: 0.02299\n","Epoch: 04 [ 6282/16204 ( 39%)], Train Loss: 0.02294\n","Epoch: 04 [ 6302/16204 ( 39%)], Train Loss: 0.02297\n","Epoch: 04 [ 6322/16204 ( 39%)], Train Loss: 0.02292\n","Epoch: 04 [ 6342/16204 ( 39%)], Train Loss: 0.02305\n","Epoch: 04 [ 6362/16204 ( 39%)], Train Loss: 0.02300\n","Epoch: 04 [ 6382/16204 ( 39%)], Train Loss: 0.02323\n","Epoch: 04 [ 6402/16204 ( 40%)], Train Loss: 0.02352\n","Epoch: 04 [ 6422/16204 ( 40%)], Train Loss: 0.02362\n","Epoch: 04 [ 6442/16204 ( 40%)], Train Loss: 0.02359\n","Epoch: 04 [ 6462/16204 ( 40%)], Train Loss: 0.02353\n","Epoch: 04 [ 6482/16204 ( 40%)], Train Loss: 0.02347\n","Epoch: 04 [ 6502/16204 ( 40%)], Train Loss: 0.02342\n","Epoch: 04 [ 6522/16204 ( 40%)], Train Loss: 0.02336\n","Epoch: 04 [ 6542/16204 ( 40%)], Train Loss: 0.02340\n","Epoch: 04 [ 6562/16204 ( 40%)], Train Loss: 0.02334\n","Epoch: 04 [ 6582/16204 ( 41%)], Train Loss: 0.02330\n","Epoch: 04 [ 6602/16204 ( 41%)], Train Loss: 0.02324\n","Epoch: 04 [ 6622/16204 ( 41%)], Train Loss: 0.02326\n","Epoch: 04 [ 6642/16204 ( 41%)], Train Loss: 0.02321\n","Epoch: 04 [ 6662/16204 ( 41%)], Train Loss: 0.02322\n","Epoch: 04 [ 6682/16204 ( 41%)], Train Loss: 0.02319\n","Epoch: 04 [ 6702/16204 ( 41%)], Train Loss: 0.02321\n","Epoch: 04 [ 6722/16204 ( 41%)], Train Loss: 0.02314\n","Epoch: 04 [ 6742/16204 ( 42%)], Train Loss: 0.02310\n","Epoch: 04 [ 6762/16204 ( 42%)], Train Loss: 0.02305\n","Epoch: 04 [ 6782/16204 ( 42%)], Train Loss: 0.02300\n","Epoch: 04 [ 6802/16204 ( 42%)], Train Loss: 0.02294\n","Epoch: 04 [ 6822/16204 ( 42%)], Train Loss: 0.02290\n","Epoch: 04 [ 6842/16204 ( 42%)], Train Loss: 0.02300\n","Epoch: 04 [ 6862/16204 ( 42%)], Train Loss: 0.02294\n","Epoch: 04 [ 6882/16204 ( 42%)], Train Loss: 0.02288\n","Epoch: 04 [ 6902/16204 ( 43%)], Train Loss: 0.02283\n","Epoch: 04 [ 6922/16204 ( 43%)], Train Loss: 0.02277\n","Epoch: 04 [ 6942/16204 ( 43%)], Train Loss: 0.02271\n","Epoch: 04 [ 6962/16204 ( 43%)], Train Loss: 0.02266\n","Epoch: 04 [ 6982/16204 ( 43%)], Train Loss: 0.02260\n","Epoch: 04 [ 7002/16204 ( 43%)], Train Loss: 0.02254\n","Epoch: 04 [ 7022/16204 ( 43%)], Train Loss: 0.02250\n","Epoch: 04 [ 7042/16204 ( 43%)], Train Loss: 0.02245\n","Epoch: 04 [ 7062/16204 ( 44%)], Train Loss: 0.02239\n","Epoch: 04 [ 7082/16204 ( 44%)], Train Loss: 0.02242\n","Epoch: 04 [ 7102/16204 ( 44%)], Train Loss: 0.02236\n","Epoch: 04 [ 7122/16204 ( 44%)], Train Loss: 0.02231\n","Epoch: 04 [ 7142/16204 ( 44%)], Train Loss: 0.02236\n","Epoch: 04 [ 7162/16204 ( 44%)], Train Loss: 0.02257\n","Epoch: 04 [ 7182/16204 ( 44%)], Train Loss: 0.02266\n","Epoch: 04 [ 7202/16204 ( 44%)], Train Loss: 0.02262\n","Epoch: 04 [ 7222/16204 ( 45%)], Train Loss: 0.02262\n","Epoch: 04 [ 7242/16204 ( 45%)], Train Loss: 0.02263\n","Epoch: 04 [ 7262/16204 ( 45%)], Train Loss: 0.02260\n","Epoch: 04 [ 7282/16204 ( 45%)], Train Loss: 0.02266\n","Epoch: 04 [ 7302/16204 ( 45%)], Train Loss: 0.02274\n","Epoch: 04 [ 7322/16204 ( 45%)], Train Loss: 0.02269\n","Epoch: 04 [ 7342/16204 ( 45%)], Train Loss: 0.02274\n","Epoch: 04 [ 7362/16204 ( 45%)], Train Loss: 0.02280\n","Epoch: 04 [ 7382/16204 ( 46%)], Train Loss: 0.02278\n","Epoch: 04 [ 7402/16204 ( 46%)], Train Loss: 0.02278\n","Epoch: 04 [ 7422/16204 ( 46%)], Train Loss: 0.02293\n","Epoch: 04 [ 7442/16204 ( 46%)], Train Loss: 0.02290\n","Epoch: 04 [ 7462/16204 ( 46%)], Train Loss: 0.02285\n","Epoch: 04 [ 7482/16204 ( 46%)], Train Loss: 0.02282\n","Epoch: 04 [ 7502/16204 ( 46%)], Train Loss: 0.02277\n","Epoch: 04 [ 7522/16204 ( 46%)], Train Loss: 0.02277\n","Epoch: 04 [ 7542/16204 ( 47%)], Train Loss: 0.02281\n","Epoch: 04 [ 7562/16204 ( 47%)], Train Loss: 0.02278\n","Epoch: 04 [ 7582/16204 ( 47%)], Train Loss: 0.02272\n","Epoch: 04 [ 7602/16204 ( 47%)], Train Loss: 0.02269\n","Epoch: 04 [ 7622/16204 ( 47%)], Train Loss: 0.02267\n","Epoch: 04 [ 7642/16204 ( 47%)], Train Loss: 0.02265\n","Epoch: 04 [ 7662/16204 ( 47%)], Train Loss: 0.02260\n","Epoch: 04 [ 7682/16204 ( 47%)], Train Loss: 0.02269\n","Epoch: 04 [ 7702/16204 ( 48%)], Train Loss: 0.02265\n","Epoch: 04 [ 7722/16204 ( 48%)], Train Loss: 0.02260\n","Epoch: 04 [ 7742/16204 ( 48%)], Train Loss: 0.02257\n","Epoch: 04 [ 7762/16204 ( 48%)], Train Loss: 0.02254\n","Epoch: 04 [ 7782/16204 ( 48%)], Train Loss: 0.02248\n","Epoch: 04 [ 7802/16204 ( 48%)], Train Loss: 0.02246\n","Epoch: 04 [ 7822/16204 ( 48%)], Train Loss: 0.02250\n","Epoch: 04 [ 7842/16204 ( 48%)], Train Loss: 0.02256\n","Epoch: 04 [ 7862/16204 ( 49%)], Train Loss: 0.02251\n","Epoch: 04 [ 7882/16204 ( 49%)], Train Loss: 0.02252\n","Epoch: 04 [ 7902/16204 ( 49%)], Train Loss: 0.02249\n","Epoch: 04 [ 7922/16204 ( 49%)], Train Loss: 0.02245\n","Epoch: 04 [ 7942/16204 ( 49%)], Train Loss: 0.02245\n","Epoch: 04 [ 7962/16204 ( 49%)], Train Loss: 0.02240\n","Epoch: 04 [ 7982/16204 ( 49%)], Train Loss: 0.02239\n","Epoch: 04 [ 8002/16204 ( 49%)], Train Loss: 0.02233\n","Epoch: 04 [ 8022/16204 ( 50%)], Train Loss: 0.02233\n","Epoch: 04 [ 8042/16204 ( 50%)], Train Loss: 0.02240\n","Epoch: 04 [ 8062/16204 ( 50%)], Train Loss: 0.02240\n","Epoch: 04 [ 8082/16204 ( 50%)], Train Loss: 0.02245\n","Epoch: 04 [ 8102/16204 ( 50%)], Train Loss: 0.02240\n","Epoch: 04 [ 8122/16204 ( 50%)], Train Loss: 0.02234\n","Epoch: 04 [ 8142/16204 ( 50%)], Train Loss: 0.02229\n","Epoch: 04 [ 8162/16204 ( 50%)], Train Loss: 0.02234\n","Epoch: 04 [ 8182/16204 ( 50%)], Train Loss: 0.02231\n","Epoch: 04 [ 8202/16204 ( 51%)], Train Loss: 0.02230\n","Epoch: 04 [ 8222/16204 ( 51%)], Train Loss: 0.02225\n","Epoch: 04 [ 8242/16204 ( 51%)], Train Loss: 0.02220\n","Epoch: 04 [ 8262/16204 ( 51%)], Train Loss: 0.02215\n","Epoch: 04 [ 8282/16204 ( 51%)], Train Loss: 0.02215\n","Epoch: 04 [ 8302/16204 ( 51%)], Train Loss: 0.02211\n","Epoch: 04 [ 8322/16204 ( 51%)], Train Loss: 0.02206\n","Epoch: 04 [ 8342/16204 ( 51%)], Train Loss: 0.02203\n","Epoch: 04 [ 8362/16204 ( 52%)], Train Loss: 0.02198\n","Epoch: 04 [ 8382/16204 ( 52%)], Train Loss: 0.02197\n","Epoch: 04 [ 8402/16204 ( 52%)], Train Loss: 0.02200\n","Epoch: 04 [ 8422/16204 ( 52%)], Train Loss: 0.02199\n","Epoch: 04 [ 8442/16204 ( 52%)], Train Loss: 0.02196\n","Epoch: 04 [ 8462/16204 ( 52%)], Train Loss: 0.02192\n","Epoch: 04 [ 8482/16204 ( 52%)], Train Loss: 0.02189\n","Epoch: 04 [ 8502/16204 ( 52%)], Train Loss: 0.02185\n","Epoch: 04 [ 8522/16204 ( 53%)], Train Loss: 0.02190\n","Epoch: 04 [ 8542/16204 ( 53%)], Train Loss: 0.02221\n","Epoch: 04 [ 8562/16204 ( 53%)], Train Loss: 0.02216\n","Epoch: 04 [ 8582/16204 ( 53%)], Train Loss: 0.02238\n","Epoch: 04 [ 8602/16204 ( 53%)], Train Loss: 0.02234\n","Epoch: 04 [ 8622/16204 ( 53%)], Train Loss: 0.02229\n","Epoch: 04 [ 8642/16204 ( 53%)], Train Loss: 0.02225\n","Epoch: 04 [ 8662/16204 ( 53%)], Train Loss: 0.02221\n","Epoch: 04 [ 8682/16204 ( 54%)], Train Loss: 0.02217\n","Epoch: 04 [ 8702/16204 ( 54%)], Train Loss: 0.02214\n","Epoch: 04 [ 8722/16204 ( 54%)], Train Loss: 0.02209\n","Epoch: 04 [ 8742/16204 ( 54%)], Train Loss: 0.02206\n","Epoch: 04 [ 8762/16204 ( 54%)], Train Loss: 0.02204\n","Epoch: 04 [ 8782/16204 ( 54%)], Train Loss: 0.02199\n","Epoch: 04 [ 8802/16204 ( 54%)], Train Loss: 0.02197\n","Epoch: 04 [ 8822/16204 ( 54%)], Train Loss: 0.02192\n","Epoch: 04 [ 8842/16204 ( 55%)], Train Loss: 0.02198\n","Epoch: 04 [ 8862/16204 ( 55%)], Train Loss: 0.02195\n","Epoch: 04 [ 8882/16204 ( 55%)], Train Loss: 0.02194\n","Epoch: 04 [ 8902/16204 ( 55%)], Train Loss: 0.02190\n","Epoch: 04 [ 8922/16204 ( 55%)], Train Loss: 0.02194\n","Epoch: 04 [ 8942/16204 ( 55%)], Train Loss: 0.02192\n","Epoch: 04 [ 8962/16204 ( 55%)], Train Loss: 0.02191\n","Epoch: 04 [ 8982/16204 ( 55%)], Train Loss: 0.02189\n","Epoch: 04 [ 9002/16204 ( 56%)], Train Loss: 0.02188\n","Epoch: 04 [ 9022/16204 ( 56%)], Train Loss: 0.02184\n","Epoch: 04 [ 9042/16204 ( 56%)], Train Loss: 0.02181\n","Epoch: 04 [ 9062/16204 ( 56%)], Train Loss: 0.02183\n","Epoch: 04 [ 9082/16204 ( 56%)], Train Loss: 0.02182\n","Epoch: 04 [ 9102/16204 ( 56%)], Train Loss: 0.02193\n","Epoch: 04 [ 9122/16204 ( 56%)], Train Loss: 0.02191\n","Epoch: 04 [ 9142/16204 ( 56%)], Train Loss: 0.02192\n","Epoch: 04 [ 9162/16204 ( 57%)], Train Loss: 0.02189\n","Epoch: 04 [ 9182/16204 ( 57%)], Train Loss: 0.02184\n","Epoch: 04 [ 9202/16204 ( 57%)], Train Loss: 0.02182\n","Epoch: 04 [ 9222/16204 ( 57%)], Train Loss: 0.02178\n","Epoch: 04 [ 9242/16204 ( 57%)], Train Loss: 0.02200\n","Epoch: 04 [ 9262/16204 ( 57%)], Train Loss: 0.02196\n","Epoch: 04 [ 9282/16204 ( 57%)], Train Loss: 0.02194\n","Epoch: 04 [ 9302/16204 ( 57%)], Train Loss: 0.02195\n","Epoch: 04 [ 9322/16204 ( 58%)], Train Loss: 0.02193\n","Epoch: 04 [ 9342/16204 ( 58%)], Train Loss: 0.02188\n","Epoch: 04 [ 9362/16204 ( 58%)], Train Loss: 0.02186\n","Epoch: 04 [ 9382/16204 ( 58%)], Train Loss: 0.02189\n","Epoch: 04 [ 9402/16204 ( 58%)], Train Loss: 0.02201\n","Epoch: 04 [ 9422/16204 ( 58%)], Train Loss: 0.02199\n","Epoch: 04 [ 9442/16204 ( 58%)], Train Loss: 0.02212\n","Epoch: 04 [ 9462/16204 ( 58%)], Train Loss: 0.02209\n","Epoch: 04 [ 9482/16204 ( 59%)], Train Loss: 0.02205\n","Epoch: 04 [ 9502/16204 ( 59%)], Train Loss: 0.02201\n","Epoch: 04 [ 9522/16204 ( 59%)], Train Loss: 0.02196\n","Epoch: 04 [ 9542/16204 ( 59%)], Train Loss: 0.02198\n","Epoch: 04 [ 9562/16204 ( 59%)], Train Loss: 0.02194\n","Epoch: 04 [ 9582/16204 ( 59%)], Train Loss: 0.02189\n","Epoch: 04 [ 9602/16204 ( 59%)], Train Loss: 0.02194\n","Epoch: 04 [ 9622/16204 ( 59%)], Train Loss: 0.02215\n","Epoch: 04 [ 9642/16204 ( 60%)], Train Loss: 0.02212\n","Epoch: 04 [ 9662/16204 ( 60%)], Train Loss: 0.02208\n","Epoch: 04 [ 9682/16204 ( 60%)], Train Loss: 0.02205\n","Epoch: 04 [ 9702/16204 ( 60%)], Train Loss: 0.02207\n","Epoch: 04 [ 9722/16204 ( 60%)], Train Loss: 0.02205\n","Epoch: 04 [ 9742/16204 ( 60%)], Train Loss: 0.02214\n","Epoch: 04 [ 9762/16204 ( 60%)], Train Loss: 0.02210\n","Epoch: 04 [ 9782/16204 ( 60%)], Train Loss: 0.02215\n","Epoch: 04 [ 9802/16204 ( 60%)], Train Loss: 0.02214\n","Epoch: 04 [ 9822/16204 ( 61%)], Train Loss: 0.02216\n","Epoch: 04 [ 9842/16204 ( 61%)], Train Loss: 0.02224\n","Epoch: 04 [ 9862/16204 ( 61%)], Train Loss: 0.02243\n","Epoch: 04 [ 9882/16204 ( 61%)], Train Loss: 0.02241\n","Epoch: 04 [ 9902/16204 ( 61%)], Train Loss: 0.02238\n","Epoch: 04 [ 9922/16204 ( 61%)], Train Loss: 0.02240\n","Epoch: 04 [ 9942/16204 ( 61%)], Train Loss: 0.02246\n","Epoch: 04 [ 9962/16204 ( 61%)], Train Loss: 0.02243\n","Epoch: 04 [ 9982/16204 ( 62%)], Train Loss: 0.02239\n","Epoch: 04 [10002/16204 ( 62%)], Train Loss: 0.02238\n","Epoch: 04 [10022/16204 ( 62%)], Train Loss: 0.02235\n","Epoch: 04 [10042/16204 ( 62%)], Train Loss: 0.02231\n","Epoch: 04 [10062/16204 ( 62%)], Train Loss: 0.02227\n","Epoch: 04 [10082/16204 ( 62%)], Train Loss: 0.02223\n","Epoch: 04 [10102/16204 ( 62%)], Train Loss: 0.02220\n","Epoch: 04 [10122/16204 ( 62%)], Train Loss: 0.02218\n","Epoch: 04 [10142/16204 ( 63%)], Train Loss: 0.02214\n","Epoch: 04 [10162/16204 ( 63%)], Train Loss: 0.02211\n","Epoch: 04 [10182/16204 ( 63%)], Train Loss: 0.02207\n","Epoch: 04 [10202/16204 ( 63%)], Train Loss: 0.02225\n","Epoch: 04 [10222/16204 ( 63%)], Train Loss: 0.02225\n","Epoch: 04 [10242/16204 ( 63%)], Train Loss: 0.02224\n","Epoch: 04 [10262/16204 ( 63%)], Train Loss: 0.02222\n","Epoch: 04 [10282/16204 ( 63%)], Train Loss: 0.02219\n","Epoch: 04 [10302/16204 ( 64%)], Train Loss: 0.02215\n","Epoch: 04 [10322/16204 ( 64%)], Train Loss: 0.02211\n","Epoch: 04 [10342/16204 ( 64%)], Train Loss: 0.02208\n","Epoch: 04 [10362/16204 ( 64%)], Train Loss: 0.02204\n","Epoch: 04 [10382/16204 ( 64%)], Train Loss: 0.02203\n","Epoch: 04 [10402/16204 ( 64%)], Train Loss: 0.02200\n","Epoch: 04 [10422/16204 ( 64%)], Train Loss: 0.02196\n","Epoch: 04 [10442/16204 ( 64%)], Train Loss: 0.02192\n","Epoch: 04 [10462/16204 ( 65%)], Train Loss: 0.02189\n","Epoch: 04 [10482/16204 ( 65%)], Train Loss: 0.02186\n","Epoch: 04 [10502/16204 ( 65%)], Train Loss: 0.02182\n","Epoch: 04 [10522/16204 ( 65%)], Train Loss: 0.02178\n","Epoch: 04 [10542/16204 ( 65%)], Train Loss: 0.02174\n","Epoch: 04 [10562/16204 ( 65%)], Train Loss: 0.02170\n","Epoch: 04 [10582/16204 ( 65%)], Train Loss: 0.02167\n","Epoch: 04 [10602/16204 ( 65%)], Train Loss: 0.02163\n","Epoch: 04 [10622/16204 ( 66%)], Train Loss: 0.02163\n","Epoch: 04 [10642/16204 ( 66%)], Train Loss: 0.02159\n","Epoch: 04 [10662/16204 ( 66%)], Train Loss: 0.02159\n","Epoch: 04 [10682/16204 ( 66%)], Train Loss: 0.02155\n","Epoch: 04 [10702/16204 ( 66%)], Train Loss: 0.02152\n","Epoch: 04 [10722/16204 ( 66%)], Train Loss: 0.02148\n","Epoch: 04 [10742/16204 ( 66%)], Train Loss: 0.02144\n","Epoch: 04 [10762/16204 ( 66%)], Train Loss: 0.02140\n","Epoch: 04 [10782/16204 ( 67%)], Train Loss: 0.02145\n","Epoch: 04 [10802/16204 ( 67%)], Train Loss: 0.02144\n","Epoch: 04 [10822/16204 ( 67%)], Train Loss: 0.02144\n","Epoch: 04 [10842/16204 ( 67%)], Train Loss: 0.02140\n","Epoch: 04 [10862/16204 ( 67%)], Train Loss: 0.02137\n","Epoch: 04 [10882/16204 ( 67%)], Train Loss: 0.02133\n","Epoch: 04 [10902/16204 ( 67%)], Train Loss: 0.02131\n","Epoch: 04 [10922/16204 ( 67%)], Train Loss: 0.02131\n","Epoch: 04 [10942/16204 ( 68%)], Train Loss: 0.02141\n","Epoch: 04 [10962/16204 ( 68%)], Train Loss: 0.02138\n","Epoch: 04 [10982/16204 ( 68%)], Train Loss: 0.02136\n","Epoch: 04 [11002/16204 ( 68%)], Train Loss: 0.02134\n","Epoch: 04 [11022/16204 ( 68%)], Train Loss: 0.02130\n","Epoch: 04 [11042/16204 ( 68%)], Train Loss: 0.02127\n","Epoch: 04 [11062/16204 ( 68%)], Train Loss: 0.02125\n","Epoch: 04 [11082/16204 ( 68%)], Train Loss: 0.02127\n","Epoch: 04 [11102/16204 ( 69%)], Train Loss: 0.02124\n","Epoch: 04 [11122/16204 ( 69%)], Train Loss: 0.02125\n","Epoch: 04 [11142/16204 ( 69%)], Train Loss: 0.02121\n","Epoch: 04 [11162/16204 ( 69%)], Train Loss: 0.02119\n","Epoch: 04 [11182/16204 ( 69%)], Train Loss: 0.02116\n","Epoch: 04 [11202/16204 ( 69%)], Train Loss: 0.02114\n","Epoch: 04 [11222/16204 ( 69%)], Train Loss: 0.02122\n","Epoch: 04 [11242/16204 ( 69%)], Train Loss: 0.02119\n","Epoch: 04 [11262/16204 ( 70%)], Train Loss: 0.02116\n","Epoch: 04 [11282/16204 ( 70%)], Train Loss: 0.02113\n","Epoch: 04 [11302/16204 ( 70%)], Train Loss: 0.02111\n","Epoch: 04 [11322/16204 ( 70%)], Train Loss: 0.02117\n","Epoch: 04 [11342/16204 ( 70%)], Train Loss: 0.02114\n","Epoch: 04 [11362/16204 ( 70%)], Train Loss: 0.02111\n","Epoch: 04 [11382/16204 ( 70%)], Train Loss: 0.02108\n","Epoch: 04 [11402/16204 ( 70%)], Train Loss: 0.02107\n","Epoch: 04 [11422/16204 ( 70%)], Train Loss: 0.02105\n","Epoch: 04 [11442/16204 ( 71%)], Train Loss: 0.02108\n","Epoch: 04 [11462/16204 ( 71%)], Train Loss: 0.02106\n","Epoch: 04 [11482/16204 ( 71%)], Train Loss: 0.02108\n","Epoch: 04 [11502/16204 ( 71%)], Train Loss: 0.02105\n","Epoch: 04 [11522/16204 ( 71%)], Train Loss: 0.02109\n","Epoch: 04 [11542/16204 ( 71%)], Train Loss: 0.02106\n","Epoch: 04 [11562/16204 ( 71%)], Train Loss: 0.02113\n","Epoch: 04 [11582/16204 ( 71%)], Train Loss: 0.02110\n","Epoch: 04 [11602/16204 ( 72%)], Train Loss: 0.02124\n","Epoch: 04 [11622/16204 ( 72%)], Train Loss: 0.02121\n","Epoch: 04 [11642/16204 ( 72%)], Train Loss: 0.02119\n","Epoch: 04 [11662/16204 ( 72%)], Train Loss: 0.02125\n","Epoch: 04 [11682/16204 ( 72%)], Train Loss: 0.02122\n","Epoch: 04 [11702/16204 ( 72%)], Train Loss: 0.02119\n","Epoch: 04 [11722/16204 ( 72%)], Train Loss: 0.02122\n","Epoch: 04 [11742/16204 ( 72%)], Train Loss: 0.02119\n","Epoch: 04 [11762/16204 ( 73%)], Train Loss: 0.02116\n","Epoch: 04 [11782/16204 ( 73%)], Train Loss: 0.02114\n","Epoch: 04 [11802/16204 ( 73%)], Train Loss: 0.02111\n","Epoch: 04 [11822/16204 ( 73%)], Train Loss: 0.02108\n","Epoch: 04 [11842/16204 ( 73%)], Train Loss: 0.02105\n","Epoch: 04 [11862/16204 ( 73%)], Train Loss: 0.02102\n","Epoch: 04 [11882/16204 ( 73%)], Train Loss: 0.02099\n","Epoch: 04 [11902/16204 ( 73%)], Train Loss: 0.02100\n","Epoch: 04 [11922/16204 ( 74%)], Train Loss: 0.02096\n","Epoch: 04 [11942/16204 ( 74%)], Train Loss: 0.02093\n","Epoch: 04 [11962/16204 ( 74%)], Train Loss: 0.02093\n","Epoch: 04 [11982/16204 ( 74%)], Train Loss: 0.02091\n","Epoch: 04 [12002/16204 ( 74%)], Train Loss: 0.02088\n","Epoch: 04 [12022/16204 ( 74%)], Train Loss: 0.02085\n","Epoch: 04 [12042/16204 ( 74%)], Train Loss: 0.02083\n","Epoch: 04 [12062/16204 ( 74%)], Train Loss: 0.02081\n","Epoch: 04 [12082/16204 ( 75%)], Train Loss: 0.02078\n","Epoch: 04 [12102/16204 ( 75%)], Train Loss: 0.02074\n","Epoch: 04 [12122/16204 ( 75%)], Train Loss: 0.02071\n","Epoch: 04 [12142/16204 ( 75%)], Train Loss: 0.02069\n","Epoch: 04 [12162/16204 ( 75%)], Train Loss: 0.02066\n","Epoch: 04 [12182/16204 ( 75%)], Train Loss: 0.02064\n","Epoch: 04 [12202/16204 ( 75%)], Train Loss: 0.02061\n","Epoch: 04 [12222/16204 ( 75%)], Train Loss: 0.02060\n","Epoch: 04 [12242/16204 ( 76%)], Train Loss: 0.02057\n","Epoch: 04 [12262/16204 ( 76%)], Train Loss: 0.02055\n","Epoch: 04 [12282/16204 ( 76%)], Train Loss: 0.02052\n","Epoch: 04 [12302/16204 ( 76%)], Train Loss: 0.02051\n","Epoch: 04 [12322/16204 ( 76%)], Train Loss: 0.02050\n","Epoch: 04 [12342/16204 ( 76%)], Train Loss: 0.02049\n","Epoch: 04 [12362/16204 ( 76%)], Train Loss: 0.02046\n","Epoch: 04 [12382/16204 ( 76%)], Train Loss: 0.02042\n","Epoch: 04 [12402/16204 ( 77%)], Train Loss: 0.02039\n","Epoch: 04 [12422/16204 ( 77%)], Train Loss: 0.02040\n","Epoch: 04 [12442/16204 ( 77%)], Train Loss: 0.02039\n","Epoch: 04 [12462/16204 ( 77%)], Train Loss: 0.02036\n","Epoch: 04 [12482/16204 ( 77%)], Train Loss: 0.02033\n","Epoch: 04 [12502/16204 ( 77%)], Train Loss: 0.02031\n","Epoch: 04 [12522/16204 ( 77%)], Train Loss: 0.02028\n","Epoch: 04 [12542/16204 ( 77%)], Train Loss: 0.02025\n","Epoch: 04 [12562/16204 ( 78%)], Train Loss: 0.02023\n","Epoch: 04 [12582/16204 ( 78%)], Train Loss: 0.02020\n","Epoch: 04 [12602/16204 ( 78%)], Train Loss: 0.02017\n","Epoch: 04 [12622/16204 ( 78%)], Train Loss: 0.02014\n","Epoch: 04 [12642/16204 ( 78%)], Train Loss: 0.02011\n","Epoch: 04 [12662/16204 ( 78%)], Train Loss: 0.02008\n","Epoch: 04 [12682/16204 ( 78%)], Train Loss: 0.02005\n","Epoch: 04 [12702/16204 ( 78%)], Train Loss: 0.02002\n","Epoch: 04 [12722/16204 ( 79%)], Train Loss: 0.02001\n","Epoch: 04 [12742/16204 ( 79%)], Train Loss: 0.01998\n","Epoch: 04 [12762/16204 ( 79%)], Train Loss: 0.01995\n","Epoch: 04 [12782/16204 ( 79%)], Train Loss: 0.01999\n","Epoch: 04 [12802/16204 ( 79%)], Train Loss: 0.01997\n","Epoch: 04 [12822/16204 ( 79%)], Train Loss: 0.01994\n","Epoch: 04 [12842/16204 ( 79%)], Train Loss: 0.01991\n","Epoch: 04 [12862/16204 ( 79%)], Train Loss: 0.01988\n","Epoch: 04 [12882/16204 ( 79%)], Train Loss: 0.01985\n","Epoch: 04 [12902/16204 ( 80%)], Train Loss: 0.01982\n","Epoch: 04 [12922/16204 ( 80%)], Train Loss: 0.01982\n","Epoch: 04 [12942/16204 ( 80%)], Train Loss: 0.01979\n","Epoch: 04 [12962/16204 ( 80%)], Train Loss: 0.01976\n","Epoch: 04 [12982/16204 ( 80%)], Train Loss: 0.01974\n","Epoch: 04 [13002/16204 ( 80%)], Train Loss: 0.01971\n","Epoch: 04 [13022/16204 ( 80%)], Train Loss: 0.01968\n","Epoch: 04 [13042/16204 ( 80%)], Train Loss: 0.01967\n","Epoch: 04 [13062/16204 ( 81%)], Train Loss: 0.01965\n","Epoch: 04 [13082/16204 ( 81%)], Train Loss: 0.01964\n","Epoch: 04 [13102/16204 ( 81%)], Train Loss: 0.01961\n","Epoch: 04 [13122/16204 ( 81%)], Train Loss: 0.01959\n","Epoch: 04 [13142/16204 ( 81%)], Train Loss: 0.01982\n","Epoch: 04 [13162/16204 ( 81%)], Train Loss: 0.01997\n","Epoch: 04 [13182/16204 ( 81%)], Train Loss: 0.01999\n","Epoch: 04 [13202/16204 ( 81%)], Train Loss: 0.01998\n","Epoch: 04 [13222/16204 ( 82%)], Train Loss: 0.01999\n","Epoch: 04 [13242/16204 ( 82%)], Train Loss: 0.01998\n","Epoch: 04 [13262/16204 ( 82%)], Train Loss: 0.01998\n","Epoch: 04 [13282/16204 ( 82%)], Train Loss: 0.01996\n","Epoch: 04 [13302/16204 ( 82%)], Train Loss: 0.01994\n","Epoch: 04 [13322/16204 ( 82%)], Train Loss: 0.01993\n","Epoch: 04 [13342/16204 ( 82%)], Train Loss: 0.01991\n","Epoch: 04 [13362/16204 ( 82%)], Train Loss: 0.01988\n","Epoch: 04 [13382/16204 ( 83%)], Train Loss: 0.01985\n","Epoch: 04 [13402/16204 ( 83%)], Train Loss: 0.01984\n","Epoch: 04 [13422/16204 ( 83%)], Train Loss: 0.01982\n","Epoch: 04 [13442/16204 ( 83%)], Train Loss: 0.01980\n","Epoch: 04 [13462/16204 ( 83%)], Train Loss: 0.01977\n","Epoch: 04 [13482/16204 ( 83%)], Train Loss: 0.01974\n","Epoch: 04 [13502/16204 ( 83%)], Train Loss: 0.01972\n","Epoch: 04 [13522/16204 ( 83%)], Train Loss: 0.01969\n","Epoch: 04 [13542/16204 ( 84%)], Train Loss: 0.01967\n","Epoch: 04 [13562/16204 ( 84%)], Train Loss: 0.01968\n","Epoch: 04 [13582/16204 ( 84%)], Train Loss: 0.01965\n","Epoch: 04 [13602/16204 ( 84%)], Train Loss: 0.01963\n","Epoch: 04 [13622/16204 ( 84%)], Train Loss: 0.01976\n","Epoch: 04 [13642/16204 ( 84%)], Train Loss: 0.01973\n","Epoch: 04 [13662/16204 ( 84%)], Train Loss: 0.01971\n","Epoch: 04 [13682/16204 ( 84%)], Train Loss: 0.01972\n","Epoch: 04 [13702/16204 ( 85%)], Train Loss: 0.01970\n","Epoch: 04 [13722/16204 ( 85%)], Train Loss: 0.01991\n","Epoch: 04 [13742/16204 ( 85%)], Train Loss: 0.01996\n","Epoch: 04 [13762/16204 ( 85%)], Train Loss: 0.02001\n","Epoch: 04 [13782/16204 ( 85%)], Train Loss: 0.01998\n","Epoch: 04 [13802/16204 ( 85%)], Train Loss: 0.02000\n","Epoch: 04 [13822/16204 ( 85%)], Train Loss: 0.01999\n","Epoch: 04 [13842/16204 ( 85%)], Train Loss: 0.01998\n","Epoch: 04 [13862/16204 ( 86%)], Train Loss: 0.02003\n","Epoch: 04 [13882/16204 ( 86%)], Train Loss: 0.02005\n","Epoch: 04 [13902/16204 ( 86%)], Train Loss: 0.02004\n","Epoch: 04 [13922/16204 ( 86%)], Train Loss: 0.02002\n","Epoch: 04 [13942/16204 ( 86%)], Train Loss: 0.02002\n","Epoch: 04 [13962/16204 ( 86%)], Train Loss: 0.02000\n","Epoch: 04 [13982/16204 ( 86%)], Train Loss: 0.01999\n","Epoch: 04 [14002/16204 ( 86%)], Train Loss: 0.02000\n","Epoch: 04 [14022/16204 ( 87%)], Train Loss: 0.01998\n","Epoch: 04 [14042/16204 ( 87%)], Train Loss: 0.01998\n","Epoch: 04 [14062/16204 ( 87%)], Train Loss: 0.01997\n","Epoch: 04 [14082/16204 ( 87%)], Train Loss: 0.01997\n","Epoch: 04 [14102/16204 ( 87%)], Train Loss: 0.01995\n","Epoch: 04 [14122/16204 ( 87%)], Train Loss: 0.01994\n","Epoch: 04 [14142/16204 ( 87%)], Train Loss: 0.01993\n","Epoch: 04 [14162/16204 ( 87%)], Train Loss: 0.01991\n","Epoch: 04 [14182/16204 ( 88%)], Train Loss: 0.01988\n","Epoch: 04 [14202/16204 ( 88%)], Train Loss: 0.01987\n","Epoch: 04 [14222/16204 ( 88%)], Train Loss: 0.01985\n","Epoch: 04 [14242/16204 ( 88%)], Train Loss: 0.01983\n","Epoch: 04 [14262/16204 ( 88%)], Train Loss: 0.01982\n","Epoch: 04 [14282/16204 ( 88%)], Train Loss: 0.01979\n","Epoch: 04 [14302/16204 ( 88%)], Train Loss: 0.01978\n","Epoch: 04 [14322/16204 ( 88%)], Train Loss: 0.01978\n","Epoch: 04 [14342/16204 ( 89%)], Train Loss: 0.01975\n","Epoch: 04 [14362/16204 ( 89%)], Train Loss: 0.01973\n","Epoch: 04 [14382/16204 ( 89%)], Train Loss: 0.01971\n","Epoch: 04 [14402/16204 ( 89%)], Train Loss: 0.01969\n","Epoch: 04 [14422/16204 ( 89%)], Train Loss: 0.01967\n","Epoch: 04 [14442/16204 ( 89%)], Train Loss: 0.01964\n","Epoch: 04 [14462/16204 ( 89%)], Train Loss: 0.01962\n","Epoch: 04 [14482/16204 ( 89%)], Train Loss: 0.01966\n","Epoch: 04 [14502/16204 ( 89%)], Train Loss: 0.01964\n","Epoch: 04 [14522/16204 ( 90%)], Train Loss: 0.01963\n","Epoch: 04 [14542/16204 ( 90%)], Train Loss: 0.01962\n","Epoch: 04 [14562/16204 ( 90%)], Train Loss: 0.01960\n","Epoch: 04 [14582/16204 ( 90%)], Train Loss: 0.01959\n","Epoch: 04 [14602/16204 ( 90%)], Train Loss: 0.01956\n","Epoch: 04 [14622/16204 ( 90%)], Train Loss: 0.01963\n","Epoch: 04 [14642/16204 ( 90%)], Train Loss: 0.01961\n","Epoch: 04 [14662/16204 ( 90%)], Train Loss: 0.01958\n","Epoch: 04 [14682/16204 ( 91%)], Train Loss: 0.01956\n","Epoch: 04 [14702/16204 ( 91%)], Train Loss: 0.01955\n","Epoch: 04 [14722/16204 ( 91%)], Train Loss: 0.01952\n","Epoch: 04 [14742/16204 ( 91%)], Train Loss: 0.01951\n","Epoch: 04 [14762/16204 ( 91%)], Train Loss: 0.01948\n","Epoch: 04 [14782/16204 ( 91%)], Train Loss: 0.01953\n","Epoch: 04 [14802/16204 ( 91%)], Train Loss: 0.01950\n","Epoch: 04 [14822/16204 ( 91%)], Train Loss: 0.01948\n","Epoch: 04 [14842/16204 ( 92%)], Train Loss: 0.01945\n","Epoch: 04 [14862/16204 ( 92%)], Train Loss: 0.01943\n","Epoch: 04 [14882/16204 ( 92%)], Train Loss: 0.01940\n","Epoch: 04 [14902/16204 ( 92%)], Train Loss: 0.01938\n","Epoch: 04 [14922/16204 ( 92%)], Train Loss: 0.01935\n","Epoch: 04 [14942/16204 ( 92%)], Train Loss: 0.01933\n","Epoch: 04 [14962/16204 ( 92%)], Train Loss: 0.01933\n","Epoch: 04 [14982/16204 ( 92%)], Train Loss: 0.01948\n","Epoch: 04 [15002/16204 ( 93%)], Train Loss: 0.01945\n","Epoch: 04 [15022/16204 ( 93%)], Train Loss: 0.01943\n","Epoch: 04 [15042/16204 ( 93%)], Train Loss: 0.01940\n","Epoch: 04 [15062/16204 ( 93%)], Train Loss: 0.01942\n","Epoch: 04 [15082/16204 ( 93%)], Train Loss: 0.01939\n","Epoch: 04 [15102/16204 ( 93%)], Train Loss: 0.01948\n","Epoch: 04 [15122/16204 ( 93%)], Train Loss: 0.01946\n","Epoch: 04 [15142/16204 ( 93%)], Train Loss: 0.01944\n","Epoch: 04 [15162/16204 ( 94%)], Train Loss: 0.01942\n","Epoch: 04 [15182/16204 ( 94%)], Train Loss: 0.01939\n","Epoch: 04 [15202/16204 ( 94%)], Train Loss: 0.01938\n","Epoch: 04 [15222/16204 ( 94%)], Train Loss: 0.01936\n","Epoch: 04 [15242/16204 ( 94%)], Train Loss: 0.01951\n","Epoch: 04 [15262/16204 ( 94%)], Train Loss: 0.01949\n","Epoch: 04 [15282/16204 ( 94%)], Train Loss: 0.01947\n","Epoch: 04 [15302/16204 ( 94%)], Train Loss: 0.01953\n","Epoch: 04 [15322/16204 ( 95%)], Train Loss: 0.01964\n","Epoch: 04 [15342/16204 ( 95%)], Train Loss: 0.01963\n","Epoch: 04 [15362/16204 ( 95%)], Train Loss: 0.01969\n","Epoch: 04 [15382/16204 ( 95%)], Train Loss: 0.01967\n","Epoch: 04 [15402/16204 ( 95%)], Train Loss: 0.01966\n","Epoch: 04 [15422/16204 ( 95%)], Train Loss: 0.01964\n","Epoch: 04 [15442/16204 ( 95%)], Train Loss: 0.01962\n","Epoch: 04 [15462/16204 ( 95%)], Train Loss: 0.01961\n","Epoch: 04 [15482/16204 ( 96%)], Train Loss: 0.01959\n","Epoch: 04 [15502/16204 ( 96%)], Train Loss: 0.01958\n","Epoch: 04 [15522/16204 ( 96%)], Train Loss: 0.01961\n","Epoch: 04 [15542/16204 ( 96%)], Train Loss: 0.01958\n","Epoch: 04 [15562/16204 ( 96%)], Train Loss: 0.01956\n","Epoch: 04 [15582/16204 ( 96%)], Train Loss: 0.01954\n","Epoch: 04 [15602/16204 ( 96%)], Train Loss: 0.01951\n","Epoch: 04 [15622/16204 ( 96%)], Train Loss: 0.01953\n","Epoch: 04 [15642/16204 ( 97%)], Train Loss: 0.01951\n","Epoch: 04 [15662/16204 ( 97%)], Train Loss: 0.01949\n","Epoch: 04 [15682/16204 ( 97%)], Train Loss: 0.01952\n","Epoch: 04 [15702/16204 ( 97%)], Train Loss: 0.01951\n","Epoch: 04 [15722/16204 ( 97%)], Train Loss: 0.01950\n","Epoch: 04 [15742/16204 ( 97%)], Train Loss: 0.01947\n","Epoch: 04 [15762/16204 ( 97%)], Train Loss: 0.01945\n","Epoch: 04 [15782/16204 ( 97%)], Train Loss: 0.01943\n","Epoch: 04 [15802/16204 ( 98%)], Train Loss: 0.01942\n","Epoch: 04 [15822/16204 ( 98%)], Train Loss: 0.01940\n","Epoch: 04 [15842/16204 ( 98%)], Train Loss: 0.01938\n","Epoch: 04 [15862/16204 ( 98%)], Train Loss: 0.01936\n","Epoch: 04 [15882/16204 ( 98%)], Train Loss: 0.01934\n","Epoch: 04 [15902/16204 ( 98%)], Train Loss: 0.01932\n","Epoch: 04 [15922/16204 ( 98%)], Train Loss: 0.01929\n","Epoch: 04 [15942/16204 ( 98%)], Train Loss: 0.01928\n","Epoch: 04 [15962/16204 ( 99%)], Train Loss: 0.01926\n","Epoch: 04 [15982/16204 ( 99%)], Train Loss: 0.01923\n","Epoch: 04 [16002/16204 ( 99%)], Train Loss: 0.01933\n","Epoch: 04 [16022/16204 ( 99%)], Train Loss: 0.01938\n","Epoch: 04 [16042/16204 ( 99%)], Train Loss: 0.01938\n","Epoch: 04 [16062/16204 ( 99%)], Train Loss: 0.01935\n","Epoch: 04 [16082/16204 ( 99%)], Train Loss: 0.01934\n","Epoch: 04 [16102/16204 ( 99%)], Train Loss: 0.01932\n","Epoch: 04 [16122/16204 ( 99%)], Train Loss: 0.01930\n","Epoch: 04 [16142/16204 (100%)], Train Loss: 0.01932\n","Epoch: 04 [16162/16204 (100%)], Train Loss: 0.01930\n","Epoch: 04 [16182/16204 (100%)], Train Loss: 0.01928\n","Epoch: 04 [16202/16204 (100%)], Train Loss: 0.01926\n","Epoch: 04 [16204/16204 (100%)], Train Loss: 0.01926\n","----Validation Results Summary----\n","Epoch: [4] Valid Loss: 0.00000\n","\n","Epoch: 05 [    2/16204 (  0%)], Train Loss: 0.02898\n","Epoch: 05 [   22/16204 (  0%)], Train Loss: 0.03919\n","Epoch: 05 [   42/16204 (  0%)], Train Loss: 0.02354\n","Epoch: 05 [   62/16204 (  0%)], Train Loss: 0.02903\n","Epoch: 05 [   82/16204 (  1%)], Train Loss: 0.03242\n","Epoch: 05 [  102/16204 (  1%)], Train Loss: 0.02613\n","Epoch: 05 [  122/16204 (  1%)], Train Loss: 0.02237\n","Epoch: 05 [  142/16204 (  1%)], Train Loss: 0.02337\n","Epoch: 05 [  162/16204 (  1%)], Train Loss: 0.02113\n","Epoch: 05 [  182/16204 (  1%)], Train Loss: 0.01946\n","Epoch: 05 [  202/16204 (  1%)], Train Loss: 0.01802\n","Epoch: 05 [  222/16204 (  1%)], Train Loss: 0.01744\n","Epoch: 05 [  242/16204 (  1%)], Train Loss: 0.01802\n","Epoch: 05 [  262/16204 (  2%)], Train Loss: 0.01676\n","Epoch: 05 [  282/16204 (  2%)], Train Loss: 0.01564\n","Epoch: 05 [  302/16204 (  2%)], Train Loss: 0.01479\n","Epoch: 05 [  322/16204 (  2%)], Train Loss: 0.01397\n","Epoch: 05 [  342/16204 (  2%)], Train Loss: 0.01359\n","Epoch: 05 [  362/16204 (  2%)], Train Loss: 0.01304\n","Epoch: 05 [  382/16204 (  2%)], Train Loss: 0.01306\n","Epoch: 05 [  402/16204 (  2%)], Train Loss: 0.01361\n","Epoch: 05 [  422/16204 (  3%)], Train Loss: 0.01314\n","Epoch: 05 [  442/16204 (  3%)], Train Loss: 0.01263\n","Epoch: 05 [  462/16204 (  3%)], Train Loss: 0.01210\n","Epoch: 05 [  482/16204 (  3%)], Train Loss: 0.01160\n","Epoch: 05 [  502/16204 (  3%)], Train Loss: 0.01118\n","Epoch: 05 [  522/16204 (  3%)], Train Loss: 0.01088\n","Epoch: 05 [  542/16204 (  3%)], Train Loss: 0.01053\n","Epoch: 05 [  562/16204 (  3%)], Train Loss: 0.01025\n","Epoch: 05 [  582/16204 (  4%)], Train Loss: 0.00995\n","Epoch: 05 [  602/16204 (  4%)], Train Loss: 0.00978\n","Epoch: 05 [  622/16204 (  4%)], Train Loss: 0.00952\n","Epoch: 05 [  642/16204 (  4%)], Train Loss: 0.00947\n","Epoch: 05 [  662/16204 (  4%)], Train Loss: 0.00922\n","Epoch: 05 [  682/16204 (  4%)], Train Loss: 0.00949\n","Epoch: 05 [  702/16204 (  4%)], Train Loss: 0.00923\n","Epoch: 05 [  722/16204 (  4%)], Train Loss: 0.00901\n","Epoch: 05 [  742/16204 (  5%)], Train Loss: 0.00977\n","Epoch: 05 [  762/16204 (  5%)], Train Loss: 0.00955\n","Epoch: 05 [  782/16204 (  5%)], Train Loss: 0.00965\n","Epoch: 05 [  802/16204 (  5%)], Train Loss: 0.01095\n","Epoch: 05 [  822/16204 (  5%)], Train Loss: 0.01096\n","Epoch: 05 [  842/16204 (  5%)], Train Loss: 0.01080\n","Epoch: 05 [  862/16204 (  5%)], Train Loss: 0.01057\n","Epoch: 05 [  882/16204 (  5%)], Train Loss: 0.01044\n","Epoch: 05 [  902/16204 (  6%)], Train Loss: 0.01028\n","Epoch: 05 [  922/16204 (  6%)], Train Loss: 0.01017\n","Epoch: 05 [  942/16204 (  6%)], Train Loss: 0.01021\n","Epoch: 05 [  962/16204 (  6%)], Train Loss: 0.01012\n","Epoch: 05 [  982/16204 (  6%)], Train Loss: 0.01002\n","Epoch: 05 [ 1002/16204 (  6%)], Train Loss: 0.00985\n","Epoch: 05 [ 1022/16204 (  6%)], Train Loss: 0.01054\n","Epoch: 05 [ 1042/16204 (  6%)], Train Loss: 0.01035\n","Epoch: 05 [ 1062/16204 (  7%)], Train Loss: 0.01018\n","Epoch: 05 [ 1082/16204 (  7%)], Train Loss: 0.01003\n","Epoch: 05 [ 1102/16204 (  7%)], Train Loss: 0.00985\n","Epoch: 05 [ 1122/16204 (  7%)], Train Loss: 0.00988\n","Epoch: 05 [ 1142/16204 (  7%)], Train Loss: 0.00973\n","Epoch: 05 [ 1162/16204 (  7%)], Train Loss: 0.00956\n","Epoch: 05 [ 1182/16204 (  7%)], Train Loss: 0.01048\n","Epoch: 05 [ 1202/16204 (  7%)], Train Loss: 0.01031\n","Epoch: 05 [ 1222/16204 (  8%)], Train Loss: 0.01023\n","Epoch: 05 [ 1242/16204 (  8%)], Train Loss: 0.01008\n","Epoch: 05 [ 1262/16204 (  8%)], Train Loss: 0.00993\n","Epoch: 05 [ 1282/16204 (  8%)], Train Loss: 0.00979\n","Epoch: 05 [ 1302/16204 (  8%)], Train Loss: 0.00969\n","Epoch: 05 [ 1322/16204 (  8%)], Train Loss: 0.00958\n","Epoch: 05 [ 1342/16204 (  8%)], Train Loss: 0.00946\n","Epoch: 05 [ 1362/16204 (  8%)], Train Loss: 0.00938\n","Epoch: 05 [ 1382/16204 (  9%)], Train Loss: 0.00928\n","Epoch: 05 [ 1402/16204 (  9%)], Train Loss: 0.00916\n","Epoch: 05 [ 1422/16204 (  9%)], Train Loss: 0.00908\n","Epoch: 05 [ 1442/16204 (  9%)], Train Loss: 0.00897\n","Epoch: 05 [ 1462/16204 (  9%)], Train Loss: 0.00941\n","Epoch: 05 [ 1482/16204 (  9%)], Train Loss: 0.00963\n","Epoch: 05 [ 1502/16204 (  9%)], Train Loss: 0.00951\n","Epoch: 05 [ 1522/16204 (  9%)], Train Loss: 0.00940\n","Epoch: 05 [ 1542/16204 ( 10%)], Train Loss: 0.00929\n","Epoch: 05 [ 1562/16204 ( 10%)], Train Loss: 0.00940\n","Epoch: 05 [ 1582/16204 ( 10%)], Train Loss: 0.00928\n","Epoch: 05 [ 1602/16204 ( 10%)], Train Loss: 0.00917\n","Epoch: 05 [ 1622/16204 ( 10%)], Train Loss: 0.00910\n","Epoch: 05 [ 1642/16204 ( 10%)], Train Loss: 0.00902\n","Epoch: 05 [ 1662/16204 ( 10%)], Train Loss: 0.00902\n","Epoch: 05 [ 1682/16204 ( 10%)], Train Loss: 0.00892\n","Epoch: 05 [ 1702/16204 ( 11%)], Train Loss: 0.00883\n","Epoch: 05 [ 1722/16204 ( 11%)], Train Loss: 0.00874\n","Epoch: 05 [ 1742/16204 ( 11%)], Train Loss: 0.00866\n","Epoch: 05 [ 1762/16204 ( 11%)], Train Loss: 0.00861\n","Epoch: 05 [ 1782/16204 ( 11%)], Train Loss: 0.00853\n","Epoch: 05 [ 1802/16204 ( 11%)], Train Loss: 0.00844\n","Epoch: 05 [ 1822/16204 ( 11%)], Train Loss: 0.00926\n","Epoch: 05 [ 1842/16204 ( 11%)], Train Loss: 0.00917\n","Epoch: 05 [ 1862/16204 ( 11%)], Train Loss: 0.00922\n","Epoch: 05 [ 1882/16204 ( 12%)], Train Loss: 0.00931\n","Epoch: 05 [ 1902/16204 ( 12%)], Train Loss: 0.00923\n","Epoch: 05 [ 1922/16204 ( 12%)], Train Loss: 0.00920\n","Epoch: 05 [ 1942/16204 ( 12%)], Train Loss: 0.00912\n","Epoch: 05 [ 1962/16204 ( 12%)], Train Loss: 0.00908\n","Epoch: 05 [ 1982/16204 ( 12%)], Train Loss: 0.01028\n","Epoch: 05 [ 2002/16204 ( 12%)], Train Loss: 0.01019\n","Epoch: 05 [ 2022/16204 ( 12%)], Train Loss: 0.01037\n","Epoch: 05 [ 2042/16204 ( 13%)], Train Loss: 0.01035\n","Epoch: 05 [ 2062/16204 ( 13%)], Train Loss: 0.01193\n","Epoch: 05 [ 2082/16204 ( 13%)], Train Loss: 0.01193\n","Epoch: 05 [ 2102/16204 ( 13%)], Train Loss: 0.01182\n","Epoch: 05 [ 2122/16204 ( 13%)], Train Loss: 0.01173\n","Epoch: 05 [ 2142/16204 ( 13%)], Train Loss: 0.01173\n","Epoch: 05 [ 2162/16204 ( 13%)], Train Loss: 0.01163\n","Epoch: 05 [ 2182/16204 ( 13%)], Train Loss: 0.01157\n","Epoch: 05 [ 2202/16204 ( 14%)], Train Loss: 0.01157\n","Epoch: 05 [ 2222/16204 ( 14%)], Train Loss: 0.01147\n","Epoch: 05 [ 2242/16204 ( 14%)], Train Loss: 0.01152\n","Epoch: 05 [ 2262/16204 ( 14%)], Train Loss: 0.01177\n","Epoch: 05 [ 2282/16204 ( 14%)], Train Loss: 0.01173\n","Epoch: 05 [ 2302/16204 ( 14%)], Train Loss: 0.01167\n","Epoch: 05 [ 2322/16204 ( 14%)], Train Loss: 0.01165\n","Epoch: 05 [ 2342/16204 ( 14%)], Train Loss: 0.01158\n","Epoch: 05 [ 2362/16204 ( 15%)], Train Loss: 0.01207\n","Epoch: 05 [ 2382/16204 ( 15%)], Train Loss: 0.01198\n","Epoch: 05 [ 2402/16204 ( 15%)], Train Loss: 0.01190\n","Epoch: 05 [ 2422/16204 ( 15%)], Train Loss: 0.01184\n","Epoch: 05 [ 2442/16204 ( 15%)], Train Loss: 0.01183\n","Epoch: 05 [ 2462/16204 ( 15%)], Train Loss: 0.01180\n","Epoch: 05 [ 2482/16204 ( 15%)], Train Loss: 0.01200\n","Epoch: 05 [ 2502/16204 ( 15%)], Train Loss: 0.01196\n","Epoch: 05 [ 2522/16204 ( 16%)], Train Loss: 0.01190\n","Epoch: 05 [ 2542/16204 ( 16%)], Train Loss: 0.01181\n","Epoch: 05 [ 2562/16204 ( 16%)], Train Loss: 0.01177\n","Epoch: 05 [ 2582/16204 ( 16%)], Train Loss: 0.01173\n","Epoch: 05 [ 2602/16204 ( 16%)], Train Loss: 0.01171\n","Epoch: 05 [ 2622/16204 ( 16%)], Train Loss: 0.01177\n","Epoch: 05 [ 2642/16204 ( 16%)], Train Loss: 0.01168\n","Epoch: 05 [ 2662/16204 ( 16%)], Train Loss: 0.01177\n","Epoch: 05 [ 2682/16204 ( 17%)], Train Loss: 0.01173\n","Epoch: 05 [ 2702/16204 ( 17%)], Train Loss: 0.01166\n","Epoch: 05 [ 2722/16204 ( 17%)], Train Loss: 0.01159\n","Epoch: 05 [ 2742/16204 ( 17%)], Train Loss: 0.01151\n","Epoch: 05 [ 2762/16204 ( 17%)], Train Loss: 0.01143\n","Epoch: 05 [ 2782/16204 ( 17%)], Train Loss: 0.01136\n","Epoch: 05 [ 2802/16204 ( 17%)], Train Loss: 0.01139\n","Epoch: 05 [ 2822/16204 ( 17%)], Train Loss: 0.01144\n","Epoch: 05 [ 2842/16204 ( 18%)], Train Loss: 0.01137\n","Epoch: 05 [ 2862/16204 ( 18%)], Train Loss: 0.01262\n","Epoch: 05 [ 2882/16204 ( 18%)], Train Loss: 0.01253\n","Epoch: 05 [ 2902/16204 ( 18%)], Train Loss: 0.01248\n","Epoch: 05 [ 2922/16204 ( 18%)], Train Loss: 0.01241\n","Epoch: 05 [ 2942/16204 ( 18%)], Train Loss: 0.01233\n","Epoch: 05 [ 2962/16204 ( 18%)], Train Loss: 0.01225\n","Epoch: 05 [ 2982/16204 ( 18%)], Train Loss: 0.01219\n","Epoch: 05 [ 3002/16204 ( 19%)], Train Loss: 0.01211\n","Epoch: 05 [ 3022/16204 ( 19%)], Train Loss: 0.01203\n","Epoch: 05 [ 3042/16204 ( 19%)], Train Loss: 0.01196\n","Epoch: 05 [ 3062/16204 ( 19%)], Train Loss: 0.01190\n","Epoch: 05 [ 3082/16204 ( 19%)], Train Loss: 0.01191\n","Epoch: 05 [ 3102/16204 ( 19%)], Train Loss: 0.01186\n","Epoch: 05 [ 3122/16204 ( 19%)], Train Loss: 0.01179\n","Epoch: 05 [ 3142/16204 ( 19%)], Train Loss: 0.01243\n","Epoch: 05 [ 3162/16204 ( 20%)], Train Loss: 0.01236\n","Epoch: 05 [ 3182/16204 ( 20%)], Train Loss: 0.01229\n","Epoch: 05 [ 3202/16204 ( 20%)], Train Loss: 0.01222\n","Epoch: 05 [ 3222/16204 ( 20%)], Train Loss: 0.01215\n","Epoch: 05 [ 3242/16204 ( 20%)], Train Loss: 0.01216\n","Epoch: 05 [ 3262/16204 ( 20%)], Train Loss: 0.01215\n","Epoch: 05 [ 3282/16204 ( 20%)], Train Loss: 0.01208\n","Epoch: 05 [ 3302/16204 ( 20%)], Train Loss: 0.01223\n","Epoch: 05 [ 3322/16204 ( 21%)], Train Loss: 0.01223\n","Epoch: 05 [ 3342/16204 ( 21%)], Train Loss: 0.01216\n","Epoch: 05 [ 3362/16204 ( 21%)], Train Loss: 0.01210\n","Epoch: 05 [ 3382/16204 ( 21%)], Train Loss: 0.01208\n","Epoch: 05 [ 3402/16204 ( 21%)], Train Loss: 0.01209\n","Epoch: 05 [ 3422/16204 ( 21%)], Train Loss: 0.01206\n","Epoch: 05 [ 3442/16204 ( 21%)], Train Loss: 0.01201\n","Epoch: 05 [ 3462/16204 ( 21%)], Train Loss: 0.01207\n","Epoch: 05 [ 3482/16204 ( 21%)], Train Loss: 0.01203\n","Epoch: 05 [ 3502/16204 ( 22%)], Train Loss: 0.01200\n","Epoch: 05 [ 3522/16204 ( 22%)], Train Loss: 0.01193\n","Epoch: 05 [ 3542/16204 ( 22%)], Train Loss: 0.01190\n","Epoch: 05 [ 3562/16204 ( 22%)], Train Loss: 0.01184\n","Epoch: 05 [ 3582/16204 ( 22%)], Train Loss: 0.01180\n","Epoch: 05 [ 3602/16204 ( 22%)], Train Loss: 0.01174\n","Epoch: 05 [ 3622/16204 ( 22%)], Train Loss: 0.01174\n","Epoch: 05 [ 3642/16204 ( 22%)], Train Loss: 0.01169\n","Epoch: 05 [ 3662/16204 ( 23%)], Train Loss: 0.01165\n","Epoch: 05 [ 3682/16204 ( 23%)], Train Loss: 0.01161\n","Epoch: 05 [ 3702/16204 ( 23%)], Train Loss: 0.01157\n","Epoch: 05 [ 3722/16204 ( 23%)], Train Loss: 0.01158\n","Epoch: 05 [ 3742/16204 ( 23%)], Train Loss: 0.01153\n","Epoch: 05 [ 3762/16204 ( 23%)], Train Loss: 0.01147\n","Epoch: 05 [ 3782/16204 ( 23%)], Train Loss: 0.01142\n","Epoch: 05 [ 3802/16204 ( 23%)], Train Loss: 0.01152\n","Epoch: 05 [ 3822/16204 ( 24%)], Train Loss: 0.01157\n","Epoch: 05 [ 3842/16204 ( 24%)], Train Loss: 0.01152\n","Epoch: 05 [ 3862/16204 ( 24%)], Train Loss: 0.01147\n","Epoch: 05 [ 3882/16204 ( 24%)], Train Loss: 0.01153\n","Epoch: 05 [ 3902/16204 ( 24%)], Train Loss: 0.01148\n","Epoch: 05 [ 3922/16204 ( 24%)], Train Loss: 0.01160\n","Epoch: 05 [ 3942/16204 ( 24%)], Train Loss: 0.01156\n","Epoch: 05 [ 3962/16204 ( 24%)], Train Loss: 0.01151\n","Epoch: 05 [ 3982/16204 ( 25%)], Train Loss: 0.01146\n","Epoch: 05 [ 4002/16204 ( 25%)], Train Loss: 0.01141\n","Epoch: 05 [ 4022/16204 ( 25%)], Train Loss: 0.01139\n","Epoch: 05 [ 4042/16204 ( 25%)], Train Loss: 0.01135\n","Epoch: 05 [ 4062/16204 ( 25%)], Train Loss: 0.01129\n","Epoch: 05 [ 4082/16204 ( 25%)], Train Loss: 0.01124\n","Epoch: 05 [ 4102/16204 ( 25%)], Train Loss: 0.01119\n","Epoch: 05 [ 4122/16204 ( 25%)], Train Loss: 0.01120\n","Epoch: 05 [ 4142/16204 ( 26%)], Train Loss: 0.01131\n","Epoch: 05 [ 4162/16204 ( 26%)], Train Loss: 0.01126\n","Epoch: 05 [ 4182/16204 ( 26%)], Train Loss: 0.01123\n","Epoch: 05 [ 4202/16204 ( 26%)], Train Loss: 0.01119\n","Epoch: 05 [ 4222/16204 ( 26%)], Train Loss: 0.01116\n","Epoch: 05 [ 4242/16204 ( 26%)], Train Loss: 0.01112\n","Epoch: 05 [ 4262/16204 ( 26%)], Train Loss: 0.01109\n","Epoch: 05 [ 4282/16204 ( 26%)], Train Loss: 0.01104\n","Epoch: 05 [ 4302/16204 ( 27%)], Train Loss: 0.01099\n","Epoch: 05 [ 4322/16204 ( 27%)], Train Loss: 0.01095\n","Epoch: 05 [ 4342/16204 ( 27%)], Train Loss: 0.01090\n","Epoch: 05 [ 4362/16204 ( 27%)], Train Loss: 0.01085\n","Epoch: 05 [ 4382/16204 ( 27%)], Train Loss: 0.01081\n","Epoch: 05 [ 4402/16204 ( 27%)], Train Loss: 0.01076\n","Epoch: 05 [ 4422/16204 ( 27%)], Train Loss: 0.01081\n","Epoch: 05 [ 4442/16204 ( 27%)], Train Loss: 0.01077\n","Epoch: 05 [ 4462/16204 ( 28%)], Train Loss: 0.01078\n","Epoch: 05 [ 4482/16204 ( 28%)], Train Loss: 0.01093\n","Epoch: 05 [ 4502/16204 ( 28%)], Train Loss: 0.01094\n","Epoch: 05 [ 4522/16204 ( 28%)], Train Loss: 0.01089\n","Epoch: 05 [ 4542/16204 ( 28%)], Train Loss: 0.01085\n","Epoch: 05 [ 4562/16204 ( 28%)], Train Loss: 0.01082\n","Epoch: 05 [ 4582/16204 ( 28%)], Train Loss: 0.01078\n","Epoch: 05 [ 4602/16204 ( 28%)], Train Loss: 0.01077\n","Epoch: 05 [ 4622/16204 ( 29%)], Train Loss: 0.01072\n","Epoch: 05 [ 4642/16204 ( 29%)], Train Loss: 0.01068\n","Epoch: 05 [ 4662/16204 ( 29%)], Train Loss: 0.01064\n","Epoch: 05 [ 4682/16204 ( 29%)], Train Loss: 0.01060\n","Epoch: 05 [ 4702/16204 ( 29%)], Train Loss: 0.01056\n","Epoch: 05 [ 4722/16204 ( 29%)], Train Loss: 0.01087\n","Epoch: 05 [ 4742/16204 ( 29%)], Train Loss: 0.01083\n","Epoch: 05 [ 4762/16204 ( 29%)], Train Loss: 0.01078\n","Epoch: 05 [ 4782/16204 ( 30%)], Train Loss: 0.01075\n","Epoch: 05 [ 4802/16204 ( 30%)], Train Loss: 0.01070\n","Epoch: 05 [ 4822/16204 ( 30%)], Train Loss: 0.01072\n","Epoch: 05 [ 4842/16204 ( 30%)], Train Loss: 0.01068\n","Epoch: 05 [ 4862/16204 ( 30%)], Train Loss: 0.01064\n","Epoch: 05 [ 4882/16204 ( 30%)], Train Loss: 0.01060\n","Epoch: 05 [ 4902/16204 ( 30%)], Train Loss: 0.01124\n","Epoch: 05 [ 4922/16204 ( 30%)], Train Loss: 0.01120\n","Epoch: 05 [ 4942/16204 ( 30%)], Train Loss: 0.01115\n","Epoch: 05 [ 4962/16204 ( 31%)], Train Loss: 0.01130\n","Epoch: 05 [ 4982/16204 ( 31%)], Train Loss: 0.01126\n","Epoch: 05 [ 5002/16204 ( 31%)], Train Loss: 0.01122\n","Epoch: 05 [ 5022/16204 ( 31%)], Train Loss: 0.01121\n","Epoch: 05 [ 5042/16204 ( 31%)], Train Loss: 0.01121\n","Epoch: 05 [ 5062/16204 ( 31%)], Train Loss: 0.01120\n","Epoch: 05 [ 5082/16204 ( 31%)], Train Loss: 0.01116\n","Epoch: 05 [ 5102/16204 ( 31%)], Train Loss: 0.01112\n","Epoch: 05 [ 5122/16204 ( 32%)], Train Loss: 0.01108\n","Epoch: 05 [ 5142/16204 ( 32%)], Train Loss: 0.01104\n","Epoch: 05 [ 5162/16204 ( 32%)], Train Loss: 0.01153\n","Epoch: 05 [ 5182/16204 ( 32%)], Train Loss: 0.01150\n","Epoch: 05 [ 5202/16204 ( 32%)], Train Loss: 0.01147\n","Epoch: 05 [ 5222/16204 ( 32%)], Train Loss: 0.01143\n","Epoch: 05 [ 5242/16204 ( 32%)], Train Loss: 0.01141\n","Epoch: 05 [ 5262/16204 ( 32%)], Train Loss: 0.01138\n","Epoch: 05 [ 5282/16204 ( 33%)], Train Loss: 0.01134\n","Epoch: 05 [ 5302/16204 ( 33%)], Train Loss: 0.01130\n","Epoch: 05 [ 5322/16204 ( 33%)], Train Loss: 0.01127\n","Epoch: 05 [ 5342/16204 ( 33%)], Train Loss: 0.01128\n","Epoch: 05 [ 5362/16204 ( 33%)], Train Loss: 0.01132\n","Epoch: 05 [ 5382/16204 ( 33%)], Train Loss: 0.01134\n","Epoch: 05 [ 5402/16204 ( 33%)], Train Loss: 0.01143\n","Epoch: 05 [ 5422/16204 ( 33%)], Train Loss: 0.01151\n","Epoch: 05 [ 5442/16204 ( 34%)], Train Loss: 0.01147\n","Epoch: 05 [ 5462/16204 ( 34%)], Train Loss: 0.01143\n","Epoch: 05 [ 5482/16204 ( 34%)], Train Loss: 0.01139\n","Epoch: 05 [ 5502/16204 ( 34%)], Train Loss: 0.01143\n","Epoch: 05 [ 5522/16204 ( 34%)], Train Loss: 0.01140\n","Epoch: 05 [ 5542/16204 ( 34%)], Train Loss: 0.01136\n","Epoch: 05 [ 5562/16204 ( 34%)], Train Loss: 0.01134\n","Epoch: 05 [ 5582/16204 ( 34%)], Train Loss: 0.01134\n","Epoch: 05 [ 5602/16204 ( 35%)], Train Loss: 0.01130\n","Epoch: 05 [ 5622/16204 ( 35%)], Train Loss: 0.01127\n","Epoch: 05 [ 5642/16204 ( 35%)], Train Loss: 0.01123\n","Epoch: 05 [ 5662/16204 ( 35%)], Train Loss: 0.01119\n","Epoch: 05 [ 5682/16204 ( 35%)], Train Loss: 0.01118\n","Epoch: 05 [ 5702/16204 ( 35%)], Train Loss: 0.01114\n","Epoch: 05 [ 5722/16204 ( 35%)], Train Loss: 0.01110\n","Epoch: 05 [ 5742/16204 ( 35%)], Train Loss: 0.01107\n","Epoch: 05 [ 5762/16204 ( 36%)], Train Loss: 0.01120\n","Epoch: 05 [ 5782/16204 ( 36%)], Train Loss: 0.01117\n","Epoch: 05 [ 5802/16204 ( 36%)], Train Loss: 0.01113\n","Epoch: 05 [ 5822/16204 ( 36%)], Train Loss: 0.01110\n","Epoch: 05 [ 5842/16204 ( 36%)], Train Loss: 0.01126\n","Epoch: 05 [ 5862/16204 ( 36%)], Train Loss: 0.01123\n","Epoch: 05 [ 5882/16204 ( 36%)], Train Loss: 0.01120\n","Epoch: 05 [ 5902/16204 ( 36%)], Train Loss: 0.01116\n","Epoch: 05 [ 5922/16204 ( 37%)], Train Loss: 0.01114\n","Epoch: 05 [ 5942/16204 ( 37%)], Train Loss: 0.01111\n","Epoch: 05 [ 5962/16204 ( 37%)], Train Loss: 0.01107\n","Epoch: 05 [ 5982/16204 ( 37%)], Train Loss: 0.01105\n","Epoch: 05 [ 6002/16204 ( 37%)], Train Loss: 0.01101\n","Epoch: 05 [ 6022/16204 ( 37%)], Train Loss: 0.01099\n","Epoch: 05 [ 6042/16204 ( 37%)], Train Loss: 0.01095\n","Epoch: 05 [ 6062/16204 ( 37%)], Train Loss: 0.01092\n","Epoch: 05 [ 6082/16204 ( 38%)], Train Loss: 0.01088\n","Epoch: 05 [ 6102/16204 ( 38%)], Train Loss: 0.01085\n","Epoch: 05 [ 6122/16204 ( 38%)], Train Loss: 0.01082\n","Epoch: 05 [ 6142/16204 ( 38%)], Train Loss: 0.01081\n","Epoch: 05 [ 6162/16204 ( 38%)], Train Loss: 0.01077\n","Epoch: 05 [ 6182/16204 ( 38%)], Train Loss: 0.01075\n","Epoch: 05 [ 6202/16204 ( 38%)], Train Loss: 0.01072\n","Epoch: 05 [ 6222/16204 ( 38%)], Train Loss: 0.01069\n","Epoch: 05 [ 6242/16204 ( 39%)], Train Loss: 0.01065\n","Epoch: 05 [ 6262/16204 ( 39%)], Train Loss: 0.01062\n","Epoch: 05 [ 6282/16204 ( 39%)], Train Loss: 0.01060\n","Epoch: 05 [ 6302/16204 ( 39%)], Train Loss: 0.01057\n","Epoch: 05 [ 6322/16204 ( 39%)], Train Loss: 0.01054\n","Epoch: 05 [ 6342/16204 ( 39%)], Train Loss: 0.01053\n","Epoch: 05 [ 6362/16204 ( 39%)], Train Loss: 0.01050\n","Epoch: 05 [ 6382/16204 ( 39%)], Train Loss: 0.01073\n","Epoch: 05 [ 6402/16204 ( 40%)], Train Loss: 0.01101\n","Epoch: 05 [ 6422/16204 ( 40%)], Train Loss: 0.01101\n","Epoch: 05 [ 6442/16204 ( 40%)], Train Loss: 0.01098\n","Epoch: 05 [ 6462/16204 ( 40%)], Train Loss: 0.01096\n","Epoch: 05 [ 6482/16204 ( 40%)], Train Loss: 0.01093\n","Epoch: 05 [ 6502/16204 ( 40%)], Train Loss: 0.01090\n","Epoch: 05 [ 6522/16204 ( 40%)], Train Loss: 0.01087\n","Epoch: 05 [ 6542/16204 ( 40%)], Train Loss: 0.01086\n","Epoch: 05 [ 6562/16204 ( 40%)], Train Loss: 0.01083\n","Epoch: 05 [ 6582/16204 ( 41%)], Train Loss: 0.01080\n","Epoch: 05 [ 6602/16204 ( 41%)], Train Loss: 0.01078\n","Epoch: 05 [ 6622/16204 ( 41%)], Train Loss: 0.01077\n","Epoch: 05 [ 6642/16204 ( 41%)], Train Loss: 0.01165\n","Epoch: 05 [ 6662/16204 ( 41%)], Train Loss: 0.01162\n","Epoch: 05 [ 6682/16204 ( 41%)], Train Loss: 0.01159\n","Epoch: 05 [ 6702/16204 ( 41%)], Train Loss: 0.01156\n","Epoch: 05 [ 6722/16204 ( 41%)], Train Loss: 0.01153\n","Epoch: 05 [ 6742/16204 ( 42%)], Train Loss: 0.01150\n","Epoch: 05 [ 6762/16204 ( 42%)], Train Loss: 0.01147\n","Epoch: 05 [ 6782/16204 ( 42%)], Train Loss: 0.01145\n","Epoch: 05 [ 6802/16204 ( 42%)], Train Loss: 0.01142\n","Epoch: 05 [ 6822/16204 ( 42%)], Train Loss: 0.01139\n","Epoch: 05 [ 6842/16204 ( 42%)], Train Loss: 0.01136\n","Epoch: 05 [ 6862/16204 ( 42%)], Train Loss: 0.01140\n","Epoch: 05 [ 6882/16204 ( 42%)], Train Loss: 0.01136\n","Epoch: 05 [ 6902/16204 ( 43%)], Train Loss: 0.01142\n","Epoch: 05 [ 6922/16204 ( 43%)], Train Loss: 0.01141\n","Epoch: 05 [ 6942/16204 ( 43%)], Train Loss: 0.01140\n","Epoch: 05 [ 6962/16204 ( 43%)], Train Loss: 0.01138\n","Epoch: 05 [ 6982/16204 ( 43%)], Train Loss: 0.01135\n","Epoch: 05 [ 7002/16204 ( 43%)], Train Loss: 0.01134\n","Epoch: 05 [ 7022/16204 ( 43%)], Train Loss: 0.01131\n","Epoch: 05 [ 7042/16204 ( 43%)], Train Loss: 0.01131\n","Epoch: 05 [ 7062/16204 ( 44%)], Train Loss: 0.01128\n","Epoch: 05 [ 7082/16204 ( 44%)], Train Loss: 0.01125\n","Epoch: 05 [ 7102/16204 ( 44%)], Train Loss: 0.01125\n","Epoch: 05 [ 7122/16204 ( 44%)], Train Loss: 0.01125\n","Epoch: 05 [ 7142/16204 ( 44%)], Train Loss: 0.01124\n","Epoch: 05 [ 7162/16204 ( 44%)], Train Loss: 0.01122\n","Epoch: 05 [ 7182/16204 ( 44%)], Train Loss: 0.01119\n","Epoch: 05 [ 7202/16204 ( 44%)], Train Loss: 0.01117\n","Epoch: 05 [ 7222/16204 ( 45%)], Train Loss: 0.01114\n","Epoch: 05 [ 7242/16204 ( 45%)], Train Loss: 0.01112\n","Epoch: 05 [ 7262/16204 ( 45%)], Train Loss: 0.01109\n","Epoch: 05 [ 7282/16204 ( 45%)], Train Loss: 0.01106\n","Epoch: 05 [ 7302/16204 ( 45%)], Train Loss: 0.01105\n","Epoch: 05 [ 7322/16204 ( 45%)], Train Loss: 0.01103\n","Epoch: 05 [ 7342/16204 ( 45%)], Train Loss: 0.01100\n","Epoch: 05 [ 7362/16204 ( 45%)], Train Loss: 0.01099\n","Epoch: 05 [ 7382/16204 ( 46%)], Train Loss: 0.01096\n","Epoch: 05 [ 7402/16204 ( 46%)], Train Loss: 0.01098\n","Epoch: 05 [ 7422/16204 ( 46%)], Train Loss: 0.01095\n","Epoch: 05 [ 7442/16204 ( 46%)], Train Loss: 0.01093\n","Epoch: 05 [ 7462/16204 ( 46%)], Train Loss: 0.01090\n","Epoch: 05 [ 7482/16204 ( 46%)], Train Loss: 0.01087\n","Epoch: 05 [ 7502/16204 ( 46%)], Train Loss: 0.01084\n","Epoch: 05 [ 7522/16204 ( 46%)], Train Loss: 0.01082\n","Epoch: 05 [ 7542/16204 ( 47%)], Train Loss: 0.01080\n","Epoch: 05 [ 7562/16204 ( 47%)], Train Loss: 0.01077\n","Epoch: 05 [ 7582/16204 ( 47%)], Train Loss: 0.01074\n","Epoch: 05 [ 7602/16204 ( 47%)], Train Loss: 0.01074\n","Epoch: 05 [ 7622/16204 ( 47%)], Train Loss: 0.01072\n","Epoch: 05 [ 7642/16204 ( 47%)], Train Loss: 0.01069\n","Epoch: 05 [ 7662/16204 ( 47%)], Train Loss: 0.01067\n","Epoch: 05 [ 7682/16204 ( 47%)], Train Loss: 0.01064\n","Epoch: 05 [ 7702/16204 ( 48%)], Train Loss: 0.01062\n","Epoch: 05 [ 7722/16204 ( 48%)], Train Loss: 0.01060\n","Epoch: 05 [ 7742/16204 ( 48%)], Train Loss: 0.01063\n","Epoch: 05 [ 7762/16204 ( 48%)], Train Loss: 0.01060\n","Epoch: 05 [ 7782/16204 ( 48%)], Train Loss: 0.01058\n","Epoch: 05 [ 7802/16204 ( 48%)], Train Loss: 0.01055\n","Epoch: 05 [ 7822/16204 ( 48%)], Train Loss: 0.01062\n","Epoch: 05 [ 7842/16204 ( 48%)], Train Loss: 0.01060\n","Epoch: 05 [ 7862/16204 ( 49%)], Train Loss: 0.01060\n","Epoch: 05 [ 7882/16204 ( 49%)], Train Loss: 0.01058\n","Epoch: 05 [ 7902/16204 ( 49%)], Train Loss: 0.01055\n","Epoch: 05 [ 7922/16204 ( 49%)], Train Loss: 0.01053\n","Epoch: 05 [ 7942/16204 ( 49%)], Train Loss: 0.01051\n","Epoch: 05 [ 7962/16204 ( 49%)], Train Loss: 0.01049\n","Epoch: 05 [ 7982/16204 ( 49%)], Train Loss: 0.01046\n","Epoch: 05 [ 8002/16204 ( 49%)], Train Loss: 0.01048\n","Epoch: 05 [ 8022/16204 ( 50%)], Train Loss: 0.01048\n","Epoch: 05 [ 8042/16204 ( 50%)], Train Loss: 0.01045\n","Epoch: 05 [ 8062/16204 ( 50%)], Train Loss: 0.01043\n","Epoch: 05 [ 8082/16204 ( 50%)], Train Loss: 0.01040\n","Epoch: 05 [ 8102/16204 ( 50%)], Train Loss: 0.01038\n","Epoch: 05 [ 8122/16204 ( 50%)], Train Loss: 0.01036\n","Epoch: 05 [ 8142/16204 ( 50%)], Train Loss: 0.01033\n","Epoch: 05 [ 8162/16204 ( 50%)], Train Loss: 0.01031\n","Epoch: 05 [ 8182/16204 ( 50%)], Train Loss: 0.01029\n","Epoch: 05 [ 8202/16204 ( 51%)], Train Loss: 0.01026\n","Epoch: 05 [ 8222/16204 ( 51%)], Train Loss: 0.01024\n","Epoch: 05 [ 8242/16204 ( 51%)], Train Loss: 0.01022\n","Epoch: 05 [ 8262/16204 ( 51%)], Train Loss: 0.01020\n","Epoch: 05 [ 8282/16204 ( 51%)], Train Loss: 0.01018\n","Epoch: 05 [ 8302/16204 ( 51%)], Train Loss: 0.01015\n","Epoch: 05 [ 8322/16204 ( 51%)], Train Loss: 0.01013\n","Epoch: 05 [ 8342/16204 ( 51%)], Train Loss: 0.01011\n","Epoch: 05 [ 8362/16204 ( 52%)], Train Loss: 0.01008\n","Epoch: 05 [ 8382/16204 ( 52%)], Train Loss: 0.01006\n","Epoch: 05 [ 8402/16204 ( 52%)], Train Loss: 0.01004\n","Epoch: 05 [ 8422/16204 ( 52%)], Train Loss: 0.01004\n","Epoch: 05 [ 8442/16204 ( 52%)], Train Loss: 0.01002\n","Epoch: 05 [ 8462/16204 ( 52%)], Train Loss: 0.01000\n","Epoch: 05 [ 8482/16204 ( 52%)], Train Loss: 0.00998\n","Epoch: 05 [ 8502/16204 ( 52%)], Train Loss: 0.00996\n","Epoch: 05 [ 8522/16204 ( 53%)], Train Loss: 0.00994\n","Epoch: 05 [ 8542/16204 ( 53%)], Train Loss: 0.00992\n","Epoch: 05 [ 8562/16204 ( 53%)], Train Loss: 0.00993\n","Epoch: 05 [ 8582/16204 ( 53%)], Train Loss: 0.00993\n","Epoch: 05 [ 8602/16204 ( 53%)], Train Loss: 0.00990\n","Epoch: 05 [ 8622/16204 ( 53%)], Train Loss: 0.00988\n","Epoch: 05 [ 8642/16204 ( 53%)], Train Loss: 0.00986\n","Epoch: 05 [ 8662/16204 ( 53%)], Train Loss: 0.00984\n","Epoch: 05 [ 8682/16204 ( 54%)], Train Loss: 0.00982\n","Epoch: 05 [ 8702/16204 ( 54%)], Train Loss: 0.00982\n","Epoch: 05 [ 8722/16204 ( 54%)], Train Loss: 0.00980\n","Epoch: 05 [ 8742/16204 ( 54%)], Train Loss: 0.00978\n","Epoch: 05 [ 8762/16204 ( 54%)], Train Loss: 0.00976\n","Epoch: 05 [ 8782/16204 ( 54%)], Train Loss: 0.00974\n","Epoch: 05 [ 8802/16204 ( 54%)], Train Loss: 0.00972\n","Epoch: 05 [ 8822/16204 ( 54%)], Train Loss: 0.00971\n","Epoch: 05 [ 8842/16204 ( 55%)], Train Loss: 0.00973\n","Epoch: 05 [ 8862/16204 ( 55%)], Train Loss: 0.00971\n","Epoch: 05 [ 8882/16204 ( 55%)], Train Loss: 0.00969\n","Epoch: 05 [ 8902/16204 ( 55%)], Train Loss: 0.00967\n","Epoch: 05 [ 8922/16204 ( 55%)], Train Loss: 0.00966\n","Epoch: 05 [ 8942/16204 ( 55%)], Train Loss: 0.00964\n","Epoch: 05 [ 8962/16204 ( 55%)], Train Loss: 0.00962\n","Epoch: 05 [ 8982/16204 ( 55%)], Train Loss: 0.00961\n","Epoch: 05 [ 9002/16204 ( 56%)], Train Loss: 0.00960\n","Epoch: 05 [ 9022/16204 ( 56%)], Train Loss: 0.00980\n","Epoch: 05 [ 9042/16204 ( 56%)], Train Loss: 0.00980\n","Epoch: 05 [ 9062/16204 ( 56%)], Train Loss: 0.00978\n","Epoch: 05 [ 9082/16204 ( 56%)], Train Loss: 0.00976\n","Epoch: 05 [ 9102/16204 ( 56%)], Train Loss: 0.00982\n","Epoch: 05 [ 9122/16204 ( 56%)], Train Loss: 0.00981\n","Epoch: 05 [ 9142/16204 ( 56%)], Train Loss: 0.00980\n","Epoch: 05 [ 9162/16204 ( 57%)], Train Loss: 0.00978\n","Epoch: 05 [ 9182/16204 ( 57%)], Train Loss: 0.00976\n","Epoch: 05 [ 9202/16204 ( 57%)], Train Loss: 0.00974\n","Epoch: 05 [ 9222/16204 ( 57%)], Train Loss: 0.00972\n","Epoch: 05 [ 9242/16204 ( 57%)], Train Loss: 0.00970\n","Epoch: 05 [ 9262/16204 ( 57%)], Train Loss: 0.00968\n","Epoch: 05 [ 9282/16204 ( 57%)], Train Loss: 0.00966\n","Epoch: 05 [ 9302/16204 ( 57%)], Train Loss: 0.00964\n","Epoch: 05 [ 9322/16204 ( 58%)], Train Loss: 0.00962\n","Epoch: 05 [ 9342/16204 ( 58%)], Train Loss: 0.00960\n","Epoch: 05 [ 9362/16204 ( 58%)], Train Loss: 0.00959\n","Epoch: 05 [ 9382/16204 ( 58%)], Train Loss: 0.00956\n","Epoch: 05 [ 9402/16204 ( 58%)], Train Loss: 0.00955\n","Epoch: 05 [ 9422/16204 ( 58%)], Train Loss: 0.00953\n","Epoch: 05 [ 9442/16204 ( 58%)], Train Loss: 0.00952\n","Epoch: 05 [ 9462/16204 ( 58%)], Train Loss: 0.00950\n","Epoch: 05 [ 9482/16204 ( 59%)], Train Loss: 0.00949\n","Epoch: 05 [ 9502/16204 ( 59%)], Train Loss: 0.00947\n","Epoch: 05 [ 9522/16204 ( 59%)], Train Loss: 0.00945\n","Epoch: 05 [ 9542/16204 ( 59%)], Train Loss: 0.00943\n","Epoch: 05 [ 9562/16204 ( 59%)], Train Loss: 0.00943\n","Epoch: 05 [ 9582/16204 ( 59%)], Train Loss: 0.00941\n","Epoch: 05 [ 9602/16204 ( 59%)], Train Loss: 0.00941\n","Epoch: 05 [ 9622/16204 ( 59%)], Train Loss: 0.00939\n","Epoch: 05 [ 9642/16204 ( 60%)], Train Loss: 0.00938\n","Epoch: 05 [ 9662/16204 ( 60%)], Train Loss: 0.00937\n","Epoch: 05 [ 9682/16204 ( 60%)], Train Loss: 0.00936\n","Epoch: 05 [ 9702/16204 ( 60%)], Train Loss: 0.00934\n","Epoch: 05 [ 9722/16204 ( 60%)], Train Loss: 0.00934\n","Epoch: 05 [ 9742/16204 ( 60%)], Train Loss: 0.00932\n","Epoch: 05 [ 9762/16204 ( 60%)], Train Loss: 0.00931\n","Epoch: 05 [ 9782/16204 ( 60%)], Train Loss: 0.00930\n","Epoch: 05 [ 9802/16204 ( 60%)], Train Loss: 0.00928\n","Epoch: 05 [ 9822/16204 ( 61%)], Train Loss: 0.00926\n","Epoch: 05 [ 9842/16204 ( 61%)], Train Loss: 0.00924\n","Epoch: 05 [ 9862/16204 ( 61%)], Train Loss: 0.00923\n","Epoch: 05 [ 9882/16204 ( 61%)], Train Loss: 0.00921\n","Epoch: 05 [ 9902/16204 ( 61%)], Train Loss: 0.00919\n","Epoch: 05 [ 9922/16204 ( 61%)], Train Loss: 0.00931\n","Epoch: 05 [ 9942/16204 ( 61%)], Train Loss: 0.00929\n","Epoch: 05 [ 9962/16204 ( 61%)], Train Loss: 0.00928\n","Epoch: 05 [ 9982/16204 ( 62%)], Train Loss: 0.00926\n","Epoch: 05 [10002/16204 ( 62%)], Train Loss: 0.00924\n","Epoch: 05 [10022/16204 ( 62%)], Train Loss: 0.00923\n","Epoch: 05 [10042/16204 ( 62%)], Train Loss: 0.00921\n","Epoch: 05 [10062/16204 ( 62%)], Train Loss: 0.00922\n","Epoch: 05 [10082/16204 ( 62%)], Train Loss: 0.00920\n","Epoch: 05 [10102/16204 ( 62%)], Train Loss: 0.00918\n","Epoch: 05 [10122/16204 ( 62%)], Train Loss: 0.00917\n","Epoch: 05 [10142/16204 ( 63%)], Train Loss: 0.00915\n","Epoch: 05 [10162/16204 ( 63%)], Train Loss: 0.00914\n","Epoch: 05 [10182/16204 ( 63%)], Train Loss: 0.00912\n","Epoch: 05 [10202/16204 ( 63%)], Train Loss: 0.00911\n","Epoch: 05 [10222/16204 ( 63%)], Train Loss: 0.00910\n","Epoch: 05 [10242/16204 ( 63%)], Train Loss: 0.00911\n","Epoch: 05 [10262/16204 ( 63%)], Train Loss: 0.00910\n","Epoch: 05 [10282/16204 ( 63%)], Train Loss: 0.00908\n","Epoch: 05 [10302/16204 ( 64%)], Train Loss: 0.00908\n","Epoch: 05 [10322/16204 ( 64%)], Train Loss: 0.00907\n","Epoch: 05 [10342/16204 ( 64%)], Train Loss: 0.00905\n","Epoch: 05 [10362/16204 ( 64%)], Train Loss: 0.00911\n","Epoch: 05 [10382/16204 ( 64%)], Train Loss: 0.00909\n","Epoch: 05 [10402/16204 ( 64%)], Train Loss: 0.00909\n","Epoch: 05 [10422/16204 ( 64%)], Train Loss: 0.00908\n","Epoch: 05 [10442/16204 ( 64%)], Train Loss: 0.00906\n","Epoch: 05 [10462/16204 ( 65%)], Train Loss: 0.00905\n","Epoch: 05 [10482/16204 ( 65%)], Train Loss: 0.00904\n","Epoch: 05 [10502/16204 ( 65%)], Train Loss: 0.00902\n","Epoch: 05 [10522/16204 ( 65%)], Train Loss: 0.00900\n","Epoch: 05 [10542/16204 ( 65%)], Train Loss: 0.00899\n","Epoch: 05 [10562/16204 ( 65%)], Train Loss: 0.00898\n","Epoch: 05 [10582/16204 ( 65%)], Train Loss: 0.00897\n","Epoch: 05 [10602/16204 ( 65%)], Train Loss: 0.00895\n","Epoch: 05 [10622/16204 ( 66%)], Train Loss: 0.00893\n","Epoch: 05 [10642/16204 ( 66%)], Train Loss: 0.00895\n","Epoch: 05 [10662/16204 ( 66%)], Train Loss: 0.00894\n","Epoch: 05 [10682/16204 ( 66%)], Train Loss: 0.00892\n","Epoch: 05 [10702/16204 ( 66%)], Train Loss: 0.00891\n","Epoch: 05 [10722/16204 ( 66%)], Train Loss: 0.00889\n","Epoch: 05 [10742/16204 ( 66%)], Train Loss: 0.00888\n","Epoch: 05 [10762/16204 ( 66%)], Train Loss: 0.00886\n","Epoch: 05 [10782/16204 ( 67%)], Train Loss: 0.00885\n","Epoch: 05 [10802/16204 ( 67%)], Train Loss: 0.00883\n","Epoch: 05 [10822/16204 ( 67%)], Train Loss: 0.00882\n","Epoch: 05 [10842/16204 ( 67%)], Train Loss: 0.00880\n","Epoch: 05 [10862/16204 ( 67%)], Train Loss: 0.00879\n","Epoch: 05 [10882/16204 ( 67%)], Train Loss: 0.00878\n","Epoch: 05 [10902/16204 ( 67%)], Train Loss: 0.00877\n","Epoch: 05 [10922/16204 ( 67%)], Train Loss: 0.00877\n","Epoch: 05 [10942/16204 ( 68%)], Train Loss: 0.00878\n","Epoch: 05 [10962/16204 ( 68%)], Train Loss: 0.00876\n","Epoch: 05 [10982/16204 ( 68%)], Train Loss: 0.00875\n","Epoch: 05 [11002/16204 ( 68%)], Train Loss: 0.00873\n","Epoch: 05 [11022/16204 ( 68%)], Train Loss: 0.00872\n","Epoch: 05 [11042/16204 ( 68%)], Train Loss: 0.00871\n","Epoch: 05 [11062/16204 ( 68%)], Train Loss: 0.00869\n","Epoch: 05 [11082/16204 ( 68%)], Train Loss: 0.00883\n","Epoch: 05 [11102/16204 ( 69%)], Train Loss: 0.00881\n","Epoch: 05 [11122/16204 ( 69%)], Train Loss: 0.00880\n","Epoch: 05 [11142/16204 ( 69%)], Train Loss: 0.00878\n","Epoch: 05 [11162/16204 ( 69%)], Train Loss: 0.00877\n","Epoch: 05 [11182/16204 ( 69%)], Train Loss: 0.00875\n","Epoch: 05 [11202/16204 ( 69%)], Train Loss: 0.00874\n","Epoch: 05 [11222/16204 ( 69%)], Train Loss: 0.00872\n","Epoch: 05 [11242/16204 ( 69%)], Train Loss: 0.00873\n","Epoch: 05 [11262/16204 ( 70%)], Train Loss: 0.00871\n","Epoch: 05 [11282/16204 ( 70%)], Train Loss: 0.00870\n","Epoch: 05 [11302/16204 ( 70%)], Train Loss: 0.00868\n","Epoch: 05 [11322/16204 ( 70%)], Train Loss: 0.00867\n","Epoch: 05 [11342/16204 ( 70%)], Train Loss: 0.00865\n","Epoch: 05 [11362/16204 ( 70%)], Train Loss: 0.00864\n","Epoch: 05 [11382/16204 ( 70%)], Train Loss: 0.00868\n","Epoch: 05 [11402/16204 ( 70%)], Train Loss: 0.00867\n","Epoch: 05 [11422/16204 ( 70%)], Train Loss: 0.00865\n","Epoch: 05 [11442/16204 ( 71%)], Train Loss: 0.00871\n","Epoch: 05 [11462/16204 ( 71%)], Train Loss: 0.00870\n","Epoch: 05 [11482/16204 ( 71%)], Train Loss: 0.00869\n","Epoch: 05 [11502/16204 ( 71%)], Train Loss: 0.00867\n","Epoch: 05 [11522/16204 ( 71%)], Train Loss: 0.00866\n","Epoch: 05 [11542/16204 ( 71%)], Train Loss: 0.00864\n","Epoch: 05 [11562/16204 ( 71%)], Train Loss: 0.00863\n","Epoch: 05 [11582/16204 ( 71%)], Train Loss: 0.00862\n","Epoch: 05 [11602/16204 ( 72%)], Train Loss: 0.00895\n","Epoch: 05 [11622/16204 ( 72%)], Train Loss: 0.00894\n","Epoch: 05 [11642/16204 ( 72%)], Train Loss: 0.00893\n","Epoch: 05 [11662/16204 ( 72%)], Train Loss: 0.00892\n","Epoch: 05 [11682/16204 ( 72%)], Train Loss: 0.00891\n","Epoch: 05 [11702/16204 ( 72%)], Train Loss: 0.00890\n","Epoch: 05 [11722/16204 ( 72%)], Train Loss: 0.00892\n","Epoch: 05 [11742/16204 ( 72%)], Train Loss: 0.00891\n","Epoch: 05 [11762/16204 ( 73%)], Train Loss: 0.00893\n","Epoch: 05 [11782/16204 ( 73%)], Train Loss: 0.00892\n","Epoch: 05 [11802/16204 ( 73%)], Train Loss: 0.00891\n","Epoch: 05 [11822/16204 ( 73%)], Train Loss: 0.00890\n","Epoch: 05 [11842/16204 ( 73%)], Train Loss: 0.00889\n","Epoch: 05 [11862/16204 ( 73%)], Train Loss: 0.00888\n","Epoch: 05 [11882/16204 ( 73%)], Train Loss: 0.00886\n","Epoch: 05 [11902/16204 ( 73%)], Train Loss: 0.00885\n","Epoch: 05 [11922/16204 ( 74%)], Train Loss: 0.00884\n","Epoch: 05 [11942/16204 ( 74%)], Train Loss: 0.00883\n","Epoch: 05 [11962/16204 ( 74%)], Train Loss: 0.00881\n","Epoch: 05 [11982/16204 ( 74%)], Train Loss: 0.00880\n","Epoch: 05 [12002/16204 ( 74%)], Train Loss: 0.00879\n","Epoch: 05 [12022/16204 ( 74%)], Train Loss: 0.00878\n","Epoch: 05 [12042/16204 ( 74%)], Train Loss: 0.00876\n","Epoch: 05 [12062/16204 ( 74%)], Train Loss: 0.00875\n","Epoch: 05 [12082/16204 ( 75%)], Train Loss: 0.00874\n","Epoch: 05 [12102/16204 ( 75%)], Train Loss: 0.00872\n","Epoch: 05 [12122/16204 ( 75%)], Train Loss: 0.00871\n","Epoch: 05 [12142/16204 ( 75%)], Train Loss: 0.00873\n","Epoch: 05 [12162/16204 ( 75%)], Train Loss: 0.00873\n","Epoch: 05 [12182/16204 ( 75%)], Train Loss: 0.00871\n","Epoch: 05 [12202/16204 ( 75%)], Train Loss: 0.00871\n","Epoch: 05 [12222/16204 ( 75%)], Train Loss: 0.00870\n","Epoch: 05 [12242/16204 ( 76%)], Train Loss: 0.00868\n","Epoch: 05 [12262/16204 ( 76%)], Train Loss: 0.00867\n","Epoch: 05 [12282/16204 ( 76%)], Train Loss: 0.00866\n","Epoch: 05 [12302/16204 ( 76%)], Train Loss: 0.00867\n","Epoch: 05 [12322/16204 ( 76%)], Train Loss: 0.00881\n","Epoch: 05 [12342/16204 ( 76%)], Train Loss: 0.00880\n","Epoch: 05 [12362/16204 ( 76%)], Train Loss: 0.00878\n","Epoch: 05 [12382/16204 ( 76%)], Train Loss: 0.00877\n","Epoch: 05 [12402/16204 ( 77%)], Train Loss: 0.00878\n","Epoch: 05 [12422/16204 ( 77%)], Train Loss: 0.00876\n","Epoch: 05 [12442/16204 ( 77%)], Train Loss: 0.00875\n","Epoch: 05 [12462/16204 ( 77%)], Train Loss: 0.00876\n","Epoch: 05 [12482/16204 ( 77%)], Train Loss: 0.00874\n","Epoch: 05 [12502/16204 ( 77%)], Train Loss: 0.00873\n","Epoch: 05 [12522/16204 ( 77%)], Train Loss: 0.00872\n","Epoch: 05 [12542/16204 ( 77%)], Train Loss: 0.00871\n","Epoch: 05 [12562/16204 ( 78%)], Train Loss: 0.00869\n","Epoch: 05 [12582/16204 ( 78%)], Train Loss: 0.00868\n","Epoch: 05 [12602/16204 ( 78%)], Train Loss: 0.00867\n","Epoch: 05 [12622/16204 ( 78%)], Train Loss: 0.00866\n","Epoch: 05 [12642/16204 ( 78%)], Train Loss: 0.00865\n","Epoch: 05 [12662/16204 ( 78%)], Train Loss: 0.00864\n","Epoch: 05 [12682/16204 ( 78%)], Train Loss: 0.00863\n","Epoch: 05 [12702/16204 ( 78%)], Train Loss: 0.00861\n","Epoch: 05 [12722/16204 ( 79%)], Train Loss: 0.00860\n","Epoch: 05 [12742/16204 ( 79%)], Train Loss: 0.00859\n","Epoch: 05 [12762/16204 ( 79%)], Train Loss: 0.00857\n","Epoch: 05 [12782/16204 ( 79%)], Train Loss: 0.00868\n","Epoch: 05 [12802/16204 ( 79%)], Train Loss: 0.00867\n","Epoch: 05 [12822/16204 ( 79%)], Train Loss: 0.00865\n","Epoch: 05 [12842/16204 ( 79%)], Train Loss: 0.00865\n","Epoch: 05 [12862/16204 ( 79%)], Train Loss: 0.00864\n","Epoch: 05 [12882/16204 ( 79%)], Train Loss: 0.00863\n","Epoch: 05 [12902/16204 ( 80%)], Train Loss: 0.00862\n","Epoch: 05 [12922/16204 ( 80%)], Train Loss: 0.00861\n","Epoch: 05 [12942/16204 ( 80%)], Train Loss: 0.00859\n","Epoch: 05 [12962/16204 ( 80%)], Train Loss: 0.00858\n","Epoch: 05 [12982/16204 ( 80%)], Train Loss: 0.00857\n","Epoch: 05 [13002/16204 ( 80%)], Train Loss: 0.00856\n","Epoch: 05 [13022/16204 ( 80%)], Train Loss: 0.00854\n","Epoch: 05 [13042/16204 ( 80%)], Train Loss: 0.00854\n","Epoch: 05 [13062/16204 ( 81%)], Train Loss: 0.00854\n","Epoch: 05 [13082/16204 ( 81%)], Train Loss: 0.00853\n","Epoch: 05 [13102/16204 ( 81%)], Train Loss: 0.00852\n","Epoch: 05 [13122/16204 ( 81%)], Train Loss: 0.00851\n","Epoch: 05 [13142/16204 ( 81%)], Train Loss: 0.00850\n","Epoch: 05 [13162/16204 ( 81%)], Train Loss: 0.00850\n","Epoch: 05 [13182/16204 ( 81%)], Train Loss: 0.00851\n","Epoch: 05 [13202/16204 ( 81%)], Train Loss: 0.00849\n","Epoch: 05 [13222/16204 ( 82%)], Train Loss: 0.00848\n","Epoch: 05 [13242/16204 ( 82%)], Train Loss: 0.00847\n","Epoch: 05 [13262/16204 ( 82%)], Train Loss: 0.00846\n","Epoch: 05 [13282/16204 ( 82%)], Train Loss: 0.00845\n","Epoch: 05 [13302/16204 ( 82%)], Train Loss: 0.00843\n","Epoch: 05 [13322/16204 ( 82%)], Train Loss: 0.00842\n","Epoch: 05 [13342/16204 ( 82%)], Train Loss: 0.00841\n","Epoch: 05 [13362/16204 ( 82%)], Train Loss: 0.00840\n","Epoch: 05 [13382/16204 ( 83%)], Train Loss: 0.00839\n","Epoch: 05 [13402/16204 ( 83%)], Train Loss: 0.00837\n","Epoch: 05 [13422/16204 ( 83%)], Train Loss: 0.00836\n","Epoch: 05 [13442/16204 ( 83%)], Train Loss: 0.00835\n","Epoch: 05 [13462/16204 ( 83%)], Train Loss: 0.00834\n","Epoch: 05 [13482/16204 ( 83%)], Train Loss: 0.00834\n","Epoch: 05 [13502/16204 ( 83%)], Train Loss: 0.00833\n","Epoch: 05 [13522/16204 ( 83%)], Train Loss: 0.00832\n","Epoch: 05 [13542/16204 ( 84%)], Train Loss: 0.00831\n","Epoch: 05 [13562/16204 ( 84%)], Train Loss: 0.00830\n","Epoch: 05 [13582/16204 ( 84%)], Train Loss: 0.00830\n","Epoch: 05 [13602/16204 ( 84%)], Train Loss: 0.00829\n","Epoch: 05 [13622/16204 ( 84%)], Train Loss: 0.00829\n","Epoch: 05 [13642/16204 ( 84%)], Train Loss: 0.00828\n","Epoch: 05 [13662/16204 ( 84%)], Train Loss: 0.00827\n","Epoch: 05 [13682/16204 ( 84%)], Train Loss: 0.00825\n","Epoch: 05 [13702/16204 ( 85%)], Train Loss: 0.00824\n","Epoch: 05 [13722/16204 ( 85%)], Train Loss: 0.00823\n","Epoch: 05 [13742/16204 ( 85%)], Train Loss: 0.00823\n","Epoch: 05 [13762/16204 ( 85%)], Train Loss: 0.00835\n","Epoch: 05 [13782/16204 ( 85%)], Train Loss: 0.00834\n","Epoch: 05 [13802/16204 ( 85%)], Train Loss: 0.00832\n","Epoch: 05 [13822/16204 ( 85%)], Train Loss: 0.00831\n","Epoch: 05 [13842/16204 ( 85%)], Train Loss: 0.00830\n","Epoch: 05 [13862/16204 ( 86%)], Train Loss: 0.00829\n","Epoch: 05 [13882/16204 ( 86%)], Train Loss: 0.00829\n","Epoch: 05 [13902/16204 ( 86%)], Train Loss: 0.00829\n","Epoch: 05 [13922/16204 ( 86%)], Train Loss: 0.00828\n","Epoch: 05 [13942/16204 ( 86%)], Train Loss: 0.00829\n","Epoch: 05 [13962/16204 ( 86%)], Train Loss: 0.00827\n","Epoch: 05 [13982/16204 ( 86%)], Train Loss: 0.00826\n","Epoch: 05 [14002/16204 ( 86%)], Train Loss: 0.00825\n","Epoch: 05 [14022/16204 ( 87%)], Train Loss: 0.00824\n","Epoch: 05 [14042/16204 ( 87%)], Train Loss: 0.00823\n","Epoch: 05 [14062/16204 ( 87%)], Train Loss: 0.00822\n","Epoch: 05 [14082/16204 ( 87%)], Train Loss: 0.00821\n","Epoch: 05 [14102/16204 ( 87%)], Train Loss: 0.00821\n","Epoch: 05 [14122/16204 ( 87%)], Train Loss: 0.00820\n","Epoch: 05 [14142/16204 ( 87%)], Train Loss: 0.00819\n","Epoch: 05 [14162/16204 ( 87%)], Train Loss: 0.00818\n","Epoch: 05 [14182/16204 ( 88%)], Train Loss: 0.00817\n","Epoch: 05 [14202/16204 ( 88%)], Train Loss: 0.00816\n","Epoch: 05 [14222/16204 ( 88%)], Train Loss: 0.00815\n","Epoch: 05 [14242/16204 ( 88%)], Train Loss: 0.00814\n","Epoch: 05 [14262/16204 ( 88%)], Train Loss: 0.00813\n","Epoch: 05 [14282/16204 ( 88%)], Train Loss: 0.00812\n","Epoch: 05 [14302/16204 ( 88%)], Train Loss: 0.00811\n","Epoch: 05 [14322/16204 ( 88%)], Train Loss: 0.00810\n","Epoch: 05 [14342/16204 ( 89%)], Train Loss: 0.00809\n","Epoch: 05 [14362/16204 ( 89%)], Train Loss: 0.00807\n","Epoch: 05 [14382/16204 ( 89%)], Train Loss: 0.00806\n","Epoch: 05 [14402/16204 ( 89%)], Train Loss: 0.00805\n","Epoch: 05 [14422/16204 ( 89%)], Train Loss: 0.00804\n","Epoch: 05 [14442/16204 ( 89%)], Train Loss: 0.00803\n","Epoch: 05 [14462/16204 ( 89%)], Train Loss: 0.00802\n","Epoch: 05 [14482/16204 ( 89%)], Train Loss: 0.00802\n","Epoch: 05 [14502/16204 ( 89%)], Train Loss: 0.00801\n","Epoch: 05 [14522/16204 ( 90%)], Train Loss: 0.00799\n","Epoch: 05 [14542/16204 ( 90%)], Train Loss: 0.00799\n","Epoch: 05 [14562/16204 ( 90%)], Train Loss: 0.00798\n","Epoch: 05 [14582/16204 ( 90%)], Train Loss: 0.00808\n","Epoch: 05 [14602/16204 ( 90%)], Train Loss: 0.00807\n","Epoch: 05 [14622/16204 ( 90%)], Train Loss: 0.00806\n","Epoch: 05 [14642/16204 ( 90%)], Train Loss: 0.00805\n","Epoch: 05 [14662/16204 ( 90%)], Train Loss: 0.00804\n","Epoch: 05 [14682/16204 ( 91%)], Train Loss: 0.00803\n","Epoch: 05 [14702/16204 ( 91%)], Train Loss: 0.00803\n","Epoch: 05 [14722/16204 ( 91%)], Train Loss: 0.00802\n","Epoch: 05 [14742/16204 ( 91%)], Train Loss: 0.00801\n","Epoch: 05 [14762/16204 ( 91%)], Train Loss: 0.00800\n","Epoch: 05 [14782/16204 ( 91%)], Train Loss: 0.00799\n","Epoch: 05 [14802/16204 ( 91%)], Train Loss: 0.00798\n","Epoch: 05 [14822/16204 ( 91%)], Train Loss: 0.00797\n","Epoch: 05 [14842/16204 ( 92%)], Train Loss: 0.00796\n","Epoch: 05 [14862/16204 ( 92%)], Train Loss: 0.00795\n","Epoch: 05 [14882/16204 ( 92%)], Train Loss: 0.00794\n","Epoch: 05 [14902/16204 ( 92%)], Train Loss: 0.00793\n","Epoch: 05 [14922/16204 ( 92%)], Train Loss: 0.00792\n","Epoch: 05 [14942/16204 ( 92%)], Train Loss: 0.00791\n","Epoch: 05 [14962/16204 ( 92%)], Train Loss: 0.00790\n","Epoch: 05 [14982/16204 ( 92%)], Train Loss: 0.00798\n","Epoch: 05 [15002/16204 ( 93%)], Train Loss: 0.00799\n","Epoch: 05 [15022/16204 ( 93%)], Train Loss: 0.00798\n","Epoch: 05 [15042/16204 ( 93%)], Train Loss: 0.00797\n","Epoch: 05 [15062/16204 ( 93%)], Train Loss: 0.00796\n","Epoch: 05 [15082/16204 ( 93%)], Train Loss: 0.00795\n","Epoch: 05 [15102/16204 ( 93%)], Train Loss: 0.00794\n","Epoch: 05 [15122/16204 ( 93%)], Train Loss: 0.00793\n","Epoch: 05 [15142/16204 ( 93%)], Train Loss: 0.00793\n","Epoch: 05 [15162/16204 ( 94%)], Train Loss: 0.00792\n","Epoch: 05 [15182/16204 ( 94%)], Train Loss: 0.00791\n","Epoch: 05 [15202/16204 ( 94%)], Train Loss: 0.00790\n","Epoch: 05 [15222/16204 ( 94%)], Train Loss: 0.00790\n","Epoch: 05 [15242/16204 ( 94%)], Train Loss: 0.00789\n","Epoch: 05 [15262/16204 ( 94%)], Train Loss: 0.00788\n","Epoch: 05 [15282/16204 ( 94%)], Train Loss: 0.00787\n","Epoch: 05 [15302/16204 ( 94%)], Train Loss: 0.00788\n","Epoch: 05 [15322/16204 ( 95%)], Train Loss: 0.00787\n","Epoch: 05 [15342/16204 ( 95%)], Train Loss: 0.00788\n","Epoch: 05 [15362/16204 ( 95%)], Train Loss: 0.00787\n","Epoch: 05 [15382/16204 ( 95%)], Train Loss: 0.00787\n","Epoch: 05 [15402/16204 ( 95%)], Train Loss: 0.00786\n","Epoch: 05 [15422/16204 ( 95%)], Train Loss: 0.00786\n","Epoch: 05 [15442/16204 ( 95%)], Train Loss: 0.00785\n","Epoch: 05 [15462/16204 ( 95%)], Train Loss: 0.00784\n","Epoch: 05 [15482/16204 ( 96%)], Train Loss: 0.00783\n","Epoch: 05 [15502/16204 ( 96%)], Train Loss: 0.00782\n","Epoch: 05 [15522/16204 ( 96%)], Train Loss: 0.00781\n","Epoch: 05 [15542/16204 ( 96%)], Train Loss: 0.00780\n","Epoch: 05 [15562/16204 ( 96%)], Train Loss: 0.00779\n","Epoch: 05 [15582/16204 ( 96%)], Train Loss: 0.00779\n","Epoch: 05 [15602/16204 ( 96%)], Train Loss: 0.00778\n","Epoch: 05 [15622/16204 ( 96%)], Train Loss: 0.00777\n","Epoch: 05 [15642/16204 ( 97%)], Train Loss: 0.00776\n","Epoch: 05 [15662/16204 ( 97%)], Train Loss: 0.00775\n","Epoch: 05 [15682/16204 ( 97%)], Train Loss: 0.00774\n","Epoch: 05 [15702/16204 ( 97%)], Train Loss: 0.00773\n","Epoch: 05 [15722/16204 ( 97%)], Train Loss: 0.00772\n","Epoch: 05 [15742/16204 ( 97%)], Train Loss: 0.00771\n","Epoch: 05 [15762/16204 ( 97%)], Train Loss: 0.00770\n","Epoch: 05 [15782/16204 ( 97%)], Train Loss: 0.00769\n","Epoch: 05 [15802/16204 ( 98%)], Train Loss: 0.00768\n","Epoch: 05 [15822/16204 ( 98%)], Train Loss: 0.00767\n","Epoch: 05 [15842/16204 ( 98%)], Train Loss: 0.00766\n","Epoch: 05 [15862/16204 ( 98%)], Train Loss: 0.00765\n","Epoch: 05 [15882/16204 ( 98%)], Train Loss: 0.00765\n","Epoch: 05 [15902/16204 ( 98%)], Train Loss: 0.00764\n","Epoch: 05 [15922/16204 ( 98%)], Train Loss: 0.00763\n","Epoch: 05 [15942/16204 ( 98%)], Train Loss: 0.00762\n","Epoch: 05 [15962/16204 ( 99%)], Train Loss: 0.00761\n","Epoch: 05 [15982/16204 ( 99%)], Train Loss: 0.00760\n","Epoch: 05 [16002/16204 ( 99%)], Train Loss: 0.00760\n","Epoch: 05 [16022/16204 ( 99%)], Train Loss: 0.00759\n","Epoch: 05 [16042/16204 ( 99%)], Train Loss: 0.00758\n","Epoch: 05 [16062/16204 ( 99%)], Train Loss: 0.00757\n","Epoch: 05 [16082/16204 ( 99%)], Train Loss: 0.00758\n","Epoch: 05 [16102/16204 ( 99%)], Train Loss: 0.00757\n","Epoch: 05 [16122/16204 ( 99%)], Train Loss: 0.00756\n","Epoch: 05 [16142/16204 (100%)], Train Loss: 0.00755\n","Epoch: 05 [16162/16204 (100%)], Train Loss: 0.00754\n","Epoch: 05 [16182/16204 (100%)], Train Loss: 0.00754\n","Epoch: 05 [16202/16204 (100%)], Train Loss: 0.00753\n","Epoch: 05 [16204/16204 (100%)], Train Loss: 0.00753\n","----Validation Results Summary----\n","Epoch: [5] Valid Loss: 0.00000\n","\n","Epoch: 06 [    2/16204 (  0%)], Train Loss: 0.00000\n","Epoch: 06 [   22/16204 (  0%)], Train Loss: 0.00098\n","Epoch: 06 [   42/16204 (  0%)], Train Loss: 0.00433\n","Epoch: 06 [   62/16204 (  0%)], Train Loss: 0.00302\n","Epoch: 06 [   82/16204 (  1%)], Train Loss: 0.00347\n","Epoch: 06 [  102/16204 (  1%)], Train Loss: 0.00280\n","Epoch: 06 [  122/16204 (  1%)], Train Loss: 0.00241\n","Epoch: 06 [  142/16204 (  1%)], Train Loss: 0.00208\n","Epoch: 06 [  162/16204 (  1%)], Train Loss: 0.00222\n","Epoch: 06 [  182/16204 (  1%)], Train Loss: 0.00204\n","Epoch: 06 [  202/16204 (  1%)], Train Loss: 0.00186\n","Epoch: 06 [  222/16204 (  1%)], Train Loss: 0.00170\n","Epoch: 06 [  242/16204 (  1%)], Train Loss: 0.00158\n","Epoch: 06 [  262/16204 (  2%)], Train Loss: 0.00147\n","Epoch: 06 [  282/16204 (  2%)], Train Loss: 0.00141\n","Epoch: 06 [  302/16204 (  2%)], Train Loss: 0.00132\n","Epoch: 06 [  322/16204 (  2%)], Train Loss: 0.00125\n","Epoch: 06 [  342/16204 (  2%)], Train Loss: 0.00118\n","Epoch: 06 [  362/16204 (  2%)], Train Loss: 0.00113\n","Epoch: 06 [  382/16204 (  2%)], Train Loss: 0.00145\n","Epoch: 06 [  402/16204 (  2%)], Train Loss: 0.00139\n","Epoch: 06 [  422/16204 (  3%)], Train Loss: 0.00134\n","Epoch: 06 [  442/16204 (  3%)], Train Loss: 0.00128\n","Epoch: 06 [  462/16204 (  3%)], Train Loss: 0.00123\n","Epoch: 06 [  482/16204 (  3%)], Train Loss: 0.00119\n","Epoch: 06 [  502/16204 (  3%)], Train Loss: 0.00138\n","Epoch: 06 [  522/16204 (  3%)], Train Loss: 0.00133\n","Epoch: 06 [  542/16204 (  3%)], Train Loss: 0.00167\n","Epoch: 06 [  562/16204 (  3%)], Train Loss: 0.00161\n","Epoch: 06 [  582/16204 (  4%)], Train Loss: 0.00217\n","Epoch: 06 [  602/16204 (  4%)], Train Loss: 0.00211\n","Epoch: 06 [  622/16204 (  4%)], Train Loss: 0.00205\n","Epoch: 06 [  642/16204 (  4%)], Train Loss: 0.00202\n","Epoch: 06 [  662/16204 (  4%)], Train Loss: 0.00198\n","Epoch: 06 [  682/16204 (  4%)], Train Loss: 0.00193\n","Epoch: 06 [  702/16204 (  4%)], Train Loss: 0.00188\n","Epoch: 06 [  722/16204 (  4%)], Train Loss: 0.00183\n","Epoch: 06 [  742/16204 (  5%)], Train Loss: 0.00213\n","Epoch: 06 [  762/16204 (  5%)], Train Loss: 0.00214\n","Epoch: 06 [  782/16204 (  5%)], Train Loss: 0.00209\n","Epoch: 06 [  802/16204 (  5%)], Train Loss: 0.00206\n","Epoch: 06 [  822/16204 (  5%)], Train Loss: 0.00202\n","Epoch: 06 [  842/16204 (  5%)], Train Loss: 0.00198\n","Epoch: 06 [  862/16204 (  5%)], Train Loss: 0.00194\n","Epoch: 06 [  882/16204 (  5%)], Train Loss: 0.00189\n","Epoch: 06 [  902/16204 (  6%)], Train Loss: 0.00186\n","Epoch: 06 [  922/16204 (  6%)], Train Loss: 0.00182\n","Epoch: 06 [  942/16204 (  6%)], Train Loss: 0.00179\n","Epoch: 06 [  962/16204 (  6%)], Train Loss: 0.00176\n","Epoch: 06 [  982/16204 (  6%)], Train Loss: 0.00177\n","Epoch: 06 [ 1002/16204 (  6%)], Train Loss: 0.00173\n","Epoch: 06 [ 1022/16204 (  6%)], Train Loss: 0.00180\n","Epoch: 06 [ 1042/16204 (  6%)], Train Loss: 0.00177\n","Epoch: 06 [ 1062/16204 (  7%)], Train Loss: 0.00174\n","Epoch: 06 [ 1082/16204 (  7%)], Train Loss: 0.00171\n","Epoch: 06 [ 1102/16204 (  7%)], Train Loss: 0.00168\n","Epoch: 06 [ 1122/16204 (  7%)], Train Loss: 0.00167\n","Epoch: 06 [ 1142/16204 (  7%)], Train Loss: 0.00164\n","Epoch: 06 [ 1162/16204 (  7%)], Train Loss: 0.00162\n","Epoch: 06 [ 1182/16204 (  7%)], Train Loss: 0.00164\n","Epoch: 06 [ 1202/16204 (  7%)], Train Loss: 0.00162\n","Epoch: 06 [ 1222/16204 (  8%)], Train Loss: 0.00169\n","Epoch: 06 [ 1242/16204 (  8%)], Train Loss: 0.00180\n","Epoch: 06 [ 1262/16204 (  8%)], Train Loss: 0.00179\n","Epoch: 06 [ 1282/16204 (  8%)], Train Loss: 0.00177\n","Epoch: 06 [ 1302/16204 (  8%)], Train Loss: 0.00174\n","Epoch: 06 [ 1322/16204 (  8%)], Train Loss: 0.00177\n","Epoch: 06 [ 1342/16204 (  8%)], Train Loss: 0.00175\n","Epoch: 06 [ 1362/16204 (  8%)], Train Loss: 0.00172\n","Epoch: 06 [ 1382/16204 (  9%)], Train Loss: 0.00171\n","Epoch: 06 [ 1402/16204 (  9%)], Train Loss: 0.00169\n","Epoch: 06 [ 1422/16204 (  9%)], Train Loss: 0.00167\n","Epoch: 06 [ 1442/16204 (  9%)], Train Loss: 0.00165\n","Epoch: 06 [ 1462/16204 (  9%)], Train Loss: 0.00163\n","Epoch: 06 [ 1482/16204 (  9%)], Train Loss: 0.00167\n","Epoch: 06 [ 1502/16204 (  9%)], Train Loss: 0.00165\n","Epoch: 06 [ 1522/16204 (  9%)], Train Loss: 0.00179\n","Epoch: 06 [ 1542/16204 ( 10%)], Train Loss: 0.00176\n","Epoch: 06 [ 1562/16204 ( 10%)], Train Loss: 0.00174\n","Epoch: 06 [ 1582/16204 ( 10%)], Train Loss: 0.00177\n","Epoch: 06 [ 1602/16204 ( 10%)], Train Loss: 0.00175\n","Epoch: 06 [ 1622/16204 ( 10%)], Train Loss: 0.00175\n","Epoch: 06 [ 1642/16204 ( 10%)], Train Loss: 0.00173\n","Epoch: 06 [ 1662/16204 ( 10%)], Train Loss: 0.00171\n","Epoch: 06 [ 1682/16204 ( 10%)], Train Loss: 0.00170\n","Epoch: 06 [ 1702/16204 ( 11%)], Train Loss: 0.00168\n","Epoch: 06 [ 1722/16204 ( 11%)], Train Loss: 0.00168\n","Epoch: 06 [ 1742/16204 ( 11%)], Train Loss: 0.00166\n","Epoch: 06 [ 1762/16204 ( 11%)], Train Loss: 0.00166\n","Epoch: 06 [ 1782/16204 ( 11%)], Train Loss: 0.00164\n","Epoch: 06 [ 1802/16204 ( 11%)], Train Loss: 0.00163\n","Epoch: 06 [ 1822/16204 ( 11%)], Train Loss: 0.00161\n","Epoch: 06 [ 1842/16204 ( 11%)], Train Loss: 0.00159\n","Epoch: 06 [ 1862/16204 ( 11%)], Train Loss: 0.00158\n","Epoch: 06 [ 1882/16204 ( 12%)], Train Loss: 0.00157\n","Epoch: 06 [ 1902/16204 ( 12%)], Train Loss: 0.00163\n","Epoch: 06 [ 1922/16204 ( 12%)], Train Loss: 0.00161\n","Epoch: 06 [ 1942/16204 ( 12%)], Train Loss: 0.00160\n","Epoch: 06 [ 1962/16204 ( 12%)], Train Loss: 0.00158\n","Epoch: 06 [ 1982/16204 ( 12%)], Train Loss: 0.00161\n","Epoch: 06 [ 2002/16204 ( 12%)], Train Loss: 0.00159\n","Epoch: 06 [ 2022/16204 ( 12%)], Train Loss: 0.00157\n","Epoch: 06 [ 2042/16204 ( 13%)], Train Loss: 0.00156\n","Epoch: 06 [ 2062/16204 ( 13%)], Train Loss: 0.00155\n","Epoch: 06 [ 2082/16204 ( 13%)], Train Loss: 0.00153\n","Epoch: 06 [ 2102/16204 ( 13%)], Train Loss: 0.00152\n","Epoch: 06 [ 2122/16204 ( 13%)], Train Loss: 0.00151\n","Epoch: 06 [ 2142/16204 ( 13%)], Train Loss: 0.00149\n","Epoch: 06 [ 2162/16204 ( 13%)], Train Loss: 0.00148\n","Epoch: 06 [ 2182/16204 ( 13%)], Train Loss: 0.00147\n","Epoch: 06 [ 2202/16204 ( 14%)], Train Loss: 0.00145\n","Epoch: 06 [ 2222/16204 ( 14%)], Train Loss: 0.00144\n","Epoch: 06 [ 2242/16204 ( 14%)], Train Loss: 0.00171\n","Epoch: 06 [ 2262/16204 ( 14%)], Train Loss: 0.00170\n","Epoch: 06 [ 2282/16204 ( 14%)], Train Loss: 0.00169\n","Epoch: 06 [ 2302/16204 ( 14%)], Train Loss: 0.00168\n","Epoch: 06 [ 2322/16204 ( 14%)], Train Loss: 0.00166\n","Epoch: 06 [ 2342/16204 ( 14%)], Train Loss: 0.00165\n","Epoch: 06 [ 2362/16204 ( 15%)], Train Loss: 0.00168\n","Epoch: 06 [ 2382/16204 ( 15%)], Train Loss: 0.00167\n","Epoch: 06 [ 2402/16204 ( 15%)], Train Loss: 0.00165\n","Epoch: 06 [ 2422/16204 ( 15%)], Train Loss: 0.00164\n","Epoch: 06 [ 2442/16204 ( 15%)], Train Loss: 0.00164\n","Epoch: 06 [ 2462/16204 ( 15%)], Train Loss: 0.00162\n","Epoch: 06 [ 2482/16204 ( 15%)], Train Loss: 0.00161\n","Epoch: 06 [ 2502/16204 ( 15%)], Train Loss: 0.00161\n","Epoch: 06 [ 2522/16204 ( 16%)], Train Loss: 0.00162\n","Epoch: 06 [ 2542/16204 ( 16%)], Train Loss: 0.00161\n","Epoch: 06 [ 2562/16204 ( 16%)], Train Loss: 0.00160\n","Epoch: 06 [ 2582/16204 ( 16%)], Train Loss: 0.00159\n","Epoch: 06 [ 2602/16204 ( 16%)], Train Loss: 0.00158\n","Epoch: 06 [ 2622/16204 ( 16%)], Train Loss: 0.00166\n","Epoch: 06 [ 2642/16204 ( 16%)], Train Loss: 0.00165\n","Epoch: 06 [ 2662/16204 ( 16%)], Train Loss: 0.00164\n","Epoch: 06 [ 2682/16204 ( 17%)], Train Loss: 0.00163\n","Epoch: 06 [ 2702/16204 ( 17%)], Train Loss: 0.00162\n","Epoch: 06 [ 2722/16204 ( 17%)], Train Loss: 0.00161\n","Epoch: 06 [ 2742/16204 ( 17%)], Train Loss: 0.00160\n","Epoch: 06 [ 2762/16204 ( 17%)], Train Loss: 0.00174\n","Epoch: 06 [ 2782/16204 ( 17%)], Train Loss: 0.00173\n","Epoch: 06 [ 2802/16204 ( 17%)], Train Loss: 0.00172\n","Epoch: 06 [ 2822/16204 ( 17%)], Train Loss: 0.00171\n","Epoch: 06 [ 2842/16204 ( 18%)], Train Loss: 0.00170\n","Epoch: 06 [ 2862/16204 ( 18%)], Train Loss: 0.00169\n","Epoch: 06 [ 2882/16204 ( 18%)], Train Loss: 0.00167\n","Epoch: 06 [ 2902/16204 ( 18%)], Train Loss: 0.00166\n","Epoch: 06 [ 2922/16204 ( 18%)], Train Loss: 0.00166\n","Epoch: 06 [ 2942/16204 ( 18%)], Train Loss: 0.00165\n","Epoch: 06 [ 2962/16204 ( 18%)], Train Loss: 0.00164\n","Epoch: 06 [ 2982/16204 ( 18%)], Train Loss: 0.00162\n","Epoch: 06 [ 3002/16204 ( 19%)], Train Loss: 0.00161\n","Epoch: 06 [ 3022/16204 ( 19%)], Train Loss: 0.00160\n","Epoch: 06 [ 3042/16204 ( 19%)], Train Loss: 0.00160\n","Epoch: 06 [ 3062/16204 ( 19%)], Train Loss: 0.00159\n","Epoch: 06 [ 3082/16204 ( 19%)], Train Loss: 0.00160\n","Epoch: 06 [ 3102/16204 ( 19%)], Train Loss: 0.00159\n","Epoch: 06 [ 3122/16204 ( 19%)], Train Loss: 0.00158\n","Epoch: 06 [ 3142/16204 ( 19%)], Train Loss: 0.00160\n","Epoch: 06 [ 3162/16204 ( 20%)], Train Loss: 0.00159\n","Epoch: 06 [ 3182/16204 ( 20%)], Train Loss: 0.00159\n","Epoch: 06 [ 3202/16204 ( 20%)], Train Loss: 0.00158\n","Epoch: 06 [ 3222/16204 ( 20%)], Train Loss: 0.00157\n","Epoch: 06 [ 3242/16204 ( 20%)], Train Loss: 0.00156\n","Epoch: 06 [ 3262/16204 ( 20%)], Train Loss: 0.00156\n","Epoch: 06 [ 3282/16204 ( 20%)], Train Loss: 0.00155\n","Epoch: 06 [ 3302/16204 ( 20%)], Train Loss: 0.00154\n","Epoch: 06 [ 3322/16204 ( 21%)], Train Loss: 0.00153\n","Epoch: 06 [ 3342/16204 ( 21%)], Train Loss: 0.00152\n","Epoch: 06 [ 3362/16204 ( 21%)], Train Loss: 0.00153\n","Epoch: 06 [ 3382/16204 ( 21%)], Train Loss: 0.00153\n","Epoch: 06 [ 3402/16204 ( 21%)], Train Loss: 0.00162\n","Epoch: 06 [ 3422/16204 ( 21%)], Train Loss: 0.00161\n","Epoch: 06 [ 3442/16204 ( 21%)], Train Loss: 0.00160\n","Epoch: 06 [ 3462/16204 ( 21%)], Train Loss: 0.00159\n","Epoch: 06 [ 3482/16204 ( 21%)], Train Loss: 0.00159\n","Epoch: 06 [ 3502/16204 ( 22%)], Train Loss: 0.00158\n","Epoch: 06 [ 3522/16204 ( 22%)], Train Loss: 0.00157\n","Epoch: 06 [ 3542/16204 ( 22%)], Train Loss: 0.00157\n","Epoch: 06 [ 3562/16204 ( 22%)], Train Loss: 0.00156\n","Epoch: 06 [ 3582/16204 ( 22%)], Train Loss: 0.00155\n","Epoch: 06 [ 3602/16204 ( 22%)], Train Loss: 0.00154\n","Epoch: 06 [ 3622/16204 ( 22%)], Train Loss: 0.00153\n","Epoch: 06 [ 3642/16204 ( 22%)], Train Loss: 0.00179\n","Epoch: 06 [ 3662/16204 ( 23%)], Train Loss: 0.00185\n","Epoch: 06 [ 3682/16204 ( 23%)], Train Loss: 0.00184\n","Epoch: 06 [ 3702/16204 ( 23%)], Train Loss: 0.00183\n","Epoch: 06 [ 3722/16204 ( 23%)], Train Loss: 0.00182\n","Epoch: 06 [ 3742/16204 ( 23%)], Train Loss: 0.00181\n","Epoch: 06 [ 3762/16204 ( 23%)], Train Loss: 0.00181\n","Epoch: 06 [ 3782/16204 ( 23%)], Train Loss: 0.00180\n","Epoch: 06 [ 3802/16204 ( 23%)], Train Loss: 0.00193\n","Epoch: 06 [ 3822/16204 ( 24%)], Train Loss: 0.00192\n","Epoch: 06 [ 3842/16204 ( 24%)], Train Loss: 0.00192\n","Epoch: 06 [ 3862/16204 ( 24%)], Train Loss: 0.00191\n","Epoch: 06 [ 3882/16204 ( 24%)], Train Loss: 0.00190\n","Epoch: 06 [ 3902/16204 ( 24%)], Train Loss: 0.00189\n","Epoch: 06 [ 3922/16204 ( 24%)], Train Loss: 0.00189\n","Epoch: 06 [ 3942/16204 ( 24%)], Train Loss: 0.00189\n","Epoch: 06 [ 3962/16204 ( 24%)], Train Loss: 0.00188\n","Epoch: 06 [ 3982/16204 ( 25%)], Train Loss: 0.00189\n","Epoch: 06 [ 4002/16204 ( 25%)], Train Loss: 0.00188\n","Epoch: 06 [ 4022/16204 ( 25%)], Train Loss: 0.00187\n","Epoch: 06 [ 4042/16204 ( 25%)], Train Loss: 0.00187\n","Epoch: 06 [ 4062/16204 ( 25%)], Train Loss: 0.00186\n","Epoch: 06 [ 4082/16204 ( 25%)], Train Loss: 0.00185\n","Epoch: 06 [ 4102/16204 ( 25%)], Train Loss: 0.00184\n","Epoch: 06 [ 4122/16204 ( 25%)], Train Loss: 0.00185\n","Epoch: 06 [ 4142/16204 ( 26%)], Train Loss: 0.00188\n","Epoch: 06 [ 4162/16204 ( 26%)], Train Loss: 0.00187\n","Epoch: 06 [ 4182/16204 ( 26%)], Train Loss: 0.00196\n","Epoch: 06 [ 4202/16204 ( 26%)], Train Loss: 0.00195\n","Epoch: 06 [ 4222/16204 ( 26%)], Train Loss: 0.00195\n","Epoch: 06 [ 4242/16204 ( 26%)], Train Loss: 0.00194\n","Epoch: 06 [ 4262/16204 ( 26%)], Train Loss: 0.00193\n","Epoch: 06 [ 4282/16204 ( 26%)], Train Loss: 0.00192\n","Epoch: 06 [ 4302/16204 ( 27%)], Train Loss: 0.00194\n","Epoch: 06 [ 4322/16204 ( 27%)], Train Loss: 0.00193\n","Epoch: 06 [ 4342/16204 ( 27%)], Train Loss: 0.00193\n","Epoch: 06 [ 4362/16204 ( 27%)], Train Loss: 0.00194\n","Epoch: 06 [ 4382/16204 ( 27%)], Train Loss: 0.00196\n","Epoch: 06 [ 4402/16204 ( 27%)], Train Loss: 0.00195\n","Epoch: 06 [ 4422/16204 ( 27%)], Train Loss: 0.00194\n","Epoch: 06 [ 4442/16204 ( 27%)], Train Loss: 0.00193\n","Epoch: 06 [ 4462/16204 ( 28%)], Train Loss: 0.00193\n","Epoch: 06 [ 4482/16204 ( 28%)], Train Loss: 0.00192\n","Epoch: 06 [ 4502/16204 ( 28%)], Train Loss: 0.00192\n","Epoch: 06 [ 4522/16204 ( 28%)], Train Loss: 0.00191\n","Epoch: 06 [ 4542/16204 ( 28%)], Train Loss: 0.00191\n","Epoch: 06 [ 4562/16204 ( 28%)], Train Loss: 0.00190\n","Epoch: 06 [ 4582/16204 ( 28%)], Train Loss: 0.00189\n","Epoch: 06 [ 4602/16204 ( 28%)], Train Loss: 0.00188\n","Epoch: 06 [ 4622/16204 ( 29%)], Train Loss: 0.00188\n","Epoch: 06 [ 4642/16204 ( 29%)], Train Loss: 0.00187\n","Epoch: 06 [ 4662/16204 ( 29%)], Train Loss: 0.00186\n","Epoch: 06 [ 4682/16204 ( 29%)], Train Loss: 0.00186\n","Epoch: 06 [ 4702/16204 ( 29%)], Train Loss: 0.00186\n","Epoch: 06 [ 4722/16204 ( 29%)], Train Loss: 0.00186\n","Epoch: 06 [ 4742/16204 ( 29%)], Train Loss: 0.00185\n","Epoch: 06 [ 4762/16204 ( 29%)], Train Loss: 0.00185\n","Epoch: 06 [ 4782/16204 ( 30%)], Train Loss: 0.00184\n","Epoch: 06 [ 4802/16204 ( 30%)], Train Loss: 0.00183\n","Epoch: 06 [ 4822/16204 ( 30%)], Train Loss: 0.00182\n","Epoch: 06 [ 4842/16204 ( 30%)], Train Loss: 0.00182\n","Epoch: 06 [ 4862/16204 ( 30%)], Train Loss: 0.00181\n","Epoch: 06 [ 4882/16204 ( 30%)], Train Loss: 0.00181\n","Epoch: 06 [ 4902/16204 ( 30%)], Train Loss: 0.00180\n","Epoch: 06 [ 4922/16204 ( 30%)], Train Loss: 0.00179\n","Epoch: 06 [ 4942/16204 ( 30%)], Train Loss: 0.00178\n","Epoch: 06 [ 4962/16204 ( 31%)], Train Loss: 0.00178\n","Epoch: 06 [ 4982/16204 ( 31%)], Train Loss: 0.00177\n","Epoch: 06 [ 5002/16204 ( 31%)], Train Loss: 0.00177\n","Epoch: 06 [ 5022/16204 ( 31%)], Train Loss: 0.00176\n","Epoch: 06 [ 5042/16204 ( 31%)], Train Loss: 0.00175\n","Epoch: 06 [ 5062/16204 ( 31%)], Train Loss: 0.00176\n","Epoch: 06 [ 5082/16204 ( 31%)], Train Loss: 0.00175\n","Epoch: 06 [ 5102/16204 ( 31%)], Train Loss: 0.00175\n","Epoch: 06 [ 5122/16204 ( 32%)], Train Loss: 0.00174\n","Epoch: 06 [ 5142/16204 ( 32%)], Train Loss: 0.00174\n","Epoch: 06 [ 5162/16204 ( 32%)], Train Loss: 0.00173\n","Epoch: 06 [ 5182/16204 ( 32%)], Train Loss: 0.00172\n","Epoch: 06 [ 5202/16204 ( 32%)], Train Loss: 0.00172\n","Epoch: 06 [ 5222/16204 ( 32%)], Train Loss: 0.00171\n","Epoch: 06 [ 5242/16204 ( 32%)], Train Loss: 0.00171\n","Epoch: 06 [ 5262/16204 ( 32%)], Train Loss: 0.00170\n","Epoch: 06 [ 5282/16204 ( 33%)], Train Loss: 0.00169\n","Epoch: 06 [ 5302/16204 ( 33%)], Train Loss: 0.00169\n","Epoch: 06 [ 5322/16204 ( 33%)], Train Loss: 0.00168\n","Epoch: 06 [ 5342/16204 ( 33%)], Train Loss: 0.00168\n","Epoch: 06 [ 5362/16204 ( 33%)], Train Loss: 0.00168\n","Epoch: 06 [ 5382/16204 ( 33%)], Train Loss: 0.00167\n","Epoch: 06 [ 5402/16204 ( 33%)], Train Loss: 0.00166\n","Epoch: 06 [ 5422/16204 ( 33%)], Train Loss: 0.00166\n","Epoch: 06 [ 5442/16204 ( 34%)], Train Loss: 0.00165\n","Epoch: 06 [ 5462/16204 ( 34%)], Train Loss: 0.00165\n","Epoch: 06 [ 5482/16204 ( 34%)], Train Loss: 0.00165\n","Epoch: 06 [ 5502/16204 ( 34%)], Train Loss: 0.00164\n","Epoch: 06 [ 5522/16204 ( 34%)], Train Loss: 0.00164\n","Epoch: 06 [ 5542/16204 ( 34%)], Train Loss: 0.00163\n","Epoch: 06 [ 5562/16204 ( 34%)], Train Loss: 0.00162\n","Epoch: 06 [ 5582/16204 ( 34%)], Train Loss: 0.00162\n","Epoch: 06 [ 5602/16204 ( 35%)], Train Loss: 0.00161\n","Epoch: 06 [ 5622/16204 ( 35%)], Train Loss: 0.00161\n","Epoch: 06 [ 5642/16204 ( 35%)], Train Loss: 0.00161\n","Epoch: 06 [ 5662/16204 ( 35%)], Train Loss: 0.00160\n","Epoch: 06 [ 5682/16204 ( 35%)], Train Loss: 0.00160\n","Epoch: 06 [ 5702/16204 ( 35%)], Train Loss: 0.00160\n","Epoch: 06 [ 5722/16204 ( 35%)], Train Loss: 0.00159\n","Epoch: 06 [ 5742/16204 ( 35%)], Train Loss: 0.00159\n","Epoch: 06 [ 5762/16204 ( 36%)], Train Loss: 0.00158\n","Epoch: 06 [ 5782/16204 ( 36%)], Train Loss: 0.00158\n","Epoch: 06 [ 5802/16204 ( 36%)], Train Loss: 0.00157\n","Epoch: 06 [ 5822/16204 ( 36%)], Train Loss: 0.00157\n","Epoch: 06 [ 5842/16204 ( 36%)], Train Loss: 0.00162\n","Epoch: 06 [ 5862/16204 ( 36%)], Train Loss: 0.00162\n","Epoch: 06 [ 5882/16204 ( 36%)], Train Loss: 0.00161\n","Epoch: 06 [ 5902/16204 ( 36%)], Train Loss: 0.00161\n","Epoch: 06 [ 5922/16204 ( 37%)], Train Loss: 0.00161\n","Epoch: 06 [ 5942/16204 ( 37%)], Train Loss: 0.00161\n","Epoch: 06 [ 5962/16204 ( 37%)], Train Loss: 0.00161\n","Epoch: 06 [ 5982/16204 ( 37%)], Train Loss: 0.00160\n","Epoch: 06 [ 6002/16204 ( 37%)], Train Loss: 0.00160\n","Epoch: 06 [ 6022/16204 ( 37%)], Train Loss: 0.00159\n","Epoch: 06 [ 6042/16204 ( 37%)], Train Loss: 0.00159\n","Epoch: 06 [ 6062/16204 ( 37%)], Train Loss: 0.00158\n","Epoch: 06 [ 6082/16204 ( 38%)], Train Loss: 0.00158\n","Epoch: 06 [ 6102/16204 ( 38%)], Train Loss: 0.00157\n","Epoch: 06 [ 6122/16204 ( 38%)], Train Loss: 0.00157\n","Epoch: 06 [ 6142/16204 ( 38%)], Train Loss: 0.00156\n","Epoch: 06 [ 6162/16204 ( 38%)], Train Loss: 0.00156\n","Epoch: 06 [ 6182/16204 ( 38%)], Train Loss: 0.00155\n","Epoch: 06 [ 6202/16204 ( 38%)], Train Loss: 0.00155\n","Epoch: 06 [ 6222/16204 ( 38%)], Train Loss: 0.00155\n","Epoch: 06 [ 6242/16204 ( 39%)], Train Loss: 0.00154\n","Epoch: 06 [ 6262/16204 ( 39%)], Train Loss: 0.00154\n","Epoch: 06 [ 6282/16204 ( 39%)], Train Loss: 0.00154\n","Epoch: 06 [ 6302/16204 ( 39%)], Train Loss: 0.00153\n","Epoch: 06 [ 6322/16204 ( 39%)], Train Loss: 0.00153\n","Epoch: 06 [ 6342/16204 ( 39%)], Train Loss: 0.00152\n","Epoch: 06 [ 6362/16204 ( 39%)], Train Loss: 0.00152\n","Epoch: 06 [ 6382/16204 ( 39%)], Train Loss: 0.00152\n","Epoch: 06 [ 6402/16204 ( 40%)], Train Loss: 0.00152\n","Epoch: 06 [ 6422/16204 ( 40%)], Train Loss: 0.00151\n","Epoch: 06 [ 6442/16204 ( 40%)], Train Loss: 0.00151\n","Epoch: 06 [ 6462/16204 ( 40%)], Train Loss: 0.00153\n","Epoch: 06 [ 6482/16204 ( 40%)], Train Loss: 0.00153\n","Epoch: 06 [ 6502/16204 ( 40%)], Train Loss: 0.00152\n","Epoch: 06 [ 6522/16204 ( 40%)], Train Loss: 0.00152\n","Epoch: 06 [ 6542/16204 ( 40%)], Train Loss: 0.00151\n","Epoch: 06 [ 6562/16204 ( 40%)], Train Loss: 0.00151\n","Epoch: 06 [ 6582/16204 ( 41%)], Train Loss: 0.00151\n","Epoch: 06 [ 6602/16204 ( 41%)], Train Loss: 0.00150\n","Epoch: 06 [ 6622/16204 ( 41%)], Train Loss: 0.00150\n","Epoch: 06 [ 6642/16204 ( 41%)], Train Loss: 0.00150\n","Epoch: 06 [ 6662/16204 ( 41%)], Train Loss: 0.00149\n","Epoch: 06 [ 6682/16204 ( 41%)], Train Loss: 0.00149\n","Epoch: 06 [ 6702/16204 ( 41%)], Train Loss: 0.00149\n","Epoch: 06 [ 6722/16204 ( 41%)], Train Loss: 0.00149\n","Epoch: 06 [ 6742/16204 ( 42%)], Train Loss: 0.00148\n","Epoch: 06 [ 6762/16204 ( 42%)], Train Loss: 0.00148\n","Epoch: 06 [ 6782/16204 ( 42%)], Train Loss: 0.00147\n","Epoch: 06 [ 6802/16204 ( 42%)], Train Loss: 0.00147\n","Epoch: 06 [ 6822/16204 ( 42%)], Train Loss: 0.00147\n","Epoch: 06 [ 6842/16204 ( 42%)], Train Loss: 0.00147\n","Epoch: 06 [ 6862/16204 ( 42%)], Train Loss: 0.00146\n","Epoch: 06 [ 6882/16204 ( 42%)], Train Loss: 0.00146\n","Epoch: 06 [ 6902/16204 ( 43%)], Train Loss: 0.00146\n","Epoch: 06 [ 6922/16204 ( 43%)], Train Loss: 0.00146\n","Epoch: 06 [ 6942/16204 ( 43%)], Train Loss: 0.00146\n","Epoch: 06 [ 6962/16204 ( 43%)], Train Loss: 0.00146\n","Epoch: 06 [ 6982/16204 ( 43%)], Train Loss: 0.00145\n","Epoch: 06 [ 7002/16204 ( 43%)], Train Loss: 0.00145\n","Epoch: 06 [ 7022/16204 ( 43%)], Train Loss: 0.00144\n","Epoch: 06 [ 7042/16204 ( 43%)], Train Loss: 0.00144\n","Epoch: 06 [ 7062/16204 ( 44%)], Train Loss: 0.00144\n","Epoch: 06 [ 7082/16204 ( 44%)], Train Loss: 0.00143\n","Epoch: 06 [ 7102/16204 ( 44%)], Train Loss: 0.00143\n","Epoch: 06 [ 7122/16204 ( 44%)], Train Loss: 0.00143\n","Epoch: 06 [ 7142/16204 ( 44%)], Train Loss: 0.00143\n","Epoch: 06 [ 7162/16204 ( 44%)], Train Loss: 0.00143\n","Epoch: 06 [ 7182/16204 ( 44%)], Train Loss: 0.00142\n","Epoch: 06 [ 7202/16204 ( 44%)], Train Loss: 0.00142\n","Epoch: 06 [ 7222/16204 ( 45%)], Train Loss: 0.00142\n","Epoch: 06 [ 7242/16204 ( 45%)], Train Loss: 0.00141\n","Epoch: 06 [ 7262/16204 ( 45%)], Train Loss: 0.00141\n","Epoch: 06 [ 7282/16204 ( 45%)], Train Loss: 0.00141\n","Epoch: 06 [ 7302/16204 ( 45%)], Train Loss: 0.00140\n","Epoch: 06 [ 7322/16204 ( 45%)], Train Loss: 0.00140\n","Epoch: 06 [ 7342/16204 ( 45%)], Train Loss: 0.00140\n","Epoch: 06 [ 7362/16204 ( 45%)], Train Loss: 0.00140\n","Epoch: 06 [ 7382/16204 ( 46%)], Train Loss: 0.00139\n","Epoch: 06 [ 7402/16204 ( 46%)], Train Loss: 0.00139\n","Epoch: 06 [ 7422/16204 ( 46%)], Train Loss: 0.00138\n","Epoch: 06 [ 7442/16204 ( 46%)], Train Loss: 0.00138\n","Epoch: 06 [ 7462/16204 ( 46%)], Train Loss: 0.00138\n","Epoch: 06 [ 7482/16204 ( 46%)], Train Loss: 0.00138\n","Epoch: 06 [ 7502/16204 ( 46%)], Train Loss: 0.00138\n","Epoch: 06 [ 7522/16204 ( 46%)], Train Loss: 0.00137\n","Epoch: 06 [ 7542/16204 ( 47%)], Train Loss: 0.00137\n","Epoch: 06 [ 7562/16204 ( 47%)], Train Loss: 0.00137\n","Epoch: 06 [ 7582/16204 ( 47%)], Train Loss: 0.00137\n","Epoch: 06 [ 7602/16204 ( 47%)], Train Loss: 0.00137\n","Epoch: 06 [ 7622/16204 ( 47%)], Train Loss: 0.00137\n","Epoch: 06 [ 7642/16204 ( 47%)], Train Loss: 0.00136\n","Epoch: 06 [ 7662/16204 ( 47%)], Train Loss: 0.00137\n","Epoch: 06 [ 7682/16204 ( 47%)], Train Loss: 0.00136\n","Epoch: 06 [ 7702/16204 ( 48%)], Train Loss: 0.00136\n","Epoch: 06 [ 7722/16204 ( 48%)], Train Loss: 0.00152\n","Epoch: 06 [ 7742/16204 ( 48%)], Train Loss: 0.00152\n","Epoch: 06 [ 7762/16204 ( 48%)], Train Loss: 0.00151\n","Epoch: 06 [ 7782/16204 ( 48%)], Train Loss: 0.00151\n","Epoch: 06 [ 7802/16204 ( 48%)], Train Loss: 0.00151\n","Epoch: 06 [ 7822/16204 ( 48%)], Train Loss: 0.00150\n","Epoch: 06 [ 7842/16204 ( 48%)], Train Loss: 0.00150\n","Epoch: 06 [ 7862/16204 ( 49%)], Train Loss: 0.00150\n","Epoch: 06 [ 7882/16204 ( 49%)], Train Loss: 0.00150\n","Epoch: 06 [ 7902/16204 ( 49%)], Train Loss: 0.00149\n","Epoch: 06 [ 7922/16204 ( 49%)], Train Loss: 0.00150\n","Epoch: 06 [ 7942/16204 ( 49%)], Train Loss: 0.00150\n","Epoch: 06 [ 7962/16204 ( 49%)], Train Loss: 0.00149\n","Epoch: 06 [ 7982/16204 ( 49%)], Train Loss: 0.00149\n","Epoch: 06 [ 8002/16204 ( 49%)], Train Loss: 0.00149\n","Epoch: 06 [ 8022/16204 ( 50%)], Train Loss: 0.00148\n","Epoch: 06 [ 8042/16204 ( 50%)], Train Loss: 0.00148\n","Epoch: 06 [ 8062/16204 ( 50%)], Train Loss: 0.00148\n","Epoch: 06 [ 8082/16204 ( 50%)], Train Loss: 0.00148\n","Epoch: 06 [ 8102/16204 ( 50%)], Train Loss: 0.00148\n","Epoch: 06 [ 8122/16204 ( 50%)], Train Loss: 0.00147\n","Epoch: 06 [ 8142/16204 ( 50%)], Train Loss: 0.00147\n","Epoch: 06 [ 8162/16204 ( 50%)], Train Loss: 0.00147\n","Epoch: 06 [ 8182/16204 ( 50%)], Train Loss: 0.00146\n","Epoch: 06 [ 8202/16204 ( 51%)], Train Loss: 0.00146\n","Epoch: 06 [ 8222/16204 ( 51%)], Train Loss: 0.00146\n","Epoch: 06 [ 8242/16204 ( 51%)], Train Loss: 0.00145\n","Epoch: 06 [ 8262/16204 ( 51%)], Train Loss: 0.00145\n","Epoch: 06 [ 8282/16204 ( 51%)], Train Loss: 0.00145\n","Epoch: 06 [ 8302/16204 ( 51%)], Train Loss: 0.00144\n","Epoch: 06 [ 8322/16204 ( 51%)], Train Loss: 0.00144\n","Epoch: 06 [ 8342/16204 ( 51%)], Train Loss: 0.00144\n","Epoch: 06 [ 8362/16204 ( 52%)], Train Loss: 0.00144\n","Epoch: 06 [ 8382/16204 ( 52%)], Train Loss: 0.00143\n","Epoch: 06 [ 8402/16204 ( 52%)], Train Loss: 0.00143\n","Epoch: 06 [ 8422/16204 ( 52%)], Train Loss: 0.00143\n","Epoch: 06 [ 8442/16204 ( 52%)], Train Loss: 0.00143\n","Epoch: 06 [ 8462/16204 ( 52%)], Train Loss: 0.00142\n","Epoch: 06 [ 8482/16204 ( 52%)], Train Loss: 0.00142\n","Epoch: 06 [ 8502/16204 ( 52%)], Train Loss: 0.00142\n","Epoch: 06 [ 8522/16204 ( 53%)], Train Loss: 0.00142\n","Epoch: 06 [ 8542/16204 ( 53%)], Train Loss: 0.00141\n","Epoch: 06 [ 8562/16204 ( 53%)], Train Loss: 0.00142\n","Epoch: 06 [ 8582/16204 ( 53%)], Train Loss: 0.00141\n","Epoch: 06 [ 8602/16204 ( 53%)], Train Loss: 0.00141\n","Epoch: 06 [ 8622/16204 ( 53%)], Train Loss: 0.00141\n","Epoch: 06 [ 8642/16204 ( 53%)], Train Loss: 0.00140\n","Epoch: 06 [ 8662/16204 ( 53%)], Train Loss: 0.00140\n","Epoch: 06 [ 8682/16204 ( 54%)], Train Loss: 0.00140\n","Epoch: 06 [ 8702/16204 ( 54%)], Train Loss: 0.00140\n","Epoch: 06 [ 8722/16204 ( 54%)], Train Loss: 0.00139\n","Epoch: 06 [ 8742/16204 ( 54%)], Train Loss: 0.00139\n","Epoch: 06 [ 8762/16204 ( 54%)], Train Loss: 0.00139\n","Epoch: 06 [ 8782/16204 ( 54%)], Train Loss: 0.00138\n","Epoch: 06 [ 8802/16204 ( 54%)], Train Loss: 0.00138\n","Epoch: 06 [ 8822/16204 ( 54%)], Train Loss: 0.00138\n","Epoch: 06 [ 8842/16204 ( 55%)], Train Loss: 0.00138\n","Epoch: 06 [ 8862/16204 ( 55%)], Train Loss: 0.00138\n","Epoch: 06 [ 8882/16204 ( 55%)], Train Loss: 0.00137\n","Epoch: 06 [ 8902/16204 ( 55%)], Train Loss: 0.00137\n","Epoch: 06 [ 8922/16204 ( 55%)], Train Loss: 0.00137\n","Epoch: 06 [ 8942/16204 ( 55%)], Train Loss: 0.00137\n","Epoch: 06 [ 8962/16204 ( 55%)], Train Loss: 0.00137\n","Epoch: 06 [ 8982/16204 ( 55%)], Train Loss: 0.00136\n","Epoch: 06 [ 9002/16204 ( 56%)], Train Loss: 0.00136\n","Epoch: 06 [ 9022/16204 ( 56%)], Train Loss: 0.00136\n","Epoch: 06 [ 9042/16204 ( 56%)], Train Loss: 0.00136\n","Epoch: 06 [ 9062/16204 ( 56%)], Train Loss: 0.00135\n","Epoch: 06 [ 9082/16204 ( 56%)], Train Loss: 0.00136\n","Epoch: 06 [ 9102/16204 ( 56%)], Train Loss: 0.00136\n","Epoch: 06 [ 9122/16204 ( 56%)], Train Loss: 0.00135\n","Epoch: 06 [ 9142/16204 ( 56%)], Train Loss: 0.00135\n","Epoch: 06 [ 9162/16204 ( 57%)], Train Loss: 0.00135\n","Epoch: 06 [ 9182/16204 ( 57%)], Train Loss: 0.00135\n","Epoch: 06 [ 9202/16204 ( 57%)], Train Loss: 0.00135\n","Epoch: 06 [ 9222/16204 ( 57%)], Train Loss: 0.00135\n","Epoch: 06 [ 9242/16204 ( 57%)], Train Loss: 0.00134\n","Epoch: 06 [ 9262/16204 ( 57%)], Train Loss: 0.00134\n","Epoch: 06 [ 9282/16204 ( 57%)], Train Loss: 0.00134\n","Epoch: 06 [ 9302/16204 ( 57%)], Train Loss: 0.00134\n","Epoch: 06 [ 9322/16204 ( 58%)], Train Loss: 0.00133\n","Epoch: 06 [ 9342/16204 ( 58%)], Train Loss: 0.00133\n","Epoch: 06 [ 9362/16204 ( 58%)], Train Loss: 0.00133\n","Epoch: 06 [ 9382/16204 ( 58%)], Train Loss: 0.00133\n","Epoch: 06 [ 9402/16204 ( 58%)], Train Loss: 0.00132\n","Epoch: 06 [ 9422/16204 ( 58%)], Train Loss: 0.00132\n","Epoch: 06 [ 9442/16204 ( 58%)], Train Loss: 0.00132\n","Epoch: 06 [ 9462/16204 ( 58%)], Train Loss: 0.00132\n","Epoch: 06 [ 9482/16204 ( 59%)], Train Loss: 0.00132\n","Epoch: 06 [ 9502/16204 ( 59%)], Train Loss: 0.00131\n","Epoch: 06 [ 9522/16204 ( 59%)], Train Loss: 0.00131\n","Epoch: 06 [ 9542/16204 ( 59%)], Train Loss: 0.00131\n","Epoch: 06 [ 9562/16204 ( 59%)], Train Loss: 0.00130\n","Epoch: 06 [ 9582/16204 ( 59%)], Train Loss: 0.00130\n","Epoch: 06 [ 9602/16204 ( 59%)], Train Loss: 0.00130\n","Epoch: 06 [ 9622/16204 ( 59%)], Train Loss: 0.00130\n","Epoch: 06 [ 9642/16204 ( 60%)], Train Loss: 0.00129\n","Epoch: 06 [ 9662/16204 ( 60%)], Train Loss: 0.00129\n","Epoch: 06 [ 9682/16204 ( 60%)], Train Loss: 0.00129\n","Epoch: 06 [ 9702/16204 ( 60%)], Train Loss: 0.00129\n","Epoch: 06 [ 9722/16204 ( 60%)], Train Loss: 0.00128\n","Epoch: 06 [ 9742/16204 ( 60%)], Train Loss: 0.00128\n","Epoch: 06 [ 9762/16204 ( 60%)], Train Loss: 0.00128\n","Epoch: 06 [ 9782/16204 ( 60%)], Train Loss: 0.00128\n","Epoch: 06 [ 9802/16204 ( 60%)], Train Loss: 0.00128\n","Epoch: 06 [ 9822/16204 ( 61%)], Train Loss: 0.00127\n","Epoch: 06 [ 9842/16204 ( 61%)], Train Loss: 0.00127\n","Epoch: 06 [ 9862/16204 ( 61%)], Train Loss: 0.00127\n","Epoch: 06 [ 9882/16204 ( 61%)], Train Loss: 0.00127\n","Epoch: 06 [ 9902/16204 ( 61%)], Train Loss: 0.00126\n","Epoch: 06 [ 9922/16204 ( 61%)], Train Loss: 0.00126\n","Epoch: 06 [ 9942/16204 ( 61%)], Train Loss: 0.00126\n","Epoch: 06 [ 9962/16204 ( 61%)], Train Loss: 0.00126\n","Epoch: 06 [ 9982/16204 ( 62%)], Train Loss: 0.00125\n","Epoch: 06 [10002/16204 ( 62%)], Train Loss: 0.00125\n","Epoch: 06 [10022/16204 ( 62%)], Train Loss: 0.00125\n","Epoch: 06 [10042/16204 ( 62%)], Train Loss: 0.00125\n","Epoch: 06 [10062/16204 ( 62%)], Train Loss: 0.00125\n","Epoch: 06 [10082/16204 ( 62%)], Train Loss: 0.00124\n","Epoch: 06 [10102/16204 ( 62%)], Train Loss: 0.00124\n","Epoch: 06 [10122/16204 ( 62%)], Train Loss: 0.00124\n","Epoch: 06 [10142/16204 ( 63%)], Train Loss: 0.00124\n","Epoch: 06 [10162/16204 ( 63%)], Train Loss: 0.00124\n","Epoch: 06 [10182/16204 ( 63%)], Train Loss: 0.00123\n","Epoch: 06 [10202/16204 ( 63%)], Train Loss: 0.00123\n","Epoch: 06 [10222/16204 ( 63%)], Train Loss: 0.00123\n","Epoch: 06 [10242/16204 ( 63%)], Train Loss: 0.00123\n","Epoch: 06 [10262/16204 ( 63%)], Train Loss: 0.00123\n","Epoch: 06 [10282/16204 ( 63%)], Train Loss: 0.00122\n","Epoch: 06 [10302/16204 ( 64%)], Train Loss: 0.00122\n","Epoch: 06 [10322/16204 ( 64%)], Train Loss: 0.00122\n","Epoch: 06 [10342/16204 ( 64%)], Train Loss: 0.00122\n","Epoch: 06 [10362/16204 ( 64%)], Train Loss: 0.00122\n","Epoch: 06 [10382/16204 ( 64%)], Train Loss: 0.00121\n","Epoch: 06 [10402/16204 ( 64%)], Train Loss: 0.00121\n","Epoch: 06 [10422/16204 ( 64%)], Train Loss: 0.00121\n","Epoch: 06 [10442/16204 ( 64%)], Train Loss: 0.00121\n","Epoch: 06 [10462/16204 ( 65%)], Train Loss: 0.00120\n","Epoch: 06 [10482/16204 ( 65%)], Train Loss: 0.00120\n","Epoch: 06 [10502/16204 ( 65%)], Train Loss: 0.00120\n","Epoch: 06 [10522/16204 ( 65%)], Train Loss: 0.00120\n","Epoch: 06 [10542/16204 ( 65%)], Train Loss: 0.00120\n","Epoch: 06 [10562/16204 ( 65%)], Train Loss: 0.00119\n","Epoch: 06 [10582/16204 ( 65%)], Train Loss: 0.00119\n","Epoch: 06 [10602/16204 ( 65%)], Train Loss: 0.00119\n","Epoch: 06 [10622/16204 ( 66%)], Train Loss: 0.00119\n","Epoch: 06 [10642/16204 ( 66%)], Train Loss: 0.00119\n","Epoch: 06 [10662/16204 ( 66%)], Train Loss: 0.00118\n","Epoch: 06 [10682/16204 ( 66%)], Train Loss: 0.00118\n","Epoch: 06 [10702/16204 ( 66%)], Train Loss: 0.00118\n","Epoch: 06 [10722/16204 ( 66%)], Train Loss: 0.00118\n","Epoch: 06 [10742/16204 ( 66%)], Train Loss: 0.00118\n","Epoch: 06 [10762/16204 ( 66%)], Train Loss: 0.00118\n","Epoch: 06 [10782/16204 ( 67%)], Train Loss: 0.00118\n","Epoch: 06 [10802/16204 ( 67%)], Train Loss: 0.00117\n","Epoch: 06 [10822/16204 ( 67%)], Train Loss: 0.00117\n","Epoch: 06 [10842/16204 ( 67%)], Train Loss: 0.00121\n","Epoch: 06 [10862/16204 ( 67%)], Train Loss: 0.00121\n","Epoch: 06 [10882/16204 ( 67%)], Train Loss: 0.00120\n","Epoch: 06 [10902/16204 ( 67%)], Train Loss: 0.00120\n","Epoch: 06 [10922/16204 ( 67%)], Train Loss: 0.00121\n","Epoch: 06 [10942/16204 ( 68%)], Train Loss: 0.00121\n","Epoch: 06 [10962/16204 ( 68%)], Train Loss: 0.00121\n","Epoch: 06 [10982/16204 ( 68%)], Train Loss: 0.00120\n","Epoch: 06 [11002/16204 ( 68%)], Train Loss: 0.00120\n","Epoch: 06 [11022/16204 ( 68%)], Train Loss: 0.00120\n","Epoch: 06 [11042/16204 ( 68%)], Train Loss: 0.00120\n","Epoch: 06 [11062/16204 ( 68%)], Train Loss: 0.00120\n","Epoch: 06 [11082/16204 ( 68%)], Train Loss: 0.00119\n","Epoch: 06 [11102/16204 ( 69%)], Train Loss: 0.00120\n","Epoch: 06 [11122/16204 ( 69%)], Train Loss: 0.00119\n","Epoch: 06 [11142/16204 ( 69%)], Train Loss: 0.00120\n","Epoch: 06 [11162/16204 ( 69%)], Train Loss: 0.00120\n","Epoch: 06 [11182/16204 ( 69%)], Train Loss: 0.00119\n","Epoch: 06 [11202/16204 ( 69%)], Train Loss: 0.00119\n","Epoch: 06 [11222/16204 ( 69%)], Train Loss: 0.00119\n","Epoch: 06 [11242/16204 ( 69%)], Train Loss: 0.00119\n","Epoch: 06 [11262/16204 ( 70%)], Train Loss: 0.00119\n","Epoch: 06 [11282/16204 ( 70%)], Train Loss: 0.00119\n","Epoch: 06 [11302/16204 ( 70%)], Train Loss: 0.00119\n","Epoch: 06 [11322/16204 ( 70%)], Train Loss: 0.00118\n","Epoch: 06 [11342/16204 ( 70%)], Train Loss: 0.00118\n","Epoch: 06 [11362/16204 ( 70%)], Train Loss: 0.00118\n","Epoch: 06 [11382/16204 ( 70%)], Train Loss: 0.00118\n","Epoch: 06 [11402/16204 ( 70%)], Train Loss: 0.00118\n","Epoch: 06 [11422/16204 ( 70%)], Train Loss: 0.00117\n","Epoch: 06 [11442/16204 ( 71%)], Train Loss: 0.00117\n","Epoch: 06 [11462/16204 ( 71%)], Train Loss: 0.00117\n","Epoch: 06 [11482/16204 ( 71%)], Train Loss: 0.00117\n","Epoch: 06 [11502/16204 ( 71%)], Train Loss: 0.00117\n","Epoch: 06 [11522/16204 ( 71%)], Train Loss: 0.00117\n","Epoch: 06 [11542/16204 ( 71%)], Train Loss: 0.00116\n","Epoch: 06 [11562/16204 ( 71%)], Train Loss: 0.00116\n","Epoch: 06 [11582/16204 ( 71%)], Train Loss: 0.00116\n","Epoch: 06 [11602/16204 ( 72%)], Train Loss: 0.00116\n","Epoch: 06 [11622/16204 ( 72%)], Train Loss: 0.00116\n","Epoch: 06 [11642/16204 ( 72%)], Train Loss: 0.00116\n","Epoch: 06 [11662/16204 ( 72%)], Train Loss: 0.00115\n","Epoch: 06 [11682/16204 ( 72%)], Train Loss: 0.00115\n","Epoch: 06 [11702/16204 ( 72%)], Train Loss: 0.00115\n","Epoch: 06 [11722/16204 ( 72%)], Train Loss: 0.00115\n","Epoch: 06 [11742/16204 ( 72%)], Train Loss: 0.00115\n","Epoch: 06 [11762/16204 ( 73%)], Train Loss: 0.00115\n","Epoch: 06 [11782/16204 ( 73%)], Train Loss: 0.00115\n","Epoch: 06 [11802/16204 ( 73%)], Train Loss: 0.00115\n","Epoch: 06 [11822/16204 ( 73%)], Train Loss: 0.00114\n","Epoch: 06 [11842/16204 ( 73%)], Train Loss: 0.00114\n","Epoch: 06 [11862/16204 ( 73%)], Train Loss: 0.00114\n","Epoch: 06 [11882/16204 ( 73%)], Train Loss: 0.00114\n","Epoch: 06 [11902/16204 ( 73%)], Train Loss: 0.00114\n","Epoch: 06 [11922/16204 ( 74%)], Train Loss: 0.00114\n","Epoch: 06 [11942/16204 ( 74%)], Train Loss: 0.00113\n","Epoch: 06 [11962/16204 ( 74%)], Train Loss: 0.00113\n","Epoch: 06 [11982/16204 ( 74%)], Train Loss: 0.00113\n","Epoch: 06 [12002/16204 ( 74%)], Train Loss: 0.00113\n","Epoch: 06 [12022/16204 ( 74%)], Train Loss: 0.00113\n","Epoch: 06 [12042/16204 ( 74%)], Train Loss: 0.00113\n","Epoch: 06 [12062/16204 ( 74%)], Train Loss: 0.00113\n","Epoch: 06 [12082/16204 ( 75%)], Train Loss: 0.00113\n","Epoch: 06 [12102/16204 ( 75%)], Train Loss: 0.00113\n","Epoch: 06 [12122/16204 ( 75%)], Train Loss: 0.00112\n","Epoch: 06 [12142/16204 ( 75%)], Train Loss: 0.00112\n","Epoch: 06 [12162/16204 ( 75%)], Train Loss: 0.00112\n","Epoch: 06 [12182/16204 ( 75%)], Train Loss: 0.00112\n","Epoch: 06 [12202/16204 ( 75%)], Train Loss: 0.00112\n","Epoch: 06 [12222/16204 ( 75%)], Train Loss: 0.00112\n","Epoch: 06 [12242/16204 ( 76%)], Train Loss: 0.00111\n","Epoch: 06 [12262/16204 ( 76%)], Train Loss: 0.00111\n","Epoch: 06 [12282/16204 ( 76%)], Train Loss: 0.00111\n","Epoch: 06 [12302/16204 ( 76%)], Train Loss: 0.00111\n","Epoch: 06 [12322/16204 ( 76%)], Train Loss: 0.00111\n","Epoch: 06 [12342/16204 ( 76%)], Train Loss: 0.00111\n","Epoch: 06 [12362/16204 ( 76%)], Train Loss: 0.00110\n","Epoch: 06 [12382/16204 ( 76%)], Train Loss: 0.00110\n","Epoch: 06 [12402/16204 ( 77%)], Train Loss: 0.00110\n","Epoch: 06 [12422/16204 ( 77%)], Train Loss: 0.00110\n","Epoch: 06 [12442/16204 ( 77%)], Train Loss: 0.00110\n","Epoch: 06 [12462/16204 ( 77%)], Train Loss: 0.00110\n","Epoch: 06 [12482/16204 ( 77%)], Train Loss: 0.00109\n","Epoch: 06 [12502/16204 ( 77%)], Train Loss: 0.00109\n","Epoch: 06 [12522/16204 ( 77%)], Train Loss: 0.00109\n","Epoch: 06 [12542/16204 ( 77%)], Train Loss: 0.00109\n","Epoch: 06 [12562/16204 ( 78%)], Train Loss: 0.00109\n","Epoch: 06 [12582/16204 ( 78%)], Train Loss: 0.00109\n","Epoch: 06 [12602/16204 ( 78%)], Train Loss: 0.00109\n","Epoch: 06 [12622/16204 ( 78%)], Train Loss: 0.00108\n","Epoch: 06 [12642/16204 ( 78%)], Train Loss: 0.00108\n","Epoch: 06 [12662/16204 ( 78%)], Train Loss: 0.00108\n","Epoch: 06 [12682/16204 ( 78%)], Train Loss: 0.00108\n","Epoch: 06 [12702/16204 ( 78%)], Train Loss: 0.00108\n","Epoch: 06 [12722/16204 ( 79%)], Train Loss: 0.00108\n","Epoch: 06 [12742/16204 ( 79%)], Train Loss: 0.00107\n","Epoch: 06 [12762/16204 ( 79%)], Train Loss: 0.00107\n","Epoch: 06 [12782/16204 ( 79%)], Train Loss: 0.00107\n","Epoch: 06 [12802/16204 ( 79%)], Train Loss: 0.00107\n","Epoch: 06 [12822/16204 ( 79%)], Train Loss: 0.00107\n","Epoch: 06 [12842/16204 ( 79%)], Train Loss: 0.00107\n","Epoch: 06 [12862/16204 ( 79%)], Train Loss: 0.00107\n","Epoch: 06 [12882/16204 ( 79%)], Train Loss: 0.00106\n","Epoch: 06 [12902/16204 ( 80%)], Train Loss: 0.00106\n","Epoch: 06 [12922/16204 ( 80%)], Train Loss: 0.00106\n","Epoch: 06 [12942/16204 ( 80%)], Train Loss: 0.00106\n","Epoch: 06 [12962/16204 ( 80%)], Train Loss: 0.00106\n","Epoch: 06 [12982/16204 ( 80%)], Train Loss: 0.00106\n","Epoch: 06 [13002/16204 ( 80%)], Train Loss: 0.00106\n","Epoch: 06 [13022/16204 ( 80%)], Train Loss: 0.00106\n","Epoch: 06 [13042/16204 ( 80%)], Train Loss: 0.00105\n","Epoch: 06 [13062/16204 ( 81%)], Train Loss: 0.00105\n","Epoch: 06 [13082/16204 ( 81%)], Train Loss: 0.00105\n","Epoch: 06 [13102/16204 ( 81%)], Train Loss: 0.00105\n","Epoch: 06 [13122/16204 ( 81%)], Train Loss: 0.00105\n","Epoch: 06 [13142/16204 ( 81%)], Train Loss: 0.00105\n","Epoch: 06 [13162/16204 ( 81%)], Train Loss: 0.00105\n","Epoch: 06 [13182/16204 ( 81%)], Train Loss: 0.00104\n","Epoch: 06 [13202/16204 ( 81%)], Train Loss: 0.00104\n","Epoch: 06 [13222/16204 ( 82%)], Train Loss: 0.00104\n","Epoch: 06 [13242/16204 ( 82%)], Train Loss: 0.00104\n","Epoch: 06 [13262/16204 ( 82%)], Train Loss: 0.00104\n","Epoch: 06 [13282/16204 ( 82%)], Train Loss: 0.00104\n","Epoch: 06 [13302/16204 ( 82%)], Train Loss: 0.00104\n","Epoch: 06 [13322/16204 ( 82%)], Train Loss: 0.00103\n","Epoch: 06 [13342/16204 ( 82%)], Train Loss: 0.00103\n","Epoch: 06 [13362/16204 ( 82%)], Train Loss: 0.00103\n","Epoch: 06 [13382/16204 ( 83%)], Train Loss: 0.00103\n","Epoch: 06 [13402/16204 ( 83%)], Train Loss: 0.00103\n","Epoch: 06 [13422/16204 ( 83%)], Train Loss: 0.00103\n","Epoch: 06 [13442/16204 ( 83%)], Train Loss: 0.00103\n","Epoch: 06 [13462/16204 ( 83%)], Train Loss: 0.00103\n","Epoch: 06 [13482/16204 ( 83%)], Train Loss: 0.00102\n","Epoch: 06 [13502/16204 ( 83%)], Train Loss: 0.00102\n","Epoch: 06 [13522/16204 ( 83%)], Train Loss: 0.00102\n","Epoch: 06 [13542/16204 ( 84%)], Train Loss: 0.00102\n","Epoch: 06 [13562/16204 ( 84%)], Train Loss: 0.00102\n","Epoch: 06 [13582/16204 ( 84%)], Train Loss: 0.00102\n","Epoch: 06 [13602/16204 ( 84%)], Train Loss: 0.00102\n","Epoch: 06 [13622/16204 ( 84%)], Train Loss: 0.00102\n","Epoch: 06 [13642/16204 ( 84%)], Train Loss: 0.00101\n","Epoch: 06 [13662/16204 ( 84%)], Train Loss: 0.00101\n","Epoch: 06 [13682/16204 ( 84%)], Train Loss: 0.00101\n","Epoch: 06 [13702/16204 ( 85%)], Train Loss: 0.00101\n","Epoch: 06 [13722/16204 ( 85%)], Train Loss: 0.00101\n","Epoch: 06 [13742/16204 ( 85%)], Train Loss: 0.00101\n","Epoch: 06 [13762/16204 ( 85%)], Train Loss: 0.00101\n","Epoch: 06 [13782/16204 ( 85%)], Train Loss: 0.00101\n","Epoch: 06 [13802/16204 ( 85%)], Train Loss: 0.00101\n","Epoch: 06 [13822/16204 ( 85%)], Train Loss: 0.00101\n","Epoch: 06 [13842/16204 ( 85%)], Train Loss: 0.00101\n","Epoch: 06 [13862/16204 ( 86%)], Train Loss: 0.00101\n","Epoch: 06 [13882/16204 ( 86%)], Train Loss: 0.00101\n","Epoch: 06 [13902/16204 ( 86%)], Train Loss: 0.00100\n","Epoch: 06 [13922/16204 ( 86%)], Train Loss: 0.00100\n","Epoch: 06 [13942/16204 ( 86%)], Train Loss: 0.00100\n","Epoch: 06 [13962/16204 ( 86%)], Train Loss: 0.00100\n","Epoch: 06 [13982/16204 ( 86%)], Train Loss: 0.00100\n","Epoch: 06 [14002/16204 ( 86%)], Train Loss: 0.00100\n","Epoch: 06 [14022/16204 ( 87%)], Train Loss: 0.00100\n","Epoch: 06 [14042/16204 ( 87%)], Train Loss: 0.00100\n","Epoch: 06 [14062/16204 ( 87%)], Train Loss: 0.00100\n","Epoch: 06 [14082/16204 ( 87%)], Train Loss: 0.00099\n","Epoch: 06 [14102/16204 ( 87%)], Train Loss: 0.00099\n","Epoch: 06 [14122/16204 ( 87%)], Train Loss: 0.00099\n","Epoch: 06 [14142/16204 ( 87%)], Train Loss: 0.00099\n","Epoch: 06 [14162/16204 ( 87%)], Train Loss: 0.00099\n","Epoch: 06 [14182/16204 ( 88%)], Train Loss: 0.00099\n","Epoch: 06 [14202/16204 ( 88%)], Train Loss: 0.00099\n","Epoch: 06 [14222/16204 ( 88%)], Train Loss: 0.00099\n","Epoch: 06 [14242/16204 ( 88%)], Train Loss: 0.00098\n","Epoch: 06 [14262/16204 ( 88%)], Train Loss: 0.00098\n","Epoch: 06 [14282/16204 ( 88%)], Train Loss: 0.00098\n","Epoch: 06 [14302/16204 ( 88%)], Train Loss: 0.00098\n","Epoch: 06 [14322/16204 ( 88%)], Train Loss: 0.00098\n","Epoch: 06 [14342/16204 ( 89%)], Train Loss: 0.00098\n","Epoch: 06 [14362/16204 ( 89%)], Train Loss: 0.00098\n","Epoch: 06 [14382/16204 ( 89%)], Train Loss: 0.00098\n","Epoch: 06 [14402/16204 ( 89%)], Train Loss: 0.00097\n","Epoch: 06 [14422/16204 ( 89%)], Train Loss: 0.00097\n","Epoch: 06 [14442/16204 ( 89%)], Train Loss: 0.00097\n","Epoch: 06 [14462/16204 ( 89%)], Train Loss: 0.00097\n","Epoch: 06 [14482/16204 ( 89%)], Train Loss: 0.00097\n","Epoch: 06 [14502/16204 ( 89%)], Train Loss: 0.00097\n","Epoch: 06 [14522/16204 ( 90%)], Train Loss: 0.00097\n","Epoch: 06 [14542/16204 ( 90%)], Train Loss: 0.00097\n","Epoch: 06 [14562/16204 ( 90%)], Train Loss: 0.00097\n","Epoch: 06 [14582/16204 ( 90%)], Train Loss: 0.00097\n","Epoch: 06 [14602/16204 ( 90%)], Train Loss: 0.00097\n","Epoch: 06 [14622/16204 ( 90%)], Train Loss: 0.00097\n","Epoch: 06 [14642/16204 ( 90%)], Train Loss: 0.00097\n","Epoch: 06 [14662/16204 ( 90%)], Train Loss: 0.00097\n","Epoch: 06 [14682/16204 ( 91%)], Train Loss: 0.00096\n","Epoch: 06 [14702/16204 ( 91%)], Train Loss: 0.00096\n","Epoch: 06 [14722/16204 ( 91%)], Train Loss: 0.00096\n","Epoch: 06 [14742/16204 ( 91%)], Train Loss: 0.00096\n","Epoch: 06 [14762/16204 ( 91%)], Train Loss: 0.00096\n","Epoch: 06 [14782/16204 ( 91%)], Train Loss: 0.00096\n","Epoch: 06 [14802/16204 ( 91%)], Train Loss: 0.00096\n","Epoch: 06 [14822/16204 ( 91%)], Train Loss: 0.00096\n","Epoch: 06 [14842/16204 ( 92%)], Train Loss: 0.00096\n","Epoch: 06 [14862/16204 ( 92%)], Train Loss: 0.00095\n","Epoch: 06 [14882/16204 ( 92%)], Train Loss: 0.00095\n","Epoch: 06 [14902/16204 ( 92%)], Train Loss: 0.00095\n","Epoch: 06 [14922/16204 ( 92%)], Train Loss: 0.00095\n","Epoch: 06 [14942/16204 ( 92%)], Train Loss: 0.00095\n","Epoch: 06 [14962/16204 ( 92%)], Train Loss: 0.00095\n","Epoch: 06 [14982/16204 ( 92%)], Train Loss: 0.00099\n","Epoch: 06 [15002/16204 ( 93%)], Train Loss: 0.00099\n","Epoch: 06 [15022/16204 ( 93%)], Train Loss: 0.00098\n","Epoch: 06 [15042/16204 ( 93%)], Train Loss: 0.00098\n","Epoch: 06 [15062/16204 ( 93%)], Train Loss: 0.00098\n","Epoch: 06 [15082/16204 ( 93%)], Train Loss: 0.00098\n","Epoch: 06 [15102/16204 ( 93%)], Train Loss: 0.00098\n","Epoch: 06 [15122/16204 ( 93%)], Train Loss: 0.00098\n","Epoch: 06 [15142/16204 ( 93%)], Train Loss: 0.00098\n","Epoch: 06 [15162/16204 ( 94%)], Train Loss: 0.00098\n","Epoch: 06 [15182/16204 ( 94%)], Train Loss: 0.00098\n","Epoch: 06 [15202/16204 ( 94%)], Train Loss: 0.00097\n","Epoch: 06 [15222/16204 ( 94%)], Train Loss: 0.00097\n","Epoch: 06 [15242/16204 ( 94%)], Train Loss: 0.00097\n","Epoch: 06 [15262/16204 ( 94%)], Train Loss: 0.00097\n","Epoch: 06 [15282/16204 ( 94%)], Train Loss: 0.00097\n","Epoch: 06 [15302/16204 ( 94%)], Train Loss: 0.00097\n","Epoch: 06 [15322/16204 ( 95%)], Train Loss: 0.00097\n","Epoch: 06 [15342/16204 ( 95%)], Train Loss: 0.00097\n","Epoch: 06 [15362/16204 ( 95%)], Train Loss: 0.00097\n","Epoch: 06 [15382/16204 ( 95%)], Train Loss: 0.00097\n","Epoch: 06 [15402/16204 ( 95%)], Train Loss: 0.00096\n","Epoch: 06 [15422/16204 ( 95%)], Train Loss: 0.00100\n","Epoch: 06 [15442/16204 ( 95%)], Train Loss: 0.00100\n","Epoch: 06 [15462/16204 ( 95%)], Train Loss: 0.00100\n","Epoch: 06 [15482/16204 ( 96%)], Train Loss: 0.00100\n","Epoch: 06 [15502/16204 ( 96%)], Train Loss: 0.00100\n","Epoch: 06 [15522/16204 ( 96%)], Train Loss: 0.00099\n","Epoch: 06 [15542/16204 ( 96%)], Train Loss: 0.00099\n","Epoch: 06 [15562/16204 ( 96%)], Train Loss: 0.00099\n","Epoch: 06 [15582/16204 ( 96%)], Train Loss: 0.00099\n","Epoch: 06 [15602/16204 ( 96%)], Train Loss: 0.00099\n","Epoch: 06 [15622/16204 ( 96%)], Train Loss: 0.00099\n","Epoch: 06 [15642/16204 ( 97%)], Train Loss: 0.00099\n","Epoch: 06 [15662/16204 ( 97%)], Train Loss: 0.00099\n","Epoch: 06 [15682/16204 ( 97%)], Train Loss: 0.00098\n","Epoch: 06 [15702/16204 ( 97%)], Train Loss: 0.00098\n","Epoch: 06 [15722/16204 ( 97%)], Train Loss: 0.00098\n","Epoch: 06 [15742/16204 ( 97%)], Train Loss: 0.00098\n","Epoch: 06 [15762/16204 ( 97%)], Train Loss: 0.00098\n","Epoch: 06 [15782/16204 ( 97%)], Train Loss: 0.00098\n","Epoch: 06 [15802/16204 ( 98%)], Train Loss: 0.00098\n","Epoch: 06 [15822/16204 ( 98%)], Train Loss: 0.00098\n","Epoch: 06 [15842/16204 ( 98%)], Train Loss: 0.00098\n","Epoch: 06 [15862/16204 ( 98%)], Train Loss: 0.00097\n","Epoch: 06 [15882/16204 ( 98%)], Train Loss: 0.00097\n","Epoch: 06 [15902/16204 ( 98%)], Train Loss: 0.00097\n","Epoch: 06 [15922/16204 ( 98%)], Train Loss: 0.00097\n","Epoch: 06 [15942/16204 ( 98%)], Train Loss: 0.00097\n","Epoch: 06 [15962/16204 ( 99%)], Train Loss: 0.00097\n","Epoch: 06 [15982/16204 ( 99%)], Train Loss: 0.00097\n","Epoch: 06 [16002/16204 ( 99%)], Train Loss: 0.00097\n","Epoch: 06 [16022/16204 ( 99%)], Train Loss: 0.00097\n","Epoch: 06 [16042/16204 ( 99%)], Train Loss: 0.00096\n","Epoch: 06 [16062/16204 ( 99%)], Train Loss: 0.00096\n","Epoch: 06 [16082/16204 ( 99%)], Train Loss: 0.00096\n","Epoch: 06 [16102/16204 ( 99%)], Train Loss: 0.00096\n","Epoch: 06 [16122/16204 ( 99%)], Train Loss: 0.00096\n","Epoch: 06 [16142/16204 (100%)], Train Loss: 0.00096\n","Epoch: 06 [16162/16204 (100%)], Train Loss: 0.00096\n","Epoch: 06 [16182/16204 (100%)], Train Loss: 0.00096\n","Epoch: 06 [16202/16204 (100%)], Train Loss: 0.00096\n","Epoch: 06 [16204/16204 (100%)], Train Loss: 0.00096\n","----Validation Results Summary----\n","Epoch: [6] Valid Loss: 0.00000\n","\n","Total Training Time: 24391.167369127274secs, Average Training Time per Epoch: 3484.452481303896secs.\n","Total Validation Time: 2.0886800289154053secs, Average Validation Time per Epoch: 0.29838286127362934secs.\n"]}]},{"cell_type":"code","metadata":{"id":"DkjRIhdbwjHx","execution":{"iopub.status.busy":"2021-08-17T20:36:49.722586Z","iopub.execute_input":"2021-08-17T20:36:49.723011Z","iopub.status.idle":"2021-08-17T20:36:49.727659Z","shell.execute_reply.started":"2021-08-17T20:36:49.722978Z","shell.execute_reply":"2021-08-17T20:36:49.726609Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1633891220226,"user_tz":-540,"elapsed":52,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["# example for training second fold\n","\n","# for fold in range(1, 2):\n","#     print();print()\n","#     print('-'*50)\n","#     print(f'FOLD: {fold}')\n","#     print('-'*50)\n","#     run(train, fold)"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7uIwhUECP4iP"},"source":["### Thanks and please do Upvote!"]}]}