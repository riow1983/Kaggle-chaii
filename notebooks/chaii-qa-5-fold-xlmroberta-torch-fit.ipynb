{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"name":"chaii-qa-5-fold-xlmroberta-torch-fit.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"4598218598064e8bb4096396b889b75c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c218725a8be0401a93014ed27ed3d652","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b8f631b168714081a3c4fa29d784d4a3","IPY_MODEL_23808bc0ad63443092f5afcebd963768","IPY_MODEL_1166003d02a8405891370efd12238d21"]}},"c218725a8be0401a93014ed27ed3d652":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b8f631b168714081a3c4fa29d784d4a3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_fdbf353ad44348109f1a58602c70dad3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_937b9536275548d59ffe6547b70a741f"}},"23808bc0ad63443092f5afcebd963768":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_f03338f54b16411383e7765b5bb2613b","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":606,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":606,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2023686459074c0eafac81159e81d9c0"}},"1166003d02a8405891370efd12238d21":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5616e0935dd442248f643fb121f5f3eb","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 606/606 [00:00&lt;00:00, 17.4kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6ef9507965ce4091bc8877080ab12309"}},"fdbf353ad44348109f1a58602c70dad3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"937b9536275548d59ffe6547b70a741f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f03338f54b16411383e7765b5bb2613b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"2023686459074c0eafac81159e81d9c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5616e0935dd442248f643fb121f5f3eb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6ef9507965ce4091bc8877080ab12309":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4683532a86b04805a2a5242a58f00ec7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_02bc64bb20a949c69f61d16ebb4485e5","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_6b2406c87dbe41698e83599422aecb19","IPY_MODEL_f561347775e44ecbbb88b5b9fcf6fdf5","IPY_MODEL_ddc8478c0dbe44ddbb45e798e0035715"]}},"02bc64bb20a949c69f61d16ebb4485e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6b2406c87dbe41698e83599422aecb19":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_197520097a45483387434ad52aeda82d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0f8312b700a747f984e29f3ab087aaeb"}},"f561347775e44ecbbb88b5b9fcf6fdf5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7fcb89fe7d76429b9ce9480e4c6b0dc8","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":179,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":179,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a5bf850ed5294d7fa5599ed74223eda5"}},"ddc8478c0dbe44ddbb45e798e0035715":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a0efcabd09584a788d7881a811142466","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 179/179 [00:00&lt;00:00, 5.36kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c369e85f86344ecb8fb2033caaa9bb27"}},"197520097a45483387434ad52aeda82d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0f8312b700a747f984e29f3ab087aaeb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7fcb89fe7d76429b9ce9480e4c6b0dc8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a5bf850ed5294d7fa5599ed74223eda5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a0efcabd09584a788d7881a811142466":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c369e85f86344ecb8fb2033caaa9bb27":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a66c6836263d4c21877e2ea98ab9f072":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_10e84d144c174ee5bdc3662b00b5c80d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_383586bcd6a8422a99a407f4c7ae4c5b","IPY_MODEL_f0984b11c507479a8786ad01065134b3","IPY_MODEL_66efb023564d44bd980bc4afdd159aad"]}},"10e84d144c174ee5bdc3662b00b5c80d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"383586bcd6a8422a99a407f4c7ae4c5b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8d71f2b74e9b48cca35ba04042070fb7","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5645e917734f4d3696f0c9a386627e17"}},"f0984b11c507479a8786ad01065134b3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_14ad85623a3f4d0d87e93d0044114220","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":5069051,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":5069051,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_46d38670110c43499dab2991bbe3c839"}},"66efb023564d44bd980bc4afdd159aad":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7885162769b8418da46480fe656bd67f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 4.83M/4.83M [00:00&lt;00:00, 12.3MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d29e1c00d7c84bf0a212a62576d2b263"}},"8d71f2b74e9b48cca35ba04042070fb7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5645e917734f4d3696f0c9a386627e17":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"14ad85623a3f4d0d87e93d0044114220":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"46d38670110c43499dab2991bbe3c839":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7885162769b8418da46480fe656bd67f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d29e1c00d7c84bf0a212a62576d2b263":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"817e72053f424abe9de89e853bc9c5a9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_3271b014e2af458da814356a27831e68","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_5171b7831c1c4ae5b29a4655293a0872","IPY_MODEL_1abe42b782de473d99d4a092b762885f","IPY_MODEL_c1beb75215d0498987da95b27e87d7bf"]}},"3271b014e2af458da814356a27831e68":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5171b7831c1c4ae5b29a4655293a0872":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b0d4176f59ef4b50992ac9f85bb970de","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_80c0169c63924df29bd8962d7ebe2591"}},"1abe42b782de473d99d4a092b762885f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_247ebc661dad433d923b10a2fcdd7324","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":150,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":150,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1a4c9a962714485fba22e6704dcd5c95"}},"c1beb75215d0498987da95b27e87d7bf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_66bb6e0583e443e3aa80608798eee32e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 150/150 [00:00&lt;00:00, 4.53kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1c4594e41ed4453fb5807a50e1e3e369"}},"b0d4176f59ef4b50992ac9f85bb970de":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"80c0169c63924df29bd8962d7ebe2591":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"247ebc661dad433d923b10a2fcdd7324":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"1a4c9a962714485fba22e6704dcd5c95":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"66bb6e0583e443e3aa80608798eee32e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1c4594e41ed4453fb5807a50e1e3e369":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"097cb4330ad74b9d9abb28f39308f09e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_7bd0dbd63aa74fd295cc1995063c89ab","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a004c02f56e8406686f6597c630e93aa","IPY_MODEL_a7e4f0a02f364a48861dcff855078a7f","IPY_MODEL_a5e327ca2c924b81acfa814087dc530a"]}},"7bd0dbd63aa74fd295cc1995063c89ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a004c02f56e8406686f6597c630e93aa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_acae6a67afca466c94e2e5466eb1314a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_91fd27bfb48c473988e27cb3daad4119"}},"a7e4f0a02f364a48861dcff855078a7f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_955d658ca92f49cb98767ddb8e751cd9","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":2239666418,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":2239666418,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9b17b6ce7fcc40a99eed8d1bfefbaab6"}},"a5e327ca2c924b81acfa814087dc530a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_64399f9508794b239db4ed7001e10c76","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 2.09G/2.09G [00:58&lt;00:00, 40.7MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6d37eb5f725d464ba344d26a8509b35d"}},"acae6a67afca466c94e2e5466eb1314a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"91fd27bfb48c473988e27cb3daad4119":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"955d658ca92f49cb98767ddb8e751cd9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"9b17b6ce7fcc40a99eed8d1bfefbaab6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"64399f9508794b239db4ed7001e10c76":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6d37eb5f725d464ba344d26a8509b35d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RkuY4GVbQ1k4","executionInfo":{"status":"ok","timestamp":1635050611668,"user_tz":-540,"elapsed":32327,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"38ecbcd5-e675-4906-c55b-2e43c413e375"},"source":["# Kaggle or Colab\n","import sys\n","import os\n","if 'kaggle_web_client' in sys.modules:\n","    # Do something\n","    pass\n","elif 'google.colab' in sys.modules:\n","    # Do something\n","    from google.colab import drive\n","    drive.mount(\"/content/drive\")\n","\n","    comp_name_official = \"chaii-hindi-and-tamil-question-answering\"\n","    comp_name_local = \"Kaggle-chaii\"\n","\n","    !pip install --upgrade --force-reinstall --no-deps kaggle\n","    import json\n","    f = open(\"/content/drive/MyDrive/colab_notebooks/kaggle/kaggle.json\", \"r\")\n","    json_data = json.load(f)\n","    os.environ[\"KAGGLE_USERNAME\"] = json_data[\"username\"]\n","    os.environ[\"KAGGLE_KEY\"] = json_data[\"key\"]\n","\n","    %cd /content/drive/MyDrive/colab_notebooks/kaggle/{comp_name_local}/notebooks\n","\n","    dname = \"chaii-qa-5-fold-xlmroberta-torch-fit\"\n","    !mkdir {dname}\n","    #%cd /content/drive/MyDrive/colab_notebooks/kaggle/{comp_name_local}/notebooks/{dname}"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Collecting kaggle\n","  Downloading kaggle-1.5.12.tar.gz (58 kB)\n","\u001b[K     |████████████████████████████████| 58 kB 3.6 MB/s \n","\u001b[?25hBuilding wheels for collected packages: kaggle\n","  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73051 sha256=eac6ad6c2c64e8d3d7e818aba4fa0ee8de6edea6a7f097f1bc530a7c296bb4ad\n","  Stored in directory: /root/.cache/pip/wheels/62/d6/58/5853130f941e75b2177d281eb7e44b4a98ed46dd155f556dc5\n","Successfully built kaggle\n","Installing collected packages: kaggle\n","  Attempting uninstall: kaggle\n","    Found existing installation: kaggle 1.5.12\n","    Uninstalling kaggle-1.5.12:\n","      Successfully uninstalled kaggle-1.5.12\n","Successfully installed kaggle-1.5.12\n","/content/drive/MyDrive/colab_notebooks/kaggle/Kaggle-chaii/notebooks\n","mkdir: cannot create directory ‘chaii-qa-5-fold-xlmroberta-torch-fit’: File exists\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a0xZ62k7SB1g","executionInfo":{"status":"ok","timestamp":1635050619781,"user_tz":-540,"elapsed":8278,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"b9eedc30-cff2-422a-bbca-5a2e98481ffa"},"source":["if 'kaggle_web_client' in sys.modules:\n","    # Do something\n","    pass\n","elif 'google.colab' in sys.modules:\n","    #!pip install transformers\n","    !pip install transformers[sentencepiece]\n","\n","    # import yaml\n","    # with open(f'./config_notebook/config.yaml') as file:\n","    #     cfg = yaml.load(file, Loader=yaml.FullLoader) # Loader is recommended\n","    # print(\"Config:\\n\", cfg)"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers[sentencepiece]\n","  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n","\u001b[K     |████████████████████████████████| 2.9 MB 7.8 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 45.3 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (21.0)\n","Collecting huggingface-hub>=0.0.17\n","  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n","\u001b[K     |████████████████████████████████| 56 kB 4.6 MB/s \n","\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 38.0 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 49.9 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (1.19.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.3.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2019.12.20)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (4.8.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (4.62.3)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.17.3)\n","Collecting sentencepiece!=0.1.92,>=0.1.91\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 41.7 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers[sentencepiece]) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers[sentencepiece]) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers[sentencepiece]) (3.6.0)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers[sentencepiece]) (1.15.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (2021.5.30)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[sentencepiece]) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[sentencepiece]) (1.0.1)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers, sentencepiece\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.0.19 pyyaml-6.0 sacremoses-0.0.46 sentencepiece-0.1.96 tokenizers-0.10.3 transformers-4.11.3\n"]}]},{"cell_type":"markdown","metadata":{"id":"uQdNDkrBP4hj"},"source":["<h2>chaii QA - 5 Fold XLMRoberta Finetuning in Torch w/o Trainer API</h2>\n","    \n","<h3><span \"style: color=#444\">Introduction</span></h3>\n","\n","The kernel implements 5-Fold XLMRoberta QA Model without using the Trainer API from HuggingFace.\n","\n","This is a three part kernel,\n","\n","- [External Data - MLQA, XQUAD Preprocessing](https://www.kaggle.com/rhtsingh/external-data-mlqa-xquad-preprocessing) which preprocesses the Hindi Corpus of MLQA and XQUAD. I have used these data for training.\n","\n","- [chaii QA - 5 Fold XLMRoberta Torch | FIT](https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-fit/edit) This kernel is the current kernel and used for Finetuning (FIT) on competition + external data.\n","\n","- [chaii QA - 5 Fold XLMRoberta Torch | Infer](https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-infer) The Inference kernel where we ensemble our 5 Fold XLMRoberta Models and do the submission.\n","\n","<h3><span \"style: color=#444\">Techniques</span></h3>\n","\n","The kernel has implementation for below techniques, click on the links to learn more -\n","\n"," - [External Data Preprocessing + Training](https://www.kaggle.com/rhtsingh/external-data-mlqa-xquad-preprocessing)\n"," \n"," - [Mixed Precision Training using APEX](https://www.kaggle.com/rhtsingh/swa-apex-amp-interpreting-transformers-in-torch)\n"," \n"," - [Gradient Accumulation](https://www.kaggle.com/rhtsingh/speeding-up-transformer-w-optimization-strategies)\n"," \n"," - [Gradient Clipping](https://www.kaggle.com/rhtsingh/swa-apex-amp-interpreting-transformers-in-torch)\n"," \n"," - [Grouped Layerwise Learning Rate Decay](https://www.kaggle.com/rhtsingh/guide-to-huggingface-schedulers-differential-lrs)\n"," \n"," - [Utilizing Intermediate Transformer Representations](https://www.kaggle.com/rhtsingh/utilizing-transformer-representations-efficiently)\n"," \n"," - [Layer Initialization](https://www.kaggle.com/rhtsingh/on-stability-of-few-sample-transformer-fine-tuning)\n"," \n"," - [5-Fold Training](https://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-fit)\n"," \n"," - etc.\n"," \n","<h3><span \"style: color=#444\">References</span></h3>\n","I would like to acknowledge below kagglers work and resources without whom this kernel wouldn't have been possible,  \n","\n","- [roberta inference 5 folds by Abhishek](https://www.kaggle.com/abhishek/roberta-inference-5-folds)\n","\n","- [ChAII - EDA & Baseline by Darek](https://www.kaggle.com/thedrcat/chaii-eda-baseline) due to whom i found the great notebook below from HuggingFace,\n","\n","- [How to fine-tune a model on question answering](https://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb)\n","\n","- [HuggingFace QA Examples](https://github.com/huggingface/transformers/tree/master/examples/pytorch/question-answering)\n","\n","- [TensorFlow 2.0 Question Answering - Collections of gold solutions](https://www.kaggle.com/c/tensorflow2-question-answering/discussion/127409) these solutions are gold and highly recommend everyone to give a detailed read."]},{"cell_type":"markdown","metadata":{"id":"vI-eiJRPP4hw"},"source":["<h3><span style=\"color=#444\">Note</span></h3>\n","\n","The below points are worth noting,\n","\n"," - I haven't used FP16 because due to some reason this fails and model never starts training.\n"," - These are the original hyperparamters and setting that I have used for training my models.\n"," - I tried few pooling layers but none of them performed better than simple one.\n"," - Gradient clipping reduces model performance.\n"," - <span style=\"color:#2E86C1\">Training 5 Folds at once will give OOM issue on Kaggle and Colab. I have trained one fold at a time i.e. After 1st fold is completed I save the model as a dataset then train second fold. Repeat this process until all 5 folds are completed. Training each fold takes around 50 minuts on Kaggle.</span>\n"," - <span style=\"color:#2E86C1\">I have modified the preprocessing code a lot from original used in HuggingFace notebook since I am not using HuggingFace Datasets API as well. So I had to handle the sequence-ids specially since they contain None values which torch doesn't support.</span>"]},{"cell_type":"markdown","metadata":{"id":"8ykS_qCDP4hz"},"source":["### Install APEX"]},{"cell_type":"code","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","id":"aCY6yvR6ET3s","execution":{"iopub.status.busy":"2021-08-17T20:22:25.164246Z","iopub.execute_input":"2021-08-17T20:22:25.164729Z","iopub.status.idle":"2021-08-17T20:22:25.170948Z","shell.execute_reply.started":"2021-08-17T20:22:25.164627Z","shell.execute_reply":"2021-08-17T20:22:25.169352Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1635050619786,"user_tz":-540,"elapsed":37,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["# %%writefile setup.sh\n","# export CUDA_HOME=/usr/local/cuda-10.1\n","# git clone https://github.com/NVIDIA/apex\n","# cd apex\n","# pip install -v --disable-pip-version-check --no-cache-dir ./"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"-l2Jsav9ET3v","execution":{"iopub.status.busy":"2021-08-17T20:22:26.935257Z","iopub.execute_input":"2021-08-17T20:22:26.935919Z","iopub.status.idle":"2021-08-17T20:22:26.939334Z","shell.execute_reply.started":"2021-08-17T20:22:26.935881Z","shell.execute_reply":"2021-08-17T20:22:26.938488Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1635050619791,"user_tz":-540,"elapsed":34,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["# %%capture\n","# !sh setup.sh"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xFbbPlPgP4h7"},"source":["### Import Dependencies"]},{"cell_type":"code","metadata":{"id":"E4l6PirHET3x","execution":{"iopub.status.busy":"2021-08-17T20:23:50.232396Z","iopub.execute_input":"2021-08-17T20:23:50.232892Z","iopub.status.idle":"2021-08-17T20:24:01.076652Z","shell.execute_reply.started":"2021-08-17T20:23:50.232861Z","shell.execute_reply":"2021-08-17T20:24:01.075658Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635050635249,"user_tz":-540,"elapsed":15485,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"32873788-1e03-4fc1-e171-8cb4f00afb26"},"source":["#import os\n","import gc\n","gc.enable()\n","import math\n","#mport json\n","import time\n","import random\n","import multiprocessing\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=UserWarning)\n","\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm, trange\n","from sklearn import model_selection\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn import Parameter\n","import torch.optim as optim\n","from torch.utils.data import (\n","    Dataset, DataLoader,\n","    SequentialSampler, RandomSampler\n",")\n","from torch.utils.data.distributed import DistributedSampler\n","\n","try:\n","    from apex import amp\n","    APEX_INSTALLED = True\n","except ImportError:\n","    APEX_INSTALLED = False\n","\n","import transformers\n","from transformers import (\n","    WEIGHTS_NAME,\n","    AdamW,\n","    AutoConfig,\n","    AutoModel,\n","    AutoTokenizer,\n","    get_cosine_schedule_with_warmup,\n","    get_linear_schedule_with_warmup,\n","    logging,\n","    MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n",")\n","logging.set_verbosity_warning()\n","logging.set_verbosity_error()\n","\n","def fix_all_seeds(seed):\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","def optimal_num_of_loader_workers():\n","    num_cpus = multiprocessing.cpu_count()\n","    num_gpus = torch.cuda.device_count()\n","    optimal_value = min(num_cpus, num_gpus*4) if num_gpus else num_cpus - 1\n","    return optimal_value\n","\n","print(f\"Apex AMP Installed :: {APEX_INSTALLED}\")\n","MODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())\n","MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Apex AMP Installed :: False\n"]}]},{"cell_type":"markdown","metadata":{"id":"y0OvcnMaP4h9"},"source":["### Training Configuration"]},{"cell_type":"code","metadata":{"id":"LUx5XplNET3y","execution":{"iopub.status.busy":"2021-08-17T20:24:13.391322Z","iopub.execute_input":"2021-08-17T20:24:13.391696Z","iopub.status.idle":"2021-08-17T20:24:13.398397Z","shell.execute_reply.started":"2021-08-17T20:24:13.391662Z","shell.execute_reply":"2021-08-17T20:24:13.39732Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1635050635252,"user_tz":-540,"elapsed":55,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["class Config:\n","    # model\n","    model_type = 'xlm_roberta'\n","    model_name_or_path = \"deepset/xlm-roberta-large-squad2\"\n","    config_name = \"deepset/xlm-roberta-large-squad2\"\n","    fp16 = True if APEX_INSTALLED else False\n","    fp16_opt_level = \"O1\"\n","    gradient_accumulation_steps = 2\n","\n","    # tokenizer\n","    tokenizer_name = \"deepset/xlm-roberta-large-squad2\"\n","    max_seq_length = 384 #512 #384\n","    doc_stride = 128 #80 #128\n","\n","    # train\n","    epochs = 1 #2 #7 #1\n","    train_batch_size = 4 #2 #4\n","    eval_batch_size = 8 #4 #8\n","\n","    # optimizer\n","    optimizer_type = 'AdamW'\n","    learning_rate = 1.5e-5\n","    weight_decay = 1e-2\n","    epsilon = 1e-8\n","    max_grad_norm = 1.0\n","\n","    # scheduler\n","    decay_name = 'linear-warmup'\n","    warmup_ratio = 0.1\n","\n","    # logging\n","    logging_steps = 10\n","\n","    # evaluate\n","    output_dir = dname #'output'\n","    seed = 2021"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HYno9o1OP4h-"},"source":["### Data Factory"]},{"cell_type":"code","metadata":{"id":"X_eRZQrzET3z","execution":{"iopub.status.busy":"2021-08-17T20:24:26.939105Z","iopub.execute_input":"2021-08-17T20:24:26.939437Z","iopub.status.idle":"2021-08-17T20:24:27.776039Z","shell.execute_reply.started":"2021-08-17T20:24:26.939409Z","shell.execute_reply":"2021-08-17T20:24:27.775204Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1635050638699,"user_tz":-540,"elapsed":3043,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["train = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/train.csv')\n","test = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/test.csv')\n","external_mlqa = pd.read_csv('../input/mlqa-hindi-processed/mlqa_hindi.csv')\n","external_xquad = pd.read_csv('../input/mlqa-hindi-processed/xquad.csv')\n","external_train = pd.concat([external_mlqa, external_xquad])\n","\n","def create_folds(data, num_splits):\n","    data[\"kfold\"] = -1\n","    kf = model_selection.StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=2021)\n","    for f, (t_, v_) in enumerate(kf.split(X=data, y=data['language'])):\n","        data.loc[v_, 'kfold'] = f\n","    return data\n","\n","#### RIOW\n","# train = create_folds(train, num_splits=5)\n","# external_train[\"kfold\"] = -1\n","# external_train['id'] = list(np.arange(1, len(external_train)+1))\n","# train = pd.concat([train, external_train]).reset_index(drop=True)\n","\n","external_train['id'] = list(np.arange(1, len(external_train)+1))\n","\n","# # Drop tamil\n","# train = train[train[\"language\"]!=\"tamil\"].reset_index(drop=True)\n","\n","train = pd.concat([train, external_train]).reset_index(drop=True)\n","train = create_folds(train, num_splits=5)\n","#### RIOWRIOW\n","\n","def convert_answers(row):\n","    return {'answer_start': [row[0]], 'text': [row[1]]}\n","\n","train['answers'] = train[['answer_start', 'answer_text']].apply(convert_answers, axis=1)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dAoAJUVaP4iA"},"source":["### Covert Examples to Features (Preprocess)"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-12T15:50:26.947214Z","iopub.execute_input":"2021-08-12T15:50:26.947589Z","iopub.status.idle":"2021-08-12T15:50:26.960916Z","shell.execute_reply.started":"2021-08-12T15:50:26.947551Z","shell.execute_reply":"2021-08-12T15:50:26.959064Z"},"id":"dxbZdct1ET3z","trusted":true,"executionInfo":{"status":"ok","timestamp":1635050638700,"user_tz":-540,"elapsed":43,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["def prepare_train_features(args, example, tokenizer):\n","    example[\"question\"] = example[\"question\"].lstrip()\n","    tokenized_example = tokenizer(\n","        example[\"question\"],\n","        example[\"context\"],\n","        truncation=\"only_second\",\n","        max_length=args.max_seq_length,\n","        stride=args.doc_stride,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True,\n","        padding=\"max_length\",\n","    )\n","\n","    sample_mapping = tokenized_example.pop(\"overflow_to_sample_mapping\")\n","    offset_mapping = tokenized_example.pop(\"offset_mapping\")\n","\n","    features = []\n","    for i, offsets in enumerate(offset_mapping):\n","        feature = {}\n","\n","        input_ids = tokenized_example[\"input_ids\"][i]\n","        attention_mask = tokenized_example[\"attention_mask\"][i]\n","\n","        feature['input_ids'] = input_ids\n","        feature['attention_mask'] = attention_mask\n","        feature['offset_mapping'] = offsets\n","\n","        cls_index = input_ids.index(tokenizer.cls_token_id)\n","        sequence_ids = tokenized_example.sequence_ids(i)\n","\n","        sample_index = sample_mapping[i]\n","        answers = example[\"answers\"]\n","\n","        if len(answers[\"answer_start\"]) == 0:\n","            feature[\"start_position\"] = cls_index\n","            feature[\"end_position\"] = cls_index\n","        else:\n","            start_char = answers[\"answer_start\"][0]\n","            end_char = start_char + len(answers[\"text\"][0])\n","\n","            token_start_index = 0\n","            while sequence_ids[token_start_index] != 1:\n","                token_start_index += 1\n","\n","            token_end_index = len(input_ids) - 1\n","            while sequence_ids[token_end_index] != 1:\n","                token_end_index -= 1\n","\n","            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n","                feature[\"start_position\"] = cls_index\n","                feature[\"end_position\"] = cls_index\n","            else:\n","                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n","                    token_start_index += 1\n","                feature[\"start_position\"] = token_start_index - 1\n","                while offsets[token_end_index][1] >= end_char:\n","                    token_end_index -= 1\n","                feature[\"end_position\"] = token_end_index + 1\n","\n","        features.append(feature)\n","    return features"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a3SIS_xAP4iC"},"source":["### Dataset Retriever"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-12T15:50:26.962738Z","iopub.execute_input":"2021-08-12T15:50:26.963118Z","iopub.status.idle":"2021-08-12T15:50:26.97542Z","shell.execute_reply.started":"2021-08-12T15:50:26.963075Z","shell.execute_reply":"2021-08-12T15:50:26.974431Z"},"id":"6TuzHdjmET30","trusted":true,"executionInfo":{"status":"ok","timestamp":1635050638703,"user_tz":-540,"elapsed":37,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["class DatasetRetriever(Dataset):\n","    def __init__(self, features, mode='train'):\n","        super(DatasetRetriever, self).__init__()\n","        self.features = features\n","        self.mode = mode\n","        \n","    def __len__(self):\n","        return len(self.features)\n","    \n","    def __getitem__(self, item):   \n","        feature = self.features[item]\n","        if self.mode == 'train':\n","            return {\n","                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n","                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n","                'offset_mapping':torch.tensor(feature['offset_mapping'], dtype=torch.long),\n","                'start_position':torch.tensor(feature['start_position'], dtype=torch.long),\n","                'end_position':torch.tensor(feature['end_position'], dtype=torch.long)\n","            }\n","        else:\n","            return {\n","                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n","                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n","                'offset_mapping':feature['offset_mapping'],\n","                'sequence_ids':feature['sequence_ids'],\n","                'id':feature['example_id'],\n","                'context': feature['context'],\n","                'question': feature['question']\n","            }"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tpH-2nPqP4iE"},"source":["### Model"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-12T15:50:26.976977Z","iopub.execute_input":"2021-08-12T15:50:26.977627Z","iopub.status.idle":"2021-08-12T15:50:26.990227Z","shell.execute_reply.started":"2021-08-12T15:50:26.97747Z","shell.execute_reply":"2021-08-12T15:50:26.989443Z"},"id":"9OxhKqxcET31","trusted":true,"executionInfo":{"status":"ok","timestamp":1635050638704,"user_tz":-540,"elapsed":33,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["class Model(nn.Module):\n","    def __init__(self, modelname_or_path, config):\n","        super(Model, self).__init__()\n","        self.config = config\n","        self.xlm_roberta = AutoModel.from_pretrained(modelname_or_path, config=config)\n","        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","        self._init_weights(self.qa_outputs)\n","        \n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","\n","    def forward(\n","        self, \n","        input_ids, \n","        attention_mask=None, \n","        # token_type_ids=None\n","    ):\n","        outputs = self.xlm_roberta(\n","            input_ids,\n","            attention_mask=attention_mask,\n","        )\n","\n","        sequence_output = outputs[0]\n","        pooled_output = outputs[1]\n","        \n","        # sequence_output = self.dropout(sequence_output)\n","        qa_logits = self.qa_outputs(sequence_output)\n","        \n","        start_logits, end_logits = qa_logits.split(1, dim=-1)\n","        start_logits = start_logits.squeeze(-1)\n","        end_logits = end_logits.squeeze(-1)\n","    \n","        return start_logits, end_logits"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xdMx6plqP4iG"},"source":["### Loss"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-12T15:50:26.991891Z","iopub.execute_input":"2021-08-12T15:50:26.99237Z","iopub.status.idle":"2021-08-12T15:50:27.000188Z","shell.execute_reply.started":"2021-08-12T15:50:26.992334Z","shell.execute_reply":"2021-08-12T15:50:26.999374Z"},"id":"SxuNrJqqET32","trusted":true,"executionInfo":{"status":"ok","timestamp":1635050638705,"user_tz":-540,"elapsed":30,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["def loss_fn(preds, labels):\n","    start_preds, end_preds = preds\n","    start_labels, end_labels = labels\n","    \n","    start_loss = nn.CrossEntropyLoss(ignore_index=-1)(start_preds, start_labels)\n","    end_loss = nn.CrossEntropyLoss(ignore_index=-1)(end_preds, end_labels)\n","    total_loss = (start_loss + end_loss) / 2\n","    return total_loss"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jUMtX08cP4iH"},"source":["### Grouped Layerwise Learning Rate Decay"]},{"cell_type":"code","metadata":{"id":"vf6HVcu2ET34","execution":{"iopub.status.busy":"2021-08-17T20:25:36.36033Z","iopub.execute_input":"2021-08-17T20:25:36.361058Z","iopub.status.idle":"2021-08-17T20:25:36.381524Z","shell.execute_reply.started":"2021-08-17T20:25:36.361009Z","shell.execute_reply":"2021-08-17T20:25:36.380328Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1635050638706,"user_tz":-540,"elapsed":28,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["def get_optimizer_grouped_parameters(args, model):\n","    no_decay = [\"bias\", \"LayerNorm.weight\"]\n","    group1=['layer.0.','layer.1.','layer.2.','layer.3.']\n","    group2=['layer.4.','layer.5.','layer.6.','layer.7.']    \n","    group3=['layer.8.','layer.9.','layer.10.','layer.11.']\n","    group_all=['layer.0.','layer.1.','layer.2.','layer.3.','layer.4.','layer.5.','layer.6.','layer.7.','layer.8.','layer.9.','layer.10.','layer.11.']\n","    optimizer_grouped_parameters = [\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': args.weight_decay},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': args.weight_decay, 'lr': args.learning_rate/2.6},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': args.weight_decay, 'lr': args.learning_rate},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': args.weight_decay, 'lr': args.learning_rate*2.6},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': 0.0},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': 0.0, 'lr': args.learning_rate/2.6},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': 0.0, 'lr': args.learning_rate},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': 0.0, 'lr': args.learning_rate*2.6},\n","        {'params': [p for n, p in model.named_parameters() if args.model_type not in n], 'lr':args.learning_rate*20, \"weight_decay\": 0.0},\n","    ]\n","    return optimizer_grouped_parameters"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oXz_0fIQP4iI"},"source":["### Metric Logger"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-12T15:50:27.021442Z","iopub.execute_input":"2021-08-12T15:50:27.021952Z","iopub.status.idle":"2021-08-12T15:50:27.03234Z","shell.execute_reply.started":"2021-08-12T15:50:27.021912Z","shell.execute_reply":"2021-08-12T15:50:27.03156Z"},"id":"bkFB-iMcET34","trusted":true,"executionInfo":{"status":"ok","timestamp":1635050638753,"user_tz":-540,"elapsed":71,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["class AverageMeter(object):\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","        self.max = 0\n","        self.min = 1e5\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","        if val > self.max:\n","            self.max = val\n","        if val < self.min:\n","            self.min = val"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y946gQxtP4iJ"},"source":["### Utilities"]},{"cell_type":"code","metadata":{"id":"spFRutV0ET34","execution":{"iopub.status.busy":"2021-08-17T20:26:30.014223Z","iopub.execute_input":"2021-08-17T20:26:30.014623Z","iopub.status.idle":"2021-08-17T20:26:30.030566Z","shell.execute_reply.started":"2021-08-17T20:26:30.01459Z","shell.execute_reply":"2021-08-17T20:26:30.029723Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1635050639314,"user_tz":-540,"elapsed":114,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["def make_model(args):\n","    config = AutoConfig.from_pretrained(args.config_name)\n","    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name)\n","    model = Model(args.model_name_or_path, config=config)\n","    return config, tokenizer, model\n","\n","def make_optimizer(args, model):\n","    # optimizer_grouped_parameters = get_optimizer_grouped_parameters(args, model)\n","    no_decay = [\"bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\n","            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","            \"weight_decay\": args.weight_decay,\n","        },\n","        {\n","            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n","            \"weight_decay\": 0.0,\n","        },\n","    ]\n","    if args.optimizer_type == \"AdamW\":\n","        optimizer = AdamW(\n","            optimizer_grouped_parameters,\n","            lr=args.learning_rate,\n","            eps=args.epsilon,\n","            correct_bias=True\n","        )\n","        return optimizer\n","\n","def make_scheduler(\n","    args, optimizer, \n","    num_warmup_steps, \n","    num_training_steps\n","):\n","    if args.decay_name == \"cosine-warmup\":\n","        scheduler = get_cosine_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=num_warmup_steps,\n","            num_training_steps=num_training_steps\n","        )\n","    else:\n","        scheduler = get_linear_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=num_warmup_steps,\n","            num_training_steps=num_training_steps\n","        )\n","    return scheduler    \n","\n","def make_loader(\n","    args, data, \n","    tokenizer, fold\n","):\n","    train_set, valid_set = data[data['kfold']!=fold], data[data['kfold']==fold]\n","    \n","    train_features, valid_features = [[] for _ in range(2)]\n","    for i, row in train_set.iterrows():\n","        train_features += prepare_train_features(args, row, tokenizer)\n","    for i, row in valid_set.iterrows():\n","        valid_features += prepare_train_features(args, row, tokenizer)\n","\n","    train_dataset = DatasetRetriever(train_features)\n","    valid_dataset = DatasetRetriever(valid_features)\n","    print(f\"Num examples Train= {len(train_dataset)}, Num examples Valid={len(valid_dataset)}\")\n","    \n","    train_sampler = RandomSampler(train_dataset)\n","    valid_sampler = SequentialSampler(valid_dataset)\n","\n","    train_dataloader = DataLoader(\n","        train_dataset,\n","        batch_size=args.train_batch_size,\n","        sampler=train_sampler,\n","        num_workers=optimal_num_of_loader_workers(),\n","        pin_memory=True,\n","        drop_last=False \n","    )\n","\n","    valid_dataloader = DataLoader(\n","        valid_dataset,\n","        batch_size=args.eval_batch_size, \n","        sampler=valid_sampler,\n","        num_workers=optimal_num_of_loader_workers(),\n","        pin_memory=True, \n","        drop_last=False\n","    )\n","\n","    return train_dataloader, valid_dataloader"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i-zfYJGxP4iK"},"source":["### Trainer"]},{"cell_type":"code","metadata":{"id":"iFLvh1VQET35","execution":{"iopub.status.busy":"2021-08-17T20:26:36.310455Z","iopub.execute_input":"2021-08-17T20:26:36.310827Z","iopub.status.idle":"2021-08-17T20:26:36.325575Z","shell.execute_reply.started":"2021-08-17T20:26:36.310796Z","shell.execute_reply":"2021-08-17T20:26:36.324417Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1635050639316,"user_tz":-540,"elapsed":106,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["class Trainer:\n","    def __init__(\n","        self, model, tokenizer, \n","        optimizer, scheduler\n","    ):\n","        self.model = model\n","        self.tokenizer = tokenizer\n","        self.optimizer = optimizer\n","        self.scheduler = scheduler\n","\n","    def train(\n","        self, args, \n","        train_dataloader, \n","        epoch, result_dict\n","    ):\n","        count = 0\n","        losses = AverageMeter()\n","        \n","        self.model.zero_grad()\n","        self.model.train()\n","        \n","        fix_all_seeds(args.seed)\n","        \n","        for batch_idx, batch_data in enumerate(train_dataloader):\n","            input_ids, attention_mask, targets_start, targets_end = \\\n","                batch_data['input_ids'], batch_data['attention_mask'], \\\n","                    batch_data['start_position'], batch_data['end_position']\n","            \n","            input_ids, attention_mask, targets_start, targets_end = \\\n","                input_ids.cuda(), attention_mask.cuda(), targets_start.cuda(), targets_end.cuda()\n","\n","            outputs_start, outputs_end = self.model(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","            )\n","            \n","            loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n","            loss = loss / args.gradient_accumulation_steps\n","\n","            if args.fp16:\n","                with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n","                    scaled_loss.backward()\n","            else:\n","                loss.backward()\n","\n","            count += input_ids.size(0)\n","            losses.update(loss.item(), input_ids.size(0))\n","\n","            # if args.fp16:\n","            #     torch.nn.utils.clip_grad_norm_(amp.master_params(self.optimizer), args.max_grad_norm)\n","            # else:\n","            #     torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.max_grad_norm)\n","\n","            if batch_idx % args.gradient_accumulation_steps == 0 or batch_idx == len(train_dataloader) - 1:\n","                self.optimizer.step()\n","                self.scheduler.step()\n","                self.optimizer.zero_grad()\n","\n","            if (batch_idx % args.logging_steps == 0) or (batch_idx+1)==len(train_dataloader):\n","                _s = str(len(str(len(train_dataloader.sampler))))\n","                ret = [\n","                    ('Epoch: {:0>2} [{: >' + _s + '}/{} ({: >3.0f}%)]').format(epoch, count, len(train_dataloader.sampler), 100 * count / len(train_dataloader.sampler)),\n","                    'Train Loss: {: >4.5f}'.format(losses.avg),\n","                ]\n","                print(', '.join(ret))\n","\n","        result_dict['train_loss'].append(losses.avg)\n","        return result_dict"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7pmOF0dxP4iL"},"source":["### Evaluator"]},{"cell_type":"code","metadata":{"id":"1a8kG2UYET36","execution":{"iopub.status.busy":"2021-08-17T20:26:39.517516Z","iopub.execute_input":"2021-08-17T20:26:39.517886Z","iopub.status.idle":"2021-08-17T20:26:39.528591Z","shell.execute_reply.started":"2021-08-17T20:26:39.517856Z","shell.execute_reply":"2021-08-17T20:26:39.527569Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1635050639317,"user_tz":-540,"elapsed":105,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["class Evaluator:\n","    def __init__(self, model):\n","        self.model = model\n","    \n","    def save(self, result, output_dir):\n","        with open(f'{output_dir}/result_dict.json', 'w') as f:\n","            f.write(json.dumps(result, sort_keys=True, indent=4, ensure_ascii=False))\n","\n","    def evaluate(self, valid_dataloader, epoch, result_dict):\n","        losses = AverageMeter()\n","        for batch_idx, batch_data in enumerate(valid_dataloader):\n","            self.model = self.model.eval()\n","            input_ids, attention_mask, targets_start, targets_end = \\\n","                batch_data['input_ids'], batch_data['attention_mask'], \\\n","                    batch_data['start_position'], batch_data['end_position']\n","            \n","            input_ids, attention_mask, targets_start, targets_end = \\\n","                input_ids.cuda(), attention_mask.cuda(), targets_start.cuda(), targets_end.cuda()\n","            \n","            with torch.no_grad():            \n","                outputs_start, outputs_end = self.model(\n","                    input_ids=input_ids,\n","                    attention_mask=attention_mask,\n","                )\n","                \n","                loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n","                losses.update(loss.item(), input_ids.size(0))\n","                \n","        print('----Validation Results Summary----')\n","        print('Epoch: [{}] Valid Loss: {: >4.5f}'.format(epoch, losses.avg))\n","        result_dict['val_loss'].append(losses.avg)        \n","        return result_dict"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cAZPoRJeP4iM"},"source":["### Initialize Training"]},{"cell_type":"code","metadata":{"id":"v-gUDyq2ET37","execution":{"iopub.status.busy":"2021-08-17T20:26:44.41837Z","iopub.execute_input":"2021-08-17T20:26:44.418722Z","iopub.status.idle":"2021-08-17T20:26:44.428918Z","shell.execute_reply.started":"2021-08-17T20:26:44.418691Z","shell.execute_reply":"2021-08-17T20:26:44.428086Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1635050639318,"user_tz":-540,"elapsed":102,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["def init_training(args, data, fold):\n","    fix_all_seeds(args.seed)\n","    \n","    if not os.path.exists(args.output_dir):\n","        os.makedirs(args.output_dir)\n","    \n","    # model\n","    model_config, tokenizer, model = make_model(args)\n","    if torch.cuda.device_count() >= 1:\n","        print('Model pushed to {} GPU(s), type {}.'.format(\n","            torch.cuda.device_count(), \n","            torch.cuda.get_device_name(0))\n","        )\n","        model = model.cuda() \n","    else:\n","        raise ValueError('CPU training is not supported')\n","    \n","    # data loaders\n","    train_dataloader, valid_dataloader = make_loader(args, data, tokenizer, fold)\n","\n","    # optimizer\n","    optimizer = make_optimizer(args, model)\n","\n","    # scheduler\n","    num_training_steps = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps) * args.epochs\n","    if args.warmup_ratio > 0:\n","        num_warmup_steps = int(args.warmup_ratio * num_training_steps)\n","    else:\n","        num_warmup_steps = 0\n","    print(f\"Total Training Steps: {num_training_steps}, Total Warmup Steps: {num_warmup_steps}\")\n","    scheduler = make_scheduler(args, optimizer, num_warmup_steps, num_training_steps)\n","\n","    # mixed precision training with NVIDIA Apex\n","    if args.fp16:\n","        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n","    \n","    result_dict = {\n","        'epoch':[], \n","        'train_loss': [], \n","        'val_loss' : [], \n","        'best_val_loss': np.inf\n","    }\n","\n","    return (\n","        model, model_config, tokenizer, optimizer, scheduler, \n","        train_dataloader, valid_dataloader, result_dict\n","    )"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g6ZBX-5aP4iN"},"source":["### Run"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-12T15:50:27.097353Z","iopub.execute_input":"2021-08-12T15:50:27.097724Z","iopub.status.idle":"2021-08-12T15:50:27.112113Z","shell.execute_reply.started":"2021-08-12T15:50:27.097688Z","shell.execute_reply":"2021-08-12T15:50:27.111224Z"},"id":"39ei5Bm5ET37","trusted":true,"executionInfo":{"status":"ok","timestamp":1635050639322,"user_tz":-540,"elapsed":103,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["def run(data, fold):\n","    args = Config()\n","    model, model_config, tokenizer, optimizer, scheduler, train_dataloader, \\\n","        valid_dataloader, result_dict = init_training(args, data, fold)\n","    \n","    trainer = Trainer(model, tokenizer, optimizer, scheduler)\n","    evaluator = Evaluator(model)\n","\n","    train_time_list = []\n","    valid_time_list = []\n","\n","    for epoch in range(args.epochs):\n","        result_dict['epoch'].append(epoch)\n","\n","        # Train\n","        torch.cuda.synchronize()\n","        tic1 = time.time()\n","        result_dict = trainer.train(\n","            args, train_dataloader, \n","            epoch, result_dict\n","        )\n","        torch.cuda.synchronize()\n","        tic2 = time.time() \n","        train_time_list.append(tic2 - tic1)\n","        \n","        # Evaluate\n","        torch.cuda.synchronize()\n","        tic3 = time.time()\n","        result_dict = evaluator.evaluate(\n","            valid_dataloader, epoch, result_dict\n","        )\n","        torch.cuda.synchronize()\n","        tic4 = time.time() \n","        valid_time_list.append(tic4 - tic3)\n","            \n","        output_dir = os.path.join(args.output_dir, f\"checkpoint-fold-{fold}\")\n","        if result_dict['val_loss'][-1] < result_dict['best_val_loss']:\n","            print(\"{} Epoch, Best epoch was updated! Valid Loss: {: >4.5f}\".format(epoch, result_dict['val_loss'][-1]))\n","            result_dict[\"best_val_loss\"] = result_dict['val_loss'][-1]        \n","            \n","            os.makedirs(output_dir, exist_ok=True)\n","            torch.save(model.state_dict(), f\"{output_dir}/pytorch_model.bin\")\n","            model_config.save_pretrained(output_dir)\n","            tokenizer.save_pretrained(output_dir)\n","            print(f\"Saving model checkpoint to {output_dir}.\")\n","            \n","        print()\n","\n","    evaluator.save(result_dict, output_dir)\n","    \n","    print(f\"Total Training Time: {np.sum(train_time_list)}secs, Average Training Time per Epoch: {np.mean(train_time_list)}secs.\")\n","    print(f\"Total Validation Time: {np.sum(valid_time_list)}secs, Average Validation Time per Epoch: {np.mean(valid_time_list)}secs.\")\n","    \n","    torch.cuda.empty_cache()\n","    del trainer, evaluator\n","    del model, model_config, tokenizer\n","    del optimizer, scheduler\n","    del train_dataloader, valid_dataloader, result_dict\n","    gc.collect()"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"mPaGnnCnbhWl","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["4598218598064e8bb4096396b889b75c","c218725a8be0401a93014ed27ed3d652","b8f631b168714081a3c4fa29d784d4a3","23808bc0ad63443092f5afcebd963768","1166003d02a8405891370efd12238d21","fdbf353ad44348109f1a58602c70dad3","937b9536275548d59ffe6547b70a741f","f03338f54b16411383e7765b5bb2613b","2023686459074c0eafac81159e81d9c0","5616e0935dd442248f643fb121f5f3eb","6ef9507965ce4091bc8877080ab12309","4683532a86b04805a2a5242a58f00ec7","02bc64bb20a949c69f61d16ebb4485e5","6b2406c87dbe41698e83599422aecb19","f561347775e44ecbbb88b5b9fcf6fdf5","ddc8478c0dbe44ddbb45e798e0035715","197520097a45483387434ad52aeda82d","0f8312b700a747f984e29f3ab087aaeb","7fcb89fe7d76429b9ce9480e4c6b0dc8","a5bf850ed5294d7fa5599ed74223eda5","a0efcabd09584a788d7881a811142466","c369e85f86344ecb8fb2033caaa9bb27","a66c6836263d4c21877e2ea98ab9f072","10e84d144c174ee5bdc3662b00b5c80d","383586bcd6a8422a99a407f4c7ae4c5b","f0984b11c507479a8786ad01065134b3","66efb023564d44bd980bc4afdd159aad","8d71f2b74e9b48cca35ba04042070fb7","5645e917734f4d3696f0c9a386627e17","14ad85623a3f4d0d87e93d0044114220","46d38670110c43499dab2991bbe3c839","7885162769b8418da46480fe656bd67f","d29e1c00d7c84bf0a212a62576d2b263","817e72053f424abe9de89e853bc9c5a9","3271b014e2af458da814356a27831e68","5171b7831c1c4ae5b29a4655293a0872","1abe42b782de473d99d4a092b762885f","c1beb75215d0498987da95b27e87d7bf","b0d4176f59ef4b50992ac9f85bb970de","80c0169c63924df29bd8962d7ebe2591","247ebc661dad433d923b10a2fcdd7324","1a4c9a962714485fba22e6704dcd5c95","66bb6e0583e443e3aa80608798eee32e","1c4594e41ed4453fb5807a50e1e3e369","097cb4330ad74b9d9abb28f39308f09e","7bd0dbd63aa74fd295cc1995063c89ab","a004c02f56e8406686f6597c630e93aa","a7e4f0a02f364a48861dcff855078a7f","a5e327ca2c924b81acfa814087dc530a","acae6a67afca466c94e2e5466eb1314a","91fd27bfb48c473988e27cb3daad4119","955d658ca92f49cb98767ddb8e751cd9","9b17b6ce7fcc40a99eed8d1bfefbaab6","64399f9508794b239db4ed7001e10c76","6d37eb5f725d464ba344d26a8509b35d"]},"executionInfo":{"status":"ok","timestamp":1635060144351,"user_tz":-540,"elapsed":1902766,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"4b05551f-50cd-4e97-98fa-94c7b83db2dc"},"source":["for fold in range(3,6):\n","    print();print()\n","    print('-'*50)\n","    print(f'FOLD: {fold}')\n","    print('-'*50)\n","    run(train, fold)"],"execution_count":19,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","\n","--------------------------------------------------\n","FOLD: 3\n","--------------------------------------------------\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4598218598064e8bb4096396b889b75c","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/606 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4683532a86b04805a2a5242a58f00ec7","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/179 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a66c6836263d4c21877e2ea98ab9f072","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/4.83M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"817e72053f424abe9de89e853bc9c5a9","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/150 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"097cb4330ad74b9d9abb28f39308f09e","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/2.09G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"output_type":"stream","name":"stdout","text":["Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n","Num examples Train= 18765, Num examples Valid=4304\n","Total Training Steps: 2346, Total Warmup Steps: 234\n","Epoch: 00 [    4/18765 (  0%)], Train Loss: 2.83550\n","Epoch: 00 [   44/18765 (  0%)], Train Loss: 2.92596\n","Epoch: 00 [   84/18765 (  0%)], Train Loss: 2.88841\n","Epoch: 00 [  124/18765 (  1%)], Train Loss: 2.83932\n","Epoch: 00 [  164/18765 (  1%)], Train Loss: 2.77994\n","Epoch: 00 [  204/18765 (  1%)], Train Loss: 2.71142\n","Epoch: 00 [  244/18765 (  1%)], Train Loss: 2.63200\n","Epoch: 00 [  284/18765 (  2%)], Train Loss: 2.54595\n","Epoch: 00 [  324/18765 (  2%)], Train Loss: 2.43741\n","Epoch: 00 [  364/18765 (  2%)], Train Loss: 2.33767\n","Epoch: 00 [  404/18765 (  2%)], Train Loss: 2.23204\n","Epoch: 00 [  444/18765 (  2%)], Train Loss: 2.10473\n","Epoch: 00 [  484/18765 (  3%)], Train Loss: 1.97282\n","Epoch: 00 [  524/18765 (  3%)], Train Loss: 1.88409\n","Epoch: 00 [  564/18765 (  3%)], Train Loss: 1.79097\n","Epoch: 00 [  604/18765 (  3%)], Train Loss: 1.70640\n","Epoch: 00 [  644/18765 (  3%)], Train Loss: 1.62915\n","Epoch: 00 [  684/18765 (  4%)], Train Loss: 1.57151\n","Epoch: 00 [  724/18765 (  4%)], Train Loss: 1.50584\n","Epoch: 00 [  764/18765 (  4%)], Train Loss: 1.44642\n","Epoch: 00 [  804/18765 (  4%)], Train Loss: 1.38910\n","Epoch: 00 [  844/18765 (  4%)], Train Loss: 1.34622\n","Epoch: 00 [  884/18765 (  5%)], Train Loss: 1.30582\n","Epoch: 00 [  924/18765 (  5%)], Train Loss: 1.26315\n","Epoch: 00 [  964/18765 (  5%)], Train Loss: 1.22938\n","Epoch: 00 [ 1004/18765 (  5%)], Train Loss: 1.18790\n","Epoch: 00 [ 1044/18765 (  6%)], Train Loss: 1.16610\n","Epoch: 00 [ 1084/18765 (  6%)], Train Loss: 1.13105\n","Epoch: 00 [ 1124/18765 (  6%)], Train Loss: 1.10795\n","Epoch: 00 [ 1164/18765 (  6%)], Train Loss: 1.08452\n","Epoch: 00 [ 1204/18765 (  6%)], Train Loss: 1.06333\n","Epoch: 00 [ 1244/18765 (  7%)], Train Loss: 1.04228\n","Epoch: 00 [ 1284/18765 (  7%)], Train Loss: 1.03298\n","Epoch: 00 [ 1324/18765 (  7%)], Train Loss: 1.01009\n","Epoch: 00 [ 1364/18765 (  7%)], Train Loss: 0.98998\n","Epoch: 00 [ 1404/18765 (  7%)], Train Loss: 0.96647\n","Epoch: 00 [ 1444/18765 (  8%)], Train Loss: 0.95119\n","Epoch: 00 [ 1484/18765 (  8%)], Train Loss: 0.93403\n","Epoch: 00 [ 1524/18765 (  8%)], Train Loss: 0.92049\n","Epoch: 00 [ 1564/18765 (  8%)], Train Loss: 0.90720\n","Epoch: 00 [ 1604/18765 (  9%)], Train Loss: 0.89861\n","Epoch: 00 [ 1644/18765 (  9%)], Train Loss: 0.88476\n","Epoch: 00 [ 1684/18765 (  9%)], Train Loss: 0.87244\n","Epoch: 00 [ 1724/18765 (  9%)], Train Loss: 0.86275\n","Epoch: 00 [ 1764/18765 (  9%)], Train Loss: 0.85258\n","Epoch: 00 [ 1804/18765 ( 10%)], Train Loss: 0.84263\n","Epoch: 00 [ 1844/18765 ( 10%)], Train Loss: 0.83539\n","Epoch: 00 [ 1884/18765 ( 10%)], Train Loss: 0.82742\n","Epoch: 00 [ 1924/18765 ( 10%)], Train Loss: 0.81781\n","Epoch: 00 [ 1964/18765 ( 10%)], Train Loss: 0.80635\n","Epoch: 00 [ 2004/18765 ( 11%)], Train Loss: 0.79851\n","Epoch: 00 [ 2044/18765 ( 11%)], Train Loss: 0.78893\n","Epoch: 00 [ 2084/18765 ( 11%)], Train Loss: 0.77961\n","Epoch: 00 [ 2124/18765 ( 11%)], Train Loss: 0.77760\n","Epoch: 00 [ 2164/18765 ( 12%)], Train Loss: 0.77135\n","Epoch: 00 [ 2204/18765 ( 12%)], Train Loss: 0.76419\n","Epoch: 00 [ 2244/18765 ( 12%)], Train Loss: 0.75696\n","Epoch: 00 [ 2284/18765 ( 12%)], Train Loss: 0.75121\n","Epoch: 00 [ 2324/18765 ( 12%)], Train Loss: 0.74474\n","Epoch: 00 [ 2364/18765 ( 13%)], Train Loss: 0.73838\n","Epoch: 00 [ 2404/18765 ( 13%)], Train Loss: 0.73239\n","Epoch: 00 [ 2444/18765 ( 13%)], Train Loss: 0.72863\n","Epoch: 00 [ 2484/18765 ( 13%)], Train Loss: 0.72390\n","Epoch: 00 [ 2524/18765 ( 13%)], Train Loss: 0.71568\n","Epoch: 00 [ 2564/18765 ( 14%)], Train Loss: 0.70920\n","Epoch: 00 [ 2604/18765 ( 14%)], Train Loss: 0.70131\n","Epoch: 00 [ 2644/18765 ( 14%)], Train Loss: 0.69665\n","Epoch: 00 [ 2684/18765 ( 14%)], Train Loss: 0.69056\n","Epoch: 00 [ 2724/18765 ( 15%)], Train Loss: 0.68386\n","Epoch: 00 [ 2764/18765 ( 15%)], Train Loss: 0.67966\n","Epoch: 00 [ 2804/18765 ( 15%)], Train Loss: 0.67435\n","Epoch: 00 [ 2844/18765 ( 15%)], Train Loss: 0.66874\n","Epoch: 00 [ 2884/18765 ( 15%)], Train Loss: 0.66421\n","Epoch: 00 [ 2924/18765 ( 16%)], Train Loss: 0.66270\n","Epoch: 00 [ 2964/18765 ( 16%)], Train Loss: 0.66019\n","Epoch: 00 [ 3004/18765 ( 16%)], Train Loss: 0.65886\n","Epoch: 00 [ 3044/18765 ( 16%)], Train Loss: 0.65670\n","Epoch: 00 [ 3084/18765 ( 16%)], Train Loss: 0.65309\n","Epoch: 00 [ 3124/18765 ( 17%)], Train Loss: 0.64993\n","Epoch: 00 [ 3164/18765 ( 17%)], Train Loss: 0.64773\n","Epoch: 00 [ 3204/18765 ( 17%)], Train Loss: 0.64327\n","Epoch: 00 [ 3244/18765 ( 17%)], Train Loss: 0.63932\n","Epoch: 00 [ 3284/18765 ( 18%)], Train Loss: 0.63651\n","Epoch: 00 [ 3324/18765 ( 18%)], Train Loss: 0.63184\n","Epoch: 00 [ 3364/18765 ( 18%)], Train Loss: 0.62691\n","Epoch: 00 [ 3404/18765 ( 18%)], Train Loss: 0.62408\n","Epoch: 00 [ 3444/18765 ( 18%)], Train Loss: 0.61991\n","Epoch: 00 [ 3484/18765 ( 19%)], Train Loss: 0.61617\n","Epoch: 00 [ 3524/18765 ( 19%)], Train Loss: 0.61228\n","Epoch: 00 [ 3564/18765 ( 19%)], Train Loss: 0.61084\n","Epoch: 00 [ 3604/18765 ( 19%)], Train Loss: 0.60961\n","Epoch: 00 [ 3644/18765 ( 19%)], Train Loss: 0.60564\n","Epoch: 00 [ 3684/18765 ( 20%)], Train Loss: 0.60369\n","Epoch: 00 [ 3724/18765 ( 20%)], Train Loss: 0.60025\n","Epoch: 00 [ 3764/18765 ( 20%)], Train Loss: 0.59931\n","Epoch: 00 [ 3804/18765 ( 20%)], Train Loss: 0.59653\n","Epoch: 00 [ 3844/18765 ( 20%)], Train Loss: 0.59341\n","Epoch: 00 [ 3884/18765 ( 21%)], Train Loss: 0.59161\n","Epoch: 00 [ 3924/18765 ( 21%)], Train Loss: 0.58820\n","Epoch: 00 [ 3964/18765 ( 21%)], Train Loss: 0.58637\n","Epoch: 00 [ 4004/18765 ( 21%)], Train Loss: 0.58463\n","Epoch: 00 [ 4044/18765 ( 22%)], Train Loss: 0.58101\n","Epoch: 00 [ 4084/18765 ( 22%)], Train Loss: 0.57780\n","Epoch: 00 [ 4124/18765 ( 22%)], Train Loss: 0.57426\n","Epoch: 00 [ 4164/18765 ( 22%)], Train Loss: 0.57170\n","Epoch: 00 [ 4204/18765 ( 22%)], Train Loss: 0.56979\n","Epoch: 00 [ 4244/18765 ( 23%)], Train Loss: 0.56802\n","Epoch: 00 [ 4284/18765 ( 23%)], Train Loss: 0.56597\n","Epoch: 00 [ 4324/18765 ( 23%)], Train Loss: 0.56324\n","Epoch: 00 [ 4364/18765 ( 23%)], Train Loss: 0.56012\n","Epoch: 00 [ 4404/18765 ( 23%)], Train Loss: 0.55844\n","Epoch: 00 [ 4444/18765 ( 24%)], Train Loss: 0.55650\n","Epoch: 00 [ 4484/18765 ( 24%)], Train Loss: 0.55629\n","Epoch: 00 [ 4524/18765 ( 24%)], Train Loss: 0.55374\n","Epoch: 00 [ 4564/18765 ( 24%)], Train Loss: 0.55166\n","Epoch: 00 [ 4604/18765 ( 25%)], Train Loss: 0.54937\n","Epoch: 00 [ 4644/18765 ( 25%)], Train Loss: 0.54673\n","Epoch: 00 [ 4684/18765 ( 25%)], Train Loss: 0.54750\n","Epoch: 00 [ 4724/18765 ( 25%)], Train Loss: 0.54697\n","Epoch: 00 [ 4764/18765 ( 25%)], Train Loss: 0.54461\n","Epoch: 00 [ 4804/18765 ( 26%)], Train Loss: 0.54482\n","Epoch: 00 [ 4844/18765 ( 26%)], Train Loss: 0.54353\n","Epoch: 00 [ 4884/18765 ( 26%)], Train Loss: 0.54216\n","Epoch: 00 [ 4924/18765 ( 26%)], Train Loss: 0.54031\n","Epoch: 00 [ 4964/18765 ( 26%)], Train Loss: 0.54142\n","Epoch: 00 [ 5004/18765 ( 27%)], Train Loss: 0.54049\n","Epoch: 00 [ 5044/18765 ( 27%)], Train Loss: 0.53851\n","Epoch: 00 [ 5084/18765 ( 27%)], Train Loss: 0.53737\n","Epoch: 00 [ 5124/18765 ( 27%)], Train Loss: 0.53543\n","Epoch: 00 [ 5164/18765 ( 28%)], Train Loss: 0.53406\n","Epoch: 00 [ 5204/18765 ( 28%)], Train Loss: 0.53173\n","Epoch: 00 [ 5244/18765 ( 28%)], Train Loss: 0.53086\n","Epoch: 00 [ 5284/18765 ( 28%)], Train Loss: 0.53075\n","Epoch: 00 [ 5324/18765 ( 28%)], Train Loss: 0.52973\n","Epoch: 00 [ 5364/18765 ( 29%)], Train Loss: 0.52884\n","Epoch: 00 [ 5404/18765 ( 29%)], Train Loss: 0.52854\n","Epoch: 00 [ 5444/18765 ( 29%)], Train Loss: 0.52729\n","Epoch: 00 [ 5484/18765 ( 29%)], Train Loss: 0.52575\n","Epoch: 00 [ 5524/18765 ( 29%)], Train Loss: 0.52414\n","Epoch: 00 [ 5564/18765 ( 30%)], Train Loss: 0.52422\n","Epoch: 00 [ 5604/18765 ( 30%)], Train Loss: 0.52228\n","Epoch: 00 [ 5644/18765 ( 30%)], Train Loss: 0.52018\n","Epoch: 00 [ 5684/18765 ( 30%)], Train Loss: 0.51902\n","Epoch: 00 [ 5724/18765 ( 31%)], Train Loss: 0.51826\n","Epoch: 00 [ 5764/18765 ( 31%)], Train Loss: 0.51781\n","Epoch: 00 [ 5804/18765 ( 31%)], Train Loss: 0.51597\n","Epoch: 00 [ 5844/18765 ( 31%)], Train Loss: 0.51440\n","Epoch: 00 [ 5884/18765 ( 31%)], Train Loss: 0.51199\n","Epoch: 00 [ 5924/18765 ( 32%)], Train Loss: 0.51055\n","Epoch: 00 [ 5964/18765 ( 32%)], Train Loss: 0.50934\n","Epoch: 00 [ 6004/18765 ( 32%)], Train Loss: 0.50705\n","Epoch: 00 [ 6044/18765 ( 32%)], Train Loss: 0.50715\n","Epoch: 00 [ 6084/18765 ( 32%)], Train Loss: 0.50540\n","Epoch: 00 [ 6124/18765 ( 33%)], Train Loss: 0.50375\n","Epoch: 00 [ 6164/18765 ( 33%)], Train Loss: 0.50320\n","Epoch: 00 [ 6204/18765 ( 33%)], Train Loss: 0.50171\n","Epoch: 00 [ 6244/18765 ( 33%)], Train Loss: 0.50081\n","Epoch: 00 [ 6284/18765 ( 33%)], Train Loss: 0.50086\n","Epoch: 00 [ 6324/18765 ( 34%)], Train Loss: 0.49995\n","Epoch: 00 [ 6364/18765 ( 34%)], Train Loss: 0.49934\n","Epoch: 00 [ 6404/18765 ( 34%)], Train Loss: 0.49866\n","Epoch: 00 [ 6444/18765 ( 34%)], Train Loss: 0.49706\n","Epoch: 00 [ 6484/18765 ( 35%)], Train Loss: 0.49593\n","Epoch: 00 [ 6524/18765 ( 35%)], Train Loss: 0.49451\n","Epoch: 00 [ 6564/18765 ( 35%)], Train Loss: 0.49374\n","Epoch: 00 [ 6604/18765 ( 35%)], Train Loss: 0.49292\n","Epoch: 00 [ 6644/18765 ( 35%)], Train Loss: 0.49137\n","Epoch: 00 [ 6684/18765 ( 36%)], Train Loss: 0.49014\n","Epoch: 00 [ 6724/18765 ( 36%)], Train Loss: 0.49037\n","Epoch: 00 [ 6764/18765 ( 36%)], Train Loss: 0.49027\n","Epoch: 00 [ 6804/18765 ( 36%)], Train Loss: 0.48993\n","Epoch: 00 [ 6844/18765 ( 36%)], Train Loss: 0.48968\n","Epoch: 00 [ 6884/18765 ( 37%)], Train Loss: 0.48853\n","Epoch: 00 [ 6924/18765 ( 37%)], Train Loss: 0.48748\n","Epoch: 00 [ 6964/18765 ( 37%)], Train Loss: 0.48794\n","Epoch: 00 [ 7004/18765 ( 37%)], Train Loss: 0.48710\n","Epoch: 00 [ 7044/18765 ( 38%)], Train Loss: 0.48642\n","Epoch: 00 [ 7084/18765 ( 38%)], Train Loss: 0.48533\n","Epoch: 00 [ 7124/18765 ( 38%)], Train Loss: 0.48362\n","Epoch: 00 [ 7164/18765 ( 38%)], Train Loss: 0.48206\n","Epoch: 00 [ 7204/18765 ( 38%)], Train Loss: 0.48035\n","Epoch: 00 [ 7244/18765 ( 39%)], Train Loss: 0.48037\n","Epoch: 00 [ 7284/18765 ( 39%)], Train Loss: 0.47895\n","Epoch: 00 [ 7324/18765 ( 39%)], Train Loss: 0.47786\n","Epoch: 00 [ 7364/18765 ( 39%)], Train Loss: 0.47793\n","Epoch: 00 [ 7404/18765 ( 39%)], Train Loss: 0.47755\n","Epoch: 00 [ 7444/18765 ( 40%)], Train Loss: 0.47656\n","Epoch: 00 [ 7484/18765 ( 40%)], Train Loss: 0.47582\n","Epoch: 00 [ 7524/18765 ( 40%)], Train Loss: 0.47578\n","Epoch: 00 [ 7564/18765 ( 40%)], Train Loss: 0.47415\n","Epoch: 00 [ 7604/18765 ( 41%)], Train Loss: 0.47357\n","Epoch: 00 [ 7644/18765 ( 41%)], Train Loss: 0.47210\n","Epoch: 00 [ 7684/18765 ( 41%)], Train Loss: 0.47095\n","Epoch: 00 [ 7724/18765 ( 41%)], Train Loss: 0.47045\n","Epoch: 00 [ 7764/18765 ( 41%)], Train Loss: 0.47077\n","Epoch: 00 [ 7804/18765 ( 42%)], Train Loss: 0.47133\n","Epoch: 00 [ 7844/18765 ( 42%)], Train Loss: 0.47038\n","Epoch: 00 [ 7884/18765 ( 42%)], Train Loss: 0.46977\n","Epoch: 00 [ 7924/18765 ( 42%)], Train Loss: 0.46918\n","Epoch: 00 [ 7964/18765 ( 42%)], Train Loss: 0.46841\n","Epoch: 00 [ 8004/18765 ( 43%)], Train Loss: 0.46773\n","Epoch: 00 [ 8044/18765 ( 43%)], Train Loss: 0.46740\n","Epoch: 00 [ 8084/18765 ( 43%)], Train Loss: 0.46684\n","Epoch: 00 [ 8124/18765 ( 43%)], Train Loss: 0.46647\n","Epoch: 00 [ 8164/18765 ( 44%)], Train Loss: 0.46580\n","Epoch: 00 [ 8204/18765 ( 44%)], Train Loss: 0.46630\n","Epoch: 00 [ 8244/18765 ( 44%)], Train Loss: 0.46525\n","Epoch: 00 [ 8284/18765 ( 44%)], Train Loss: 0.46414\n","Epoch: 00 [ 8324/18765 ( 44%)], Train Loss: 0.46449\n","Epoch: 00 [ 8364/18765 ( 45%)], Train Loss: 0.46355\n","Epoch: 00 [ 8404/18765 ( 45%)], Train Loss: 0.46283\n","Epoch: 00 [ 8444/18765 ( 45%)], Train Loss: 0.46277\n","Epoch: 00 [ 8484/18765 ( 45%)], Train Loss: 0.46251\n","Epoch: 00 [ 8524/18765 ( 45%)], Train Loss: 0.46169\n","Epoch: 00 [ 8564/18765 ( 46%)], Train Loss: 0.46036\n","Epoch: 00 [ 8604/18765 ( 46%)], Train Loss: 0.45939\n","Epoch: 00 [ 8644/18765 ( 46%)], Train Loss: 0.45903\n","Epoch: 00 [ 8684/18765 ( 46%)], Train Loss: 0.45792\n","Epoch: 00 [ 8724/18765 ( 46%)], Train Loss: 0.45732\n","Epoch: 00 [ 8764/18765 ( 47%)], Train Loss: 0.45706\n","Epoch: 00 [ 8804/18765 ( 47%)], Train Loss: 0.45624\n","Epoch: 00 [ 8844/18765 ( 47%)], Train Loss: 0.45677\n","Epoch: 00 [ 8884/18765 ( 47%)], Train Loss: 0.45657\n","Epoch: 00 [ 8924/18765 ( 48%)], Train Loss: 0.45576\n","Epoch: 00 [ 8964/18765 ( 48%)], Train Loss: 0.45643\n","Epoch: 00 [ 9004/18765 ( 48%)], Train Loss: 0.45669\n","Epoch: 00 [ 9044/18765 ( 48%)], Train Loss: 0.45586\n","Epoch: 00 [ 9084/18765 ( 48%)], Train Loss: 0.45471\n","Epoch: 00 [ 9124/18765 ( 49%)], Train Loss: 0.45458\n","Epoch: 00 [ 9164/18765 ( 49%)], Train Loss: 0.45365\n","Epoch: 00 [ 9204/18765 ( 49%)], Train Loss: 0.45271\n","Epoch: 00 [ 9244/18765 ( 49%)], Train Loss: 0.45230\n","Epoch: 00 [ 9284/18765 ( 49%)], Train Loss: 0.45204\n","Epoch: 00 [ 9324/18765 ( 50%)], Train Loss: 0.45134\n","Epoch: 00 [ 9364/18765 ( 50%)], Train Loss: 0.45072\n","Epoch: 00 [ 9404/18765 ( 50%)], Train Loss: 0.45017\n","Epoch: 00 [ 9444/18765 ( 50%)], Train Loss: 0.44981\n","Epoch: 00 [ 9484/18765 ( 51%)], Train Loss: 0.44944\n","Epoch: 00 [ 9524/18765 ( 51%)], Train Loss: 0.44949\n","Epoch: 00 [ 9564/18765 ( 51%)], Train Loss: 0.44851\n","Epoch: 00 [ 9604/18765 ( 51%)], Train Loss: 0.44789\n","Epoch: 00 [ 9644/18765 ( 51%)], Train Loss: 0.44686\n","Epoch: 00 [ 9684/18765 ( 52%)], Train Loss: 0.44657\n","Epoch: 00 [ 9724/18765 ( 52%)], Train Loss: 0.44660\n","Epoch: 00 [ 9764/18765 ( 52%)], Train Loss: 0.44566\n","Epoch: 00 [ 9804/18765 ( 52%)], Train Loss: 0.44475\n","Epoch: 00 [ 9844/18765 ( 52%)], Train Loss: 0.44431\n","Epoch: 00 [ 9884/18765 ( 53%)], Train Loss: 0.44420\n","Epoch: 00 [ 9924/18765 ( 53%)], Train Loss: 0.44364\n","Epoch: 00 [ 9964/18765 ( 53%)], Train Loss: 0.44276\n","Epoch: 00 [10004/18765 ( 53%)], Train Loss: 0.44229\n","Epoch: 00 [10044/18765 ( 54%)], Train Loss: 0.44237\n","Epoch: 00 [10084/18765 ( 54%)], Train Loss: 0.44179\n","Epoch: 00 [10124/18765 ( 54%)], Train Loss: 0.44062\n","Epoch: 00 [10164/18765 ( 54%)], Train Loss: 0.43960\n","Epoch: 00 [10204/18765 ( 54%)], Train Loss: 0.43855\n","Epoch: 00 [10244/18765 ( 55%)], Train Loss: 0.43790\n","Epoch: 00 [10284/18765 ( 55%)], Train Loss: 0.43812\n","Epoch: 00 [10324/18765 ( 55%)], Train Loss: 0.43744\n","Epoch: 00 [10364/18765 ( 55%)], Train Loss: 0.43652\n","Epoch: 00 [10404/18765 ( 55%)], Train Loss: 0.43679\n","Epoch: 00 [10444/18765 ( 56%)], Train Loss: 0.43647\n","Epoch: 00 [10484/18765 ( 56%)], Train Loss: 0.43556\n","Epoch: 00 [10524/18765 ( 56%)], Train Loss: 0.43603\n","Epoch: 00 [10564/18765 ( 56%)], Train Loss: 0.43627\n","Epoch: 00 [10604/18765 ( 57%)], Train Loss: 0.43553\n","Epoch: 00 [10644/18765 ( 57%)], Train Loss: 0.43527\n","Epoch: 00 [10684/18765 ( 57%)], Train Loss: 0.43458\n","Epoch: 00 [10724/18765 ( 57%)], Train Loss: 0.43456\n","Epoch: 00 [10764/18765 ( 57%)], Train Loss: 0.43404\n","Epoch: 00 [10804/18765 ( 58%)], Train Loss: 0.43368\n","Epoch: 00 [10844/18765 ( 58%)], Train Loss: 0.43315\n","Epoch: 00 [10884/18765 ( 58%)], Train Loss: 0.43267\n","Epoch: 00 [10924/18765 ( 58%)], Train Loss: 0.43214\n","Epoch: 00 [10964/18765 ( 58%)], Train Loss: 0.43145\n","Epoch: 00 [11004/18765 ( 59%)], Train Loss: 0.43206\n","Epoch: 00 [11044/18765 ( 59%)], Train Loss: 0.43207\n","Epoch: 00 [11084/18765 ( 59%)], Train Loss: 0.43186\n","Epoch: 00 [11124/18765 ( 59%)], Train Loss: 0.43128\n","Epoch: 00 [11164/18765 ( 59%)], Train Loss: 0.43131\n","Epoch: 00 [11204/18765 ( 60%)], Train Loss: 0.43055\n","Epoch: 00 [11244/18765 ( 60%)], Train Loss: 0.43007\n","Epoch: 00 [11284/18765 ( 60%)], Train Loss: 0.42954\n","Epoch: 00 [11324/18765 ( 60%)], Train Loss: 0.42870\n","Epoch: 00 [11364/18765 ( 61%)], Train Loss: 0.42863\n","Epoch: 00 [11404/18765 ( 61%)], Train Loss: 0.42832\n","Epoch: 00 [11444/18765 ( 61%)], Train Loss: 0.42789\n","Epoch: 00 [11484/18765 ( 61%)], Train Loss: 0.42753\n","Epoch: 00 [11524/18765 ( 61%)], Train Loss: 0.42719\n","Epoch: 00 [11564/18765 ( 62%)], Train Loss: 0.42709\n","Epoch: 00 [11604/18765 ( 62%)], Train Loss: 0.42729\n","Epoch: 00 [11644/18765 ( 62%)], Train Loss: 0.42673\n","Epoch: 00 [11684/18765 ( 62%)], Train Loss: 0.42621\n","Epoch: 00 [11724/18765 ( 62%)], Train Loss: 0.42605\n","Epoch: 00 [11764/18765 ( 63%)], Train Loss: 0.42662\n","Epoch: 00 [11804/18765 ( 63%)], Train Loss: 0.42621\n","Epoch: 00 [11844/18765 ( 63%)], Train Loss: 0.42625\n","Epoch: 00 [11884/18765 ( 63%)], Train Loss: 0.42656\n","Epoch: 00 [11924/18765 ( 64%)], Train Loss: 0.42635\n","Epoch: 00 [11964/18765 ( 64%)], Train Loss: 0.42598\n","Epoch: 00 [12004/18765 ( 64%)], Train Loss: 0.42551\n","Epoch: 00 [12044/18765 ( 64%)], Train Loss: 0.42508\n","Epoch: 00 [12084/18765 ( 64%)], Train Loss: 0.42436\n","Epoch: 00 [12124/18765 ( 65%)], Train Loss: 0.42368\n","Epoch: 00 [12164/18765 ( 65%)], Train Loss: 0.42304\n","Epoch: 00 [12204/18765 ( 65%)], Train Loss: 0.42298\n","Epoch: 00 [12244/18765 ( 65%)], Train Loss: 0.42305\n","Epoch: 00 [12284/18765 ( 65%)], Train Loss: 0.42255\n","Epoch: 00 [12324/18765 ( 66%)], Train Loss: 0.42232\n","Epoch: 00 [12364/18765 ( 66%)], Train Loss: 0.42191\n","Epoch: 00 [12404/18765 ( 66%)], Train Loss: 0.42138\n","Epoch: 00 [12444/18765 ( 66%)], Train Loss: 0.42140\n","Epoch: 00 [12484/18765 ( 67%)], Train Loss: 0.42174\n","Epoch: 00 [12524/18765 ( 67%)], Train Loss: 0.42070\n","Epoch: 00 [12564/18765 ( 67%)], Train Loss: 0.42036\n","Epoch: 00 [12604/18765 ( 67%)], Train Loss: 0.41948\n","Epoch: 00 [12644/18765 ( 67%)], Train Loss: 0.41987\n","Epoch: 00 [12684/18765 ( 68%)], Train Loss: 0.41930\n","Epoch: 00 [12724/18765 ( 68%)], Train Loss: 0.41889\n","Epoch: 00 [12764/18765 ( 68%)], Train Loss: 0.41878\n","Epoch: 00 [12804/18765 ( 68%)], Train Loss: 0.41860\n","Epoch: 00 [12844/18765 ( 68%)], Train Loss: 0.41827\n","Epoch: 00 [12884/18765 ( 69%)], Train Loss: 0.41763\n","Epoch: 00 [12924/18765 ( 69%)], Train Loss: 0.41727\n","Epoch: 00 [12964/18765 ( 69%)], Train Loss: 0.41656\n","Epoch: 00 [13004/18765 ( 69%)], Train Loss: 0.41660\n","Epoch: 00 [13044/18765 ( 70%)], Train Loss: 0.41619\n","Epoch: 00 [13084/18765 ( 70%)], Train Loss: 0.41580\n","Epoch: 00 [13124/18765 ( 70%)], Train Loss: 0.41549\n","Epoch: 00 [13164/18765 ( 70%)], Train Loss: 0.41484\n","Epoch: 00 [13204/18765 ( 70%)], Train Loss: 0.41472\n","Epoch: 00 [13244/18765 ( 71%)], Train Loss: 0.41443\n","Epoch: 00 [13284/18765 ( 71%)], Train Loss: 0.41417\n","Epoch: 00 [13324/18765 ( 71%)], Train Loss: 0.41428\n","Epoch: 00 [13364/18765 ( 71%)], Train Loss: 0.41380\n","Epoch: 00 [13404/18765 ( 71%)], Train Loss: 0.41374\n","Epoch: 00 [13444/18765 ( 72%)], Train Loss: 0.41324\n","Epoch: 00 [13484/18765 ( 72%)], Train Loss: 0.41287\n","Epoch: 00 [13524/18765 ( 72%)], Train Loss: 0.41279\n","Epoch: 00 [13564/18765 ( 72%)], Train Loss: 0.41258\n","Epoch: 00 [13604/18765 ( 72%)], Train Loss: 0.41284\n","Epoch: 00 [13644/18765 ( 73%)], Train Loss: 0.41200\n","Epoch: 00 [13684/18765 ( 73%)], Train Loss: 0.41182\n","Epoch: 00 [13724/18765 ( 73%)], Train Loss: 0.41120\n","Epoch: 00 [13764/18765 ( 73%)], Train Loss: 0.41111\n","Epoch: 00 [13804/18765 ( 74%)], Train Loss: 0.41073\n","Epoch: 00 [13844/18765 ( 74%)], Train Loss: 0.41042\n","Epoch: 00 [13884/18765 ( 74%)], Train Loss: 0.40990\n","Epoch: 00 [13924/18765 ( 74%)], Train Loss: 0.40971\n","Epoch: 00 [13964/18765 ( 74%)], Train Loss: 0.40915\n","Epoch: 00 [14004/18765 ( 75%)], Train Loss: 0.40900\n","Epoch: 00 [14044/18765 ( 75%)], Train Loss: 0.40863\n","Epoch: 00 [14084/18765 ( 75%)], Train Loss: 0.40805\n","Epoch: 00 [14124/18765 ( 75%)], Train Loss: 0.40822\n","Epoch: 00 [14164/18765 ( 75%)], Train Loss: 0.40771\n","Epoch: 00 [14204/18765 ( 76%)], Train Loss: 0.40723\n","Epoch: 00 [14244/18765 ( 76%)], Train Loss: 0.40665\n","Epoch: 00 [14284/18765 ( 76%)], Train Loss: 0.40602\n","Epoch: 00 [14324/18765 ( 76%)], Train Loss: 0.40582\n","Epoch: 00 [14364/18765 ( 77%)], Train Loss: 0.40521\n","Epoch: 00 [14404/18765 ( 77%)], Train Loss: 0.40477\n","Epoch: 00 [14444/18765 ( 77%)], Train Loss: 0.40446\n","Epoch: 00 [14484/18765 ( 77%)], Train Loss: 0.40466\n","Epoch: 00 [14524/18765 ( 77%)], Train Loss: 0.40489\n","Epoch: 00 [14564/18765 ( 78%)], Train Loss: 0.40465\n","Epoch: 00 [14604/18765 ( 78%)], Train Loss: 0.40426\n","Epoch: 00 [14644/18765 ( 78%)], Train Loss: 0.40450\n","Epoch: 00 [14684/18765 ( 78%)], Train Loss: 0.40464\n","Epoch: 00 [14724/18765 ( 78%)], Train Loss: 0.40427\n","Epoch: 00 [14764/18765 ( 79%)], Train Loss: 0.40406\n","Epoch: 00 [14804/18765 ( 79%)], Train Loss: 0.40377\n","Epoch: 00 [14844/18765 ( 79%)], Train Loss: 0.40347\n","Epoch: 00 [14884/18765 ( 79%)], Train Loss: 0.40294\n","Epoch: 00 [14924/18765 ( 80%)], Train Loss: 0.40255\n","Epoch: 00 [14964/18765 ( 80%)], Train Loss: 0.40197\n","Epoch: 00 [15004/18765 ( 80%)], Train Loss: 0.40161\n","Epoch: 00 [15044/18765 ( 80%)], Train Loss: 0.40104\n","Epoch: 00 [15084/18765 ( 80%)], Train Loss: 0.40080\n","Epoch: 00 [15124/18765 ( 81%)], Train Loss: 0.40076\n","Epoch: 00 [15164/18765 ( 81%)], Train Loss: 0.40089\n","Epoch: 00 [15204/18765 ( 81%)], Train Loss: 0.40158\n","Epoch: 00 [15244/18765 ( 81%)], Train Loss: 0.40103\n","Epoch: 00 [15284/18765 ( 81%)], Train Loss: 0.40141\n","Epoch: 00 [15324/18765 ( 82%)], Train Loss: 0.40091\n","Epoch: 00 [15364/18765 ( 82%)], Train Loss: 0.40054\n","Epoch: 00 [15404/18765 ( 82%)], Train Loss: 0.40036\n","Epoch: 00 [15444/18765 ( 82%)], Train Loss: 0.39993\n","Epoch: 00 [15484/18765 ( 83%)], Train Loss: 0.39976\n","Epoch: 00 [15524/18765 ( 83%)], Train Loss: 0.39991\n","Epoch: 00 [15564/18765 ( 83%)], Train Loss: 0.39991\n","Epoch: 00 [15604/18765 ( 83%)], Train Loss: 0.39944\n","Epoch: 00 [15644/18765 ( 83%)], Train Loss: 0.39897\n","Epoch: 00 [15684/18765 ( 84%)], Train Loss: 0.39874\n","Epoch: 00 [15724/18765 ( 84%)], Train Loss: 0.39929\n","Epoch: 00 [15764/18765 ( 84%)], Train Loss: 0.39901\n","Epoch: 00 [15804/18765 ( 84%)], Train Loss: 0.39880\n","Epoch: 00 [15844/18765 ( 84%)], Train Loss: 0.39843\n","Epoch: 00 [15884/18765 ( 85%)], Train Loss: 0.39805\n","Epoch: 00 [15924/18765 ( 85%)], Train Loss: 0.39772\n","Epoch: 00 [15964/18765 ( 85%)], Train Loss: 0.39775\n","Epoch: 00 [16004/18765 ( 85%)], Train Loss: 0.39756\n","Epoch: 00 [16044/18765 ( 85%)], Train Loss: 0.39743\n","Epoch: 00 [16084/18765 ( 86%)], Train Loss: 0.39740\n","Epoch: 00 [16124/18765 ( 86%)], Train Loss: 0.39675\n","Epoch: 00 [16164/18765 ( 86%)], Train Loss: 0.39646\n","Epoch: 00 [16204/18765 ( 86%)], Train Loss: 0.39600\n","Epoch: 00 [16244/18765 ( 87%)], Train Loss: 0.39573\n","Epoch: 00 [16284/18765 ( 87%)], Train Loss: 0.39595\n","Epoch: 00 [16324/18765 ( 87%)], Train Loss: 0.39578\n","Epoch: 00 [16364/18765 ( 87%)], Train Loss: 0.39536\n","Epoch: 00 [16404/18765 ( 87%)], Train Loss: 0.39528\n","Epoch: 00 [16444/18765 ( 88%)], Train Loss: 0.39487\n","Epoch: 00 [16484/18765 ( 88%)], Train Loss: 0.39467\n","Epoch: 00 [16524/18765 ( 88%)], Train Loss: 0.39479\n","Epoch: 00 [16564/18765 ( 88%)], Train Loss: 0.39487\n","Epoch: 00 [16604/18765 ( 88%)], Train Loss: 0.39422\n","Epoch: 00 [16644/18765 ( 89%)], Train Loss: 0.39371\n","Epoch: 00 [16684/18765 ( 89%)], Train Loss: 0.39338\n","Epoch: 00 [16724/18765 ( 89%)], Train Loss: 0.39317\n","Epoch: 00 [16764/18765 ( 89%)], Train Loss: 0.39283\n","Epoch: 00 [16804/18765 ( 90%)], Train Loss: 0.39261\n","Epoch: 00 [16844/18765 ( 90%)], Train Loss: 0.39254\n","Epoch: 00 [16884/18765 ( 90%)], Train Loss: 0.39244\n","Epoch: 00 [16924/18765 ( 90%)], Train Loss: 0.39200\n","Epoch: 00 [16964/18765 ( 90%)], Train Loss: 0.39166\n","Epoch: 00 [17004/18765 ( 91%)], Train Loss: 0.39139\n","Epoch: 00 [17044/18765 ( 91%)], Train Loss: 0.39105\n","Epoch: 00 [17084/18765 ( 91%)], Train Loss: 0.39072\n","Epoch: 00 [17124/18765 ( 91%)], Train Loss: 0.39068\n","Epoch: 00 [17164/18765 ( 91%)], Train Loss: 0.39020\n","Epoch: 00 [17204/18765 ( 92%)], Train Loss: 0.38993\n","Epoch: 00 [17244/18765 ( 92%)], Train Loss: 0.38984\n","Epoch: 00 [17284/18765 ( 92%)], Train Loss: 0.38996\n","Epoch: 00 [17324/18765 ( 92%)], Train Loss: 0.39027\n","Epoch: 00 [17364/18765 ( 93%)], Train Loss: 0.38985\n","Epoch: 00 [17404/18765 ( 93%)], Train Loss: 0.39012\n","Epoch: 00 [17444/18765 ( 93%)], Train Loss: 0.39002\n","Epoch: 00 [17484/18765 ( 93%)], Train Loss: 0.39003\n","Epoch: 00 [17524/18765 ( 93%)], Train Loss: 0.38992\n","Epoch: 00 [17564/18765 ( 94%)], Train Loss: 0.38976\n","Epoch: 00 [17604/18765 ( 94%)], Train Loss: 0.38938\n","Epoch: 00 [17644/18765 ( 94%)], Train Loss: 0.38907\n","Epoch: 00 [17684/18765 ( 94%)], Train Loss: 0.38932\n","Epoch: 00 [17724/18765 ( 94%)], Train Loss: 0.38904\n","Epoch: 00 [17764/18765 ( 95%)], Train Loss: 0.38914\n","Epoch: 00 [17804/18765 ( 95%)], Train Loss: 0.38878\n","Epoch: 00 [17844/18765 ( 95%)], Train Loss: 0.38845\n","Epoch: 00 [17884/18765 ( 95%)], Train Loss: 0.38808\n","Epoch: 00 [17924/18765 ( 96%)], Train Loss: 0.38785\n","Epoch: 00 [17964/18765 ( 96%)], Train Loss: 0.38763\n","Epoch: 00 [18004/18765 ( 96%)], Train Loss: 0.38733\n","Epoch: 00 [18044/18765 ( 96%)], Train Loss: 0.38685\n","Epoch: 00 [18084/18765 ( 96%)], Train Loss: 0.38647\n","Epoch: 00 [18124/18765 ( 97%)], Train Loss: 0.38609\n","Epoch: 00 [18164/18765 ( 97%)], Train Loss: 0.38583\n","Epoch: 00 [18204/18765 ( 97%)], Train Loss: 0.38544\n","Epoch: 00 [18244/18765 ( 97%)], Train Loss: 0.38498\n","Epoch: 00 [18284/18765 ( 97%)], Train Loss: 0.38442\n","Epoch: 00 [18324/18765 ( 98%)], Train Loss: 0.38408\n","Epoch: 00 [18364/18765 ( 98%)], Train Loss: 0.38417\n","Epoch: 00 [18404/18765 ( 98%)], Train Loss: 0.38426\n","Epoch: 00 [18444/18765 ( 98%)], Train Loss: 0.38448\n","Epoch: 00 [18484/18765 ( 99%)], Train Loss: 0.38455\n","Epoch: 00 [18524/18765 ( 99%)], Train Loss: 0.38422\n","Epoch: 00 [18564/18765 ( 99%)], Train Loss: 0.38389\n","Epoch: 00 [18604/18765 ( 99%)], Train Loss: 0.38335\n","Epoch: 00 [18644/18765 ( 99%)], Train Loss: 0.38297\n","Epoch: 00 [18684/18765 (100%)], Train Loss: 0.38297\n","Epoch: 00 [18724/18765 (100%)], Train Loss: 0.38305\n","Epoch: 00 [18764/18765 (100%)], Train Loss: 0.38329\n","Epoch: 00 [18765/18765 (100%)], Train Loss: 0.38336\n","----Validation Results Summary----\n","Epoch: [0] Valid Loss: 0.61498\n","0 Epoch, Best epoch was updated! Valid Loss: 0.61498\n","Saving model checkpoint to chaii-qa-5-fold-xlmroberta-torch-fit/checkpoint-fold-3.\n","\n","Total Training Time: 2620.555602788925secs, Average Training Time per Epoch: 2620.555602788925secs.\n","Total Validation Time: 181.55473566055298secs, Average Validation Time per Epoch: 181.55473566055298secs.\n","\n","\n","--------------------------------------------------\n","FOLD: 4\n","--------------------------------------------------\n","Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n","Num examples Train= 18701, Num examples Valid=4368\n","Total Training Steps: 2338, Total Warmup Steps: 233\n","Epoch: 00 [    4/18701 (  0%)], Train Loss: 2.97787\n","Epoch: 00 [   44/18701 (  0%)], Train Loss: 2.90730\n","Epoch: 00 [   84/18701 (  0%)], Train Loss: 2.88351\n","Epoch: 00 [  124/18701 (  1%)], Train Loss: 2.84616\n","Epoch: 00 [  164/18701 (  1%)], Train Loss: 2.80570\n","Epoch: 00 [  204/18701 (  1%)], Train Loss: 2.74120\n","Epoch: 00 [  244/18701 (  1%)], Train Loss: 2.65955\n","Epoch: 00 [  284/18701 (  2%)], Train Loss: 2.56138\n","Epoch: 00 [  324/18701 (  2%)], Train Loss: 2.43368\n","Epoch: 00 [  364/18701 (  2%)], Train Loss: 2.31393\n","Epoch: 00 [  404/18701 (  2%)], Train Loss: 2.19521\n","Epoch: 00 [  444/18701 (  2%)], Train Loss: 2.08758\n","Epoch: 00 [  484/18701 (  3%)], Train Loss: 1.94848\n","Epoch: 00 [  524/18701 (  3%)], Train Loss: 1.84774\n","Epoch: 00 [  564/18701 (  3%)], Train Loss: 1.75949\n","Epoch: 00 [  604/18701 (  3%)], Train Loss: 1.68607\n","Epoch: 00 [  644/18701 (  3%)], Train Loss: 1.61019\n","Epoch: 00 [  684/18701 (  4%)], Train Loss: 1.55311\n","Epoch: 00 [  724/18701 (  4%)], Train Loss: 1.51078\n","Epoch: 00 [  764/18701 (  4%)], Train Loss: 1.46481\n","Epoch: 00 [  804/18701 (  4%)], Train Loss: 1.41960\n","Epoch: 00 [  844/18701 (  5%)], Train Loss: 1.37494\n","Epoch: 00 [  884/18701 (  5%)], Train Loss: 1.33249\n","Epoch: 00 [  924/18701 (  5%)], Train Loss: 1.30294\n","Epoch: 00 [  964/18701 (  5%)], Train Loss: 1.27226\n","Epoch: 00 [ 1004/18701 (  5%)], Train Loss: 1.23214\n","Epoch: 00 [ 1044/18701 (  6%)], Train Loss: 1.19770\n","Epoch: 00 [ 1084/18701 (  6%)], Train Loss: 1.17387\n","Epoch: 00 [ 1124/18701 (  6%)], Train Loss: 1.15230\n","Epoch: 00 [ 1164/18701 (  6%)], Train Loss: 1.12917\n","Epoch: 00 [ 1204/18701 (  6%)], Train Loss: 1.10658\n","Epoch: 00 [ 1244/18701 (  7%)], Train Loss: 1.08897\n","Epoch: 00 [ 1284/18701 (  7%)], Train Loss: 1.06941\n","Epoch: 00 [ 1324/18701 (  7%)], Train Loss: 1.05124\n","Epoch: 00 [ 1364/18701 (  7%)], Train Loss: 1.03145\n","Epoch: 00 [ 1404/18701 (  8%)], Train Loss: 1.01425\n","Epoch: 00 [ 1444/18701 (  8%)], Train Loss: 1.00021\n","Epoch: 00 [ 1484/18701 (  8%)], Train Loss: 0.98224\n","Epoch: 00 [ 1524/18701 (  8%)], Train Loss: 0.96781\n","Epoch: 00 [ 1564/18701 (  8%)], Train Loss: 0.95665\n","Epoch: 00 [ 1604/18701 (  9%)], Train Loss: 0.94600\n","Epoch: 00 [ 1644/18701 (  9%)], Train Loss: 0.93223\n","Epoch: 00 [ 1684/18701 (  9%)], Train Loss: 0.92134\n","Epoch: 00 [ 1724/18701 (  9%)], Train Loss: 0.90770\n","Epoch: 00 [ 1764/18701 (  9%)], Train Loss: 0.90237\n","Epoch: 00 [ 1804/18701 ( 10%)], Train Loss: 0.88904\n","Epoch: 00 [ 1844/18701 ( 10%)], Train Loss: 0.87918\n","Epoch: 00 [ 1884/18701 ( 10%)], Train Loss: 0.87525\n","Epoch: 00 [ 1924/18701 ( 10%)], Train Loss: 0.86821\n","Epoch: 00 [ 1964/18701 ( 11%)], Train Loss: 0.85815\n","Epoch: 00 [ 2004/18701 ( 11%)], Train Loss: 0.84671\n","Epoch: 00 [ 2044/18701 ( 11%)], Train Loss: 0.84098\n","Epoch: 00 [ 2084/18701 ( 11%)], Train Loss: 0.83254\n","Epoch: 00 [ 2124/18701 ( 11%)], Train Loss: 0.82152\n","Epoch: 00 [ 2164/18701 ( 12%)], Train Loss: 0.81647\n","Epoch: 00 [ 2204/18701 ( 12%)], Train Loss: 0.80551\n","Epoch: 00 [ 2244/18701 ( 12%)], Train Loss: 0.80020\n","Epoch: 00 [ 2284/18701 ( 12%)], Train Loss: 0.79038\n","Epoch: 00 [ 2324/18701 ( 12%)], Train Loss: 0.78117\n","Epoch: 00 [ 2364/18701 ( 13%)], Train Loss: 0.77660\n","Epoch: 00 [ 2404/18701 ( 13%)], Train Loss: 0.77122\n","Epoch: 00 [ 2444/18701 ( 13%)], Train Loss: 0.76163\n","Epoch: 00 [ 2484/18701 ( 13%)], Train Loss: 0.75682\n","Epoch: 00 [ 2524/18701 ( 13%)], Train Loss: 0.75460\n","Epoch: 00 [ 2564/18701 ( 14%)], Train Loss: 0.75127\n","Epoch: 00 [ 2604/18701 ( 14%)], Train Loss: 0.74677\n","Epoch: 00 [ 2644/18701 ( 14%)], Train Loss: 0.74009\n","Epoch: 00 [ 2684/18701 ( 14%)], Train Loss: 0.73800\n","Epoch: 00 [ 2724/18701 ( 15%)], Train Loss: 0.73152\n","Epoch: 00 [ 2764/18701 ( 15%)], Train Loss: 0.72606\n","Epoch: 00 [ 2804/18701 ( 15%)], Train Loss: 0.72197\n","Epoch: 00 [ 2844/18701 ( 15%)], Train Loss: 0.72331\n","Epoch: 00 [ 2884/18701 ( 15%)], Train Loss: 0.71823\n","Epoch: 00 [ 2924/18701 ( 16%)], Train Loss: 0.71298\n","Epoch: 00 [ 2964/18701 ( 16%)], Train Loss: 0.70943\n","Epoch: 00 [ 3004/18701 ( 16%)], Train Loss: 0.70377\n","Epoch: 00 [ 3044/18701 ( 16%)], Train Loss: 0.70041\n","Epoch: 00 [ 3084/18701 ( 16%)], Train Loss: 0.69542\n","Epoch: 00 [ 3124/18701 ( 17%)], Train Loss: 0.69074\n","Epoch: 00 [ 3164/18701 ( 17%)], Train Loss: 0.68794\n","Epoch: 00 [ 3204/18701 ( 17%)], Train Loss: 0.68095\n","Epoch: 00 [ 3244/18701 ( 17%)], Train Loss: 0.67511\n","Epoch: 00 [ 3284/18701 ( 18%)], Train Loss: 0.67041\n","Epoch: 00 [ 3324/18701 ( 18%)], Train Loss: 0.66788\n","Epoch: 00 [ 3364/18701 ( 18%)], Train Loss: 0.66411\n","Epoch: 00 [ 3404/18701 ( 18%)], Train Loss: 0.65874\n","Epoch: 00 [ 3444/18701 ( 18%)], Train Loss: 0.65647\n","Epoch: 00 [ 3484/18701 ( 19%)], Train Loss: 0.65283\n","Epoch: 00 [ 3524/18701 ( 19%)], Train Loss: 0.64974\n","Epoch: 00 [ 3564/18701 ( 19%)], Train Loss: 0.64787\n","Epoch: 00 [ 3604/18701 ( 19%)], Train Loss: 0.64321\n","Epoch: 00 [ 3644/18701 ( 19%)], Train Loss: 0.63814\n","Epoch: 00 [ 3684/18701 ( 20%)], Train Loss: 0.63813\n","Epoch: 00 [ 3724/18701 ( 20%)], Train Loss: 0.63552\n","Epoch: 00 [ 3764/18701 ( 20%)], Train Loss: 0.63251\n","Epoch: 00 [ 3804/18701 ( 20%)], Train Loss: 0.62916\n","Epoch: 00 [ 3844/18701 ( 21%)], Train Loss: 0.62637\n","Epoch: 00 [ 3884/18701 ( 21%)], Train Loss: 0.62253\n","Epoch: 00 [ 3924/18701 ( 21%)], Train Loss: 0.62062\n","Epoch: 00 [ 3964/18701 ( 21%)], Train Loss: 0.61896\n","Epoch: 00 [ 4004/18701 ( 21%)], Train Loss: 0.61728\n","Epoch: 00 [ 4044/18701 ( 22%)], Train Loss: 0.61266\n","Epoch: 00 [ 4084/18701 ( 22%)], Train Loss: 0.60961\n","Epoch: 00 [ 4124/18701 ( 22%)], Train Loss: 0.60689\n","Epoch: 00 [ 4164/18701 ( 22%)], Train Loss: 0.60404\n","Epoch: 00 [ 4204/18701 ( 22%)], Train Loss: 0.60066\n","Epoch: 00 [ 4244/18701 ( 23%)], Train Loss: 0.59809\n","Epoch: 00 [ 4284/18701 ( 23%)], Train Loss: 0.59728\n","Epoch: 00 [ 4324/18701 ( 23%)], Train Loss: 0.59339\n","Epoch: 00 [ 4364/18701 ( 23%)], Train Loss: 0.59066\n","Epoch: 00 [ 4404/18701 ( 24%)], Train Loss: 0.58832\n","Epoch: 00 [ 4444/18701 ( 24%)], Train Loss: 0.58421\n","Epoch: 00 [ 4484/18701 ( 24%)], Train Loss: 0.58402\n","Epoch: 00 [ 4524/18701 ( 24%)], Train Loss: 0.58348\n","Epoch: 00 [ 4564/18701 ( 24%)], Train Loss: 0.58063\n","Epoch: 00 [ 4604/18701 ( 25%)], Train Loss: 0.57792\n","Epoch: 00 [ 4644/18701 ( 25%)], Train Loss: 0.57545\n","Epoch: 00 [ 4684/18701 ( 25%)], Train Loss: 0.57217\n","Epoch: 00 [ 4724/18701 ( 25%)], Train Loss: 0.57046\n","Epoch: 00 [ 4764/18701 ( 25%)], Train Loss: 0.56897\n","Epoch: 00 [ 4804/18701 ( 26%)], Train Loss: 0.56802\n","Epoch: 00 [ 4844/18701 ( 26%)], Train Loss: 0.56529\n","Epoch: 00 [ 4884/18701 ( 26%)], Train Loss: 0.56352\n","Epoch: 00 [ 4924/18701 ( 26%)], Train Loss: 0.56155\n","Epoch: 00 [ 4964/18701 ( 27%)], Train Loss: 0.55952\n","Epoch: 00 [ 5004/18701 ( 27%)], Train Loss: 0.55766\n","Epoch: 00 [ 5044/18701 ( 27%)], Train Loss: 0.55751\n","Epoch: 00 [ 5084/18701 ( 27%)], Train Loss: 0.55665\n","Epoch: 00 [ 5124/18701 ( 27%)], Train Loss: 0.55524\n","Epoch: 00 [ 5164/18701 ( 28%)], Train Loss: 0.55289\n","Epoch: 00 [ 5204/18701 ( 28%)], Train Loss: 0.55120\n","Epoch: 00 [ 5244/18701 ( 28%)], Train Loss: 0.55075\n","Epoch: 00 [ 5284/18701 ( 28%)], Train Loss: 0.54839\n","Epoch: 00 [ 5324/18701 ( 28%)], Train Loss: 0.54845\n","Epoch: 00 [ 5364/18701 ( 29%)], Train Loss: 0.54789\n","Epoch: 00 [ 5404/18701 ( 29%)], Train Loss: 0.54636\n","Epoch: 00 [ 5444/18701 ( 29%)], Train Loss: 0.54358\n","Epoch: 00 [ 5484/18701 ( 29%)], Train Loss: 0.54147\n","Epoch: 00 [ 5524/18701 ( 30%)], Train Loss: 0.53900\n","Epoch: 00 [ 5564/18701 ( 30%)], Train Loss: 0.53848\n","Epoch: 00 [ 5604/18701 ( 30%)], Train Loss: 0.53794\n","Epoch: 00 [ 5644/18701 ( 30%)], Train Loss: 0.53638\n","Epoch: 00 [ 5684/18701 ( 30%)], Train Loss: 0.53503\n","Epoch: 00 [ 5724/18701 ( 31%)], Train Loss: 0.53375\n","Epoch: 00 [ 5764/18701 ( 31%)], Train Loss: 0.53283\n","Epoch: 00 [ 5804/18701 ( 31%)], Train Loss: 0.53186\n","Epoch: 00 [ 5844/18701 ( 31%)], Train Loss: 0.53070\n","Epoch: 00 [ 5884/18701 ( 31%)], Train Loss: 0.52911\n","Epoch: 00 [ 5924/18701 ( 32%)], Train Loss: 0.52745\n","Epoch: 00 [ 5964/18701 ( 32%)], Train Loss: 0.52582\n","Epoch: 00 [ 6004/18701 ( 32%)], Train Loss: 0.52501\n","Epoch: 00 [ 6044/18701 ( 32%)], Train Loss: 0.52500\n","Epoch: 00 [ 6084/18701 ( 33%)], Train Loss: 0.52375\n","Epoch: 00 [ 6124/18701 ( 33%)], Train Loss: 0.52171\n","Epoch: 00 [ 6164/18701 ( 33%)], Train Loss: 0.52103\n","Epoch: 00 [ 6204/18701 ( 33%)], Train Loss: 0.51961\n","Epoch: 00 [ 6244/18701 ( 33%)], Train Loss: 0.51820\n","Epoch: 00 [ 6284/18701 ( 34%)], Train Loss: 0.51690\n","Epoch: 00 [ 6324/18701 ( 34%)], Train Loss: 0.51636\n","Epoch: 00 [ 6364/18701 ( 34%)], Train Loss: 0.51367\n","Epoch: 00 [ 6404/18701 ( 34%)], Train Loss: 0.51337\n","Epoch: 00 [ 6444/18701 ( 34%)], Train Loss: 0.51242\n","Epoch: 00 [ 6484/18701 ( 35%)], Train Loss: 0.51141\n","Epoch: 00 [ 6524/18701 ( 35%)], Train Loss: 0.51091\n","Epoch: 00 [ 6564/18701 ( 35%)], Train Loss: 0.51131\n","Epoch: 00 [ 6604/18701 ( 35%)], Train Loss: 0.51033\n","Epoch: 00 [ 6644/18701 ( 36%)], Train Loss: 0.51105\n","Epoch: 00 [ 6684/18701 ( 36%)], Train Loss: 0.50969\n","Epoch: 00 [ 6724/18701 ( 36%)], Train Loss: 0.50809\n","Epoch: 00 [ 6764/18701 ( 36%)], Train Loss: 0.50726\n","Epoch: 00 [ 6804/18701 ( 36%)], Train Loss: 0.50612\n","Epoch: 00 [ 6844/18701 ( 37%)], Train Loss: 0.50467\n","Epoch: 00 [ 6884/18701 ( 37%)], Train Loss: 0.50450\n","Epoch: 00 [ 6924/18701 ( 37%)], Train Loss: 0.50387\n","Epoch: 00 [ 6964/18701 ( 37%)], Train Loss: 0.50386\n","Epoch: 00 [ 7004/18701 ( 37%)], Train Loss: 0.50351\n","Epoch: 00 [ 7044/18701 ( 38%)], Train Loss: 0.50280\n","Epoch: 00 [ 7084/18701 ( 38%)], Train Loss: 0.50197\n","Epoch: 00 [ 7124/18701 ( 38%)], Train Loss: 0.50069\n","Epoch: 00 [ 7164/18701 ( 38%)], Train Loss: 0.50092\n","Epoch: 00 [ 7204/18701 ( 39%)], Train Loss: 0.50053\n","Epoch: 00 [ 7244/18701 ( 39%)], Train Loss: 0.50078\n","Epoch: 00 [ 7284/18701 ( 39%)], Train Loss: 0.50049\n","Epoch: 00 [ 7324/18701 ( 39%)], Train Loss: 0.50011\n","Epoch: 00 [ 7364/18701 ( 39%)], Train Loss: 0.49928\n","Epoch: 00 [ 7404/18701 ( 40%)], Train Loss: 0.49799\n","Epoch: 00 [ 7444/18701 ( 40%)], Train Loss: 0.49642\n","Epoch: 00 [ 7484/18701 ( 40%)], Train Loss: 0.49527\n","Epoch: 00 [ 7524/18701 ( 40%)], Train Loss: 0.49555\n","Epoch: 00 [ 7564/18701 ( 40%)], Train Loss: 0.49456\n","Epoch: 00 [ 7604/18701 ( 41%)], Train Loss: 0.49414\n","Epoch: 00 [ 7644/18701 ( 41%)], Train Loss: 0.49346\n","Epoch: 00 [ 7684/18701 ( 41%)], Train Loss: 0.49159\n","Epoch: 00 [ 7724/18701 ( 41%)], Train Loss: 0.49066\n","Epoch: 00 [ 7764/18701 ( 42%)], Train Loss: 0.49034\n","Epoch: 00 [ 7804/18701 ( 42%)], Train Loss: 0.48912\n","Epoch: 00 [ 7844/18701 ( 42%)], Train Loss: 0.48826\n","Epoch: 00 [ 7884/18701 ( 42%)], Train Loss: 0.48690\n","Epoch: 00 [ 7924/18701 ( 42%)], Train Loss: 0.48746\n","Epoch: 00 [ 7964/18701 ( 43%)], Train Loss: 0.48746\n","Epoch: 00 [ 8004/18701 ( 43%)], Train Loss: 0.48655\n","Epoch: 00 [ 8044/18701 ( 43%)], Train Loss: 0.48597\n","Epoch: 00 [ 8084/18701 ( 43%)], Train Loss: 0.48467\n","Epoch: 00 [ 8124/18701 ( 43%)], Train Loss: 0.48461\n","Epoch: 00 [ 8164/18701 ( 44%)], Train Loss: 0.48363\n","Epoch: 00 [ 8204/18701 ( 44%)], Train Loss: 0.48285\n","Epoch: 00 [ 8244/18701 ( 44%)], Train Loss: 0.48233\n","Epoch: 00 [ 8284/18701 ( 44%)], Train Loss: 0.48267\n","Epoch: 00 [ 8324/18701 ( 45%)], Train Loss: 0.48164\n","Epoch: 00 [ 8364/18701 ( 45%)], Train Loss: 0.48045\n","Epoch: 00 [ 8404/18701 ( 45%)], Train Loss: 0.47992\n","Epoch: 00 [ 8444/18701 ( 45%)], Train Loss: 0.47868\n","Epoch: 00 [ 8484/18701 ( 45%)], Train Loss: 0.47681\n","Epoch: 00 [ 8524/18701 ( 46%)], Train Loss: 0.47607\n","Epoch: 00 [ 8564/18701 ( 46%)], Train Loss: 0.47617\n","Epoch: 00 [ 8604/18701 ( 46%)], Train Loss: 0.47511\n","Epoch: 00 [ 8644/18701 ( 46%)], Train Loss: 0.47375\n","Epoch: 00 [ 8684/18701 ( 46%)], Train Loss: 0.47383\n","Epoch: 00 [ 8724/18701 ( 47%)], Train Loss: 0.47335\n","Epoch: 00 [ 8764/18701 ( 47%)], Train Loss: 0.47220\n","Epoch: 00 [ 8804/18701 ( 47%)], Train Loss: 0.47119\n","Epoch: 00 [ 8844/18701 ( 47%)], Train Loss: 0.47061\n","Epoch: 00 [ 8884/18701 ( 48%)], Train Loss: 0.46921\n","Epoch: 00 [ 8924/18701 ( 48%)], Train Loss: 0.46813\n","Epoch: 00 [ 8964/18701 ( 48%)], Train Loss: 0.46744\n","Epoch: 00 [ 9004/18701 ( 48%)], Train Loss: 0.46716\n","Epoch: 00 [ 9044/18701 ( 48%)], Train Loss: 0.46650\n","Epoch: 00 [ 9084/18701 ( 49%)], Train Loss: 0.46624\n","Epoch: 00 [ 9124/18701 ( 49%)], Train Loss: 0.46527\n","Epoch: 00 [ 9164/18701 ( 49%)], Train Loss: 0.46421\n","Epoch: 00 [ 9204/18701 ( 49%)], Train Loss: 0.46390\n","Epoch: 00 [ 9244/18701 ( 49%)], Train Loss: 0.46370\n","Epoch: 00 [ 9284/18701 ( 50%)], Train Loss: 0.46341\n","Epoch: 00 [ 9324/18701 ( 50%)], Train Loss: 0.46302\n","Epoch: 00 [ 9364/18701 ( 50%)], Train Loss: 0.46236\n","Epoch: 00 [ 9404/18701 ( 50%)], Train Loss: 0.46247\n","Epoch: 00 [ 9444/18701 ( 50%)], Train Loss: 0.46203\n","Epoch: 00 [ 9484/18701 ( 51%)], Train Loss: 0.46179\n","Epoch: 00 [ 9524/18701 ( 51%)], Train Loss: 0.46201\n","Epoch: 00 [ 9564/18701 ( 51%)], Train Loss: 0.46158\n","Epoch: 00 [ 9604/18701 ( 51%)], Train Loss: 0.46063\n","Epoch: 00 [ 9644/18701 ( 52%)], Train Loss: 0.45998\n","Epoch: 00 [ 9684/18701 ( 52%)], Train Loss: 0.45911\n","Epoch: 00 [ 9724/18701 ( 52%)], Train Loss: 0.45875\n","Epoch: 00 [ 9764/18701 ( 52%)], Train Loss: 0.45844\n","Epoch: 00 [ 9804/18701 ( 52%)], Train Loss: 0.45774\n","Epoch: 00 [ 9844/18701 ( 53%)], Train Loss: 0.45744\n","Epoch: 00 [ 9884/18701 ( 53%)], Train Loss: 0.45662\n","Epoch: 00 [ 9924/18701 ( 53%)], Train Loss: 0.45636\n","Epoch: 00 [ 9964/18701 ( 53%)], Train Loss: 0.45580\n","Epoch: 00 [10004/18701 ( 53%)], Train Loss: 0.45566\n","Epoch: 00 [10044/18701 ( 54%)], Train Loss: 0.45556\n","Epoch: 00 [10084/18701 ( 54%)], Train Loss: 0.45509\n","Epoch: 00 [10124/18701 ( 54%)], Train Loss: 0.45464\n","Epoch: 00 [10164/18701 ( 54%)], Train Loss: 0.45421\n","Epoch: 00 [10204/18701 ( 55%)], Train Loss: 0.45344\n","Epoch: 00 [10244/18701 ( 55%)], Train Loss: 0.45350\n","Epoch: 00 [10284/18701 ( 55%)], Train Loss: 0.45310\n","Epoch: 00 [10324/18701 ( 55%)], Train Loss: 0.45279\n","Epoch: 00 [10364/18701 ( 55%)], Train Loss: 0.45213\n","Epoch: 00 [10404/18701 ( 56%)], Train Loss: 0.45179\n","Epoch: 00 [10444/18701 ( 56%)], Train Loss: 0.45142\n","Epoch: 00 [10484/18701 ( 56%)], Train Loss: 0.45082\n","Epoch: 00 [10524/18701 ( 56%)], Train Loss: 0.45045\n","Epoch: 00 [10564/18701 ( 56%)], Train Loss: 0.44988\n","Epoch: 00 [10604/18701 ( 57%)], Train Loss: 0.44918\n","Epoch: 00 [10644/18701 ( 57%)], Train Loss: 0.44870\n","Epoch: 00 [10684/18701 ( 57%)], Train Loss: 0.44818\n","Epoch: 00 [10724/18701 ( 57%)], Train Loss: 0.44848\n","Epoch: 00 [10764/18701 ( 58%)], Train Loss: 0.44783\n","Epoch: 00 [10804/18701 ( 58%)], Train Loss: 0.44776\n","Epoch: 00 [10844/18701 ( 58%)], Train Loss: 0.44755\n","Epoch: 00 [10884/18701 ( 58%)], Train Loss: 0.44679\n","Epoch: 00 [10924/18701 ( 58%)], Train Loss: 0.44618\n","Epoch: 00 [10964/18701 ( 59%)], Train Loss: 0.44598\n","Epoch: 00 [11004/18701 ( 59%)], Train Loss: 0.44537\n","Epoch: 00 [11044/18701 ( 59%)], Train Loss: 0.44504\n","Epoch: 00 [11084/18701 ( 59%)], Train Loss: 0.44452\n","Epoch: 00 [11124/18701 ( 59%)], Train Loss: 0.44428\n","Epoch: 00 [11164/18701 ( 60%)], Train Loss: 0.44364\n","Epoch: 00 [11204/18701 ( 60%)], Train Loss: 0.44257\n","Epoch: 00 [11244/18701 ( 60%)], Train Loss: 0.44172\n","Epoch: 00 [11284/18701 ( 60%)], Train Loss: 0.44069\n","Epoch: 00 [11324/18701 ( 61%)], Train Loss: 0.44092\n","Epoch: 00 [11364/18701 ( 61%)], Train Loss: 0.44088\n","Epoch: 00 [11404/18701 ( 61%)], Train Loss: 0.44032\n","Epoch: 00 [11444/18701 ( 61%)], Train Loss: 0.43981\n","Epoch: 00 [11484/18701 ( 61%)], Train Loss: 0.43948\n","Epoch: 00 [11524/18701 ( 62%)], Train Loss: 0.43890\n","Epoch: 00 [11564/18701 ( 62%)], Train Loss: 0.43870\n","Epoch: 00 [11604/18701 ( 62%)], Train Loss: 0.43804\n","Epoch: 00 [11644/18701 ( 62%)], Train Loss: 0.43763\n","Epoch: 00 [11684/18701 ( 62%)], Train Loss: 0.43678\n","Epoch: 00 [11724/18701 ( 63%)], Train Loss: 0.43645\n","Epoch: 00 [11764/18701 ( 63%)], Train Loss: 0.43583\n","Epoch: 00 [11804/18701 ( 63%)], Train Loss: 0.43557\n","Epoch: 00 [11844/18701 ( 63%)], Train Loss: 0.43504\n","Epoch: 00 [11884/18701 ( 64%)], Train Loss: 0.43474\n","Epoch: 00 [11924/18701 ( 64%)], Train Loss: 0.43432\n","Epoch: 00 [11964/18701 ( 64%)], Train Loss: 0.43351\n","Epoch: 00 [12004/18701 ( 64%)], Train Loss: 0.43306\n","Epoch: 00 [12044/18701 ( 64%)], Train Loss: 0.43242\n","Epoch: 00 [12084/18701 ( 65%)], Train Loss: 0.43208\n","Epoch: 00 [12124/18701 ( 65%)], Train Loss: 0.43171\n","Epoch: 00 [12164/18701 ( 65%)], Train Loss: 0.43156\n","Epoch: 00 [12204/18701 ( 65%)], Train Loss: 0.43091\n","Epoch: 00 [12244/18701 ( 65%)], Train Loss: 0.43052\n","Epoch: 00 [12284/18701 ( 66%)], Train Loss: 0.43020\n","Epoch: 00 [12324/18701 ( 66%)], Train Loss: 0.42979\n","Epoch: 00 [12364/18701 ( 66%)], Train Loss: 0.42953\n","Epoch: 00 [12404/18701 ( 66%)], Train Loss: 0.42910\n","Epoch: 00 [12444/18701 ( 67%)], Train Loss: 0.42840\n","Epoch: 00 [12484/18701 ( 67%)], Train Loss: 0.42786\n","Epoch: 00 [12524/18701 ( 67%)], Train Loss: 0.42728\n","Epoch: 00 [12564/18701 ( 67%)], Train Loss: 0.42666\n","Epoch: 00 [12604/18701 ( 67%)], Train Loss: 0.42633\n","Epoch: 00 [12644/18701 ( 68%)], Train Loss: 0.42591\n","Epoch: 00 [12684/18701 ( 68%)], Train Loss: 0.42605\n","Epoch: 00 [12724/18701 ( 68%)], Train Loss: 0.42573\n","Epoch: 00 [12764/18701 ( 68%)], Train Loss: 0.42524\n","Epoch: 00 [12804/18701 ( 68%)], Train Loss: 0.42467\n","Epoch: 00 [12844/18701 ( 69%)], Train Loss: 0.42422\n","Epoch: 00 [12884/18701 ( 69%)], Train Loss: 0.42388\n","Epoch: 00 [12924/18701 ( 69%)], Train Loss: 0.42351\n","Epoch: 00 [12964/18701 ( 69%)], Train Loss: 0.42326\n","Epoch: 00 [13004/18701 ( 70%)], Train Loss: 0.42292\n","Epoch: 00 [13044/18701 ( 70%)], Train Loss: 0.42242\n","Epoch: 00 [13084/18701 ( 70%)], Train Loss: 0.42237\n","Epoch: 00 [13124/18701 ( 70%)], Train Loss: 0.42187\n","Epoch: 00 [13164/18701 ( 70%)], Train Loss: 0.42181\n","Epoch: 00 [13204/18701 ( 71%)], Train Loss: 0.42139\n","Epoch: 00 [13244/18701 ( 71%)], Train Loss: 0.42073\n","Epoch: 00 [13284/18701 ( 71%)], Train Loss: 0.42008\n","Epoch: 00 [13324/18701 ( 71%)], Train Loss: 0.41953\n","Epoch: 00 [13364/18701 ( 71%)], Train Loss: 0.41903\n","Epoch: 00 [13404/18701 ( 72%)], Train Loss: 0.41866\n","Epoch: 00 [13444/18701 ( 72%)], Train Loss: 0.41818\n","Epoch: 00 [13484/18701 ( 72%)], Train Loss: 0.41772\n","Epoch: 00 [13524/18701 ( 72%)], Train Loss: 0.41694\n","Epoch: 00 [13564/18701 ( 73%)], Train Loss: 0.41647\n","Epoch: 00 [13604/18701 ( 73%)], Train Loss: 0.41629\n","Epoch: 00 [13644/18701 ( 73%)], Train Loss: 0.41565\n","Epoch: 00 [13684/18701 ( 73%)], Train Loss: 0.41512\n","Epoch: 00 [13724/18701 ( 73%)], Train Loss: 0.41485\n","Epoch: 00 [13764/18701 ( 74%)], Train Loss: 0.41452\n","Epoch: 00 [13804/18701 ( 74%)], Train Loss: 0.41411\n","Epoch: 00 [13844/18701 ( 74%)], Train Loss: 0.41382\n","Epoch: 00 [13884/18701 ( 74%)], Train Loss: 0.41350\n","Epoch: 00 [13924/18701 ( 74%)], Train Loss: 0.41271\n","Epoch: 00 [13964/18701 ( 75%)], Train Loss: 0.41324\n","Epoch: 00 [14004/18701 ( 75%)], Train Loss: 0.41257\n","Epoch: 00 [14044/18701 ( 75%)], Train Loss: 0.41202\n","Epoch: 00 [14084/18701 ( 75%)], Train Loss: 0.41144\n","Epoch: 00 [14124/18701 ( 76%)], Train Loss: 0.41114\n","Epoch: 00 [14164/18701 ( 76%)], Train Loss: 0.41051\n","Epoch: 00 [14204/18701 ( 76%)], Train Loss: 0.41040\n","Epoch: 00 [14244/18701 ( 76%)], Train Loss: 0.40992\n","Epoch: 00 [14284/18701 ( 76%)], Train Loss: 0.40979\n","Epoch: 00 [14324/18701 ( 77%)], Train Loss: 0.40954\n","Epoch: 00 [14364/18701 ( 77%)], Train Loss: 0.40938\n","Epoch: 00 [14404/18701 ( 77%)], Train Loss: 0.40860\n","Epoch: 00 [14444/18701 ( 77%)], Train Loss: 0.40812\n","Epoch: 00 [14484/18701 ( 77%)], Train Loss: 0.40866\n","Epoch: 00 [14524/18701 ( 78%)], Train Loss: 0.40857\n","Epoch: 00 [14564/18701 ( 78%)], Train Loss: 0.40852\n","Epoch: 00 [14604/18701 ( 78%)], Train Loss: 0.40799\n","Epoch: 00 [14644/18701 ( 78%)], Train Loss: 0.40763\n","Epoch: 00 [14684/18701 ( 79%)], Train Loss: 0.40742\n","Epoch: 00 [14724/18701 ( 79%)], Train Loss: 0.40721\n","Epoch: 00 [14764/18701 ( 79%)], Train Loss: 0.40717\n","Epoch: 00 [14804/18701 ( 79%)], Train Loss: 0.40680\n","Epoch: 00 [14844/18701 ( 79%)], Train Loss: 0.40665\n","Epoch: 00 [14884/18701 ( 80%)], Train Loss: 0.40649\n","Epoch: 00 [14924/18701 ( 80%)], Train Loss: 0.40691\n","Epoch: 00 [14964/18701 ( 80%)], Train Loss: 0.40686\n","Epoch: 00 [15004/18701 ( 80%)], Train Loss: 0.40653\n","Epoch: 00 [15044/18701 ( 80%)], Train Loss: 0.40588\n","Epoch: 00 [15084/18701 ( 81%)], Train Loss: 0.40557\n","Epoch: 00 [15124/18701 ( 81%)], Train Loss: 0.40542\n","Epoch: 00 [15164/18701 ( 81%)], Train Loss: 0.40488\n","Epoch: 00 [15204/18701 ( 81%)], Train Loss: 0.40459\n","Epoch: 00 [15244/18701 ( 82%)], Train Loss: 0.40449\n","Epoch: 00 [15284/18701 ( 82%)], Train Loss: 0.40392\n","Epoch: 00 [15324/18701 ( 82%)], Train Loss: 0.40400\n","Epoch: 00 [15364/18701 ( 82%)], Train Loss: 0.40440\n","Epoch: 00 [15404/18701 ( 82%)], Train Loss: 0.40385\n","Epoch: 00 [15444/18701 ( 83%)], Train Loss: 0.40389\n","Epoch: 00 [15484/18701 ( 83%)], Train Loss: 0.40402\n","Epoch: 00 [15524/18701 ( 83%)], Train Loss: 0.40359\n","Epoch: 00 [15564/18701 ( 83%)], Train Loss: 0.40322\n","Epoch: 00 [15604/18701 ( 83%)], Train Loss: 0.40259\n","Epoch: 00 [15644/18701 ( 84%)], Train Loss: 0.40234\n","Epoch: 00 [15684/18701 ( 84%)], Train Loss: 0.40210\n","Epoch: 00 [15724/18701 ( 84%)], Train Loss: 0.40218\n","Epoch: 00 [15764/18701 ( 84%)], Train Loss: 0.40216\n","Epoch: 00 [15804/18701 ( 85%)], Train Loss: 0.40227\n","Epoch: 00 [15844/18701 ( 85%)], Train Loss: 0.40183\n","Epoch: 00 [15884/18701 ( 85%)], Train Loss: 0.40164\n","Epoch: 00 [15924/18701 ( 85%)], Train Loss: 0.40137\n","Epoch: 00 [15964/18701 ( 85%)], Train Loss: 0.40095\n","Epoch: 00 [16004/18701 ( 86%)], Train Loss: 0.40101\n","Epoch: 00 [16044/18701 ( 86%)], Train Loss: 0.40123\n","Epoch: 00 [16084/18701 ( 86%)], Train Loss: 0.40118\n","Epoch: 00 [16124/18701 ( 86%)], Train Loss: 0.40119\n","Epoch: 00 [16164/18701 ( 86%)], Train Loss: 0.40117\n","Epoch: 00 [16204/18701 ( 87%)], Train Loss: 0.40130\n","Epoch: 00 [16244/18701 ( 87%)], Train Loss: 0.40096\n","Epoch: 00 [16284/18701 ( 87%)], Train Loss: 0.40064\n","Epoch: 00 [16324/18701 ( 87%)], Train Loss: 0.40043\n","Epoch: 00 [16364/18701 ( 88%)], Train Loss: 0.40079\n","Epoch: 00 [16404/18701 ( 88%)], Train Loss: 0.40070\n","Epoch: 00 [16444/18701 ( 88%)], Train Loss: 0.40033\n","Epoch: 00 [16484/18701 ( 88%)], Train Loss: 0.40023\n","Epoch: 00 [16524/18701 ( 88%)], Train Loss: 0.39987\n","Epoch: 00 [16564/18701 ( 89%)], Train Loss: 0.39987\n","Epoch: 00 [16604/18701 ( 89%)], Train Loss: 0.39952\n","Epoch: 00 [16644/18701 ( 89%)], Train Loss: 0.39975\n","Epoch: 00 [16684/18701 ( 89%)], Train Loss: 0.39984\n","Epoch: 00 [16724/18701 ( 89%)], Train Loss: 0.39975\n","Epoch: 00 [16764/18701 ( 90%)], Train Loss: 0.39961\n","Epoch: 00 [16804/18701 ( 90%)], Train Loss: 0.39903\n","Epoch: 00 [16844/18701 ( 90%)], Train Loss: 0.39875\n","Epoch: 00 [16884/18701 ( 90%)], Train Loss: 0.39847\n","Epoch: 00 [16924/18701 ( 90%)], Train Loss: 0.39819\n","Epoch: 00 [16964/18701 ( 91%)], Train Loss: 0.39794\n","Epoch: 00 [17004/18701 ( 91%)], Train Loss: 0.39733\n","Epoch: 00 [17044/18701 ( 91%)], Train Loss: 0.39784\n","Epoch: 00 [17084/18701 ( 91%)], Train Loss: 0.39738\n","Epoch: 00 [17124/18701 ( 92%)], Train Loss: 0.39705\n","Epoch: 00 [17164/18701 ( 92%)], Train Loss: 0.39681\n","Epoch: 00 [17204/18701 ( 92%)], Train Loss: 0.39644\n","Epoch: 00 [17244/18701 ( 92%)], Train Loss: 0.39633\n","Epoch: 00 [17284/18701 ( 92%)], Train Loss: 0.39625\n","Epoch: 00 [17324/18701 ( 93%)], Train Loss: 0.39582\n","Epoch: 00 [17364/18701 ( 93%)], Train Loss: 0.39548\n","Epoch: 00 [17404/18701 ( 93%)], Train Loss: 0.39542\n","Epoch: 00 [17444/18701 ( 93%)], Train Loss: 0.39514\n","Epoch: 00 [17484/18701 ( 93%)], Train Loss: 0.39496\n","Epoch: 00 [17524/18701 ( 94%)], Train Loss: 0.39443\n","Epoch: 00 [17564/18701 ( 94%)], Train Loss: 0.39423\n","Epoch: 00 [17604/18701 ( 94%)], Train Loss: 0.39428\n","Epoch: 00 [17644/18701 ( 94%)], Train Loss: 0.39377\n","Epoch: 00 [17684/18701 ( 95%)], Train Loss: 0.39329\n","Epoch: 00 [17724/18701 ( 95%)], Train Loss: 0.39293\n","Epoch: 00 [17764/18701 ( 95%)], Train Loss: 0.39287\n","Epoch: 00 [17804/18701 ( 95%)], Train Loss: 0.39281\n","Epoch: 00 [17844/18701 ( 95%)], Train Loss: 0.39248\n","Epoch: 00 [17884/18701 ( 96%)], Train Loss: 0.39219\n","Epoch: 00 [17924/18701 ( 96%)], Train Loss: 0.39160\n","Epoch: 00 [17964/18701 ( 96%)], Train Loss: 0.39099\n","Epoch: 00 [18004/18701 ( 96%)], Train Loss: 0.39063\n","Epoch: 00 [18044/18701 ( 96%)], Train Loss: 0.39018\n","Epoch: 00 [18084/18701 ( 97%)], Train Loss: 0.38971\n","Epoch: 00 [18124/18701 ( 97%)], Train Loss: 0.38940\n","Epoch: 00 [18164/18701 ( 97%)], Train Loss: 0.38920\n","Epoch: 00 [18204/18701 ( 97%)], Train Loss: 0.38878\n","Epoch: 00 [18244/18701 ( 98%)], Train Loss: 0.38904\n","Epoch: 00 [18284/18701 ( 98%)], Train Loss: 0.38881\n","Epoch: 00 [18324/18701 ( 98%)], Train Loss: 0.38899\n","Epoch: 00 [18364/18701 ( 98%)], Train Loss: 0.38878\n","Epoch: 00 [18404/18701 ( 98%)], Train Loss: 0.38868\n","Epoch: 00 [18444/18701 ( 99%)], Train Loss: 0.38867\n","Epoch: 00 [18484/18701 ( 99%)], Train Loss: 0.38863\n","Epoch: 00 [18524/18701 ( 99%)], Train Loss: 0.38879\n","Epoch: 00 [18564/18701 ( 99%)], Train Loss: 0.38844\n","Epoch: 00 [18604/18701 ( 99%)], Train Loss: 0.38824\n","Epoch: 00 [18644/18701 (100%)], Train Loss: 0.38775\n","Epoch: 00 [18684/18701 (100%)], Train Loss: 0.38771\n","Epoch: 00 [18701/18701 (100%)], Train Loss: 0.38760\n","----Validation Results Summary----\n","Epoch: [0] Valid Loss: 0.58509\n","0 Epoch, Best epoch was updated! Valid Loss: 0.58509\n","Saving model checkpoint to chaii-qa-5-fold-xlmroberta-torch-fit/checkpoint-fold-4.\n","\n","Total Training Time: 2612.9915521144867secs, Average Training Time per Epoch: 2612.9915521144867secs.\n","Total Validation Time: 184.45663404464722secs, Average Validation Time per Epoch: 184.45663404464722secs.\n","\n","\n","--------------------------------------------------\n","FOLD: 5\n","--------------------------------------------------\n","Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n","Num examples Train= 23069, Num examples Valid=0\n","Total Training Steps: 2884, Total Warmup Steps: 288\n","Epoch: 00 [    4/23069 (  0%)], Train Loss: 2.92595\n","Epoch: 00 [   44/23069 (  0%)], Train Loss: 2.90102\n","Epoch: 00 [   84/23069 (  0%)], Train Loss: 2.87705\n","Epoch: 00 [  124/23069 (  1%)], Train Loss: 2.84823\n","Epoch: 00 [  164/23069 (  1%)], Train Loss: 2.81063\n","Epoch: 00 [  204/23069 (  1%)], Train Loss: 2.76044\n","Epoch: 00 [  244/23069 (  1%)], Train Loss: 2.70359\n","Epoch: 00 [  284/23069 (  1%)], Train Loss: 2.62044\n","Epoch: 00 [  324/23069 (  1%)], Train Loss: 2.52563\n","Epoch: 00 [  364/23069 (  2%)], Train Loss: 2.44480\n","Epoch: 00 [  404/23069 (  2%)], Train Loss: 2.33778\n","Epoch: 00 [  444/23069 (  2%)], Train Loss: 2.22460\n","Epoch: 00 [  484/23069 (  2%)], Train Loss: 2.11602\n","Epoch: 00 [  524/23069 (  2%)], Train Loss: 2.00113\n","Epoch: 00 [  564/23069 (  2%)], Train Loss: 1.89111\n","Epoch: 00 [  604/23069 (  3%)], Train Loss: 1.79100\n","Epoch: 00 [  644/23069 (  3%)], Train Loss: 1.71909\n","Epoch: 00 [  684/23069 (  3%)], Train Loss: 1.64372\n","Epoch: 00 [  724/23069 (  3%)], Train Loss: 1.58388\n","Epoch: 00 [  764/23069 (  3%)], Train Loss: 1.53190\n","Epoch: 00 [  804/23069 (  3%)], Train Loss: 1.47848\n","Epoch: 00 [  844/23069 (  4%)], Train Loss: 1.42825\n","Epoch: 00 [  884/23069 (  4%)], Train Loss: 1.37702\n","Epoch: 00 [  924/23069 (  4%)], Train Loss: 1.34172\n","Epoch: 00 [  964/23069 (  4%)], Train Loss: 1.30115\n","Epoch: 00 [ 1004/23069 (  4%)], Train Loss: 1.26790\n","Epoch: 00 [ 1044/23069 (  5%)], Train Loss: 1.22711\n","Epoch: 00 [ 1084/23069 (  5%)], Train Loss: 1.20627\n","Epoch: 00 [ 1124/23069 (  5%)], Train Loss: 1.18250\n","Epoch: 00 [ 1164/23069 (  5%)], Train Loss: 1.16074\n","Epoch: 00 [ 1204/23069 (  5%)], Train Loss: 1.13439\n","Epoch: 00 [ 1244/23069 (  5%)], Train Loss: 1.10710\n","Epoch: 00 [ 1284/23069 (  6%)], Train Loss: 1.08647\n","Epoch: 00 [ 1324/23069 (  6%)], Train Loss: 1.06405\n","Epoch: 00 [ 1364/23069 (  6%)], Train Loss: 1.04598\n","Epoch: 00 [ 1404/23069 (  6%)], Train Loss: 1.02811\n","Epoch: 00 [ 1444/23069 (  6%)], Train Loss: 1.01619\n","Epoch: 00 [ 1484/23069 (  6%)], Train Loss: 0.99852\n","Epoch: 00 [ 1524/23069 (  7%)], Train Loss: 0.98686\n","Epoch: 00 [ 1564/23069 (  7%)], Train Loss: 0.97369\n","Epoch: 00 [ 1604/23069 (  7%)], Train Loss: 0.96544\n","Epoch: 00 [ 1644/23069 (  7%)], Train Loss: 0.95208\n","Epoch: 00 [ 1684/23069 (  7%)], Train Loss: 0.93891\n","Epoch: 00 [ 1724/23069 (  7%)], Train Loss: 0.93081\n","Epoch: 00 [ 1764/23069 (  8%)], Train Loss: 0.92143\n","Epoch: 00 [ 1804/23069 (  8%)], Train Loss: 0.90885\n","Epoch: 00 [ 1844/23069 (  8%)], Train Loss: 0.89775\n","Epoch: 00 [ 1884/23069 (  8%)], Train Loss: 0.88948\n","Epoch: 00 [ 1924/23069 (  8%)], Train Loss: 0.88065\n","Epoch: 00 [ 1964/23069 (  9%)], Train Loss: 0.86974\n","Epoch: 00 [ 2004/23069 (  9%)], Train Loss: 0.86197\n","Epoch: 00 [ 2044/23069 (  9%)], Train Loss: 0.85305\n","Epoch: 00 [ 2084/23069 (  9%)], Train Loss: 0.84635\n","Epoch: 00 [ 2124/23069 (  9%)], Train Loss: 0.84025\n","Epoch: 00 [ 2164/23069 (  9%)], Train Loss: 0.83594\n","Epoch: 00 [ 2204/23069 ( 10%)], Train Loss: 0.83456\n","Epoch: 00 [ 2244/23069 ( 10%)], Train Loss: 0.83017\n","Epoch: 00 [ 2284/23069 ( 10%)], Train Loss: 0.82537\n","Epoch: 00 [ 2324/23069 ( 10%)], Train Loss: 0.81891\n","Epoch: 00 [ 2364/23069 ( 10%)], Train Loss: 0.81158\n","Epoch: 00 [ 2404/23069 ( 10%)], Train Loss: 0.80307\n","Epoch: 00 [ 2444/23069 ( 11%)], Train Loss: 0.79766\n","Epoch: 00 [ 2484/23069 ( 11%)], Train Loss: 0.79405\n","Epoch: 00 [ 2524/23069 ( 11%)], Train Loss: 0.78805\n","Epoch: 00 [ 2564/23069 ( 11%)], Train Loss: 0.78114\n","Epoch: 00 [ 2604/23069 ( 11%)], Train Loss: 0.77313\n","Epoch: 00 [ 2644/23069 ( 11%)], Train Loss: 0.76840\n","Epoch: 00 [ 2684/23069 ( 12%)], Train Loss: 0.76262\n","Epoch: 00 [ 2724/23069 ( 12%)], Train Loss: 0.75653\n","Epoch: 00 [ 2764/23069 ( 12%)], Train Loss: 0.75230\n","Epoch: 00 [ 2804/23069 ( 12%)], Train Loss: 0.74696\n","Epoch: 00 [ 2844/23069 ( 12%)], Train Loss: 0.74177\n","Epoch: 00 [ 2884/23069 ( 13%)], Train Loss: 0.73602\n","Epoch: 00 [ 2924/23069 ( 13%)], Train Loss: 0.73124\n","Epoch: 00 [ 2964/23069 ( 13%)], Train Loss: 0.72466\n","Epoch: 00 [ 3004/23069 ( 13%)], Train Loss: 0.72257\n","Epoch: 00 [ 3044/23069 ( 13%)], Train Loss: 0.71861\n","Epoch: 00 [ 3084/23069 ( 13%)], Train Loss: 0.71292\n","Epoch: 00 [ 3124/23069 ( 14%)], Train Loss: 0.70644\n","Epoch: 00 [ 3164/23069 ( 14%)], Train Loss: 0.70018\n","Epoch: 00 [ 3204/23069 ( 14%)], Train Loss: 0.69442\n","Epoch: 00 [ 3244/23069 ( 14%)], Train Loss: 0.69317\n","Epoch: 00 [ 3284/23069 ( 14%)], Train Loss: 0.69053\n","Epoch: 00 [ 3324/23069 ( 14%)], Train Loss: 0.68667\n","Epoch: 00 [ 3364/23069 ( 15%)], Train Loss: 0.68319\n","Epoch: 00 [ 3404/23069 ( 15%)], Train Loss: 0.68090\n","Epoch: 00 [ 3444/23069 ( 15%)], Train Loss: 0.67708\n","Epoch: 00 [ 3484/23069 ( 15%)], Train Loss: 0.67360\n","Epoch: 00 [ 3524/23069 ( 15%)], Train Loss: 0.67033\n","Epoch: 00 [ 3564/23069 ( 15%)], Train Loss: 0.66879\n","Epoch: 00 [ 3604/23069 ( 16%)], Train Loss: 0.66857\n","Epoch: 00 [ 3644/23069 ( 16%)], Train Loss: 0.66706\n","Epoch: 00 [ 3684/23069 ( 16%)], Train Loss: 0.66420\n","Epoch: 00 [ 3724/23069 ( 16%)], Train Loss: 0.66127\n","Epoch: 00 [ 3764/23069 ( 16%)], Train Loss: 0.65794\n","Epoch: 00 [ 3804/23069 ( 16%)], Train Loss: 0.65470\n","Epoch: 00 [ 3844/23069 ( 17%)], Train Loss: 0.65092\n","Epoch: 00 [ 3884/23069 ( 17%)], Train Loss: 0.64953\n","Epoch: 00 [ 3924/23069 ( 17%)], Train Loss: 0.64702\n","Epoch: 00 [ 3964/23069 ( 17%)], Train Loss: 0.64391\n","Epoch: 00 [ 4004/23069 ( 17%)], Train Loss: 0.64082\n","Epoch: 00 [ 4044/23069 ( 18%)], Train Loss: 0.63952\n","Epoch: 00 [ 4084/23069 ( 18%)], Train Loss: 0.63800\n","Epoch: 00 [ 4124/23069 ( 18%)], Train Loss: 0.63651\n","Epoch: 00 [ 4164/23069 ( 18%)], Train Loss: 0.63444\n","Epoch: 00 [ 4204/23069 ( 18%)], Train Loss: 0.63180\n","Epoch: 00 [ 4244/23069 ( 18%)], Train Loss: 0.63045\n","Epoch: 00 [ 4284/23069 ( 19%)], Train Loss: 0.62824\n","Epoch: 00 [ 4324/23069 ( 19%)], Train Loss: 0.62563\n","Epoch: 00 [ 4364/23069 ( 19%)], Train Loss: 0.62212\n","Epoch: 00 [ 4404/23069 ( 19%)], Train Loss: 0.62054\n","Epoch: 00 [ 4444/23069 ( 19%)], Train Loss: 0.61846\n","Epoch: 00 [ 4484/23069 ( 19%)], Train Loss: 0.61490\n","Epoch: 00 [ 4524/23069 ( 20%)], Train Loss: 0.61236\n","Epoch: 00 [ 4564/23069 ( 20%)], Train Loss: 0.60869\n","Epoch: 00 [ 4604/23069 ( 20%)], Train Loss: 0.60764\n","Epoch: 00 [ 4644/23069 ( 20%)], Train Loss: 0.60477\n","Epoch: 00 [ 4684/23069 ( 20%)], Train Loss: 0.60292\n","Epoch: 00 [ 4724/23069 ( 20%)], Train Loss: 0.59997\n","Epoch: 00 [ 4764/23069 ( 21%)], Train Loss: 0.59782\n","Epoch: 00 [ 4804/23069 ( 21%)], Train Loss: 0.59531\n","Epoch: 00 [ 4844/23069 ( 21%)], Train Loss: 0.59168\n","Epoch: 00 [ 4884/23069 ( 21%)], Train Loss: 0.58797\n","Epoch: 00 [ 4924/23069 ( 21%)], Train Loss: 0.58575\n","Epoch: 00 [ 4964/23069 ( 22%)], Train Loss: 0.58307\n","Epoch: 00 [ 5004/23069 ( 22%)], Train Loss: 0.58187\n","Epoch: 00 [ 5044/23069 ( 22%)], Train Loss: 0.58100\n","Epoch: 00 [ 5084/23069 ( 22%)], Train Loss: 0.58004\n","Epoch: 00 [ 5124/23069 ( 22%)], Train Loss: 0.57789\n","Epoch: 00 [ 5164/23069 ( 22%)], Train Loss: 0.57777\n","Epoch: 00 [ 5204/23069 ( 23%)], Train Loss: 0.57554\n","Epoch: 00 [ 5244/23069 ( 23%)], Train Loss: 0.57420\n","Epoch: 00 [ 5284/23069 ( 23%)], Train Loss: 0.57177\n","Epoch: 00 [ 5324/23069 ( 23%)], Train Loss: 0.56929\n","Epoch: 00 [ 5364/23069 ( 23%)], Train Loss: 0.56663\n","Epoch: 00 [ 5404/23069 ( 23%)], Train Loss: 0.56423\n","Epoch: 00 [ 5444/23069 ( 24%)], Train Loss: 0.56254\n","Epoch: 00 [ 5484/23069 ( 24%)], Train Loss: 0.56320\n","Epoch: 00 [ 5524/23069 ( 24%)], Train Loss: 0.56178\n","Epoch: 00 [ 5564/23069 ( 24%)], Train Loss: 0.56033\n","Epoch: 00 [ 5604/23069 ( 24%)], Train Loss: 0.55904\n","Epoch: 00 [ 5644/23069 ( 24%)], Train Loss: 0.55630\n","Epoch: 00 [ 5684/23069 ( 25%)], Train Loss: 0.55637\n","Epoch: 00 [ 5724/23069 ( 25%)], Train Loss: 0.55456\n","Epoch: 00 [ 5764/23069 ( 25%)], Train Loss: 0.55385\n","Epoch: 00 [ 5804/23069 ( 25%)], Train Loss: 0.55389\n","Epoch: 00 [ 5844/23069 ( 25%)], Train Loss: 0.55169\n","Epoch: 00 [ 5884/23069 ( 26%)], Train Loss: 0.55073\n","Epoch: 00 [ 5924/23069 ( 26%)], Train Loss: 0.54933\n","Epoch: 00 [ 5964/23069 ( 26%)], Train Loss: 0.54808\n","Epoch: 00 [ 6004/23069 ( 26%)], Train Loss: 0.54647\n","Epoch: 00 [ 6044/23069 ( 26%)], Train Loss: 0.54515\n","Epoch: 00 [ 6084/23069 ( 26%)], Train Loss: 0.54320\n","Epoch: 00 [ 6124/23069 ( 27%)], Train Loss: 0.54101\n","Epoch: 00 [ 6164/23069 ( 27%)], Train Loss: 0.54051\n","Epoch: 00 [ 6204/23069 ( 27%)], Train Loss: 0.53906\n","Epoch: 00 [ 6244/23069 ( 27%)], Train Loss: 0.53861\n","Epoch: 00 [ 6284/23069 ( 27%)], Train Loss: 0.53686\n","Epoch: 00 [ 6324/23069 ( 27%)], Train Loss: 0.53436\n","Epoch: 00 [ 6364/23069 ( 28%)], Train Loss: 0.53353\n","Epoch: 00 [ 6404/23069 ( 28%)], Train Loss: 0.53491\n","Epoch: 00 [ 6444/23069 ( 28%)], Train Loss: 0.53286\n","Epoch: 00 [ 6484/23069 ( 28%)], Train Loss: 0.53278\n","Epoch: 00 [ 6524/23069 ( 28%)], Train Loss: 0.53161\n","Epoch: 00 [ 6564/23069 ( 28%)], Train Loss: 0.53103\n","Epoch: 00 [ 6604/23069 ( 29%)], Train Loss: 0.53028\n","Epoch: 00 [ 6644/23069 ( 29%)], Train Loss: 0.52911\n","Epoch: 00 [ 6684/23069 ( 29%)], Train Loss: 0.52892\n","Epoch: 00 [ 6724/23069 ( 29%)], Train Loss: 0.52798\n","Epoch: 00 [ 6764/23069 ( 29%)], Train Loss: 0.52695\n","Epoch: 00 [ 6804/23069 ( 29%)], Train Loss: 0.52623\n","Epoch: 00 [ 6844/23069 ( 30%)], Train Loss: 0.52544\n","Epoch: 00 [ 6884/23069 ( 30%)], Train Loss: 0.52463\n","Epoch: 00 [ 6924/23069 ( 30%)], Train Loss: 0.52270\n","Epoch: 00 [ 6964/23069 ( 30%)], Train Loss: 0.52108\n","Epoch: 00 [ 7004/23069 ( 30%)], Train Loss: 0.51975\n","Epoch: 00 [ 7044/23069 ( 31%)], Train Loss: 0.51769\n","Epoch: 00 [ 7084/23069 ( 31%)], Train Loss: 0.51639\n","Epoch: 00 [ 7124/23069 ( 31%)], Train Loss: 0.51483\n","Epoch: 00 [ 7164/23069 ( 31%)], Train Loss: 0.51316\n","Epoch: 00 [ 7204/23069 ( 31%)], Train Loss: 0.51253\n","Epoch: 00 [ 7244/23069 ( 31%)], Train Loss: 0.51158\n","Epoch: 00 [ 7284/23069 ( 32%)], Train Loss: 0.51005\n","Epoch: 00 [ 7324/23069 ( 32%)], Train Loss: 0.50958\n","Epoch: 00 [ 7364/23069 ( 32%)], Train Loss: 0.50922\n","Epoch: 00 [ 7404/23069 ( 32%)], Train Loss: 0.50779\n","Epoch: 00 [ 7444/23069 ( 32%)], Train Loss: 0.50759\n","Epoch: 00 [ 7484/23069 ( 32%)], Train Loss: 0.50635\n","Epoch: 00 [ 7524/23069 ( 33%)], Train Loss: 0.50534\n","Epoch: 00 [ 7564/23069 ( 33%)], Train Loss: 0.50404\n","Epoch: 00 [ 7604/23069 ( 33%)], Train Loss: 0.50256\n","Epoch: 00 [ 7644/23069 ( 33%)], Train Loss: 0.50104\n","Epoch: 00 [ 7684/23069 ( 33%)], Train Loss: 0.50164\n","Epoch: 00 [ 7724/23069 ( 33%)], Train Loss: 0.50036\n","Epoch: 00 [ 7764/23069 ( 34%)], Train Loss: 0.50011\n","Epoch: 00 [ 7804/23069 ( 34%)], Train Loss: 0.50023\n","Epoch: 00 [ 7844/23069 ( 34%)], Train Loss: 0.49917\n","Epoch: 00 [ 7884/23069 ( 34%)], Train Loss: 0.49833\n","Epoch: 00 [ 7924/23069 ( 34%)], Train Loss: 0.49851\n","Epoch: 00 [ 7964/23069 ( 35%)], Train Loss: 0.49859\n","Epoch: 00 [ 8004/23069 ( 35%)], Train Loss: 0.49873\n","Epoch: 00 [ 8044/23069 ( 35%)], Train Loss: 0.49805\n","Epoch: 00 [ 8084/23069 ( 35%)], Train Loss: 0.49866\n","Epoch: 00 [ 8124/23069 ( 35%)], Train Loss: 0.49875\n","Epoch: 00 [ 8164/23069 ( 35%)], Train Loss: 0.49866\n","Epoch: 00 [ 8204/23069 ( 36%)], Train Loss: 0.49758\n","Epoch: 00 [ 8244/23069 ( 36%)], Train Loss: 0.49627\n","Epoch: 00 [ 8284/23069 ( 36%)], Train Loss: 0.49554\n","Epoch: 00 [ 8324/23069 ( 36%)], Train Loss: 0.49444\n","Epoch: 00 [ 8364/23069 ( 36%)], Train Loss: 0.49315\n","Epoch: 00 [ 8404/23069 ( 36%)], Train Loss: 0.49261\n","Epoch: 00 [ 8444/23069 ( 37%)], Train Loss: 0.49208\n","Epoch: 00 [ 8484/23069 ( 37%)], Train Loss: 0.49148\n","Epoch: 00 [ 8524/23069 ( 37%)], Train Loss: 0.49117\n","Epoch: 00 [ 8564/23069 ( 37%)], Train Loss: 0.49043\n","Epoch: 00 [ 8604/23069 ( 37%)], Train Loss: 0.48972\n","Epoch: 00 [ 8644/23069 ( 37%)], Train Loss: 0.48955\n","Epoch: 00 [ 8684/23069 ( 38%)], Train Loss: 0.48852\n","Epoch: 00 [ 8724/23069 ( 38%)], Train Loss: 0.48814\n","Epoch: 00 [ 8764/23069 ( 38%)], Train Loss: 0.48772\n","Epoch: 00 [ 8804/23069 ( 38%)], Train Loss: 0.48676\n","Epoch: 00 [ 8844/23069 ( 38%)], Train Loss: 0.48662\n","Epoch: 00 [ 8884/23069 ( 39%)], Train Loss: 0.48581\n","Epoch: 00 [ 8924/23069 ( 39%)], Train Loss: 0.48482\n","Epoch: 00 [ 8964/23069 ( 39%)], Train Loss: 0.48407\n","Epoch: 00 [ 9004/23069 ( 39%)], Train Loss: 0.48347\n","Epoch: 00 [ 9044/23069 ( 39%)], Train Loss: 0.48281\n","Epoch: 00 [ 9084/23069 ( 39%)], Train Loss: 0.48283\n","Epoch: 00 [ 9124/23069 ( 40%)], Train Loss: 0.48172\n","Epoch: 00 [ 9164/23069 ( 40%)], Train Loss: 0.48126\n","Epoch: 00 [ 9204/23069 ( 40%)], Train Loss: 0.48108\n","Epoch: 00 [ 9244/23069 ( 40%)], Train Loss: 0.48116\n","Epoch: 00 [ 9284/23069 ( 40%)], Train Loss: 0.48054\n","Epoch: 00 [ 9324/23069 ( 40%)], Train Loss: 0.47992\n","Epoch: 00 [ 9364/23069 ( 41%)], Train Loss: 0.47881\n","Epoch: 00 [ 9404/23069 ( 41%)], Train Loss: 0.47847\n","Epoch: 00 [ 9444/23069 ( 41%)], Train Loss: 0.47834\n","Epoch: 00 [ 9484/23069 ( 41%)], Train Loss: 0.47742\n","Epoch: 00 [ 9524/23069 ( 41%)], Train Loss: 0.47690\n","Epoch: 00 [ 9564/23069 ( 41%)], Train Loss: 0.47577\n","Epoch: 00 [ 9604/23069 ( 42%)], Train Loss: 0.47509\n","Epoch: 00 [ 9644/23069 ( 42%)], Train Loss: 0.47402\n","Epoch: 00 [ 9684/23069 ( 42%)], Train Loss: 0.47367\n","Epoch: 00 [ 9724/23069 ( 42%)], Train Loss: 0.47339\n","Epoch: 00 [ 9764/23069 ( 42%)], Train Loss: 0.47253\n","Epoch: 00 [ 9804/23069 ( 42%)], Train Loss: 0.47155\n","Epoch: 00 [ 9844/23069 ( 43%)], Train Loss: 0.47027\n","Epoch: 00 [ 9884/23069 ( 43%)], Train Loss: 0.46945\n","Epoch: 00 [ 9924/23069 ( 43%)], Train Loss: 0.46914\n","Epoch: 00 [ 9964/23069 ( 43%)], Train Loss: 0.46959\n","Epoch: 00 [10004/23069 ( 43%)], Train Loss: 0.46939\n","Epoch: 00 [10044/23069 ( 44%)], Train Loss: 0.46875\n","Epoch: 00 [10084/23069 ( 44%)], Train Loss: 0.46866\n","Epoch: 00 [10124/23069 ( 44%)], Train Loss: 0.46788\n","Epoch: 00 [10164/23069 ( 44%)], Train Loss: 0.46786\n","Epoch: 00 [10204/23069 ( 44%)], Train Loss: 0.46808\n","Epoch: 00 [10244/23069 ( 44%)], Train Loss: 0.46708\n","Epoch: 00 [10284/23069 ( 45%)], Train Loss: 0.46652\n","Epoch: 00 [10324/23069 ( 45%)], Train Loss: 0.46555\n","Epoch: 00 [10364/23069 ( 45%)], Train Loss: 0.46565\n","Epoch: 00 [10404/23069 ( 45%)], Train Loss: 0.46511\n","Epoch: 00 [10444/23069 ( 45%)], Train Loss: 0.46533\n","Epoch: 00 [10484/23069 ( 45%)], Train Loss: 0.46458\n","Epoch: 00 [10524/23069 ( 46%)], Train Loss: 0.46407\n","Epoch: 00 [10564/23069 ( 46%)], Train Loss: 0.46376\n","Epoch: 00 [10604/23069 ( 46%)], Train Loss: 0.46276\n","Epoch: 00 [10644/23069 ( 46%)], Train Loss: 0.46215\n","Epoch: 00 [10684/23069 ( 46%)], Train Loss: 0.46099\n","Epoch: 00 [10724/23069 ( 46%)], Train Loss: 0.46061\n","Epoch: 00 [10764/23069 ( 47%)], Train Loss: 0.46026\n","Epoch: 00 [10804/23069 ( 47%)], Train Loss: 0.46021\n","Epoch: 00 [10844/23069 ( 47%)], Train Loss: 0.46028\n","Epoch: 00 [10884/23069 ( 47%)], Train Loss: 0.46050\n","Epoch: 00 [10924/23069 ( 47%)], Train Loss: 0.45999\n","Epoch: 00 [10964/23069 ( 48%)], Train Loss: 0.45929\n","Epoch: 00 [11004/23069 ( 48%)], Train Loss: 0.45888\n","Epoch: 00 [11044/23069 ( 48%)], Train Loss: 0.45879\n","Epoch: 00 [11084/23069 ( 48%)], Train Loss: 0.45852\n","Epoch: 00 [11124/23069 ( 48%)], Train Loss: 0.45829\n","Epoch: 00 [11164/23069 ( 48%)], Train Loss: 0.45782\n","Epoch: 00 [11204/23069 ( 49%)], Train Loss: 0.45745\n","Epoch: 00 [11244/23069 ( 49%)], Train Loss: 0.45648\n","Epoch: 00 [11284/23069 ( 49%)], Train Loss: 0.45604\n","Epoch: 00 [11324/23069 ( 49%)], Train Loss: 0.45610\n","Epoch: 00 [11364/23069 ( 49%)], Train Loss: 0.45573\n","Epoch: 00 [11404/23069 ( 49%)], Train Loss: 0.45561\n","Epoch: 00 [11444/23069 ( 50%)], Train Loss: 0.45523\n","Epoch: 00 [11484/23069 ( 50%)], Train Loss: 0.45505\n","Epoch: 00 [11524/23069 ( 50%)], Train Loss: 0.45439\n","Epoch: 00 [11564/23069 ( 50%)], Train Loss: 0.45356\n","Epoch: 00 [11604/23069 ( 50%)], Train Loss: 0.45328\n","Epoch: 00 [11644/23069 ( 50%)], Train Loss: 0.45329\n","Epoch: 00 [11684/23069 ( 51%)], Train Loss: 0.45337\n","Epoch: 00 [11724/23069 ( 51%)], Train Loss: 0.45278\n","Epoch: 00 [11764/23069 ( 51%)], Train Loss: 0.45257\n","Epoch: 00 [11804/23069 ( 51%)], Train Loss: 0.45157\n","Epoch: 00 [11844/23069 ( 51%)], Train Loss: 0.45072\n","Epoch: 00 [11884/23069 ( 52%)], Train Loss: 0.44984\n","Epoch: 00 [11924/23069 ( 52%)], Train Loss: 0.44918\n","Epoch: 00 [11964/23069 ( 52%)], Train Loss: 0.44850\n","Epoch: 00 [12004/23069 ( 52%)], Train Loss: 0.44840\n","Epoch: 00 [12044/23069 ( 52%)], Train Loss: 0.44783\n","Epoch: 00 [12084/23069 ( 52%)], Train Loss: 0.44721\n","Epoch: 00 [12124/23069 ( 53%)], Train Loss: 0.44707\n","Epoch: 00 [12164/23069 ( 53%)], Train Loss: 0.44713\n","Epoch: 00 [12204/23069 ( 53%)], Train Loss: 0.44656\n","Epoch: 00 [12244/23069 ( 53%)], Train Loss: 0.44620\n","Epoch: 00 [12284/23069 ( 53%)], Train Loss: 0.44575\n","Epoch: 00 [12324/23069 ( 53%)], Train Loss: 0.44515\n","Epoch: 00 [12364/23069 ( 54%)], Train Loss: 0.44473\n","Epoch: 00 [12404/23069 ( 54%)], Train Loss: 0.44461\n","Epoch: 00 [12444/23069 ( 54%)], Train Loss: 0.44455\n","Epoch: 00 [12484/23069 ( 54%)], Train Loss: 0.44376\n","Epoch: 00 [12524/23069 ( 54%)], Train Loss: 0.44378\n","Epoch: 00 [12564/23069 ( 54%)], Train Loss: 0.44319\n","Epoch: 00 [12604/23069 ( 55%)], Train Loss: 0.44327\n","Epoch: 00 [12644/23069 ( 55%)], Train Loss: 0.44313\n","Epoch: 00 [12684/23069 ( 55%)], Train Loss: 0.44244\n","Epoch: 00 [12724/23069 ( 55%)], Train Loss: 0.44229\n","Epoch: 00 [12764/23069 ( 55%)], Train Loss: 0.44186\n","Epoch: 00 [12804/23069 ( 56%)], Train Loss: 0.44141\n","Epoch: 00 [12844/23069 ( 56%)], Train Loss: 0.44087\n","Epoch: 00 [12884/23069 ( 56%)], Train Loss: 0.44054\n","Epoch: 00 [12924/23069 ( 56%)], Train Loss: 0.44024\n","Epoch: 00 [12964/23069 ( 56%)], Train Loss: 0.43957\n","Epoch: 00 [13004/23069 ( 56%)], Train Loss: 0.43955\n","Epoch: 00 [13044/23069 ( 57%)], Train Loss: 0.43939\n","Epoch: 00 [13084/23069 ( 57%)], Train Loss: 0.43952\n","Epoch: 00 [13124/23069 ( 57%)], Train Loss: 0.43900\n","Epoch: 00 [13164/23069 ( 57%)], Train Loss: 0.43855\n","Epoch: 00 [13204/23069 ( 57%)], Train Loss: 0.43829\n","Epoch: 00 [13244/23069 ( 57%)], Train Loss: 0.43783\n","Epoch: 00 [13284/23069 ( 58%)], Train Loss: 0.43755\n","Epoch: 00 [13324/23069 ( 58%)], Train Loss: 0.43712\n","Epoch: 00 [13364/23069 ( 58%)], Train Loss: 0.43719\n","Epoch: 00 [13404/23069 ( 58%)], Train Loss: 0.43721\n","Epoch: 00 [13444/23069 ( 58%)], Train Loss: 0.43688\n","Epoch: 00 [13484/23069 ( 58%)], Train Loss: 0.43679\n","Epoch: 00 [13524/23069 ( 59%)], Train Loss: 0.43647\n","Epoch: 00 [13564/23069 ( 59%)], Train Loss: 0.43629\n","Epoch: 00 [13604/23069 ( 59%)], Train Loss: 0.43610\n","Epoch: 00 [13644/23069 ( 59%)], Train Loss: 0.43574\n","Epoch: 00 [13684/23069 ( 59%)], Train Loss: 0.43539\n","Epoch: 00 [13724/23069 ( 59%)], Train Loss: 0.43450\n","Epoch: 00 [13764/23069 ( 60%)], Train Loss: 0.43412\n","Epoch: 00 [13804/23069 ( 60%)], Train Loss: 0.43370\n","Epoch: 00 [13844/23069 ( 60%)], Train Loss: 0.43271\n","Epoch: 00 [13884/23069 ( 60%)], Train Loss: 0.43280\n","Epoch: 00 [13924/23069 ( 60%)], Train Loss: 0.43283\n","Epoch: 00 [13964/23069 ( 61%)], Train Loss: 0.43272\n","Epoch: 00 [14004/23069 ( 61%)], Train Loss: 0.43219\n","Epoch: 00 [14044/23069 ( 61%)], Train Loss: 0.43161\n","Epoch: 00 [14084/23069 ( 61%)], Train Loss: 0.43153\n","Epoch: 00 [14124/23069 ( 61%)], Train Loss: 0.43091\n","Epoch: 00 [14164/23069 ( 61%)], Train Loss: 0.43092\n","Epoch: 00 [14204/23069 ( 62%)], Train Loss: 0.43014\n","Epoch: 00 [14244/23069 ( 62%)], Train Loss: 0.42947\n","Epoch: 00 [14284/23069 ( 62%)], Train Loss: 0.42902\n","Epoch: 00 [14324/23069 ( 62%)], Train Loss: 0.42922\n","Epoch: 00 [14364/23069 ( 62%)], Train Loss: 0.42876\n","Epoch: 00 [14404/23069 ( 62%)], Train Loss: 0.42832\n","Epoch: 00 [14444/23069 ( 63%)], Train Loss: 0.42835\n","Epoch: 00 [14484/23069 ( 63%)], Train Loss: 0.42789\n","Epoch: 00 [14524/23069 ( 63%)], Train Loss: 0.42772\n","Epoch: 00 [14564/23069 ( 63%)], Train Loss: 0.42721\n","Epoch: 00 [14604/23069 ( 63%)], Train Loss: 0.42684\n","Epoch: 00 [14644/23069 ( 63%)], Train Loss: 0.42698\n","Epoch: 00 [14684/23069 ( 64%)], Train Loss: 0.42658\n","Epoch: 00 [14724/23069 ( 64%)], Train Loss: 0.42643\n","Epoch: 00 [14764/23069 ( 64%)], Train Loss: 0.42612\n","Epoch: 00 [14804/23069 ( 64%)], Train Loss: 0.42589\n","Epoch: 00 [14844/23069 ( 64%)], Train Loss: 0.42579\n","Epoch: 00 [14884/23069 ( 65%)], Train Loss: 0.42601\n","Epoch: 00 [14924/23069 ( 65%)], Train Loss: 0.42565\n","Epoch: 00 [14964/23069 ( 65%)], Train Loss: 0.42531\n","Epoch: 00 [15004/23069 ( 65%)], Train Loss: 0.42515\n","Epoch: 00 [15044/23069 ( 65%)], Train Loss: 0.42460\n","Epoch: 00 [15084/23069 ( 65%)], Train Loss: 0.42465\n","Epoch: 00 [15124/23069 ( 66%)], Train Loss: 0.42435\n","Epoch: 00 [15164/23069 ( 66%)], Train Loss: 0.42384\n","Epoch: 00 [15204/23069 ( 66%)], Train Loss: 0.42384\n","Epoch: 00 [15244/23069 ( 66%)], Train Loss: 0.42319\n","Epoch: 00 [15284/23069 ( 66%)], Train Loss: 0.42301\n","Epoch: 00 [15324/23069 ( 66%)], Train Loss: 0.42303\n","Epoch: 00 [15364/23069 ( 67%)], Train Loss: 0.42314\n","Epoch: 00 [15404/23069 ( 67%)], Train Loss: 0.42284\n","Epoch: 00 [15444/23069 ( 67%)], Train Loss: 0.42282\n","Epoch: 00 [15484/23069 ( 67%)], Train Loss: 0.42247\n","Epoch: 00 [15524/23069 ( 67%)], Train Loss: 0.42185\n","Epoch: 00 [15564/23069 ( 67%)], Train Loss: 0.42133\n","Epoch: 00 [15604/23069 ( 68%)], Train Loss: 0.42103\n","Epoch: 00 [15644/23069 ( 68%)], Train Loss: 0.42031\n","Epoch: 00 [15684/23069 ( 68%)], Train Loss: 0.42003\n","Epoch: 00 [15724/23069 ( 68%)], Train Loss: 0.42024\n","Epoch: 00 [15764/23069 ( 68%)], Train Loss: 0.42014\n","Epoch: 00 [15804/23069 ( 69%)], Train Loss: 0.41991\n","Epoch: 00 [15844/23069 ( 69%)], Train Loss: 0.41964\n","Epoch: 00 [15884/23069 ( 69%)], Train Loss: 0.41954\n","Epoch: 00 [15924/23069 ( 69%)], Train Loss: 0.41958\n","Epoch: 00 [15964/23069 ( 69%)], Train Loss: 0.41916\n","Epoch: 00 [16004/23069 ( 69%)], Train Loss: 0.41894\n","Epoch: 00 [16044/23069 ( 70%)], Train Loss: 0.41843\n","Epoch: 00 [16084/23069 ( 70%)], Train Loss: 0.41793\n","Epoch: 00 [16124/23069 ( 70%)], Train Loss: 0.41735\n","Epoch: 00 [16164/23069 ( 70%)], Train Loss: 0.41744\n","Epoch: 00 [16204/23069 ( 70%)], Train Loss: 0.41680\n","Epoch: 00 [16244/23069 ( 70%)], Train Loss: 0.41644\n","Epoch: 00 [16284/23069 ( 71%)], Train Loss: 0.41634\n","Epoch: 00 [16324/23069 ( 71%)], Train Loss: 0.41619\n","Epoch: 00 [16364/23069 ( 71%)], Train Loss: 0.41586\n","Epoch: 00 [16404/23069 ( 71%)], Train Loss: 0.41525\n","Epoch: 00 [16444/23069 ( 71%)], Train Loss: 0.41492\n","Epoch: 00 [16484/23069 ( 71%)], Train Loss: 0.41453\n","Epoch: 00 [16524/23069 ( 72%)], Train Loss: 0.41427\n","Epoch: 00 [16564/23069 ( 72%)], Train Loss: 0.41394\n","Epoch: 00 [16604/23069 ( 72%)], Train Loss: 0.41362\n","Epoch: 00 [16644/23069 ( 72%)], Train Loss: 0.41356\n","Epoch: 00 [16684/23069 ( 72%)], Train Loss: 0.41327\n","Epoch: 00 [16724/23069 ( 72%)], Train Loss: 0.41276\n","Epoch: 00 [16764/23069 ( 73%)], Train Loss: 0.41231\n","Epoch: 00 [16804/23069 ( 73%)], Train Loss: 0.41190\n","Epoch: 00 [16844/23069 ( 73%)], Train Loss: 0.41172\n","Epoch: 00 [16884/23069 ( 73%)], Train Loss: 0.41149\n","Epoch: 00 [16924/23069 ( 73%)], Train Loss: 0.41138\n","Epoch: 00 [16964/23069 ( 74%)], Train Loss: 0.41120\n","Epoch: 00 [17004/23069 ( 74%)], Train Loss: 0.41084\n","Epoch: 00 [17044/23069 ( 74%)], Train Loss: 0.41060\n","Epoch: 00 [17084/23069 ( 74%)], Train Loss: 0.40995\n","Epoch: 00 [17124/23069 ( 74%)], Train Loss: 0.40963\n","Epoch: 00 [17164/23069 ( 74%)], Train Loss: 0.40913\n","Epoch: 00 [17204/23069 ( 75%)], Train Loss: 0.40849\n","Epoch: 00 [17244/23069 ( 75%)], Train Loss: 0.40819\n","Epoch: 00 [17284/23069 ( 75%)], Train Loss: 0.40796\n","Epoch: 00 [17324/23069 ( 75%)], Train Loss: 0.40777\n","Epoch: 00 [17364/23069 ( 75%)], Train Loss: 0.40800\n","Epoch: 00 [17404/23069 ( 75%)], Train Loss: 0.40738\n","Epoch: 00 [17444/23069 ( 76%)], Train Loss: 0.40696\n","Epoch: 00 [17484/23069 ( 76%)], Train Loss: 0.40681\n","Epoch: 00 [17524/23069 ( 76%)], Train Loss: 0.40655\n","Epoch: 00 [17564/23069 ( 76%)], Train Loss: 0.40621\n","Epoch: 00 [17604/23069 ( 76%)], Train Loss: 0.40573\n","Epoch: 00 [17644/23069 ( 76%)], Train Loss: 0.40582\n","Epoch: 00 [17684/23069 ( 77%)], Train Loss: 0.40577\n","Epoch: 00 [17724/23069 ( 77%)], Train Loss: 0.40538\n","Epoch: 00 [17764/23069 ( 77%)], Train Loss: 0.40548\n","Epoch: 00 [17804/23069 ( 77%)], Train Loss: 0.40529\n","Epoch: 00 [17844/23069 ( 77%)], Train Loss: 0.40547\n","Epoch: 00 [17884/23069 ( 78%)], Train Loss: 0.40523\n","Epoch: 00 [17924/23069 ( 78%)], Train Loss: 0.40477\n","Epoch: 00 [17964/23069 ( 78%)], Train Loss: 0.40438\n","Epoch: 00 [18004/23069 ( 78%)], Train Loss: 0.40434\n","Epoch: 00 [18044/23069 ( 78%)], Train Loss: 0.40380\n","Epoch: 00 [18084/23069 ( 78%)], Train Loss: 0.40354\n","Epoch: 00 [18124/23069 ( 79%)], Train Loss: 0.40295\n","Epoch: 00 [18164/23069 ( 79%)], Train Loss: 0.40264\n","Epoch: 00 [18204/23069 ( 79%)], Train Loss: 0.40270\n","Epoch: 00 [18244/23069 ( 79%)], Train Loss: 0.40226\n","Epoch: 00 [18284/23069 ( 79%)], Train Loss: 0.40178\n","Epoch: 00 [18324/23069 ( 79%)], Train Loss: 0.40171\n","Epoch: 00 [18364/23069 ( 80%)], Train Loss: 0.40131\n","Epoch: 00 [18404/23069 ( 80%)], Train Loss: 0.40096\n","Epoch: 00 [18444/23069 ( 80%)], Train Loss: 0.40088\n","Epoch: 00 [18484/23069 ( 80%)], Train Loss: 0.40058\n","Epoch: 00 [18524/23069 ( 80%)], Train Loss: 0.40029\n","Epoch: 00 [18564/23069 ( 80%)], Train Loss: 0.39983\n","Epoch: 00 [18604/23069 ( 81%)], Train Loss: 0.39982\n","Epoch: 00 [18644/23069 ( 81%)], Train Loss: 0.39945\n","Epoch: 00 [18684/23069 ( 81%)], Train Loss: 0.39894\n","Epoch: 00 [18724/23069 ( 81%)], Train Loss: 0.39867\n","Epoch: 00 [18764/23069 ( 81%)], Train Loss: 0.39841\n","Epoch: 00 [18804/23069 ( 82%)], Train Loss: 0.39811\n","Epoch: 00 [18844/23069 ( 82%)], Train Loss: 0.39818\n","Epoch: 00 [18884/23069 ( 82%)], Train Loss: 0.39792\n","Epoch: 00 [18924/23069 ( 82%)], Train Loss: 0.39742\n","Epoch: 00 [18964/23069 ( 82%)], Train Loss: 0.39721\n","Epoch: 00 [19004/23069 ( 82%)], Train Loss: 0.39726\n","Epoch: 00 [19044/23069 ( 83%)], Train Loss: 0.39718\n","Epoch: 00 [19084/23069 ( 83%)], Train Loss: 0.39672\n","Epoch: 00 [19124/23069 ( 83%)], Train Loss: 0.39635\n","Epoch: 00 [19164/23069 ( 83%)], Train Loss: 0.39630\n","Epoch: 00 [19204/23069 ( 83%)], Train Loss: 0.39597\n","Epoch: 00 [19244/23069 ( 83%)], Train Loss: 0.39552\n","Epoch: 00 [19284/23069 ( 84%)], Train Loss: 0.39509\n","Epoch: 00 [19324/23069 ( 84%)], Train Loss: 0.39496\n","Epoch: 00 [19364/23069 ( 84%)], Train Loss: 0.39492\n","Epoch: 00 [19404/23069 ( 84%)], Train Loss: 0.39475\n","Epoch: 00 [19444/23069 ( 84%)], Train Loss: 0.39463\n","Epoch: 00 [19484/23069 ( 84%)], Train Loss: 0.39416\n","Epoch: 00 [19524/23069 ( 85%)], Train Loss: 0.39403\n","Epoch: 00 [19564/23069 ( 85%)], Train Loss: 0.39379\n","Epoch: 00 [19604/23069 ( 85%)], Train Loss: 0.39338\n","Epoch: 00 [19644/23069 ( 85%)], Train Loss: 0.39301\n","Epoch: 00 [19684/23069 ( 85%)], Train Loss: 0.39283\n","Epoch: 00 [19724/23069 ( 86%)], Train Loss: 0.39263\n","Epoch: 00 [19764/23069 ( 86%)], Train Loss: 0.39255\n","Epoch: 00 [19804/23069 ( 86%)], Train Loss: 0.39268\n","Epoch: 00 [19844/23069 ( 86%)], Train Loss: 0.39256\n","Epoch: 00 [19884/23069 ( 86%)], Train Loss: 0.39215\n","Epoch: 00 [19924/23069 ( 86%)], Train Loss: 0.39227\n","Epoch: 00 [19964/23069 ( 87%)], Train Loss: 0.39184\n","Epoch: 00 [20004/23069 ( 87%)], Train Loss: 0.39174\n","Epoch: 00 [20044/23069 ( 87%)], Train Loss: 0.39157\n","Epoch: 00 [20084/23069 ( 87%)], Train Loss: 0.39144\n","Epoch: 00 [20124/23069 ( 87%)], Train Loss: 0.39132\n","Epoch: 00 [20164/23069 ( 87%)], Train Loss: 0.39127\n","Epoch: 00 [20204/23069 ( 88%)], Train Loss: 0.39116\n","Epoch: 00 [20244/23069 ( 88%)], Train Loss: 0.39102\n","Epoch: 00 [20284/23069 ( 88%)], Train Loss: 0.39080\n","Epoch: 00 [20324/23069 ( 88%)], Train Loss: 0.39077\n","Epoch: 00 [20364/23069 ( 88%)], Train Loss: 0.39042\n","Epoch: 00 [20404/23069 ( 88%)], Train Loss: 0.39002\n","Epoch: 00 [20444/23069 ( 89%)], Train Loss: 0.38984\n","Epoch: 00 [20484/23069 ( 89%)], Train Loss: 0.38959\n","Epoch: 00 [20524/23069 ( 89%)], Train Loss: 0.38968\n","Epoch: 00 [20564/23069 ( 89%)], Train Loss: 0.38983\n","Epoch: 00 [20604/23069 ( 89%)], Train Loss: 0.38961\n","Epoch: 00 [20644/23069 ( 89%)], Train Loss: 0.38940\n","Epoch: 00 [20684/23069 ( 90%)], Train Loss: 0.38886\n","Epoch: 00 [20724/23069 ( 90%)], Train Loss: 0.38862\n","Epoch: 00 [20764/23069 ( 90%)], Train Loss: 0.38822\n","Epoch: 00 [20804/23069 ( 90%)], Train Loss: 0.38822\n","Epoch: 00 [20844/23069 ( 90%)], Train Loss: 0.38808\n","Epoch: 00 [20884/23069 ( 91%)], Train Loss: 0.38779\n","Epoch: 00 [20924/23069 ( 91%)], Train Loss: 0.38757\n","Epoch: 00 [20964/23069 ( 91%)], Train Loss: 0.38728\n","Epoch: 00 [21004/23069 ( 91%)], Train Loss: 0.38688\n","Epoch: 00 [21044/23069 ( 91%)], Train Loss: 0.38664\n","Epoch: 00 [21084/23069 ( 91%)], Train Loss: 0.38655\n","Epoch: 00 [21124/23069 ( 92%)], Train Loss: 0.38656\n","Epoch: 00 [21164/23069 ( 92%)], Train Loss: 0.38654\n","Epoch: 00 [21204/23069 ( 92%)], Train Loss: 0.38640\n","Epoch: 00 [21244/23069 ( 92%)], Train Loss: 0.38623\n","Epoch: 00 [21284/23069 ( 92%)], Train Loss: 0.38590\n","Epoch: 00 [21324/23069 ( 92%)], Train Loss: 0.38542\n","Epoch: 00 [21364/23069 ( 93%)], Train Loss: 0.38530\n","Epoch: 00 [21404/23069 ( 93%)], Train Loss: 0.38549\n","Epoch: 00 [21444/23069 ( 93%)], Train Loss: 0.38520\n","Epoch: 00 [21484/23069 ( 93%)], Train Loss: 0.38499\n","Epoch: 00 [21524/23069 ( 93%)], Train Loss: 0.38472\n","Epoch: 00 [21564/23069 ( 93%)], Train Loss: 0.38433\n","Epoch: 00 [21604/23069 ( 94%)], Train Loss: 0.38397\n","Epoch: 00 [21644/23069 ( 94%)], Train Loss: 0.38362\n","Epoch: 00 [21684/23069 ( 94%)], Train Loss: 0.38355\n","Epoch: 00 [21724/23069 ( 94%)], Train Loss: 0.38334\n","Epoch: 00 [21764/23069 ( 94%)], Train Loss: 0.38295\n","Epoch: 00 [21804/23069 ( 95%)], Train Loss: 0.38289\n","Epoch: 00 [21844/23069 ( 95%)], Train Loss: 0.38266\n","Epoch: 00 [21884/23069 ( 95%)], Train Loss: 0.38263\n","Epoch: 00 [21924/23069 ( 95%)], Train Loss: 0.38236\n","Epoch: 00 [21964/23069 ( 95%)], Train Loss: 0.38233\n","Epoch: 00 [22004/23069 ( 95%)], Train Loss: 0.38194\n","Epoch: 00 [22044/23069 ( 96%)], Train Loss: 0.38159\n","Epoch: 00 [22084/23069 ( 96%)], Train Loss: 0.38147\n","Epoch: 00 [22124/23069 ( 96%)], Train Loss: 0.38100\n","Epoch: 00 [22164/23069 ( 96%)], Train Loss: 0.38095\n","Epoch: 00 [22204/23069 ( 96%)], Train Loss: 0.38088\n","Epoch: 00 [22244/23069 ( 96%)], Train Loss: 0.38050\n","Epoch: 00 [22284/23069 ( 97%)], Train Loss: 0.38051\n","Epoch: 00 [22324/23069 ( 97%)], Train Loss: 0.38022\n","Epoch: 00 [22364/23069 ( 97%)], Train Loss: 0.38000\n","Epoch: 00 [22404/23069 ( 97%)], Train Loss: 0.37993\n","Epoch: 00 [22444/23069 ( 97%)], Train Loss: 0.37968\n","Epoch: 00 [22484/23069 ( 97%)], Train Loss: 0.37931\n","Epoch: 00 [22524/23069 ( 98%)], Train Loss: 0.37917\n","Epoch: 00 [22564/23069 ( 98%)], Train Loss: 0.37875\n","Epoch: 00 [22604/23069 ( 98%)], Train Loss: 0.37868\n","Epoch: 00 [22644/23069 ( 98%)], Train Loss: 0.37859\n","Epoch: 00 [22684/23069 ( 98%)], Train Loss: 0.37847\n","Epoch: 00 [22724/23069 ( 99%)], Train Loss: 0.37834\n","Epoch: 00 [22764/23069 ( 99%)], Train Loss: 0.37856\n","Epoch: 00 [22804/23069 ( 99%)], Train Loss: 0.37822\n","Epoch: 00 [22844/23069 ( 99%)], Train Loss: 0.37812\n","Epoch: 00 [22884/23069 ( 99%)], Train Loss: 0.37797\n","Epoch: 00 [22924/23069 ( 99%)], Train Loss: 0.37771\n","Epoch: 00 [22964/23069 (100%)], Train Loss: 0.37737\n","Epoch: 00 [23004/23069 (100%)], Train Loss: 0.37693\n","Epoch: 00 [23044/23069 (100%)], Train Loss: 0.37695\n","Epoch: 00 [23069/23069 (100%)], Train Loss: 0.37693\n","----Validation Results Summary----\n","Epoch: [0] Valid Loss: 0.00000\n","0 Epoch, Best epoch was updated! Valid Loss: 0.00000\n","Saving model checkpoint to chaii-qa-5-fold-xlmroberta-torch-fit/checkpoint-fold-5.\n","\n","Total Training Time: 3222.9414279460907secs, Average Training Time per Epoch: 3222.9414279460907secs.\n","Total Validation Time: 0.5322093963623047secs, Average Validation Time per Epoch: 0.5322093963623047secs.\n"]}]},{"cell_type":"code","metadata":{"id":"DkjRIhdbwjHx","execution":{"iopub.status.busy":"2021-08-17T20:36:49.722586Z","iopub.execute_input":"2021-08-17T20:36:49.723011Z","iopub.status.idle":"2021-08-17T20:36:49.727659Z","shell.execute_reply.started":"2021-08-17T20:36:49.722978Z","shell.execute_reply":"2021-08-17T20:36:49.726609Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1635060144354,"user_tz":-540,"elapsed":7,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["# example for training second fold\n","\n","# for fold in range(1, 2):\n","#     print();print()\n","#     print('-'*50)\n","#     print(f'FOLD: {fold}')\n","#     print('-'*50)\n","#     run(train, fold)"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7uIwhUECP4iP"},"source":["### Thanks and please do Upvote!"]}]}