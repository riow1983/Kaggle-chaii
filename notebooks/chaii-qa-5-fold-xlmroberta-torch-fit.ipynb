{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"name":"chaii-qa-5-fold-xlmroberta-torch-fit.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"49513bb6cffd483282ce3497cf547b65":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_37e3e1ed97e24dd4a6cd17b91512baa5","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_cb6bdf88e51240b1b72704aa5311ccc9","IPY_MODEL_af313fa2e19c4692b0658bce024f7b66","IPY_MODEL_d89ed0365514406cba08f06ad51fc1de"]}},"37e3e1ed97e24dd4a6cd17b91512baa5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cb6bdf88e51240b1b72704aa5311ccc9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d3bde5515f444e04adaa032cd0fcdcfc","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a3eea053f7ea42cb88c33796b2db599d"}},"af313fa2e19c4692b0658bce024f7b66":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_b89c954018714a76beece628fbe4d318","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":606,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":606,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_64ae008e76d944239d1c6d86b2868e57"}},"d89ed0365514406cba08f06ad51fc1de":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_383c486832644b619a64a69bfb1d0464","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 606/606 [00:00&lt;00:00, 26.6kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2bc04f576ee14e9ab9744865292a4f6d"}},"d3bde5515f444e04adaa032cd0fcdcfc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a3eea053f7ea42cb88c33796b2db599d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b89c954018714a76beece628fbe4d318":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"64ae008e76d944239d1c6d86b2868e57":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"383c486832644b619a64a69bfb1d0464":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2bc04f576ee14e9ab9744865292a4f6d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"34d83d56764e4c64b743681271d8fea2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_0ebab91b2b574c36bbcdb1395749126c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_11fd0a4638934ab0bcdb152e1ca789c7","IPY_MODEL_935f81171e2e47908feb2d9591a1b550","IPY_MODEL_b265548803224382920cf44851925654"]}},"0ebab91b2b574c36bbcdb1395749126c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"11fd0a4638934ab0bcdb152e1ca789c7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d76868e41fce485b9a0f3fff6ce33b00","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3f51fa8266a949f8b56fede7f46bb33a"}},"935f81171e2e47908feb2d9591a1b550":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7a6140ee506644b4957199117996b28a","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":179,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":179,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bd2a7765dd514fcea5dbf5e5cd3bf3f8"}},"b265548803224382920cf44851925654":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_dce3782940674ddf872c06dde25f18f7","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 179/179 [00:00&lt;00:00, 7.20kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_875b8feab2234acb9d1576e7393bff3a"}},"d76868e41fce485b9a0f3fff6ce33b00":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3f51fa8266a949f8b56fede7f46bb33a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7a6140ee506644b4957199117996b28a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"bd2a7765dd514fcea5dbf5e5cd3bf3f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"dce3782940674ddf872c06dde25f18f7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"875b8feab2234acb9d1576e7393bff3a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e485051d9f2c4aa090b7b0fd7d0ba420":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_7c1d0f1921c242799c55389fe26ebf9b","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f334e486d16542e7a61469437cf7432e","IPY_MODEL_ae0a4ca6396e45308d191b9a137eadd8","IPY_MODEL_bbdf1696214e44e9ac4f213d0122cd51"]}},"7c1d0f1921c242799c55389fe26ebf9b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f334e486d16542e7a61469437cf7432e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9009184ad25844f0b46e246982822ee0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7eccbec1b3ba4a60818375a9d7c74ce2"}},"ae0a4ca6396e45308d191b9a137eadd8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_b4d5112c5ef94e3092c1a2062c3d0fd6","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":5069051,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":5069051,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1f6d67730c194390bdee2b66108a29fc"}},"bbdf1696214e44e9ac4f213d0122cd51":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4824bd3cf83b49e2b09ff7769d2133ff","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 4.83M/4.83M [00:01&lt;00:00, 4.37MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d69e6a1ab26742f186378c2ee463fcc9"}},"9009184ad25844f0b46e246982822ee0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7eccbec1b3ba4a60818375a9d7c74ce2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b4d5112c5ef94e3092c1a2062c3d0fd6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"1f6d67730c194390bdee2b66108a29fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4824bd3cf83b49e2b09ff7769d2133ff":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d69e6a1ab26742f186378c2ee463fcc9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"931b5cfd9e104d818003847971fa97b8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d469c9d12e164380bc3ecfe95b73badc","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_605546891e2b42b1989b1e9e4bc8ed9b","IPY_MODEL_de97fab22dec498682892396a50a786a","IPY_MODEL_2ecbfe65d2df4fcd8826a343c220b2ef"]}},"d469c9d12e164380bc3ecfe95b73badc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"605546891e2b42b1989b1e9e4bc8ed9b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6bf64c176049446383054b9482f25232","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f8f4336a4a5d4cc0930b7f8424593cf7"}},"de97fab22dec498682892396a50a786a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ac9645d8d0b242d9b23bd192e6c5bf40","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":150,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":150,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_cf326fd5df92467f9170a6857b2497e0"}},"2ecbfe65d2df4fcd8826a343c220b2ef":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_61a87e1c75a74343881c364aefc06ee6","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 150/150 [00:00&lt;00:00, 5.79kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f3c5939e0ed946b8b174f844ac228c10"}},"6bf64c176049446383054b9482f25232":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f8f4336a4a5d4cc0930b7f8424593cf7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ac9645d8d0b242d9b23bd192e6c5bf40":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"cf326fd5df92467f9170a6857b2497e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"61a87e1c75a74343881c364aefc06ee6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f3c5939e0ed946b8b174f844ac228c10":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a3599314c1e94020b6187dd42d572f04":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_56c4bb53a2e347ba9c408ce188773316","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_8f184ff8d6b8494b93c18c94b0e9c384","IPY_MODEL_75b4a0499a8d4556ae94d0202270f0d9","IPY_MODEL_6473cad067754177bcb1c8f1639f82fc"]}},"56c4bb53a2e347ba9c408ce188773316":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8f184ff8d6b8494b93c18c94b0e9c384":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_79fe8b4f25b84bc2856f85f7f1dee043","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_49c0af75f9374a7fb088d90ee5388021"}},"75b4a0499a8d4556ae94d0202270f0d9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_0f56f724d6d14ee19005b7da5513e379","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":2239666418,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":2239666418,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_558ce906fbbb4913af29144979eb70a5"}},"6473cad067754177bcb1c8f1639f82fc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_281f93b6336b45da9b5dc08fec4ca640","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 2.09G/2.09G [00:38&lt;00:00, 60.1MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b6beb03cda294c06a6cce2ca892e7dfb"}},"79fe8b4f25b84bc2856f85f7f1dee043":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"49c0af75f9374a7fb088d90ee5388021":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0f56f724d6d14ee19005b7da5513e379":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"558ce906fbbb4913af29144979eb70a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"281f93b6336b45da9b5dc08fec4ca640":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b6beb03cda294c06a6cce2ca892e7dfb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RkuY4GVbQ1k4","executionInfo":{"status":"ok","timestamp":1634445460868,"user_tz":-540,"elapsed":50306,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"0f1ff2e6-f48e-46dc-d3a5-edcb20b09693"},"source":["# Kaggle or Colab\n","import sys\n","import os\n","if 'kaggle_web_client' in sys.modules:\n","    # Do something\n","    pass\n","elif 'google.colab' in sys.modules:\n","    # Do something\n","    from google.colab import drive\n","    drive.mount(\"/content/drive\")\n","\n","    comp_name_official = \"chaii-hindi-and-tamil-question-answering\"\n","    comp_name_local = \"Kaggle-chaii\"\n","\n","    !pip install --upgrade --force-reinstall --no-deps kaggle\n","    import json\n","    f = open(\"/content/drive/MyDrive/colab_notebooks/kaggle/kaggle.json\", \"r\")\n","    json_data = json.load(f)\n","    os.environ[\"KAGGLE_USERNAME\"] = json_data[\"username\"]\n","    os.environ[\"KAGGLE_KEY\"] = json_data[\"key\"]\n","\n","    %cd /content/drive/MyDrive/colab_notebooks/kaggle/{comp_name_local}/notebooks\n","\n","    dname = \"chaii-qa-5-fold-xlmroberta-torch-fit\"\n","    !mkdir {dname}\n","    #%cd /content/drive/MyDrive/colab_notebooks/kaggle/{comp_name_local}/notebooks/{dname}"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Collecting kaggle\n","  Downloading kaggle-1.5.12.tar.gz (58 kB)\n","\u001b[K     |████████████████████████████████| 58 kB 2.3 MB/s \n","\u001b[?25hBuilding wheels for collected packages: kaggle\n","  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73051 sha256=b4a3ea652d72e90f1b31b81d5cd531012646b82e75dadea955389ebeb00b1b17\n","  Stored in directory: /root/.cache/pip/wheels/62/d6/58/5853130f941e75b2177d281eb7e44b4a98ed46dd155f556dc5\n","Successfully built kaggle\n","Installing collected packages: kaggle\n","  Attempting uninstall: kaggle\n","    Found existing installation: kaggle 1.5.12\n","    Uninstalling kaggle-1.5.12:\n","      Successfully uninstalled kaggle-1.5.12\n","Successfully installed kaggle-1.5.12\n","/content/drive/MyDrive/colab_notebooks/kaggle/Kaggle-chaii/notebooks\n","mkdir: cannot create directory ‘chaii-qa-5-fold-xlmroberta-torch-fit’: File exists\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a0xZ62k7SB1g","executionInfo":{"status":"ok","timestamp":1634445467155,"user_tz":-540,"elapsed":6301,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"18b17ca0-416c-4db5-f145-a6d7247c260b"},"source":["if 'kaggle_web_client' in sys.modules:\n","    # Do something\n","    pass\n","elif 'google.colab' in sys.modules:\n","    #!pip install transformers\n","    !pip install transformers[sentencepiece]\n","\n","    # import yaml\n","    # with open(f'./config_notebook/config.yaml') as file:\n","    #     cfg = yaml.load(file, Loader=yaml.FullLoader) # Loader is recommended\n","    # print(\"Config:\\n\", cfg)"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers[sentencepiece]\n","  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n","\u001b[K     |████████████████████████████████| 2.9 MB 4.2 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 73.0 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (4.8.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2.23.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (21.0)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 82.3 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (4.62.3)\n","Collecting huggingface-hub>=0.0.17\n","  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n","\u001b[K     |████████████████████████████████| 56 kB 5.9 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2019.12.20)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 80.7 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.3.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (1.19.5)\n","Collecting sentencepiece!=0.1.92,>=0.1.91\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 52.3 MB/s \n","\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.17.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers[sentencepiece]) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers[sentencepiece]) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers[sentencepiece]) (3.6.0)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers[sentencepiece]) (1.15.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (2021.5.30)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[sentencepiece]) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[sentencepiece]) (1.0.1)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers, sentencepiece\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.0.19 pyyaml-6.0 sacremoses-0.0.46 sentencepiece-0.1.96 tokenizers-0.10.3 transformers-4.11.3\n"]}]},{"cell_type":"markdown","metadata":{"id":"uQdNDkrBP4hj"},"source":["<h2>chaii QA - 5 Fold XLMRoberta Finetuning in Torch w/o Trainer API</h2>\n","    \n","<h3><span \"style: color=#444\">Introduction</span></h3>\n","\n","The kernel implements 5-Fold XLMRoberta QA Model without using the Trainer API from HuggingFace.\n","\n","This is a three part kernel,\n","\n","- [External Data - MLQA, XQUAD Preprocessing](https://www.kaggle.com/rhtsingh/external-data-mlqa-xquad-preprocessing) which preprocesses the Hindi Corpus of MLQA and XQUAD. I have used these data for training.\n","\n","- [chaii QA - 5 Fold XLMRoberta Torch | FIT](https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-fit/edit) This kernel is the current kernel and used for Finetuning (FIT) on competition + external data.\n","\n","- [chaii QA - 5 Fold XLMRoberta Torch | Infer](https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-infer) The Inference kernel where we ensemble our 5 Fold XLMRoberta Models and do the submission.\n","\n","<h3><span \"style: color=#444\">Techniques</span></h3>\n","\n","The kernel has implementation for below techniques, click on the links to learn more -\n","\n"," - [External Data Preprocessing + Training](https://www.kaggle.com/rhtsingh/external-data-mlqa-xquad-preprocessing)\n"," \n"," - [Mixed Precision Training using APEX](https://www.kaggle.com/rhtsingh/swa-apex-amp-interpreting-transformers-in-torch)\n"," \n"," - [Gradient Accumulation](https://www.kaggle.com/rhtsingh/speeding-up-transformer-w-optimization-strategies)\n"," \n"," - [Gradient Clipping](https://www.kaggle.com/rhtsingh/swa-apex-amp-interpreting-transformers-in-torch)\n"," \n"," - [Grouped Layerwise Learning Rate Decay](https://www.kaggle.com/rhtsingh/guide-to-huggingface-schedulers-differential-lrs)\n"," \n"," - [Utilizing Intermediate Transformer Representations](https://www.kaggle.com/rhtsingh/utilizing-transformer-representations-efficiently)\n"," \n"," - [Layer Initialization](https://www.kaggle.com/rhtsingh/on-stability-of-few-sample-transformer-fine-tuning)\n"," \n"," - [5-Fold Training](https://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-fit)\n"," \n"," - etc.\n"," \n","<h3><span \"style: color=#444\">References</span></h3>\n","I would like to acknowledge below kagglers work and resources without whom this kernel wouldn't have been possible,  \n","\n","- [roberta inference 5 folds by Abhishek](https://www.kaggle.com/abhishek/roberta-inference-5-folds)\n","\n","- [ChAII - EDA & Baseline by Darek](https://www.kaggle.com/thedrcat/chaii-eda-baseline) due to whom i found the great notebook below from HuggingFace,\n","\n","- [How to fine-tune a model on question answering](https://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb)\n","\n","- [HuggingFace QA Examples](https://github.com/huggingface/transformers/tree/master/examples/pytorch/question-answering)\n","\n","- [TensorFlow 2.0 Question Answering - Collections of gold solutions](https://www.kaggle.com/c/tensorflow2-question-answering/discussion/127409) these solutions are gold and highly recommend everyone to give a detailed read."]},{"cell_type":"markdown","metadata":{"id":"vI-eiJRPP4hw"},"source":["<h3><span style=\"color=#444\">Note</span></h3>\n","\n","The below points are worth noting,\n","\n"," - I haven't used FP16 because due to some reason this fails and model never starts training.\n"," - These are the original hyperparamters and setting that I have used for training my models.\n"," - I tried few pooling layers but none of them performed better than simple one.\n"," - Gradient clipping reduces model performance.\n"," - <span style=\"color:#2E86C1\">Training 5 Folds at once will give OOM issue on Kaggle and Colab. I have trained one fold at a time i.e. After 1st fold is completed I save the model as a dataset then train second fold. Repeat this process until all 5 folds are completed. Training each fold takes around 50 minuts on Kaggle.</span>\n"," - <span style=\"color:#2E86C1\">I have modified the preprocessing code a lot from original used in HuggingFace notebook since I am not using HuggingFace Datasets API as well. So I had to handle the sequence-ids specially since they contain None values which torch doesn't support.</span>"]},{"cell_type":"markdown","metadata":{"id":"8ykS_qCDP4hz"},"source":["### Install APEX"]},{"cell_type":"code","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","id":"aCY6yvR6ET3s","execution":{"iopub.status.busy":"2021-08-17T20:22:25.164246Z","iopub.execute_input":"2021-08-17T20:22:25.164729Z","iopub.status.idle":"2021-08-17T20:22:25.170948Z","shell.execute_reply.started":"2021-08-17T20:22:25.164627Z","shell.execute_reply":"2021-08-17T20:22:25.169352Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1634445467157,"user_tz":-540,"elapsed":35,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["# %%writefile setup.sh\n","# export CUDA_HOME=/usr/local/cuda-10.1\n","# git clone https://github.com/NVIDIA/apex\n","# cd apex\n","# pip install -v --disable-pip-version-check --no-cache-dir ./"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"-l2Jsav9ET3v","execution":{"iopub.status.busy":"2021-08-17T20:22:26.935257Z","iopub.execute_input":"2021-08-17T20:22:26.935919Z","iopub.status.idle":"2021-08-17T20:22:26.939334Z","shell.execute_reply.started":"2021-08-17T20:22:26.935881Z","shell.execute_reply":"2021-08-17T20:22:26.938488Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1634445467158,"user_tz":-540,"elapsed":35,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["# %%capture\n","# !sh setup.sh"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xFbbPlPgP4h7"},"source":["### Import Dependencies"]},{"cell_type":"code","metadata":{"id":"E4l6PirHET3x","execution":{"iopub.status.busy":"2021-08-17T20:23:50.232396Z","iopub.execute_input":"2021-08-17T20:23:50.232892Z","iopub.status.idle":"2021-08-17T20:24:01.076652Z","shell.execute_reply.started":"2021-08-17T20:23:50.232861Z","shell.execute_reply":"2021-08-17T20:24:01.075658Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634445481501,"user_tz":-540,"elapsed":14373,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"b5ba9764-6161-43d6-bf51-31850b1bcf5c"},"source":["#import os\n","import gc\n","gc.enable()\n","import math\n","#mport json\n","import time\n","import random\n","import multiprocessing\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=UserWarning)\n","\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm, trange\n","from sklearn import model_selection\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn import Parameter\n","import torch.optim as optim\n","from torch.utils.data import (\n","    Dataset, DataLoader,\n","    SequentialSampler, RandomSampler\n",")\n","from torch.utils.data.distributed import DistributedSampler\n","\n","try:\n","    from apex import amp\n","    APEX_INSTALLED = True\n","except ImportError:\n","    APEX_INSTALLED = False\n","\n","import transformers\n","from transformers import (\n","    WEIGHTS_NAME,\n","    AdamW,\n","    AutoConfig,\n","    AutoModel,\n","    AutoTokenizer,\n","    get_cosine_schedule_with_warmup,\n","    get_linear_schedule_with_warmup,\n","    logging,\n","    MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n",")\n","logging.set_verbosity_warning()\n","logging.set_verbosity_error()\n","\n","def fix_all_seeds(seed):\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","def optimal_num_of_loader_workers():\n","    num_cpus = multiprocessing.cpu_count()\n","    num_gpus = torch.cuda.device_count()\n","    optimal_value = min(num_cpus, num_gpus*4) if num_gpus else num_cpus - 1\n","    return optimal_value\n","\n","print(f\"Apex AMP Installed :: {APEX_INSTALLED}\")\n","MODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())\n","MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Apex AMP Installed :: False\n"]}]},{"cell_type":"markdown","metadata":{"id":"y0OvcnMaP4h9"},"source":["### Training Configuration"]},{"cell_type":"code","metadata":{"id":"LUx5XplNET3y","execution":{"iopub.status.busy":"2021-08-17T20:24:13.391322Z","iopub.execute_input":"2021-08-17T20:24:13.391696Z","iopub.status.idle":"2021-08-17T20:24:13.398397Z","shell.execute_reply.started":"2021-08-17T20:24:13.391662Z","shell.execute_reply":"2021-08-17T20:24:13.39732Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1634445481503,"user_tz":-540,"elapsed":45,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["class Config:\n","    # model\n","    model_type = 'xlm_roberta'\n","    model_name_or_path = \"deepset/xlm-roberta-large-squad2\"\n","    config_name = \"deepset/xlm-roberta-large-squad2\"\n","    fp16 = True if APEX_INSTALLED else False\n","    fp16_opt_level = \"O1\"\n","    gradient_accumulation_steps = 2\n","\n","    # tokenizer\n","    tokenizer_name = \"deepset/xlm-roberta-large-squad2\"\n","    max_seq_length = 384 #512 #384\n","    doc_stride = 128 #80 #128\n","\n","    # train\n","    epochs = 2 #7 #1\n","    train_batch_size = 4 #2 #4\n","    eval_batch_size = 8 #4 #8\n","\n","    # optimizer\n","    optimizer_type = 'AdamW'\n","    learning_rate = 1.5e-5\n","    weight_decay = 1e-2\n","    epsilon = 1e-8\n","    max_grad_norm = 1.0\n","\n","    # scheduler\n","    decay_name = 'linear-warmup'\n","    warmup_ratio = 0.1\n","\n","    # logging\n","    logging_steps = 10\n","\n","    # evaluate\n","    output_dir = dname #'output'\n","    seed = 2021"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HYno9o1OP4h-"},"source":["### Data Factory"]},{"cell_type":"code","metadata":{"id":"X_eRZQrzET3z","execution":{"iopub.status.busy":"2021-08-17T20:24:26.939105Z","iopub.execute_input":"2021-08-17T20:24:26.939437Z","iopub.status.idle":"2021-08-17T20:24:27.776039Z","shell.execute_reply.started":"2021-08-17T20:24:26.939409Z","shell.execute_reply":"2021-08-17T20:24:27.775204Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1634445489918,"user_tz":-540,"elapsed":7796,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["train = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/train.csv')\n","test = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/test.csv')\n","external_mlqa = pd.read_csv('../input/mlqa-hindi-processed/mlqa_hindi.csv')\n","external_xquad = pd.read_csv('../input/mlqa-hindi-processed/xquad.csv')\n","external_train = pd.concat([external_mlqa, external_xquad])\n","\n","def create_folds(data, num_splits):\n","    data[\"kfold\"] = -1\n","    kf = model_selection.StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=2021)\n","    for f, (t_, v_) in enumerate(kf.split(X=data, y=data['language'])):\n","        data.loc[v_, 'kfold'] = f\n","    return data\n","\n","#### RIOW\n","# train = create_folds(train, num_splits=5)\n","# external_train[\"kfold\"] = -1\n","# external_train['id'] = list(np.arange(1, len(external_train)+1))\n","# train = pd.concat([train, external_train]).reset_index(drop=True)\n","\n","external_train['id'] = list(np.arange(1, len(external_train)+1))\n","\n","# # Drop tamil\n","# train = train[train[\"language\"]!=\"tamil\"].reset_index(drop=True)\n","\n","train = pd.concat([train, external_train]).reset_index(drop=True)\n","train = create_folds(train, num_splits=5)\n","#### RIOWRIOW\n","\n","def convert_answers(row):\n","    return {'answer_start': [row[0]], 'text': [row[1]]}\n","\n","train['answers'] = train[['answer_start', 'answer_text']].apply(convert_answers, axis=1)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dAoAJUVaP4iA"},"source":["### Covert Examples to Features (Preprocess)"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-12T15:50:26.947214Z","iopub.execute_input":"2021-08-12T15:50:26.947589Z","iopub.status.idle":"2021-08-12T15:50:26.960916Z","shell.execute_reply.started":"2021-08-12T15:50:26.947551Z","shell.execute_reply":"2021-08-12T15:50:26.959064Z"},"id":"dxbZdct1ET3z","trusted":true,"executionInfo":{"status":"ok","timestamp":1634445489919,"user_tz":-540,"elapsed":59,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["def prepare_train_features(args, example, tokenizer):\n","    example[\"question\"] = example[\"question\"].lstrip()\n","    tokenized_example = tokenizer(\n","        example[\"question\"],\n","        example[\"context\"],\n","        truncation=\"only_second\",\n","        max_length=args.max_seq_length,\n","        stride=args.doc_stride,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True,\n","        padding=\"max_length\",\n","    )\n","\n","    sample_mapping = tokenized_example.pop(\"overflow_to_sample_mapping\")\n","    offset_mapping = tokenized_example.pop(\"offset_mapping\")\n","\n","    features = []\n","    for i, offsets in enumerate(offset_mapping):\n","        feature = {}\n","\n","        input_ids = tokenized_example[\"input_ids\"][i]\n","        attention_mask = tokenized_example[\"attention_mask\"][i]\n","\n","        feature['input_ids'] = input_ids\n","        feature['attention_mask'] = attention_mask\n","        feature['offset_mapping'] = offsets\n","\n","        cls_index = input_ids.index(tokenizer.cls_token_id)\n","        sequence_ids = tokenized_example.sequence_ids(i)\n","\n","        sample_index = sample_mapping[i]\n","        answers = example[\"answers\"]\n","\n","        if len(answers[\"answer_start\"]) == 0:\n","            feature[\"start_position\"] = cls_index\n","            feature[\"end_position\"] = cls_index\n","        else:\n","            start_char = answers[\"answer_start\"][0]\n","            end_char = start_char + len(answers[\"text\"][0])\n","\n","            token_start_index = 0\n","            while sequence_ids[token_start_index] != 1:\n","                token_start_index += 1\n","\n","            token_end_index = len(input_ids) - 1\n","            while sequence_ids[token_end_index] != 1:\n","                token_end_index -= 1\n","\n","            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n","                feature[\"start_position\"] = cls_index\n","                feature[\"end_position\"] = cls_index\n","            else:\n","                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n","                    token_start_index += 1\n","                feature[\"start_position\"] = token_start_index - 1\n","                while offsets[token_end_index][1] >= end_char:\n","                    token_end_index -= 1\n","                feature[\"end_position\"] = token_end_index + 1\n","\n","        features.append(feature)\n","    return features"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a3SIS_xAP4iC"},"source":["### Dataset Retriever"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-12T15:50:26.962738Z","iopub.execute_input":"2021-08-12T15:50:26.963118Z","iopub.status.idle":"2021-08-12T15:50:26.97542Z","shell.execute_reply.started":"2021-08-12T15:50:26.963075Z","shell.execute_reply":"2021-08-12T15:50:26.974431Z"},"id":"6TuzHdjmET30","trusted":true,"executionInfo":{"status":"ok","timestamp":1634445489920,"user_tz":-540,"elapsed":59,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["class DatasetRetriever(Dataset):\n","    def __init__(self, features, mode='train'):\n","        super(DatasetRetriever, self).__init__()\n","        self.features = features\n","        self.mode = mode\n","        \n","    def __len__(self):\n","        return len(self.features)\n","    \n","    def __getitem__(self, item):   \n","        feature = self.features[item]\n","        if self.mode == 'train':\n","            return {\n","                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n","                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n","                'offset_mapping':torch.tensor(feature['offset_mapping'], dtype=torch.long),\n","                'start_position':torch.tensor(feature['start_position'], dtype=torch.long),\n","                'end_position':torch.tensor(feature['end_position'], dtype=torch.long)\n","            }\n","        else:\n","            return {\n","                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n","                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n","                'offset_mapping':feature['offset_mapping'],\n","                'sequence_ids':feature['sequence_ids'],\n","                'id':feature['example_id'],\n","                'context': feature['context'],\n","                'question': feature['question']\n","            }"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tpH-2nPqP4iE"},"source":["### Model"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-12T15:50:26.976977Z","iopub.execute_input":"2021-08-12T15:50:26.977627Z","iopub.status.idle":"2021-08-12T15:50:26.990227Z","shell.execute_reply.started":"2021-08-12T15:50:26.97747Z","shell.execute_reply":"2021-08-12T15:50:26.989443Z"},"id":"9OxhKqxcET31","trusted":true,"executionInfo":{"status":"ok","timestamp":1634445489921,"user_tz":-540,"elapsed":57,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["class Model(nn.Module):\n","    def __init__(self, modelname_or_path, config):\n","        super(Model, self).__init__()\n","        self.config = config\n","        self.xlm_roberta = AutoModel.from_pretrained(modelname_or_path, config=config)\n","        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","        self._init_weights(self.qa_outputs)\n","        \n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","\n","    def forward(\n","        self, \n","        input_ids, \n","        attention_mask=None, \n","        # token_type_ids=None\n","    ):\n","        outputs = self.xlm_roberta(\n","            input_ids,\n","            attention_mask=attention_mask,\n","        )\n","\n","        sequence_output = outputs[0]\n","        pooled_output = outputs[1]\n","        \n","        # sequence_output = self.dropout(sequence_output)\n","        qa_logits = self.qa_outputs(sequence_output)\n","        \n","        start_logits, end_logits = qa_logits.split(1, dim=-1)\n","        start_logits = start_logits.squeeze(-1)\n","        end_logits = end_logits.squeeze(-1)\n","    \n","        return start_logits, end_logits"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xdMx6plqP4iG"},"source":["### Loss"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-12T15:50:26.991891Z","iopub.execute_input":"2021-08-12T15:50:26.99237Z","iopub.status.idle":"2021-08-12T15:50:27.000188Z","shell.execute_reply.started":"2021-08-12T15:50:26.992334Z","shell.execute_reply":"2021-08-12T15:50:26.999374Z"},"id":"SxuNrJqqET32","trusted":true,"executionInfo":{"status":"ok","timestamp":1634445489922,"user_tz":-540,"elapsed":57,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["def loss_fn(preds, labels):\n","    start_preds, end_preds = preds\n","    start_labels, end_labels = labels\n","    \n","    start_loss = nn.CrossEntropyLoss(ignore_index=-1)(start_preds, start_labels)\n","    end_loss = nn.CrossEntropyLoss(ignore_index=-1)(end_preds, end_labels)\n","    total_loss = (start_loss + end_loss) / 2\n","    return total_loss"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jUMtX08cP4iH"},"source":["### Grouped Layerwise Learning Rate Decay"]},{"cell_type":"code","metadata":{"id":"vf6HVcu2ET34","execution":{"iopub.status.busy":"2021-08-17T20:25:36.36033Z","iopub.execute_input":"2021-08-17T20:25:36.361058Z","iopub.status.idle":"2021-08-17T20:25:36.381524Z","shell.execute_reply.started":"2021-08-17T20:25:36.361009Z","shell.execute_reply":"2021-08-17T20:25:36.380328Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1634445489922,"user_tz":-540,"elapsed":56,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["def get_optimizer_grouped_parameters(args, model):\n","    no_decay = [\"bias\", \"LayerNorm.weight\"]\n","    group1=['layer.0.','layer.1.','layer.2.','layer.3.']\n","    group2=['layer.4.','layer.5.','layer.6.','layer.7.']    \n","    group3=['layer.8.','layer.9.','layer.10.','layer.11.']\n","    group_all=['layer.0.','layer.1.','layer.2.','layer.3.','layer.4.','layer.5.','layer.6.','layer.7.','layer.8.','layer.9.','layer.10.','layer.11.']\n","    optimizer_grouped_parameters = [\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': args.weight_decay},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': args.weight_decay, 'lr': args.learning_rate/2.6},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': args.weight_decay, 'lr': args.learning_rate},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': args.weight_decay, 'lr': args.learning_rate*2.6},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': 0.0},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': 0.0, 'lr': args.learning_rate/2.6},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': 0.0, 'lr': args.learning_rate},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': 0.0, 'lr': args.learning_rate*2.6},\n","        {'params': [p for n, p in model.named_parameters() if args.model_type not in n], 'lr':args.learning_rate*20, \"weight_decay\": 0.0},\n","    ]\n","    return optimizer_grouped_parameters"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oXz_0fIQP4iI"},"source":["### Metric Logger"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-12T15:50:27.021442Z","iopub.execute_input":"2021-08-12T15:50:27.021952Z","iopub.status.idle":"2021-08-12T15:50:27.03234Z","shell.execute_reply.started":"2021-08-12T15:50:27.021912Z","shell.execute_reply":"2021-08-12T15:50:27.03156Z"},"id":"bkFB-iMcET34","trusted":true,"executionInfo":{"status":"ok","timestamp":1634445489923,"user_tz":-540,"elapsed":55,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["class AverageMeter(object):\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","        self.max = 0\n","        self.min = 1e5\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","        if val > self.max:\n","            self.max = val\n","        if val < self.min:\n","            self.min = val"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y946gQxtP4iJ"},"source":["### Utilities"]},{"cell_type":"code","metadata":{"id":"spFRutV0ET34","execution":{"iopub.status.busy":"2021-08-17T20:26:30.014223Z","iopub.execute_input":"2021-08-17T20:26:30.014623Z","iopub.status.idle":"2021-08-17T20:26:30.030566Z","shell.execute_reply.started":"2021-08-17T20:26:30.01459Z","shell.execute_reply":"2021-08-17T20:26:30.029723Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1634445489924,"user_tz":-540,"elapsed":55,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["def make_model(args):\n","    config = AutoConfig.from_pretrained(args.config_name)\n","    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name)\n","    model = Model(args.model_name_or_path, config=config)\n","    return config, tokenizer, model\n","\n","def make_optimizer(args, model):\n","    # optimizer_grouped_parameters = get_optimizer_grouped_parameters(args, model)\n","    no_decay = [\"bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\n","            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","            \"weight_decay\": args.weight_decay,\n","        },\n","        {\n","            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n","            \"weight_decay\": 0.0,\n","        },\n","    ]\n","    if args.optimizer_type == \"AdamW\":\n","        optimizer = AdamW(\n","            optimizer_grouped_parameters,\n","            lr=args.learning_rate,\n","            eps=args.epsilon,\n","            correct_bias=True\n","        )\n","        return optimizer\n","\n","def make_scheduler(\n","    args, optimizer, \n","    num_warmup_steps, \n","    num_training_steps\n","):\n","    if args.decay_name == \"cosine-warmup\":\n","        scheduler = get_cosine_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=num_warmup_steps,\n","            num_training_steps=num_training_steps\n","        )\n","    else:\n","        scheduler = get_linear_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=num_warmup_steps,\n","            num_training_steps=num_training_steps\n","        )\n","    return scheduler    \n","\n","def make_loader(\n","    args, data, \n","    tokenizer, fold\n","):\n","    train_set, valid_set = data[data['kfold']!=fold], data[data['kfold']==fold]\n","    \n","    train_features, valid_features = [[] for _ in range(2)]\n","    for i, row in train_set.iterrows():\n","        train_features += prepare_train_features(args, row, tokenizer)\n","    for i, row in valid_set.iterrows():\n","        valid_features += prepare_train_features(args, row, tokenizer)\n","\n","    train_dataset = DatasetRetriever(train_features)\n","    valid_dataset = DatasetRetriever(valid_features)\n","    print(f\"Num examples Train= {len(train_dataset)}, Num examples Valid={len(valid_dataset)}\")\n","    \n","    train_sampler = RandomSampler(train_dataset)\n","    valid_sampler = SequentialSampler(valid_dataset)\n","\n","    train_dataloader = DataLoader(\n","        train_dataset,\n","        batch_size=args.train_batch_size,\n","        sampler=train_sampler,\n","        num_workers=optimal_num_of_loader_workers(),\n","        pin_memory=True,\n","        drop_last=False \n","    )\n","\n","    valid_dataloader = DataLoader(\n","        valid_dataset,\n","        batch_size=args.eval_batch_size, \n","        sampler=valid_sampler,\n","        num_workers=optimal_num_of_loader_workers(),\n","        pin_memory=True, \n","        drop_last=False\n","    )\n","\n","    return train_dataloader, valid_dataloader"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i-zfYJGxP4iK"},"source":["### Trainer"]},{"cell_type":"code","metadata":{"id":"iFLvh1VQET35","execution":{"iopub.status.busy":"2021-08-17T20:26:36.310455Z","iopub.execute_input":"2021-08-17T20:26:36.310827Z","iopub.status.idle":"2021-08-17T20:26:36.325575Z","shell.execute_reply.started":"2021-08-17T20:26:36.310796Z","shell.execute_reply":"2021-08-17T20:26:36.324417Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1634445489925,"user_tz":-540,"elapsed":55,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["class Trainer:\n","    def __init__(\n","        self, model, tokenizer, \n","        optimizer, scheduler\n","    ):\n","        self.model = model\n","        self.tokenizer = tokenizer\n","        self.optimizer = optimizer\n","        self.scheduler = scheduler\n","\n","    def train(\n","        self, args, \n","        train_dataloader, \n","        epoch, result_dict\n","    ):\n","        count = 0\n","        losses = AverageMeter()\n","        \n","        self.model.zero_grad()\n","        self.model.train()\n","        \n","        fix_all_seeds(args.seed)\n","        \n","        for batch_idx, batch_data in enumerate(train_dataloader):\n","            input_ids, attention_mask, targets_start, targets_end = \\\n","                batch_data['input_ids'], batch_data['attention_mask'], \\\n","                    batch_data['start_position'], batch_data['end_position']\n","            \n","            input_ids, attention_mask, targets_start, targets_end = \\\n","                input_ids.cuda(), attention_mask.cuda(), targets_start.cuda(), targets_end.cuda()\n","\n","            outputs_start, outputs_end = self.model(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","            )\n","            \n","            loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n","            loss = loss / args.gradient_accumulation_steps\n","\n","            if args.fp16:\n","                with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n","                    scaled_loss.backward()\n","            else:\n","                loss.backward()\n","\n","            count += input_ids.size(0)\n","            losses.update(loss.item(), input_ids.size(0))\n","\n","            # if args.fp16:\n","            #     torch.nn.utils.clip_grad_norm_(amp.master_params(self.optimizer), args.max_grad_norm)\n","            # else:\n","            #     torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.max_grad_norm)\n","\n","            if batch_idx % args.gradient_accumulation_steps == 0 or batch_idx == len(train_dataloader) - 1:\n","                self.optimizer.step()\n","                self.scheduler.step()\n","                self.optimizer.zero_grad()\n","\n","            if (batch_idx % args.logging_steps == 0) or (batch_idx+1)==len(train_dataloader):\n","                _s = str(len(str(len(train_dataloader.sampler))))\n","                ret = [\n","                    ('Epoch: {:0>2} [{: >' + _s + '}/{} ({: >3.0f}%)]').format(epoch, count, len(train_dataloader.sampler), 100 * count / len(train_dataloader.sampler)),\n","                    'Train Loss: {: >4.5f}'.format(losses.avg),\n","                ]\n","                print(', '.join(ret))\n","\n","        result_dict['train_loss'].append(losses.avg)\n","        return result_dict"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7pmOF0dxP4iL"},"source":["### Evaluator"]},{"cell_type":"code","metadata":{"id":"1a8kG2UYET36","execution":{"iopub.status.busy":"2021-08-17T20:26:39.517516Z","iopub.execute_input":"2021-08-17T20:26:39.517886Z","iopub.status.idle":"2021-08-17T20:26:39.528591Z","shell.execute_reply.started":"2021-08-17T20:26:39.517856Z","shell.execute_reply":"2021-08-17T20:26:39.527569Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1634445489926,"user_tz":-540,"elapsed":56,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["class Evaluator:\n","    def __init__(self, model):\n","        self.model = model\n","    \n","    def save(self, result, output_dir):\n","        with open(f'{output_dir}/result_dict.json', 'w') as f:\n","            f.write(json.dumps(result, sort_keys=True, indent=4, ensure_ascii=False))\n","\n","    def evaluate(self, valid_dataloader, epoch, result_dict):\n","        losses = AverageMeter()\n","        for batch_idx, batch_data in enumerate(valid_dataloader):\n","            self.model = self.model.eval()\n","            input_ids, attention_mask, targets_start, targets_end = \\\n","                batch_data['input_ids'], batch_data['attention_mask'], \\\n","                    batch_data['start_position'], batch_data['end_position']\n","            \n","            input_ids, attention_mask, targets_start, targets_end = \\\n","                input_ids.cuda(), attention_mask.cuda(), targets_start.cuda(), targets_end.cuda()\n","            \n","            with torch.no_grad():            \n","                outputs_start, outputs_end = self.model(\n","                    input_ids=input_ids,\n","                    attention_mask=attention_mask,\n","                )\n","                \n","                loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n","                losses.update(loss.item(), input_ids.size(0))\n","                \n","        print('----Validation Results Summary----')\n","        print('Epoch: [{}] Valid Loss: {: >4.5f}'.format(epoch, losses.avg))\n","        result_dict['val_loss'].append(losses.avg)        \n","        return result_dict"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cAZPoRJeP4iM"},"source":["### Initialize Training"]},{"cell_type":"code","metadata":{"id":"v-gUDyq2ET37","execution":{"iopub.status.busy":"2021-08-17T20:26:44.41837Z","iopub.execute_input":"2021-08-17T20:26:44.418722Z","iopub.status.idle":"2021-08-17T20:26:44.428918Z","shell.execute_reply.started":"2021-08-17T20:26:44.418691Z","shell.execute_reply":"2021-08-17T20:26:44.428086Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1634445489926,"user_tz":-540,"elapsed":55,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["def init_training(args, data, fold):\n","    fix_all_seeds(args.seed)\n","    \n","    if not os.path.exists(args.output_dir):\n","        os.makedirs(args.output_dir)\n","    \n","    # model\n","    model_config, tokenizer, model = make_model(args)\n","    if torch.cuda.device_count() >= 1:\n","        print('Model pushed to {} GPU(s), type {}.'.format(\n","            torch.cuda.device_count(), \n","            torch.cuda.get_device_name(0))\n","        )\n","        model = model.cuda() \n","    else:\n","        raise ValueError('CPU training is not supported')\n","    \n","    # data loaders\n","    train_dataloader, valid_dataloader = make_loader(args, data, tokenizer, fold)\n","\n","    # optimizer\n","    optimizer = make_optimizer(args, model)\n","\n","    # scheduler\n","    num_training_steps = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps) * args.epochs\n","    if args.warmup_ratio > 0:\n","        num_warmup_steps = int(args.warmup_ratio * num_training_steps)\n","    else:\n","        num_warmup_steps = 0\n","    print(f\"Total Training Steps: {num_training_steps}, Total Warmup Steps: {num_warmup_steps}\")\n","    scheduler = make_scheduler(args, optimizer, num_warmup_steps, num_training_steps)\n","\n","    # mixed precision training with NVIDIA Apex\n","    if args.fp16:\n","        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n","    \n","    result_dict = {\n","        'epoch':[], \n","        'train_loss': [], \n","        'val_loss' : [], \n","        'best_val_loss': np.inf\n","    }\n","\n","    return (\n","        model, model_config, tokenizer, optimizer, scheduler, \n","        train_dataloader, valid_dataloader, result_dict\n","    )"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g6ZBX-5aP4iN"},"source":["### Run"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-12T15:50:27.097353Z","iopub.execute_input":"2021-08-12T15:50:27.097724Z","iopub.status.idle":"2021-08-12T15:50:27.112113Z","shell.execute_reply.started":"2021-08-12T15:50:27.097688Z","shell.execute_reply":"2021-08-12T15:50:27.111224Z"},"id":"39ei5Bm5ET37","trusted":true,"executionInfo":{"status":"ok","timestamp":1634445489927,"user_tz":-540,"elapsed":56,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["def run(data, fold):\n","    args = Config()\n","    model, model_config, tokenizer, optimizer, scheduler, train_dataloader, \\\n","        valid_dataloader, result_dict = init_training(args, data, fold)\n","    \n","    trainer = Trainer(model, tokenizer, optimizer, scheduler)\n","    evaluator = Evaluator(model)\n","\n","    train_time_list = []\n","    valid_time_list = []\n","\n","    for epoch in range(args.epochs):\n","        result_dict['epoch'].append(epoch)\n","\n","        # Train\n","        torch.cuda.synchronize()\n","        tic1 = time.time()\n","        result_dict = trainer.train(\n","            args, train_dataloader, \n","            epoch, result_dict\n","        )\n","        torch.cuda.synchronize()\n","        tic2 = time.time() \n","        train_time_list.append(tic2 - tic1)\n","        \n","        # Evaluate\n","        torch.cuda.synchronize()\n","        tic3 = time.time()\n","        result_dict = evaluator.evaluate(\n","            valid_dataloader, epoch, result_dict\n","        )\n","        torch.cuda.synchronize()\n","        tic4 = time.time() \n","        valid_time_list.append(tic4 - tic3)\n","            \n","        output_dir = os.path.join(args.output_dir, f\"checkpoint-fold-{fold}\")\n","        if result_dict['val_loss'][-1] < result_dict['best_val_loss']:\n","            print(\"{} Epoch, Best epoch was updated! Valid Loss: {: >4.5f}\".format(epoch, result_dict['val_loss'][-1]))\n","            result_dict[\"best_val_loss\"] = result_dict['val_loss'][-1]        \n","            \n","            os.makedirs(output_dir, exist_ok=True)\n","            torch.save(model.state_dict(), f\"{output_dir}/pytorch_model.bin\")\n","            model_config.save_pretrained(output_dir)\n","            tokenizer.save_pretrained(output_dir)\n","            print(f\"Saving model checkpoint to {output_dir}.\")\n","            \n","        print()\n","\n","    evaluator.save(result_dict, output_dir)\n","    \n","    print(f\"Total Training Time: {np.sum(train_time_list)}secs, Average Training Time per Epoch: {np.mean(train_time_list)}secs.\")\n","    print(f\"Total Validation Time: {np.sum(valid_time_list)}secs, Average Validation Time per Epoch: {np.mean(valid_time_list)}secs.\")\n","    \n","    torch.cuda.empty_cache()\n","    del trainer, evaluator\n","    del model, model_config, tokenizer\n","    del optimizer, scheduler\n","    del train_dataloader, valid_dataloader, result_dict\n","    gc.collect()"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"mPaGnnCnbhWl","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["49513bb6cffd483282ce3497cf547b65","37e3e1ed97e24dd4a6cd17b91512baa5","cb6bdf88e51240b1b72704aa5311ccc9","af313fa2e19c4692b0658bce024f7b66","d89ed0365514406cba08f06ad51fc1de","d3bde5515f444e04adaa032cd0fcdcfc","a3eea053f7ea42cb88c33796b2db599d","b89c954018714a76beece628fbe4d318","64ae008e76d944239d1c6d86b2868e57","383c486832644b619a64a69bfb1d0464","2bc04f576ee14e9ab9744865292a4f6d","34d83d56764e4c64b743681271d8fea2","0ebab91b2b574c36bbcdb1395749126c","11fd0a4638934ab0bcdb152e1ca789c7","935f81171e2e47908feb2d9591a1b550","b265548803224382920cf44851925654","d76868e41fce485b9a0f3fff6ce33b00","3f51fa8266a949f8b56fede7f46bb33a","7a6140ee506644b4957199117996b28a","bd2a7765dd514fcea5dbf5e5cd3bf3f8","dce3782940674ddf872c06dde25f18f7","875b8feab2234acb9d1576e7393bff3a","e485051d9f2c4aa090b7b0fd7d0ba420","7c1d0f1921c242799c55389fe26ebf9b","f334e486d16542e7a61469437cf7432e","ae0a4ca6396e45308d191b9a137eadd8","bbdf1696214e44e9ac4f213d0122cd51","9009184ad25844f0b46e246982822ee0","7eccbec1b3ba4a60818375a9d7c74ce2","b4d5112c5ef94e3092c1a2062c3d0fd6","1f6d67730c194390bdee2b66108a29fc","4824bd3cf83b49e2b09ff7769d2133ff","d69e6a1ab26742f186378c2ee463fcc9","931b5cfd9e104d818003847971fa97b8","d469c9d12e164380bc3ecfe95b73badc","605546891e2b42b1989b1e9e4bc8ed9b","de97fab22dec498682892396a50a786a","2ecbfe65d2df4fcd8826a343c220b2ef","6bf64c176049446383054b9482f25232","f8f4336a4a5d4cc0930b7f8424593cf7","ac9645d8d0b242d9b23bd192e6c5bf40","cf326fd5df92467f9170a6857b2497e0","61a87e1c75a74343881c364aefc06ee6","f3c5939e0ed946b8b174f844ac228c10","a3599314c1e94020b6187dd42d572f04","56c4bb53a2e347ba9c408ce188773316","8f184ff8d6b8494b93c18c94b0e9c384","75b4a0499a8d4556ae94d0202270f0d9","6473cad067754177bcb1c8f1639f82fc","79fe8b4f25b84bc2856f85f7f1dee043","49c0af75f9374a7fb088d90ee5388021","0f56f724d6d14ee19005b7da5513e379","558ce906fbbb4913af29144979eb70a5","281f93b6336b45da9b5dc08fec4ca640","b6beb03cda294c06a6cce2ca892e7dfb"]},"executionInfo":{"status":"ok","timestamp":1634474787108,"user_tz":-540,"elapsed":27235393,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"39860bd9-ce17-4f33-df7f-4cadca0889ee"},"source":["for fold in range(1,6):\n","    print();print()\n","    print('-'*50)\n","    print(f'FOLD: {fold}')\n","    print('-'*50)\n","    run(train, fold)"],"execution_count":19,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","\n","--------------------------------------------------\n","FOLD: 1\n","--------------------------------------------------\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"49513bb6cffd483282ce3497cf547b65","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/606 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"34d83d56764e4c64b743681271d8fea2","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/179 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e485051d9f2c4aa090b7b0fd7d0ba420","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/4.83M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"931b5cfd9e104d818003847971fa97b8","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/150 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a3599314c1e94020b6187dd42d572f04","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/2.09G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"output_type":"stream","name":"stdout","text":["Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n","Num examples Train= 18449, Num examples Valid=4620\n","Total Training Steps: 4614, Total Warmup Steps: 461\n","Epoch: 00 [    4/18449 (  0%)], Train Loss: 2.92419\n","Epoch: 00 [   44/18449 (  0%)], Train Loss: 2.89229\n","Epoch: 00 [   84/18449 (  0%)], Train Loss: 2.90368\n","Epoch: 00 [  124/18449 (  1%)], Train Loss: 2.88532\n","Epoch: 00 [  164/18449 (  1%)], Train Loss: 2.86039\n","Epoch: 00 [  204/18449 (  1%)], Train Loss: 2.83450\n","Epoch: 00 [  244/18449 (  1%)], Train Loss: 2.79651\n","Epoch: 00 [  284/18449 (  2%)], Train Loss: 2.75453\n","Epoch: 00 [  324/18449 (  2%)], Train Loss: 2.69757\n","Epoch: 00 [  364/18449 (  2%)], Train Loss: 2.63535\n","Epoch: 00 [  404/18449 (  2%)], Train Loss: 2.57537\n","Epoch: 00 [  444/18449 (  2%)], Train Loss: 2.50079\n","Epoch: 00 [  484/18449 (  3%)], Train Loss: 2.42299\n","Epoch: 00 [  524/18449 (  3%)], Train Loss: 2.35174\n","Epoch: 00 [  564/18449 (  3%)], Train Loss: 2.26446\n","Epoch: 00 [  604/18449 (  3%)], Train Loss: 2.16983\n","Epoch: 00 [  644/18449 (  3%)], Train Loss: 2.07035\n","Epoch: 00 [  684/18449 (  4%)], Train Loss: 1.99740\n","Epoch: 00 [  724/18449 (  4%)], Train Loss: 1.91731\n","Epoch: 00 [  764/18449 (  4%)], Train Loss: 1.84761\n","Epoch: 00 [  804/18449 (  4%)], Train Loss: 1.77700\n","Epoch: 00 [  844/18449 (  5%)], Train Loss: 1.71858\n","Epoch: 00 [  884/18449 (  5%)], Train Loss: 1.66879\n","Epoch: 00 [  924/18449 (  5%)], Train Loss: 1.61527\n","Epoch: 00 [  964/18449 (  5%)], Train Loss: 1.56522\n","Epoch: 00 [ 1004/18449 (  5%)], Train Loss: 1.51656\n","Epoch: 00 [ 1044/18449 (  6%)], Train Loss: 1.47441\n","Epoch: 00 [ 1084/18449 (  6%)], Train Loss: 1.43732\n","Epoch: 00 [ 1124/18449 (  6%)], Train Loss: 1.40679\n","Epoch: 00 [ 1164/18449 (  6%)], Train Loss: 1.37137\n","Epoch: 00 [ 1204/18449 (  7%)], Train Loss: 1.34022\n","Epoch: 00 [ 1244/18449 (  7%)], Train Loss: 1.30323\n","Epoch: 00 [ 1284/18449 (  7%)], Train Loss: 1.27384\n","Epoch: 00 [ 1324/18449 (  7%)], Train Loss: 1.24986\n","Epoch: 00 [ 1364/18449 (  7%)], Train Loss: 1.22628\n","Epoch: 00 [ 1404/18449 (  8%)], Train Loss: 1.20906\n","Epoch: 00 [ 1444/18449 (  8%)], Train Loss: 1.18624\n","Epoch: 00 [ 1484/18449 (  8%)], Train Loss: 1.16864\n","Epoch: 00 [ 1524/18449 (  8%)], Train Loss: 1.14850\n","Epoch: 00 [ 1564/18449 (  8%)], Train Loss: 1.12819\n","Epoch: 00 [ 1604/18449 (  9%)], Train Loss: 1.10952\n","Epoch: 00 [ 1644/18449 (  9%)], Train Loss: 1.08992\n","Epoch: 00 [ 1684/18449 (  9%)], Train Loss: 1.08083\n","Epoch: 00 [ 1724/18449 (  9%)], Train Loss: 1.06420\n","Epoch: 00 [ 1764/18449 ( 10%)], Train Loss: 1.05064\n","Epoch: 00 [ 1804/18449 ( 10%)], Train Loss: 1.03841\n","Epoch: 00 [ 1844/18449 ( 10%)], Train Loss: 1.02523\n","Epoch: 00 [ 1884/18449 ( 10%)], Train Loss: 1.00740\n","Epoch: 00 [ 1924/18449 ( 10%)], Train Loss: 0.99811\n","Epoch: 00 [ 1964/18449 ( 11%)], Train Loss: 0.98759\n","Epoch: 00 [ 2004/18449 ( 11%)], Train Loss: 0.97371\n","Epoch: 00 [ 2044/18449 ( 11%)], Train Loss: 0.96203\n","Epoch: 00 [ 2084/18449 ( 11%)], Train Loss: 0.94793\n","Epoch: 00 [ 2124/18449 ( 12%)], Train Loss: 0.93723\n","Epoch: 00 [ 2164/18449 ( 12%)], Train Loss: 0.92286\n","Epoch: 00 [ 2204/18449 ( 12%)], Train Loss: 0.91115\n","Epoch: 00 [ 2244/18449 ( 12%)], Train Loss: 0.90469\n","Epoch: 00 [ 2284/18449 ( 12%)], Train Loss: 0.89533\n","Epoch: 00 [ 2324/18449 ( 13%)], Train Loss: 0.88965\n","Epoch: 00 [ 2364/18449 ( 13%)], Train Loss: 0.88174\n","Epoch: 00 [ 2404/18449 ( 13%)], Train Loss: 0.87425\n","Epoch: 00 [ 2444/18449 ( 13%)], Train Loss: 0.86302\n","Epoch: 00 [ 2484/18449 ( 13%)], Train Loss: 0.85299\n","Epoch: 00 [ 2524/18449 ( 14%)], Train Loss: 0.84518\n","Epoch: 00 [ 2564/18449 ( 14%)], Train Loss: 0.83745\n","Epoch: 00 [ 2604/18449 ( 14%)], Train Loss: 0.82985\n","Epoch: 00 [ 2644/18449 ( 14%)], Train Loss: 0.82281\n","Epoch: 00 [ 2684/18449 ( 15%)], Train Loss: 0.81550\n","Epoch: 00 [ 2724/18449 ( 15%)], Train Loss: 0.80666\n","Epoch: 00 [ 2764/18449 ( 15%)], Train Loss: 0.80073\n","Epoch: 00 [ 2804/18449 ( 15%)], Train Loss: 0.79472\n","Epoch: 00 [ 2844/18449 ( 15%)], Train Loss: 0.78639\n","Epoch: 00 [ 2884/18449 ( 16%)], Train Loss: 0.78329\n","Epoch: 00 [ 2924/18449 ( 16%)], Train Loss: 0.77857\n","Epoch: 00 [ 2964/18449 ( 16%)], Train Loss: 0.77412\n","Epoch: 00 [ 3004/18449 ( 16%)], Train Loss: 0.77037\n","Epoch: 00 [ 3044/18449 ( 16%)], Train Loss: 0.76528\n","Epoch: 00 [ 3084/18449 ( 17%)], Train Loss: 0.76122\n","Epoch: 00 [ 3124/18449 ( 17%)], Train Loss: 0.75669\n","Epoch: 00 [ 3164/18449 ( 17%)], Train Loss: 0.75293\n","Epoch: 00 [ 3204/18449 ( 17%)], Train Loss: 0.75014\n","Epoch: 00 [ 3244/18449 ( 18%)], Train Loss: 0.74478\n","Epoch: 00 [ 3284/18449 ( 18%)], Train Loss: 0.73716\n","Epoch: 00 [ 3324/18449 ( 18%)], Train Loss: 0.73435\n","Epoch: 00 [ 3364/18449 ( 18%)], Train Loss: 0.72825\n","Epoch: 00 [ 3404/18449 ( 18%)], Train Loss: 0.72361\n","Epoch: 00 [ 3444/18449 ( 19%)], Train Loss: 0.71815\n","Epoch: 00 [ 3484/18449 ( 19%)], Train Loss: 0.71488\n","Epoch: 00 [ 3524/18449 ( 19%)], Train Loss: 0.71018\n","Epoch: 00 [ 3564/18449 ( 19%)], Train Loss: 0.70467\n","Epoch: 00 [ 3604/18449 ( 20%)], Train Loss: 0.69945\n","Epoch: 00 [ 3644/18449 ( 20%)], Train Loss: 0.69500\n","Epoch: 00 [ 3684/18449 ( 20%)], Train Loss: 0.69099\n","Epoch: 00 [ 3724/18449 ( 20%)], Train Loss: 0.68708\n","Epoch: 00 [ 3764/18449 ( 20%)], Train Loss: 0.68476\n","Epoch: 00 [ 3804/18449 ( 21%)], Train Loss: 0.67934\n","Epoch: 00 [ 3844/18449 ( 21%)], Train Loss: 0.67443\n","Epoch: 00 [ 3884/18449 ( 21%)], Train Loss: 0.67274\n","Epoch: 00 [ 3924/18449 ( 21%)], Train Loss: 0.66820\n","Epoch: 00 [ 3964/18449 ( 21%)], Train Loss: 0.66404\n","Epoch: 00 [ 4004/18449 ( 22%)], Train Loss: 0.66114\n","Epoch: 00 [ 4044/18449 ( 22%)], Train Loss: 0.65801\n","Epoch: 00 [ 4084/18449 ( 22%)], Train Loss: 0.65615\n","Epoch: 00 [ 4124/18449 ( 22%)], Train Loss: 0.65384\n","Epoch: 00 [ 4164/18449 ( 23%)], Train Loss: 0.64998\n","Epoch: 00 [ 4204/18449 ( 23%)], Train Loss: 0.64689\n","Epoch: 00 [ 4244/18449 ( 23%)], Train Loss: 0.64520\n","Epoch: 00 [ 4284/18449 ( 23%)], Train Loss: 0.64275\n","Epoch: 00 [ 4324/18449 ( 23%)], Train Loss: 0.64179\n","Epoch: 00 [ 4364/18449 ( 24%)], Train Loss: 0.64185\n","Epoch: 00 [ 4404/18449 ( 24%)], Train Loss: 0.63884\n","Epoch: 00 [ 4444/18449 ( 24%)], Train Loss: 0.63544\n","Epoch: 00 [ 4484/18449 ( 24%)], Train Loss: 0.63393\n","Epoch: 00 [ 4524/18449 ( 25%)], Train Loss: 0.63322\n","Epoch: 00 [ 4564/18449 ( 25%)], Train Loss: 0.63153\n","Epoch: 00 [ 4604/18449 ( 25%)], Train Loss: 0.62800\n","Epoch: 00 [ 4644/18449 ( 25%)], Train Loss: 0.62490\n","Epoch: 00 [ 4684/18449 ( 25%)], Train Loss: 0.62109\n","Epoch: 00 [ 4724/18449 ( 26%)], Train Loss: 0.62074\n","Epoch: 00 [ 4764/18449 ( 26%)], Train Loss: 0.61787\n","Epoch: 00 [ 4804/18449 ( 26%)], Train Loss: 0.61614\n","Epoch: 00 [ 4844/18449 ( 26%)], Train Loss: 0.61513\n","Epoch: 00 [ 4884/18449 ( 26%)], Train Loss: 0.61307\n","Epoch: 00 [ 4924/18449 ( 27%)], Train Loss: 0.60989\n","Epoch: 00 [ 4964/18449 ( 27%)], Train Loss: 0.60712\n","Epoch: 00 [ 5004/18449 ( 27%)], Train Loss: 0.60563\n","Epoch: 00 [ 5044/18449 ( 27%)], Train Loss: 0.60512\n","Epoch: 00 [ 5084/18449 ( 28%)], Train Loss: 0.60282\n","Epoch: 00 [ 5124/18449 ( 28%)], Train Loss: 0.60019\n","Epoch: 00 [ 5164/18449 ( 28%)], Train Loss: 0.59811\n","Epoch: 00 [ 5204/18449 ( 28%)], Train Loss: 0.59695\n","Epoch: 00 [ 5244/18449 ( 28%)], Train Loss: 0.59614\n","Epoch: 00 [ 5284/18449 ( 29%)], Train Loss: 0.59364\n","Epoch: 00 [ 5324/18449 ( 29%)], Train Loss: 0.59358\n","Epoch: 00 [ 5364/18449 ( 29%)], Train Loss: 0.59235\n","Epoch: 00 [ 5404/18449 ( 29%)], Train Loss: 0.58944\n","Epoch: 00 [ 5444/18449 ( 30%)], Train Loss: 0.58766\n","Epoch: 00 [ 5484/18449 ( 30%)], Train Loss: 0.58631\n","Epoch: 00 [ 5524/18449 ( 30%)], Train Loss: 0.58486\n","Epoch: 00 [ 5564/18449 ( 30%)], Train Loss: 0.58343\n","Epoch: 00 [ 5604/18449 ( 30%)], Train Loss: 0.58176\n","Epoch: 00 [ 5644/18449 ( 31%)], Train Loss: 0.58184\n","Epoch: 00 [ 5684/18449 ( 31%)], Train Loss: 0.57983\n","Epoch: 00 [ 5724/18449 ( 31%)], Train Loss: 0.57731\n","Epoch: 00 [ 5764/18449 ( 31%)], Train Loss: 0.57769\n","Epoch: 00 [ 5804/18449 ( 31%)], Train Loss: 0.57610\n","Epoch: 00 [ 5844/18449 ( 32%)], Train Loss: 0.57306\n","Epoch: 00 [ 5884/18449 ( 32%)], Train Loss: 0.57259\n","Epoch: 00 [ 5924/18449 ( 32%)], Train Loss: 0.57033\n","Epoch: 00 [ 5964/18449 ( 32%)], Train Loss: 0.56783\n","Epoch: 00 [ 6004/18449 ( 33%)], Train Loss: 0.56564\n","Epoch: 00 [ 6044/18449 ( 33%)], Train Loss: 0.56458\n","Epoch: 00 [ 6084/18449 ( 33%)], Train Loss: 0.56325\n","Epoch: 00 [ 6124/18449 ( 33%)], Train Loss: 0.56162\n","Epoch: 00 [ 6164/18449 ( 33%)], Train Loss: 0.55929\n","Epoch: 00 [ 6204/18449 ( 34%)], Train Loss: 0.55861\n","Epoch: 00 [ 6244/18449 ( 34%)], Train Loss: 0.55592\n","Epoch: 00 [ 6284/18449 ( 34%)], Train Loss: 0.55445\n","Epoch: 00 [ 6324/18449 ( 34%)], Train Loss: 0.55343\n","Epoch: 00 [ 6364/18449 ( 34%)], Train Loss: 0.55180\n","Epoch: 00 [ 6404/18449 ( 35%)], Train Loss: 0.55108\n","Epoch: 00 [ 6444/18449 ( 35%)], Train Loss: 0.55084\n","Epoch: 00 [ 6484/18449 ( 35%)], Train Loss: 0.54899\n","Epoch: 00 [ 6524/18449 ( 35%)], Train Loss: 0.54786\n","Epoch: 00 [ 6564/18449 ( 36%)], Train Loss: 0.54614\n","Epoch: 00 [ 6604/18449 ( 36%)], Train Loss: 0.54497\n","Epoch: 00 [ 6644/18449 ( 36%)], Train Loss: 0.54340\n","Epoch: 00 [ 6684/18449 ( 36%)], Train Loss: 0.54264\n","Epoch: 00 [ 6724/18449 ( 36%)], Train Loss: 0.54163\n","Epoch: 00 [ 6764/18449 ( 37%)], Train Loss: 0.54028\n","Epoch: 00 [ 6804/18449 ( 37%)], Train Loss: 0.53971\n","Epoch: 00 [ 6844/18449 ( 37%)], Train Loss: 0.53995\n","Epoch: 00 [ 6884/18449 ( 37%)], Train Loss: 0.53806\n","Epoch: 00 [ 6924/18449 ( 38%)], Train Loss: 0.53693\n","Epoch: 00 [ 6964/18449 ( 38%)], Train Loss: 0.53482\n","Epoch: 00 [ 7004/18449 ( 38%)], Train Loss: 0.53392\n","Epoch: 00 [ 7044/18449 ( 38%)], Train Loss: 0.53346\n","Epoch: 00 [ 7084/18449 ( 38%)], Train Loss: 0.53174\n","Epoch: 00 [ 7124/18449 ( 39%)], Train Loss: 0.52985\n","Epoch: 00 [ 7164/18449 ( 39%)], Train Loss: 0.52950\n","Epoch: 00 [ 7204/18449 ( 39%)], Train Loss: 0.52835\n","Epoch: 00 [ 7244/18449 ( 39%)], Train Loss: 0.52727\n","Epoch: 00 [ 7284/18449 ( 39%)], Train Loss: 0.52693\n","Epoch: 00 [ 7324/18449 ( 40%)], Train Loss: 0.52691\n","Epoch: 00 [ 7364/18449 ( 40%)], Train Loss: 0.52553\n","Epoch: 00 [ 7404/18449 ( 40%)], Train Loss: 0.52458\n","Epoch: 00 [ 7444/18449 ( 40%)], Train Loss: 0.52423\n","Epoch: 00 [ 7484/18449 ( 41%)], Train Loss: 0.52356\n","Epoch: 00 [ 7524/18449 ( 41%)], Train Loss: 0.52217\n","Epoch: 00 [ 7564/18449 ( 41%)], Train Loss: 0.52095\n","Epoch: 00 [ 7604/18449 ( 41%)], Train Loss: 0.52130\n","Epoch: 00 [ 7644/18449 ( 41%)], Train Loss: 0.52076\n","Epoch: 00 [ 7684/18449 ( 42%)], Train Loss: 0.51972\n","Epoch: 00 [ 7724/18449 ( 42%)], Train Loss: 0.51846\n","Epoch: 00 [ 7764/18449 ( 42%)], Train Loss: 0.51741\n","Epoch: 00 [ 7804/18449 ( 42%)], Train Loss: 0.51590\n","Epoch: 00 [ 7844/18449 ( 43%)], Train Loss: 0.51539\n","Epoch: 00 [ 7884/18449 ( 43%)], Train Loss: 0.51376\n","Epoch: 00 [ 7924/18449 ( 43%)], Train Loss: 0.51214\n","Epoch: 00 [ 7964/18449 ( 43%)], Train Loss: 0.51143\n","Epoch: 00 [ 8004/18449 ( 43%)], Train Loss: 0.51068\n","Epoch: 00 [ 8044/18449 ( 44%)], Train Loss: 0.50994\n","Epoch: 00 [ 8084/18449 ( 44%)], Train Loss: 0.50969\n","Epoch: 00 [ 8124/18449 ( 44%)], Train Loss: 0.50893\n","Epoch: 00 [ 8164/18449 ( 44%)], Train Loss: 0.50790\n","Epoch: 00 [ 8204/18449 ( 44%)], Train Loss: 0.50669\n","Epoch: 00 [ 8244/18449 ( 45%)], Train Loss: 0.50630\n","Epoch: 00 [ 8284/18449 ( 45%)], Train Loss: 0.50464\n","Epoch: 00 [ 8324/18449 ( 45%)], Train Loss: 0.50436\n","Epoch: 00 [ 8364/18449 ( 45%)], Train Loss: 0.50438\n","Epoch: 00 [ 8404/18449 ( 46%)], Train Loss: 0.50348\n","Epoch: 00 [ 8444/18449 ( 46%)], Train Loss: 0.50287\n","Epoch: 00 [ 8484/18449 ( 46%)], Train Loss: 0.50148\n","Epoch: 00 [ 8524/18449 ( 46%)], Train Loss: 0.50142\n","Epoch: 00 [ 8564/18449 ( 46%)], Train Loss: 0.50051\n","Epoch: 00 [ 8604/18449 ( 47%)], Train Loss: 0.49958\n","Epoch: 00 [ 8644/18449 ( 47%)], Train Loss: 0.49993\n","Epoch: 00 [ 8684/18449 ( 47%)], Train Loss: 0.50040\n","Epoch: 00 [ 8724/18449 ( 47%)], Train Loss: 0.49979\n","Epoch: 00 [ 8764/18449 ( 48%)], Train Loss: 0.49894\n","Epoch: 00 [ 8804/18449 ( 48%)], Train Loss: 0.49850\n","Epoch: 00 [ 8844/18449 ( 48%)], Train Loss: 0.49754\n","Epoch: 00 [ 8884/18449 ( 48%)], Train Loss: 0.49613\n","Epoch: 00 [ 8924/18449 ( 48%)], Train Loss: 0.49543\n","Epoch: 00 [ 8964/18449 ( 49%)], Train Loss: 0.49462\n","Epoch: 00 [ 9004/18449 ( 49%)], Train Loss: 0.49385\n","Epoch: 00 [ 9044/18449 ( 49%)], Train Loss: 0.49306\n","Epoch: 00 [ 9084/18449 ( 49%)], Train Loss: 0.49225\n","Epoch: 00 [ 9124/18449 ( 49%)], Train Loss: 0.49176\n","Epoch: 00 [ 9164/18449 ( 50%)], Train Loss: 0.49048\n","Epoch: 00 [ 9204/18449 ( 50%)], Train Loss: 0.49010\n","Epoch: 00 [ 9244/18449 ( 50%)], Train Loss: 0.48994\n","Epoch: 00 [ 9284/18449 ( 50%)], Train Loss: 0.48920\n","Epoch: 00 [ 9324/18449 ( 51%)], Train Loss: 0.48913\n","Epoch: 00 [ 9364/18449 ( 51%)], Train Loss: 0.48842\n","Epoch: 00 [ 9404/18449 ( 51%)], Train Loss: 0.48760\n","Epoch: 00 [ 9444/18449 ( 51%)], Train Loss: 0.48754\n","Epoch: 00 [ 9484/18449 ( 51%)], Train Loss: 0.48674\n","Epoch: 00 [ 9524/18449 ( 52%)], Train Loss: 0.48567\n","Epoch: 00 [ 9564/18449 ( 52%)], Train Loss: 0.48557\n","Epoch: 00 [ 9604/18449 ( 52%)], Train Loss: 0.48521\n","Epoch: 00 [ 9644/18449 ( 52%)], Train Loss: 0.48573\n","Epoch: 00 [ 9684/18449 ( 52%)], Train Loss: 0.48620\n","Epoch: 00 [ 9724/18449 ( 53%)], Train Loss: 0.48579\n","Epoch: 00 [ 9764/18449 ( 53%)], Train Loss: 0.48485\n","Epoch: 00 [ 9804/18449 ( 53%)], Train Loss: 0.48404\n","Epoch: 00 [ 9844/18449 ( 53%)], Train Loss: 0.48315\n","Epoch: 00 [ 9884/18449 ( 54%)], Train Loss: 0.48243\n","Epoch: 00 [ 9924/18449 ( 54%)], Train Loss: 0.48203\n","Epoch: 00 [ 9964/18449 ( 54%)], Train Loss: 0.48149\n","Epoch: 00 [10004/18449 ( 54%)], Train Loss: 0.48103\n","Epoch: 00 [10044/18449 ( 54%)], Train Loss: 0.48036\n","Epoch: 00 [10084/18449 ( 55%)], Train Loss: 0.48040\n","Epoch: 00 [10124/18449 ( 55%)], Train Loss: 0.47943\n","Epoch: 00 [10164/18449 ( 55%)], Train Loss: 0.47936\n","Epoch: 00 [10204/18449 ( 55%)], Train Loss: 0.47933\n","Epoch: 00 [10244/18449 ( 56%)], Train Loss: 0.47843\n","Epoch: 00 [10284/18449 ( 56%)], Train Loss: 0.47787\n","Epoch: 00 [10324/18449 ( 56%)], Train Loss: 0.47788\n","Epoch: 00 [10364/18449 ( 56%)], Train Loss: 0.47782\n","Epoch: 00 [10404/18449 ( 56%)], Train Loss: 0.47681\n","Epoch: 00 [10444/18449 ( 57%)], Train Loss: 0.47624\n","Epoch: 00 [10484/18449 ( 57%)], Train Loss: 0.47545\n","Epoch: 00 [10524/18449 ( 57%)], Train Loss: 0.47530\n","Epoch: 00 [10564/18449 ( 57%)], Train Loss: 0.47434\n","Epoch: 00 [10604/18449 ( 57%)], Train Loss: 0.47403\n","Epoch: 00 [10644/18449 ( 58%)], Train Loss: 0.47351\n","Epoch: 00 [10684/18449 ( 58%)], Train Loss: 0.47291\n","Epoch: 00 [10724/18449 ( 58%)], Train Loss: 0.47296\n","Epoch: 00 [10764/18449 ( 58%)], Train Loss: 0.47242\n","Epoch: 00 [10804/18449 ( 59%)], Train Loss: 0.47175\n","Epoch: 00 [10844/18449 ( 59%)], Train Loss: 0.47187\n","Epoch: 00 [10884/18449 ( 59%)], Train Loss: 0.47163\n","Epoch: 00 [10924/18449 ( 59%)], Train Loss: 0.47135\n","Epoch: 00 [10964/18449 ( 59%)], Train Loss: 0.47099\n","Epoch: 00 [11004/18449 ( 60%)], Train Loss: 0.47091\n","Epoch: 00 [11044/18449 ( 60%)], Train Loss: 0.47058\n","Epoch: 00 [11084/18449 ( 60%)], Train Loss: 0.47051\n","Epoch: 00 [11124/18449 ( 60%)], Train Loss: 0.47018\n","Epoch: 00 [11164/18449 ( 61%)], Train Loss: 0.47025\n","Epoch: 00 [11204/18449 ( 61%)], Train Loss: 0.47041\n","Epoch: 00 [11244/18449 ( 61%)], Train Loss: 0.47009\n","Epoch: 00 [11284/18449 ( 61%)], Train Loss: 0.47015\n","Epoch: 00 [11324/18449 ( 61%)], Train Loss: 0.46948\n","Epoch: 00 [11364/18449 ( 62%)], Train Loss: 0.46886\n","Epoch: 00 [11404/18449 ( 62%)], Train Loss: 0.46885\n","Epoch: 00 [11444/18449 ( 62%)], Train Loss: 0.46901\n","Epoch: 00 [11484/18449 ( 62%)], Train Loss: 0.46859\n","Epoch: 00 [11524/18449 ( 62%)], Train Loss: 0.46761\n","Epoch: 00 [11564/18449 ( 63%)], Train Loss: 0.46680\n","Epoch: 00 [11604/18449 ( 63%)], Train Loss: 0.46659\n","Epoch: 00 [11644/18449 ( 63%)], Train Loss: 0.46643\n","Epoch: 00 [11684/18449 ( 63%)], Train Loss: 0.46604\n","Epoch: 00 [11724/18449 ( 64%)], Train Loss: 0.46547\n","Epoch: 00 [11764/18449 ( 64%)], Train Loss: 0.46552\n","Epoch: 00 [11804/18449 ( 64%)], Train Loss: 0.46460\n","Epoch: 00 [11844/18449 ( 64%)], Train Loss: 0.46388\n","Epoch: 00 [11884/18449 ( 64%)], Train Loss: 0.46359\n","Epoch: 00 [11924/18449 ( 65%)], Train Loss: 0.46329\n","Epoch: 00 [11964/18449 ( 65%)], Train Loss: 0.46228\n","Epoch: 00 [12004/18449 ( 65%)], Train Loss: 0.46217\n","Epoch: 00 [12044/18449 ( 65%)], Train Loss: 0.46127\n","Epoch: 00 [12084/18449 ( 65%)], Train Loss: 0.46107\n","Epoch: 00 [12124/18449 ( 66%)], Train Loss: 0.46037\n","Epoch: 00 [12164/18449 ( 66%)], Train Loss: 0.45969\n","Epoch: 00 [12204/18449 ( 66%)], Train Loss: 0.45952\n","Epoch: 00 [12244/18449 ( 66%)], Train Loss: 0.45885\n","Epoch: 00 [12284/18449 ( 67%)], Train Loss: 0.45804\n","Epoch: 00 [12324/18449 ( 67%)], Train Loss: 0.45769\n","Epoch: 00 [12364/18449 ( 67%)], Train Loss: 0.45753\n","Epoch: 00 [12404/18449 ( 67%)], Train Loss: 0.45775\n","Epoch: 00 [12444/18449 ( 67%)], Train Loss: 0.45780\n","Epoch: 00 [12484/18449 ( 68%)], Train Loss: 0.45761\n","Epoch: 00 [12524/18449 ( 68%)], Train Loss: 0.45703\n","Epoch: 00 [12564/18449 ( 68%)], Train Loss: 0.45628\n","Epoch: 00 [12604/18449 ( 68%)], Train Loss: 0.45616\n","Epoch: 00 [12644/18449 ( 69%)], Train Loss: 0.45588\n","Epoch: 00 [12684/18449 ( 69%)], Train Loss: 0.45605\n","Epoch: 00 [12724/18449 ( 69%)], Train Loss: 0.45618\n","Epoch: 00 [12764/18449 ( 69%)], Train Loss: 0.45565\n","Epoch: 00 [12804/18449 ( 69%)], Train Loss: 0.45576\n","Epoch: 00 [12844/18449 ( 70%)], Train Loss: 0.45581\n","Epoch: 00 [12884/18449 ( 70%)], Train Loss: 0.45565\n","Epoch: 00 [12924/18449 ( 70%)], Train Loss: 0.45543\n","Epoch: 00 [12964/18449 ( 70%)], Train Loss: 0.45541\n","Epoch: 00 [13004/18449 ( 70%)], Train Loss: 0.45486\n","Epoch: 00 [13044/18449 ( 71%)], Train Loss: 0.45486\n","Epoch: 00 [13084/18449 ( 71%)], Train Loss: 0.45482\n","Epoch: 00 [13124/18449 ( 71%)], Train Loss: 0.45462\n","Epoch: 00 [13164/18449 ( 71%)], Train Loss: 0.45429\n","Epoch: 00 [13204/18449 ( 72%)], Train Loss: 0.45397\n","Epoch: 00 [13244/18449 ( 72%)], Train Loss: 0.45354\n","Epoch: 00 [13284/18449 ( 72%)], Train Loss: 0.45347\n","Epoch: 00 [13324/18449 ( 72%)], Train Loss: 0.45317\n","Epoch: 00 [13364/18449 ( 72%)], Train Loss: 0.45268\n","Epoch: 00 [13404/18449 ( 73%)], Train Loss: 0.45191\n","Epoch: 00 [13444/18449 ( 73%)], Train Loss: 0.45150\n","Epoch: 00 [13484/18449 ( 73%)], Train Loss: 0.45152\n","Epoch: 00 [13524/18449 ( 73%)], Train Loss: 0.45156\n","Epoch: 00 [13564/18449 ( 74%)], Train Loss: 0.45108\n","Epoch: 00 [13604/18449 ( 74%)], Train Loss: 0.45081\n","Epoch: 00 [13644/18449 ( 74%)], Train Loss: 0.45043\n","Epoch: 00 [13684/18449 ( 74%)], Train Loss: 0.45069\n","Epoch: 00 [13724/18449 ( 74%)], Train Loss: 0.45060\n","Epoch: 00 [13764/18449 ( 75%)], Train Loss: 0.45016\n","Epoch: 00 [13804/18449 ( 75%)], Train Loss: 0.44958\n","Epoch: 00 [13844/18449 ( 75%)], Train Loss: 0.44936\n","Epoch: 00 [13884/18449 ( 75%)], Train Loss: 0.44891\n","Epoch: 00 [13924/18449 ( 75%)], Train Loss: 0.44862\n","Epoch: 00 [13964/18449 ( 76%)], Train Loss: 0.44843\n","Epoch: 00 [14004/18449 ( 76%)], Train Loss: 0.44770\n","Epoch: 00 [14044/18449 ( 76%)], Train Loss: 0.44748\n","Epoch: 00 [14084/18449 ( 76%)], Train Loss: 0.44702\n","Epoch: 00 [14124/18449 ( 77%)], Train Loss: 0.44696\n","Epoch: 00 [14164/18449 ( 77%)], Train Loss: 0.44633\n","Epoch: 00 [14204/18449 ( 77%)], Train Loss: 0.44615\n","Epoch: 00 [14244/18449 ( 77%)], Train Loss: 0.44582\n","Epoch: 00 [14284/18449 ( 77%)], Train Loss: 0.44540\n","Epoch: 00 [14324/18449 ( 78%)], Train Loss: 0.44498\n","Epoch: 00 [14364/18449 ( 78%)], Train Loss: 0.44437\n","Epoch: 00 [14404/18449 ( 78%)], Train Loss: 0.44402\n","Epoch: 00 [14444/18449 ( 78%)], Train Loss: 0.44342\n","Epoch: 00 [14484/18449 ( 79%)], Train Loss: 0.44385\n","Epoch: 00 [14524/18449 ( 79%)], Train Loss: 0.44389\n","Epoch: 00 [14564/18449 ( 79%)], Train Loss: 0.44417\n","Epoch: 00 [14604/18449 ( 79%)], Train Loss: 0.44353\n","Epoch: 00 [14644/18449 ( 79%)], Train Loss: 0.44333\n","Epoch: 00 [14684/18449 ( 80%)], Train Loss: 0.44284\n","Epoch: 00 [14724/18449 ( 80%)], Train Loss: 0.44276\n","Epoch: 00 [14764/18449 ( 80%)], Train Loss: 0.44258\n","Epoch: 00 [14804/18449 ( 80%)], Train Loss: 0.44232\n","Epoch: 00 [14844/18449 ( 80%)], Train Loss: 0.44217\n","Epoch: 00 [14884/18449 ( 81%)], Train Loss: 0.44230\n","Epoch: 00 [14924/18449 ( 81%)], Train Loss: 0.44186\n","Epoch: 00 [14964/18449 ( 81%)], Train Loss: 0.44140\n","Epoch: 00 [15004/18449 ( 81%)], Train Loss: 0.44048\n","Epoch: 00 [15044/18449 ( 82%)], Train Loss: 0.44006\n","Epoch: 00 [15084/18449 ( 82%)], Train Loss: 0.44030\n","Epoch: 00 [15124/18449 ( 82%)], Train Loss: 0.43974\n","Epoch: 00 [15164/18449 ( 82%)], Train Loss: 0.43948\n","Epoch: 00 [15204/18449 ( 82%)], Train Loss: 0.43896\n","Epoch: 00 [15244/18449 ( 83%)], Train Loss: 0.43844\n","Epoch: 00 [15284/18449 ( 83%)], Train Loss: 0.43836\n","Epoch: 00 [15324/18449 ( 83%)], Train Loss: 0.43820\n","Epoch: 00 [15364/18449 ( 83%)], Train Loss: 0.43765\n","Epoch: 00 [15404/18449 ( 83%)], Train Loss: 0.43702\n","Epoch: 00 [15444/18449 ( 84%)], Train Loss: 0.43653\n","Epoch: 00 [15484/18449 ( 84%)], Train Loss: 0.43590\n","Epoch: 00 [15524/18449 ( 84%)], Train Loss: 0.43614\n","Epoch: 00 [15564/18449 ( 84%)], Train Loss: 0.43608\n","Epoch: 00 [15604/18449 ( 85%)], Train Loss: 0.43589\n","Epoch: 00 [15644/18449 ( 85%)], Train Loss: 0.43557\n","Epoch: 00 [15684/18449 ( 85%)], Train Loss: 0.43544\n","Epoch: 00 [15724/18449 ( 85%)], Train Loss: 0.43505\n","Epoch: 00 [15764/18449 ( 85%)], Train Loss: 0.43449\n","Epoch: 00 [15804/18449 ( 86%)], Train Loss: 0.43384\n","Epoch: 00 [15844/18449 ( 86%)], Train Loss: 0.43373\n","Epoch: 00 [15884/18449 ( 86%)], Train Loss: 0.43310\n","Epoch: 00 [15924/18449 ( 86%)], Train Loss: 0.43241\n","Epoch: 00 [15964/18449 ( 87%)], Train Loss: 0.43192\n","Epoch: 00 [16004/18449 ( 87%)], Train Loss: 0.43257\n","Epoch: 00 [16044/18449 ( 87%)], Train Loss: 0.43229\n","Epoch: 00 [16084/18449 ( 87%)], Train Loss: 0.43224\n","Epoch: 00 [16124/18449 ( 87%)], Train Loss: 0.43180\n","Epoch: 00 [16164/18449 ( 88%)], Train Loss: 0.43115\n","Epoch: 00 [16204/18449 ( 88%)], Train Loss: 0.43089\n","Epoch: 00 [16244/18449 ( 88%)], Train Loss: 0.43054\n","Epoch: 00 [16284/18449 ( 88%)], Train Loss: 0.43033\n","Epoch: 00 [16324/18449 ( 88%)], Train Loss: 0.42979\n","Epoch: 00 [16364/18449 ( 89%)], Train Loss: 0.42960\n","Epoch: 00 [16404/18449 ( 89%)], Train Loss: 0.42975\n","Epoch: 00 [16444/18449 ( 89%)], Train Loss: 0.42915\n","Epoch: 00 [16484/18449 ( 89%)], Train Loss: 0.42875\n","Epoch: 00 [16524/18449 ( 90%)], Train Loss: 0.42879\n","Epoch: 00 [16564/18449 ( 90%)], Train Loss: 0.42855\n","Epoch: 00 [16604/18449 ( 90%)], Train Loss: 0.42805\n","Epoch: 00 [16644/18449 ( 90%)], Train Loss: 0.42751\n","Epoch: 00 [16684/18449 ( 90%)], Train Loss: 0.42712\n","Epoch: 00 [16724/18449 ( 91%)], Train Loss: 0.42662\n","Epoch: 00 [16764/18449 ( 91%)], Train Loss: 0.42633\n","Epoch: 00 [16804/18449 ( 91%)], Train Loss: 0.42624\n","Epoch: 00 [16844/18449 ( 91%)], Train Loss: 0.42558\n","Epoch: 00 [16884/18449 ( 92%)], Train Loss: 0.42537\n","Epoch: 00 [16924/18449 ( 92%)], Train Loss: 0.42506\n","Epoch: 00 [16964/18449 ( 92%)], Train Loss: 0.42486\n","Epoch: 00 [17004/18449 ( 92%)], Train Loss: 0.42453\n","Epoch: 00 [17044/18449 ( 92%)], Train Loss: 0.42441\n","Epoch: 00 [17084/18449 ( 93%)], Train Loss: 0.42448\n","Epoch: 00 [17124/18449 ( 93%)], Train Loss: 0.42435\n","Epoch: 00 [17164/18449 ( 93%)], Train Loss: 0.42439\n","Epoch: 00 [17204/18449 ( 93%)], Train Loss: 0.42403\n","Epoch: 00 [17244/18449 ( 93%)], Train Loss: 0.42385\n","Epoch: 00 [17284/18449 ( 94%)], Train Loss: 0.42364\n","Epoch: 00 [17324/18449 ( 94%)], Train Loss: 0.42359\n","Epoch: 00 [17364/18449 ( 94%)], Train Loss: 0.42322\n","Epoch: 00 [17404/18449 ( 94%)], Train Loss: 0.42302\n","Epoch: 00 [17444/18449 ( 95%)], Train Loss: 0.42246\n","Epoch: 00 [17484/18449 ( 95%)], Train Loss: 0.42216\n","Epoch: 00 [17524/18449 ( 95%)], Train Loss: 0.42236\n","Epoch: 00 [17564/18449 ( 95%)], Train Loss: 0.42206\n","Epoch: 00 [17604/18449 ( 95%)], Train Loss: 0.42156\n","Epoch: 00 [17644/18449 ( 96%)], Train Loss: 0.42155\n","Epoch: 00 [17684/18449 ( 96%)], Train Loss: 0.42142\n","Epoch: 00 [17724/18449 ( 96%)], Train Loss: 0.42141\n","Epoch: 00 [17764/18449 ( 96%)], Train Loss: 0.42106\n","Epoch: 00 [17804/18449 ( 97%)], Train Loss: 0.42074\n","Epoch: 00 [17844/18449 ( 97%)], Train Loss: 0.42087\n","Epoch: 00 [17884/18449 ( 97%)], Train Loss: 0.42067\n","Epoch: 00 [17924/18449 ( 97%)], Train Loss: 0.42028\n","Epoch: 00 [17964/18449 ( 97%)], Train Loss: 0.41989\n","Epoch: 00 [18004/18449 ( 98%)], Train Loss: 0.41931\n","Epoch: 00 [18044/18449 ( 98%)], Train Loss: 0.41871\n","Epoch: 00 [18084/18449 ( 98%)], Train Loss: 0.41885\n","Epoch: 00 [18124/18449 ( 98%)], Train Loss: 0.41861\n","Epoch: 00 [18164/18449 ( 98%)], Train Loss: 0.41847\n","Epoch: 00 [18204/18449 ( 99%)], Train Loss: 0.41807\n","Epoch: 00 [18244/18449 ( 99%)], Train Loss: 0.41795\n","Epoch: 00 [18284/18449 ( 99%)], Train Loss: 0.41782\n","Epoch: 00 [18324/18449 ( 99%)], Train Loss: 0.41746\n","Epoch: 00 [18364/18449 (100%)], Train Loss: 0.41726\n","Epoch: 00 [18404/18449 (100%)], Train Loss: 0.41727\n","Epoch: 00 [18444/18449 (100%)], Train Loss: 0.41690\n","Epoch: 00 [18449/18449 (100%)], Train Loss: 0.41709\n","----Validation Results Summary----\n","Epoch: [0] Valid Loss: 0.62316\n","0 Epoch, Best epoch was updated! Valid Loss: 0.62316\n","Saving model checkpoint to chaii-qa-5-fold-xlmroberta-torch-fit/checkpoint-fold-1.\n","\n","Epoch: 01 [    4/18449 (  0%)], Train Loss: 0.28625\n","Epoch: 01 [   44/18449 (  0%)], Train Loss: 0.22958\n","Epoch: 01 [   84/18449 (  0%)], Train Loss: 0.40255\n","Epoch: 01 [  124/18449 (  1%)], Train Loss: 0.37741\n","Epoch: 01 [  164/18449 (  1%)], Train Loss: 0.35607\n","Epoch: 01 [  204/18449 (  1%)], Train Loss: 0.35800\n","Epoch: 01 [  244/18449 (  1%)], Train Loss: 0.35971\n","Epoch: 01 [  284/18449 (  2%)], Train Loss: 0.35954\n","Epoch: 01 [  324/18449 (  2%)], Train Loss: 0.33391\n","Epoch: 01 [  364/18449 (  2%)], Train Loss: 0.33015\n","Epoch: 01 [  404/18449 (  2%)], Train Loss: 0.35012\n","Epoch: 01 [  444/18449 (  2%)], Train Loss: 0.35117\n","Epoch: 01 [  484/18449 (  3%)], Train Loss: 0.34517\n","Epoch: 01 [  524/18449 (  3%)], Train Loss: 0.35411\n","Epoch: 01 [  564/18449 (  3%)], Train Loss: 0.34444\n","Epoch: 01 [  604/18449 (  3%)], Train Loss: 0.33966\n","Epoch: 01 [  644/18449 (  3%)], Train Loss: 0.32571\n","Epoch: 01 [  684/18449 (  4%)], Train Loss: 0.33592\n","Epoch: 01 [  724/18449 (  4%)], Train Loss: 0.32989\n","Epoch: 01 [  764/18449 (  4%)], Train Loss: 0.32206\n","Epoch: 01 [  804/18449 (  4%)], Train Loss: 0.31125\n","Epoch: 01 [  844/18449 (  5%)], Train Loss: 0.31044\n","Epoch: 01 [  884/18449 (  5%)], Train Loss: 0.30946\n","Epoch: 01 [  924/18449 (  5%)], Train Loss: 0.30279\n","Epoch: 01 [  964/18449 (  5%)], Train Loss: 0.29811\n","Epoch: 01 [ 1004/18449 (  5%)], Train Loss: 0.29190\n","Epoch: 01 [ 1044/18449 (  6%)], Train Loss: 0.28870\n","Epoch: 01 [ 1084/18449 (  6%)], Train Loss: 0.28632\n","Epoch: 01 [ 1124/18449 (  6%)], Train Loss: 0.28599\n","Epoch: 01 [ 1164/18449 (  6%)], Train Loss: 0.28314\n","Epoch: 01 [ 1204/18449 (  7%)], Train Loss: 0.28127\n","Epoch: 01 [ 1244/18449 (  7%)], Train Loss: 0.27349\n","Epoch: 01 [ 1284/18449 (  7%)], Train Loss: 0.26802\n","Epoch: 01 [ 1324/18449 (  7%)], Train Loss: 0.27087\n","Epoch: 01 [ 1364/18449 (  7%)], Train Loss: 0.26879\n","Epoch: 01 [ 1404/18449 (  8%)], Train Loss: 0.27227\n","Epoch: 01 [ 1444/18449 (  8%)], Train Loss: 0.26848\n","Epoch: 01 [ 1484/18449 (  8%)], Train Loss: 0.26884\n","Epoch: 01 [ 1524/18449 (  8%)], Train Loss: 0.26520\n","Epoch: 01 [ 1564/18449 (  8%)], Train Loss: 0.26433\n","Epoch: 01 [ 1604/18449 (  9%)], Train Loss: 0.26325\n","Epoch: 01 [ 1644/18449 (  9%)], Train Loss: 0.26188\n","Epoch: 01 [ 1684/18449 (  9%)], Train Loss: 0.26587\n","Epoch: 01 [ 1724/18449 (  9%)], Train Loss: 0.26602\n","Epoch: 01 [ 1764/18449 ( 10%)], Train Loss: 0.26546\n","Epoch: 01 [ 1804/18449 ( 10%)], Train Loss: 0.26517\n","Epoch: 01 [ 1844/18449 ( 10%)], Train Loss: 0.26207\n","Epoch: 01 [ 1884/18449 ( 10%)], Train Loss: 0.25860\n","Epoch: 01 [ 1924/18449 ( 10%)], Train Loss: 0.26068\n","Epoch: 01 [ 1964/18449 ( 11%)], Train Loss: 0.25953\n","Epoch: 01 [ 2004/18449 ( 11%)], Train Loss: 0.25543\n","Epoch: 01 [ 2044/18449 ( 11%)], Train Loss: 0.25467\n","Epoch: 01 [ 2084/18449 ( 11%)], Train Loss: 0.25155\n","Epoch: 01 [ 2124/18449 ( 12%)], Train Loss: 0.25071\n","Epoch: 01 [ 2164/18449 ( 12%)], Train Loss: 0.24731\n","Epoch: 01 [ 2204/18449 ( 12%)], Train Loss: 0.24467\n","Epoch: 01 [ 2244/18449 ( 12%)], Train Loss: 0.24577\n","Epoch: 01 [ 2284/18449 ( 12%)], Train Loss: 0.24440\n","Epoch: 01 [ 2324/18449 ( 13%)], Train Loss: 0.24422\n","Epoch: 01 [ 2364/18449 ( 13%)], Train Loss: 0.24245\n","Epoch: 01 [ 2404/18449 ( 13%)], Train Loss: 0.24093\n","Epoch: 01 [ 2444/18449 ( 13%)], Train Loss: 0.23773\n","Epoch: 01 [ 2484/18449 ( 13%)], Train Loss: 0.23447\n","Epoch: 01 [ 2524/18449 ( 14%)], Train Loss: 0.23197\n","Epoch: 01 [ 2564/18449 ( 14%)], Train Loss: 0.23049\n","Epoch: 01 [ 2604/18449 ( 14%)], Train Loss: 0.22911\n","Epoch: 01 [ 2644/18449 ( 14%)], Train Loss: 0.22681\n","Epoch: 01 [ 2684/18449 ( 15%)], Train Loss: 0.22523\n","Epoch: 01 [ 2724/18449 ( 15%)], Train Loss: 0.22253\n","Epoch: 01 [ 2764/18449 ( 15%)], Train Loss: 0.22151\n","Epoch: 01 [ 2804/18449 ( 15%)], Train Loss: 0.22079\n","Epoch: 01 [ 2844/18449 ( 15%)], Train Loss: 0.21846\n","Epoch: 01 [ 2884/18449 ( 16%)], Train Loss: 0.21737\n","Epoch: 01 [ 2924/18449 ( 16%)], Train Loss: 0.21562\n","Epoch: 01 [ 2964/18449 ( 16%)], Train Loss: 0.21607\n","Epoch: 01 [ 3004/18449 ( 16%)], Train Loss: 0.21633\n","Epoch: 01 [ 3044/18449 ( 16%)], Train Loss: 0.21556\n","Epoch: 01 [ 3084/18449 ( 17%)], Train Loss: 0.21437\n","Epoch: 01 [ 3124/18449 ( 17%)], Train Loss: 0.21312\n","Epoch: 01 [ 3164/18449 ( 17%)], Train Loss: 0.21155\n","Epoch: 01 [ 3204/18449 ( 17%)], Train Loss: 0.21145\n","Epoch: 01 [ 3244/18449 ( 18%)], Train Loss: 0.21022\n","Epoch: 01 [ 3284/18449 ( 18%)], Train Loss: 0.20798\n","Epoch: 01 [ 3324/18449 ( 18%)], Train Loss: 0.20748\n","Epoch: 01 [ 3364/18449 ( 18%)], Train Loss: 0.20678\n","Epoch: 01 [ 3404/18449 ( 18%)], Train Loss: 0.20560\n","Epoch: 01 [ 3444/18449 ( 19%)], Train Loss: 0.20380\n","Epoch: 01 [ 3484/18449 ( 19%)], Train Loss: 0.20231\n","Epoch: 01 [ 3524/18449 ( 19%)], Train Loss: 0.20044\n","Epoch: 01 [ 3564/18449 ( 19%)], Train Loss: 0.19839\n","Epoch: 01 [ 3604/18449 ( 20%)], Train Loss: 0.19709\n","Epoch: 01 [ 3644/18449 ( 20%)], Train Loss: 0.19639\n","Epoch: 01 [ 3684/18449 ( 20%)], Train Loss: 0.19530\n","Epoch: 01 [ 3724/18449 ( 20%)], Train Loss: 0.19366\n","Epoch: 01 [ 3764/18449 ( 20%)], Train Loss: 0.19324\n","Epoch: 01 [ 3804/18449 ( 21%)], Train Loss: 0.19149\n","Epoch: 01 [ 3844/18449 ( 21%)], Train Loss: 0.19044\n","Epoch: 01 [ 3884/18449 ( 21%)], Train Loss: 0.19001\n","Epoch: 01 [ 3924/18449 ( 21%)], Train Loss: 0.18890\n","Epoch: 01 [ 3964/18449 ( 21%)], Train Loss: 0.18767\n","Epoch: 01 [ 4004/18449 ( 22%)], Train Loss: 0.18644\n","Epoch: 01 [ 4044/18449 ( 22%)], Train Loss: 0.18556\n","Epoch: 01 [ 4084/18449 ( 22%)], Train Loss: 0.18569\n","Epoch: 01 [ 4124/18449 ( 22%)], Train Loss: 0.18552\n","Epoch: 01 [ 4164/18449 ( 23%)], Train Loss: 0.18426\n","Epoch: 01 [ 4204/18449 ( 23%)], Train Loss: 0.18345\n","Epoch: 01 [ 4244/18449 ( 23%)], Train Loss: 0.18276\n","Epoch: 01 [ 4284/18449 ( 23%)], Train Loss: 0.18216\n","Epoch: 01 [ 4324/18449 ( 23%)], Train Loss: 0.18151\n","Epoch: 01 [ 4364/18449 ( 24%)], Train Loss: 0.18187\n","Epoch: 01 [ 4404/18449 ( 24%)], Train Loss: 0.18101\n","Epoch: 01 [ 4444/18449 ( 24%)], Train Loss: 0.18083\n","Epoch: 01 [ 4484/18449 ( 24%)], Train Loss: 0.17992\n","Epoch: 01 [ 4524/18449 ( 25%)], Train Loss: 0.18012\n","Epoch: 01 [ 4564/18449 ( 25%)], Train Loss: 0.17989\n","Epoch: 01 [ 4604/18449 ( 25%)], Train Loss: 0.17882\n","Epoch: 01 [ 4644/18449 ( 25%)], Train Loss: 0.17783\n","Epoch: 01 [ 4684/18449 ( 25%)], Train Loss: 0.17710\n","Epoch: 01 [ 4724/18449 ( 26%)], Train Loss: 0.17732\n","Epoch: 01 [ 4764/18449 ( 26%)], Train Loss: 0.17668\n","Epoch: 01 [ 4804/18449 ( 26%)], Train Loss: 0.17674\n","Epoch: 01 [ 4844/18449 ( 26%)], Train Loss: 0.17725\n","Epoch: 01 [ 4884/18449 ( 26%)], Train Loss: 0.17720\n","Epoch: 01 [ 4924/18449 ( 27%)], Train Loss: 0.17618\n","Epoch: 01 [ 4964/18449 ( 27%)], Train Loss: 0.17558\n","Epoch: 01 [ 5004/18449 ( 27%)], Train Loss: 0.17488\n","Epoch: 01 [ 5044/18449 ( 27%)], Train Loss: 0.17480\n","Epoch: 01 [ 5084/18449 ( 28%)], Train Loss: 0.17437\n","Epoch: 01 [ 5124/18449 ( 28%)], Train Loss: 0.17340\n","Epoch: 01 [ 5164/18449 ( 28%)], Train Loss: 0.17357\n","Epoch: 01 [ 5204/18449 ( 28%)], Train Loss: 0.17340\n","Epoch: 01 [ 5244/18449 ( 28%)], Train Loss: 0.17335\n","Epoch: 01 [ 5284/18449 ( 29%)], Train Loss: 0.17239\n","Epoch: 01 [ 5324/18449 ( 29%)], Train Loss: 0.17237\n","Epoch: 01 [ 5364/18449 ( 29%)], Train Loss: 0.17212\n","Epoch: 01 [ 5404/18449 ( 29%)], Train Loss: 0.17137\n","Epoch: 01 [ 5444/18449 ( 30%)], Train Loss: 0.17094\n","Epoch: 01 [ 5484/18449 ( 30%)], Train Loss: 0.17091\n","Epoch: 01 [ 5524/18449 ( 30%)], Train Loss: 0.17126\n","Epoch: 01 [ 5564/18449 ( 30%)], Train Loss: 0.17125\n","Epoch: 01 [ 5604/18449 ( 30%)], Train Loss: 0.17129\n","Epoch: 01 [ 5644/18449 ( 31%)], Train Loss: 0.17202\n","Epoch: 01 [ 5684/18449 ( 31%)], Train Loss: 0.17129\n","Epoch: 01 [ 5724/18449 ( 31%)], Train Loss: 0.17062\n","Epoch: 01 [ 5764/18449 ( 31%)], Train Loss: 0.17100\n","Epoch: 01 [ 5804/18449 ( 31%)], Train Loss: 0.17076\n","Epoch: 01 [ 5844/18449 ( 32%)], Train Loss: 0.16989\n","Epoch: 01 [ 5884/18449 ( 32%)], Train Loss: 0.16939\n","Epoch: 01 [ 5924/18449 ( 32%)], Train Loss: 0.16870\n","Epoch: 01 [ 5964/18449 ( 32%)], Train Loss: 0.16809\n","Epoch: 01 [ 6004/18449 ( 33%)], Train Loss: 0.16742\n","Epoch: 01 [ 6044/18449 ( 33%)], Train Loss: 0.16728\n","Epoch: 01 [ 6084/18449 ( 33%)], Train Loss: 0.16681\n","Epoch: 01 [ 6124/18449 ( 33%)], Train Loss: 0.16616\n","Epoch: 01 [ 6164/18449 ( 33%)], Train Loss: 0.16570\n","Epoch: 01 [ 6204/18449 ( 34%)], Train Loss: 0.16600\n","Epoch: 01 [ 6244/18449 ( 34%)], Train Loss: 0.16509\n","Epoch: 01 [ 6284/18449 ( 34%)], Train Loss: 0.16450\n","Epoch: 01 [ 6324/18449 ( 34%)], Train Loss: 0.16425\n","Epoch: 01 [ 6364/18449 ( 34%)], Train Loss: 0.16463\n","Epoch: 01 [ 6404/18449 ( 35%)], Train Loss: 0.16480\n","Epoch: 01 [ 6444/18449 ( 35%)], Train Loss: 0.16466\n","Epoch: 01 [ 6484/18449 ( 35%)], Train Loss: 0.16433\n","Epoch: 01 [ 6524/18449 ( 35%)], Train Loss: 0.16413\n","Epoch: 01 [ 6564/18449 ( 36%)], Train Loss: 0.16368\n","Epoch: 01 [ 6604/18449 ( 36%)], Train Loss: 0.16316\n","Epoch: 01 [ 6644/18449 ( 36%)], Train Loss: 0.16242\n","Epoch: 01 [ 6684/18449 ( 36%)], Train Loss: 0.16216\n","Epoch: 01 [ 6724/18449 ( 36%)], Train Loss: 0.16145\n","Epoch: 01 [ 6764/18449 ( 37%)], Train Loss: 0.16142\n","Epoch: 01 [ 6804/18449 ( 37%)], Train Loss: 0.16124\n","Epoch: 01 [ 6844/18449 ( 37%)], Train Loss: 0.16130\n","Epoch: 01 [ 6884/18449 ( 37%)], Train Loss: 0.16062\n","Epoch: 01 [ 6924/18449 ( 38%)], Train Loss: 0.16003\n","Epoch: 01 [ 6964/18449 ( 38%)], Train Loss: 0.15949\n","Epoch: 01 [ 7004/18449 ( 38%)], Train Loss: 0.15883\n","Epoch: 01 [ 7044/18449 ( 38%)], Train Loss: 0.15887\n","Epoch: 01 [ 7084/18449 ( 38%)], Train Loss: 0.15821\n","Epoch: 01 [ 7124/18449 ( 39%)], Train Loss: 0.15750\n","Epoch: 01 [ 7164/18449 ( 39%)], Train Loss: 0.15725\n","Epoch: 01 [ 7204/18449 ( 39%)], Train Loss: 0.15666\n","Epoch: 01 [ 7244/18449 ( 39%)], Train Loss: 0.15607\n","Epoch: 01 [ 7284/18449 ( 39%)], Train Loss: 0.15597\n","Epoch: 01 [ 7324/18449 ( 40%)], Train Loss: 0.15607\n","Epoch: 01 [ 7364/18449 ( 40%)], Train Loss: 0.15537\n","Epoch: 01 [ 7404/18449 ( 40%)], Train Loss: 0.15529\n","Epoch: 01 [ 7444/18449 ( 40%)], Train Loss: 0.15530\n","Epoch: 01 [ 7484/18449 ( 41%)], Train Loss: 0.15502\n","Epoch: 01 [ 7524/18449 ( 41%)], Train Loss: 0.15431\n","Epoch: 01 [ 7564/18449 ( 41%)], Train Loss: 0.15394\n","Epoch: 01 [ 7604/18449 ( 41%)], Train Loss: 0.15410\n","Epoch: 01 [ 7644/18449 ( 41%)], Train Loss: 0.15414\n","Epoch: 01 [ 7684/18449 ( 42%)], Train Loss: 0.15380\n","Epoch: 01 [ 7724/18449 ( 42%)], Train Loss: 0.15349\n","Epoch: 01 [ 7764/18449 ( 42%)], Train Loss: 0.15312\n","Epoch: 01 [ 7804/18449 ( 42%)], Train Loss: 0.15265\n","Epoch: 01 [ 7844/18449 ( 43%)], Train Loss: 0.15245\n","Epoch: 01 [ 7884/18449 ( 43%)], Train Loss: 0.15224\n","Epoch: 01 [ 7924/18449 ( 43%)], Train Loss: 0.15176\n","Epoch: 01 [ 7964/18449 ( 43%)], Train Loss: 0.15182\n","Epoch: 01 [ 8004/18449 ( 43%)], Train Loss: 0.15171\n","Epoch: 01 [ 8044/18449 ( 44%)], Train Loss: 0.15197\n","Epoch: 01 [ 8084/18449 ( 44%)], Train Loss: 0.15217\n","Epoch: 01 [ 8124/18449 ( 44%)], Train Loss: 0.15188\n","Epoch: 01 [ 8164/18449 ( 44%)], Train Loss: 0.15137\n","Epoch: 01 [ 8204/18449 ( 44%)], Train Loss: 0.15086\n","Epoch: 01 [ 8244/18449 ( 45%)], Train Loss: 0.15066\n","Epoch: 01 [ 8284/18449 ( 45%)], Train Loss: 0.15033\n","Epoch: 01 [ 8324/18449 ( 45%)], Train Loss: 0.15025\n","Epoch: 01 [ 8364/18449 ( 45%)], Train Loss: 0.15053\n","Epoch: 01 [ 8404/18449 ( 46%)], Train Loss: 0.15018\n","Epoch: 01 [ 8444/18449 ( 46%)], Train Loss: 0.15000\n","Epoch: 01 [ 8484/18449 ( 46%)], Train Loss: 0.14942\n","Epoch: 01 [ 8524/18449 ( 46%)], Train Loss: 0.14921\n","Epoch: 01 [ 8564/18449 ( 46%)], Train Loss: 0.14891\n","Epoch: 01 [ 8604/18449 ( 47%)], Train Loss: 0.14850\n","Epoch: 01 [ 8644/18449 ( 47%)], Train Loss: 0.14845\n","Epoch: 01 [ 8684/18449 ( 47%)], Train Loss: 0.14914\n","Epoch: 01 [ 8724/18449 ( 47%)], Train Loss: 0.14923\n","Epoch: 01 [ 8764/18449 ( 48%)], Train Loss: 0.14896\n","Epoch: 01 [ 8804/18449 ( 48%)], Train Loss: 0.14888\n","Epoch: 01 [ 8844/18449 ( 48%)], Train Loss: 0.14860\n","Epoch: 01 [ 8884/18449 ( 48%)], Train Loss: 0.14815\n","Epoch: 01 [ 8924/18449 ( 48%)], Train Loss: 0.14801\n","Epoch: 01 [ 8964/18449 ( 49%)], Train Loss: 0.14766\n","Epoch: 01 [ 9004/18449 ( 49%)], Train Loss: 0.14728\n","Epoch: 01 [ 9044/18449 ( 49%)], Train Loss: 0.14675\n","Epoch: 01 [ 9084/18449 ( 49%)], Train Loss: 0.14655\n","Epoch: 01 [ 9124/18449 ( 49%)], Train Loss: 0.14619\n","Epoch: 01 [ 9164/18449 ( 50%)], Train Loss: 0.14581\n","Epoch: 01 [ 9204/18449 ( 50%)], Train Loss: 0.14566\n","Epoch: 01 [ 9244/18449 ( 50%)], Train Loss: 0.14546\n","Epoch: 01 [ 9284/18449 ( 50%)], Train Loss: 0.14499\n","Epoch: 01 [ 9324/18449 ( 51%)], Train Loss: 0.14551\n","Epoch: 01 [ 9364/18449 ( 51%)], Train Loss: 0.14542\n","Epoch: 01 [ 9404/18449 ( 51%)], Train Loss: 0.14502\n","Epoch: 01 [ 9444/18449 ( 51%)], Train Loss: 0.14523\n","Epoch: 01 [ 9484/18449 ( 51%)], Train Loss: 0.14501\n","Epoch: 01 [ 9524/18449 ( 52%)], Train Loss: 0.14472\n","Epoch: 01 [ 9564/18449 ( 52%)], Train Loss: 0.14446\n","Epoch: 01 [ 9604/18449 ( 52%)], Train Loss: 0.14432\n","Epoch: 01 [ 9644/18449 ( 52%)], Train Loss: 0.14452\n","Epoch: 01 [ 9684/18449 ( 52%)], Train Loss: 0.14474\n","Epoch: 01 [ 9724/18449 ( 53%)], Train Loss: 0.14464\n","Epoch: 01 [ 9764/18449 ( 53%)], Train Loss: 0.14437\n","Epoch: 01 [ 9804/18449 ( 53%)], Train Loss: 0.14409\n","Epoch: 01 [ 9844/18449 ( 53%)], Train Loss: 0.14380\n","Epoch: 01 [ 9884/18449 ( 54%)], Train Loss: 0.14368\n","Epoch: 01 [ 9924/18449 ( 54%)], Train Loss: 0.14376\n","Epoch: 01 [ 9964/18449 ( 54%)], Train Loss: 0.14356\n","Epoch: 01 [10004/18449 ( 54%)], Train Loss: 0.14336\n","Epoch: 01 [10044/18449 ( 54%)], Train Loss: 0.14327\n","Epoch: 01 [10084/18449 ( 55%)], Train Loss: 0.14316\n","Epoch: 01 [10124/18449 ( 55%)], Train Loss: 0.14302\n","Epoch: 01 [10164/18449 ( 55%)], Train Loss: 0.14313\n","Epoch: 01 [10204/18449 ( 55%)], Train Loss: 0.14314\n","Epoch: 01 [10244/18449 ( 56%)], Train Loss: 0.14279\n","Epoch: 01 [10284/18449 ( 56%)], Train Loss: 0.14286\n","Epoch: 01 [10324/18449 ( 56%)], Train Loss: 0.14289\n","Epoch: 01 [10364/18449 ( 56%)], Train Loss: 0.14298\n","Epoch: 01 [10404/18449 ( 56%)], Train Loss: 0.14254\n","Epoch: 01 [10444/18449 ( 57%)], Train Loss: 0.14246\n","Epoch: 01 [10484/18449 ( 57%)], Train Loss: 0.14216\n","Epoch: 01 [10524/18449 ( 57%)], Train Loss: 0.14235\n","Epoch: 01 [10564/18449 ( 57%)], Train Loss: 0.14210\n","Epoch: 01 [10604/18449 ( 57%)], Train Loss: 0.14197\n","Epoch: 01 [10644/18449 ( 58%)], Train Loss: 0.14187\n","Epoch: 01 [10684/18449 ( 58%)], Train Loss: 0.14153\n","Epoch: 01 [10724/18449 ( 58%)], Train Loss: 0.14140\n","Epoch: 01 [10764/18449 ( 58%)], Train Loss: 0.14096\n","Epoch: 01 [10804/18449 ( 59%)], Train Loss: 0.14080\n","Epoch: 01 [10844/18449 ( 59%)], Train Loss: 0.14069\n","Epoch: 01 [10884/18449 ( 59%)], Train Loss: 0.14050\n","Epoch: 01 [10924/18449 ( 59%)], Train Loss: 0.14034\n","Epoch: 01 [10964/18449 ( 59%)], Train Loss: 0.14013\n","Epoch: 01 [11004/18449 ( 60%)], Train Loss: 0.14002\n","Epoch: 01 [11044/18449 ( 60%)], Train Loss: 0.13977\n","Epoch: 01 [11084/18449 ( 60%)], Train Loss: 0.13959\n","Epoch: 01 [11124/18449 ( 60%)], Train Loss: 0.13943\n","Epoch: 01 [11164/18449 ( 61%)], Train Loss: 0.13919\n","Epoch: 01 [11204/18449 ( 61%)], Train Loss: 0.13905\n","Epoch: 01 [11244/18449 ( 61%)], Train Loss: 0.13899\n","Epoch: 01 [11284/18449 ( 61%)], Train Loss: 0.13900\n","Epoch: 01 [11324/18449 ( 61%)], Train Loss: 0.13922\n","Epoch: 01 [11364/18449 ( 62%)], Train Loss: 0.13910\n","Epoch: 01 [11404/18449 ( 62%)], Train Loss: 0.13934\n","Epoch: 01 [11444/18449 ( 62%)], Train Loss: 0.13959\n","Epoch: 01 [11484/18449 ( 62%)], Train Loss: 0.13944\n","Epoch: 01 [11524/18449 ( 62%)], Train Loss: 0.13921\n","Epoch: 01 [11564/18449 ( 63%)], Train Loss: 0.13897\n","Epoch: 01 [11604/18449 ( 63%)], Train Loss: 0.13874\n","Epoch: 01 [11644/18449 ( 63%)], Train Loss: 0.13849\n","Epoch: 01 [11684/18449 ( 63%)], Train Loss: 0.13843\n","Epoch: 01 [11724/18449 ( 64%)], Train Loss: 0.13827\n","Epoch: 01 [11764/18449 ( 64%)], Train Loss: 0.13813\n","Epoch: 01 [11804/18449 ( 64%)], Train Loss: 0.13781\n","Epoch: 01 [11844/18449 ( 64%)], Train Loss: 0.13744\n","Epoch: 01 [11884/18449 ( 64%)], Train Loss: 0.13757\n","Epoch: 01 [11924/18449 ( 65%)], Train Loss: 0.13742\n","Epoch: 01 [11964/18449 ( 65%)], Train Loss: 0.13704\n","Epoch: 01 [12004/18449 ( 65%)], Train Loss: 0.13690\n","Epoch: 01 [12044/18449 ( 65%)], Train Loss: 0.13676\n","Epoch: 01 [12084/18449 ( 65%)], Train Loss: 0.13672\n","Epoch: 01 [12124/18449 ( 66%)], Train Loss: 0.13666\n","Epoch: 01 [12164/18449 ( 66%)], Train Loss: 0.13636\n","Epoch: 01 [12204/18449 ( 66%)], Train Loss: 0.13615\n","Epoch: 01 [12244/18449 ( 66%)], Train Loss: 0.13599\n","Epoch: 01 [12284/18449 ( 67%)], Train Loss: 0.13572\n","Epoch: 01 [12324/18449 ( 67%)], Train Loss: 0.13549\n","Epoch: 01 [12364/18449 ( 67%)], Train Loss: 0.13563\n","Epoch: 01 [12404/18449 ( 67%)], Train Loss: 0.13603\n","Epoch: 01 [12444/18449 ( 67%)], Train Loss: 0.13612\n","Epoch: 01 [12484/18449 ( 68%)], Train Loss: 0.13595\n","Epoch: 01 [12524/18449 ( 68%)], Train Loss: 0.13581\n","Epoch: 01 [12564/18449 ( 68%)], Train Loss: 0.13555\n","Epoch: 01 [12604/18449 ( 68%)], Train Loss: 0.13572\n","Epoch: 01 [12644/18449 ( 69%)], Train Loss: 0.13552\n","Epoch: 01 [12684/18449 ( 69%)], Train Loss: 0.13544\n","Epoch: 01 [12724/18449 ( 69%)], Train Loss: 0.13555\n","Epoch: 01 [12764/18449 ( 69%)], Train Loss: 0.13539\n","Epoch: 01 [12804/18449 ( 69%)], Train Loss: 0.13533\n","Epoch: 01 [12844/18449 ( 70%)], Train Loss: 0.13548\n","Epoch: 01 [12884/18449 ( 70%)], Train Loss: 0.13554\n","Epoch: 01 [12924/18449 ( 70%)], Train Loss: 0.13554\n","Epoch: 01 [12964/18449 ( 70%)], Train Loss: 0.13546\n","Epoch: 01 [13004/18449 ( 70%)], Train Loss: 0.13525\n","Epoch: 01 [13044/18449 ( 71%)], Train Loss: 0.13513\n","Epoch: 01 [13084/18449 ( 71%)], Train Loss: 0.13492\n","Epoch: 01 [13124/18449 ( 71%)], Train Loss: 0.13500\n","Epoch: 01 [13164/18449 ( 71%)], Train Loss: 0.13500\n","Epoch: 01 [13204/18449 ( 72%)], Train Loss: 0.13496\n","Epoch: 01 [13244/18449 ( 72%)], Train Loss: 0.13465\n","Epoch: 01 [13284/18449 ( 72%)], Train Loss: 0.13468\n","Epoch: 01 [13324/18449 ( 72%)], Train Loss: 0.13464\n","Epoch: 01 [13364/18449 ( 72%)], Train Loss: 0.13433\n","Epoch: 01 [13404/18449 ( 73%)], Train Loss: 0.13410\n","Epoch: 01 [13444/18449 ( 73%)], Train Loss: 0.13384\n","Epoch: 01 [13484/18449 ( 73%)], Train Loss: 0.13383\n","Epoch: 01 [13524/18449 ( 73%)], Train Loss: 0.13389\n","Epoch: 01 [13564/18449 ( 74%)], Train Loss: 0.13370\n","Epoch: 01 [13604/18449 ( 74%)], Train Loss: 0.13359\n","Epoch: 01 [13644/18449 ( 74%)], Train Loss: 0.13346\n","Epoch: 01 [13684/18449 ( 74%)], Train Loss: 0.13335\n","Epoch: 01 [13724/18449 ( 74%)], Train Loss: 0.13355\n","Epoch: 01 [13764/18449 ( 75%)], Train Loss: 0.13345\n","Epoch: 01 [13804/18449 ( 75%)], Train Loss: 0.13338\n","Epoch: 01 [13844/18449 ( 75%)], Train Loss: 0.13336\n","Epoch: 01 [13884/18449 ( 75%)], Train Loss: 0.13325\n","Epoch: 01 [13924/18449 ( 75%)], Train Loss: 0.13317\n","Epoch: 01 [13964/18449 ( 76%)], Train Loss: 0.13303\n","Epoch: 01 [14004/18449 ( 76%)], Train Loss: 0.13276\n","Epoch: 01 [14044/18449 ( 76%)], Train Loss: 0.13261\n","Epoch: 01 [14084/18449 ( 76%)], Train Loss: 0.13254\n","Epoch: 01 [14124/18449 ( 77%)], Train Loss: 0.13242\n","Epoch: 01 [14164/18449 ( 77%)], Train Loss: 0.13217\n","Epoch: 01 [14204/18449 ( 77%)], Train Loss: 0.13216\n","Epoch: 01 [14244/18449 ( 77%)], Train Loss: 0.13217\n","Epoch: 01 [14284/18449 ( 77%)], Train Loss: 0.13198\n","Epoch: 01 [14324/18449 ( 78%)], Train Loss: 0.13176\n","Epoch: 01 [14364/18449 ( 78%)], Train Loss: 0.13154\n","Epoch: 01 [14404/18449 ( 78%)], Train Loss: 0.13155\n","Epoch: 01 [14444/18449 ( 78%)], Train Loss: 0.13132\n","Epoch: 01 [14484/18449 ( 79%)], Train Loss: 0.13127\n","Epoch: 01 [14524/18449 ( 79%)], Train Loss: 0.13136\n","Epoch: 01 [14564/18449 ( 79%)], Train Loss: 0.13173\n","Epoch: 01 [14604/18449 ( 79%)], Train Loss: 0.13155\n","Epoch: 01 [14644/18449 ( 79%)], Train Loss: 0.13159\n","Epoch: 01 [14684/18449 ( 80%)], Train Loss: 0.13144\n","Epoch: 01 [14724/18449 ( 80%)], Train Loss: 0.13152\n","Epoch: 01 [14764/18449 ( 80%)], Train Loss: 0.13136\n","Epoch: 01 [14804/18449 ( 80%)], Train Loss: 0.13129\n","Epoch: 01 [14844/18449 ( 80%)], Train Loss: 0.13131\n","Epoch: 01 [14884/18449 ( 81%)], Train Loss: 0.13129\n","Epoch: 01 [14924/18449 ( 81%)], Train Loss: 0.13126\n","Epoch: 01 [14964/18449 ( 81%)], Train Loss: 0.13107\n","Epoch: 01 [15004/18449 ( 81%)], Train Loss: 0.13082\n","Epoch: 01 [15044/18449 ( 82%)], Train Loss: 0.13063\n","Epoch: 01 [15084/18449 ( 82%)], Train Loss: 0.13077\n","Epoch: 01 [15124/18449 ( 82%)], Train Loss: 0.13074\n","Epoch: 01 [15164/18449 ( 82%)], Train Loss: 0.13058\n","Epoch: 01 [15204/18449 ( 82%)], Train Loss: 0.13038\n","Epoch: 01 [15244/18449 ( 83%)], Train Loss: 0.13024\n","Epoch: 01 [15284/18449 ( 83%)], Train Loss: 0.13023\n","Epoch: 01 [15324/18449 ( 83%)], Train Loss: 0.13000\n","Epoch: 01 [15364/18449 ( 83%)], Train Loss: 0.12988\n","Epoch: 01 [15404/18449 ( 83%)], Train Loss: 0.12962\n","Epoch: 01 [15444/18449 ( 84%)], Train Loss: 0.12938\n","Epoch: 01 [15484/18449 ( 84%)], Train Loss: 0.12918\n","Epoch: 01 [15524/18449 ( 84%)], Train Loss: 0.12901\n","Epoch: 01 [15564/18449 ( 84%)], Train Loss: 0.12886\n","Epoch: 01 [15604/18449 ( 85%)], Train Loss: 0.12888\n","Epoch: 01 [15644/18449 ( 85%)], Train Loss: 0.12890\n","Epoch: 01 [15684/18449 ( 85%)], Train Loss: 0.12882\n","Epoch: 01 [15724/18449 ( 85%)], Train Loss: 0.12866\n","Epoch: 01 [15764/18449 ( 85%)], Train Loss: 0.12856\n","Epoch: 01 [15804/18449 ( 86%)], Train Loss: 0.12827\n","Epoch: 01 [15844/18449 ( 86%)], Train Loss: 0.12819\n","Epoch: 01 [15884/18449 ( 86%)], Train Loss: 0.12804\n","Epoch: 01 [15924/18449 ( 86%)], Train Loss: 0.12784\n","Epoch: 01 [15964/18449 ( 87%)], Train Loss: 0.12767\n","Epoch: 01 [16004/18449 ( 87%)], Train Loss: 0.12769\n","Epoch: 01 [16044/18449 ( 87%)], Train Loss: 0.12758\n","Epoch: 01 [16084/18449 ( 87%)], Train Loss: 0.12778\n","Epoch: 01 [16124/18449 ( 87%)], Train Loss: 0.12756\n","Epoch: 01 [16164/18449 ( 88%)], Train Loss: 0.12739\n","Epoch: 01 [16204/18449 ( 88%)], Train Loss: 0.12726\n","Epoch: 01 [16244/18449 ( 88%)], Train Loss: 0.12718\n","Epoch: 01 [16284/18449 ( 88%)], Train Loss: 0.12755\n","Epoch: 01 [16324/18449 ( 88%)], Train Loss: 0.12738\n","Epoch: 01 [16364/18449 ( 89%)], Train Loss: 0.12734\n","Epoch: 01 [16404/18449 ( 89%)], Train Loss: 0.12744\n","Epoch: 01 [16444/18449 ( 89%)], Train Loss: 0.12729\n","Epoch: 01 [16484/18449 ( 89%)], Train Loss: 0.12726\n","Epoch: 01 [16524/18449 ( 90%)], Train Loss: 0.12713\n","Epoch: 01 [16564/18449 ( 90%)], Train Loss: 0.12699\n","Epoch: 01 [16604/18449 ( 90%)], Train Loss: 0.12698\n","Epoch: 01 [16644/18449 ( 90%)], Train Loss: 0.12687\n","Epoch: 01 [16684/18449 ( 90%)], Train Loss: 0.12668\n","Epoch: 01 [16724/18449 ( 91%)], Train Loss: 0.12656\n","Epoch: 01 [16764/18449 ( 91%)], Train Loss: 0.12640\n","Epoch: 01 [16804/18449 ( 91%)], Train Loss: 0.12633\n","Epoch: 01 [16844/18449 ( 91%)], Train Loss: 0.12617\n","Epoch: 01 [16884/18449 ( 92%)], Train Loss: 0.12618\n","Epoch: 01 [16924/18449 ( 92%)], Train Loss: 0.12625\n","Epoch: 01 [16964/18449 ( 92%)], Train Loss: 0.12613\n","Epoch: 01 [17004/18449 ( 92%)], Train Loss: 0.12606\n","Epoch: 01 [17044/18449 ( 92%)], Train Loss: 0.12601\n","Epoch: 01 [17084/18449 ( 93%)], Train Loss: 0.12590\n","Epoch: 01 [17124/18449 ( 93%)], Train Loss: 0.12577\n","Epoch: 01 [17164/18449 ( 93%)], Train Loss: 0.12591\n","Epoch: 01 [17204/18449 ( 93%)], Train Loss: 0.12587\n","Epoch: 01 [17244/18449 ( 93%)], Train Loss: 0.12586\n","Epoch: 01 [17284/18449 ( 94%)], Train Loss: 0.12567\n","Epoch: 01 [17324/18449 ( 94%)], Train Loss: 0.12567\n","Epoch: 01 [17364/18449 ( 94%)], Train Loss: 0.12552\n","Epoch: 01 [17404/18449 ( 94%)], Train Loss: 0.12542\n","Epoch: 01 [17444/18449 ( 95%)], Train Loss: 0.12530\n","Epoch: 01 [17484/18449 ( 95%)], Train Loss: 0.12537\n","Epoch: 01 [17524/18449 ( 95%)], Train Loss: 0.12544\n","Epoch: 01 [17564/18449 ( 95%)], Train Loss: 0.12541\n","Epoch: 01 [17604/18449 ( 95%)], Train Loss: 0.12527\n","Epoch: 01 [17644/18449 ( 96%)], Train Loss: 0.12530\n","Epoch: 01 [17684/18449 ( 96%)], Train Loss: 0.12531\n","Epoch: 01 [17724/18449 ( 96%)], Train Loss: 0.12543\n","Epoch: 01 [17764/18449 ( 96%)], Train Loss: 0.12538\n","Epoch: 01 [17804/18449 ( 97%)], Train Loss: 0.12535\n","Epoch: 01 [17844/18449 ( 97%)], Train Loss: 0.12547\n","Epoch: 01 [17884/18449 ( 97%)], Train Loss: 0.12539\n","Epoch: 01 [17924/18449 ( 97%)], Train Loss: 0.12530\n","Epoch: 01 [17964/18449 ( 97%)], Train Loss: 0.12533\n","Epoch: 01 [18004/18449 ( 98%)], Train Loss: 0.12516\n","Epoch: 01 [18044/18449 ( 98%)], Train Loss: 0.12494\n","Epoch: 01 [18084/18449 ( 98%)], Train Loss: 0.12498\n","Epoch: 01 [18124/18449 ( 98%)], Train Loss: 0.12494\n","Epoch: 01 [18164/18449 ( 98%)], Train Loss: 0.12487\n","Epoch: 01 [18204/18449 ( 99%)], Train Loss: 0.12472\n","Epoch: 01 [18244/18449 ( 99%)], Train Loss: 0.12475\n","Epoch: 01 [18284/18449 ( 99%)], Train Loss: 0.12480\n","Epoch: 01 [18324/18449 ( 99%)], Train Loss: 0.12471\n","Epoch: 01 [18364/18449 (100%)], Train Loss: 0.12471\n","Epoch: 01 [18404/18449 (100%)], Train Loss: 0.12471\n","Epoch: 01 [18444/18449 (100%)], Train Loss: 0.12456\n","Epoch: 01 [18449/18449 (100%)], Train Loss: 0.12463\n","----Validation Results Summary----\n","Epoch: [1] Valid Loss: 0.71733\n","\n","Total Training Time: 5127.9898409843445secs, Average Training Time per Epoch: 2563.9949204921722secs.\n","Total Validation Time: 388.28557872772217secs, Average Validation Time per Epoch: 194.14278936386108secs.\n","\n","\n","--------------------------------------------------\n","FOLD: 2\n","--------------------------------------------------\n","Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n","Num examples Train= 17967, Num examples Valid=5102\n","Total Training Steps: 4492, Total Warmup Steps: 449\n","Epoch: 00 [    4/17967 (  0%)], Train Loss: 2.90929\n","Epoch: 00 [   44/17967 (  0%)], Train Loss: 2.91739\n","Epoch: 00 [   84/17967 (  0%)], Train Loss: 2.89664\n","Epoch: 00 [  124/17967 (  1%)], Train Loss: 2.87783\n","Epoch: 00 [  164/17967 (  1%)], Train Loss: 2.84672\n","Epoch: 00 [  204/17967 (  1%)], Train Loss: 2.80913\n","Epoch: 00 [  244/17967 (  1%)], Train Loss: 2.77531\n","Epoch: 00 [  284/17967 (  2%)], Train Loss: 2.72952\n","Epoch: 00 [  324/17967 (  2%)], Train Loss: 2.67412\n","Epoch: 00 [  364/17967 (  2%)], Train Loss: 2.62483\n","Epoch: 00 [  404/17967 (  2%)], Train Loss: 2.56436\n","Epoch: 00 [  444/17967 (  2%)], Train Loss: 2.48704\n","Epoch: 00 [  484/17967 (  3%)], Train Loss: 2.40730\n","Epoch: 00 [  524/17967 (  3%)], Train Loss: 2.31268\n","Epoch: 00 [  564/17967 (  3%)], Train Loss: 2.20843\n","Epoch: 00 [  604/17967 (  3%)], Train Loss: 2.10172\n","Epoch: 00 [  644/17967 (  4%)], Train Loss: 2.02851\n","Epoch: 00 [  684/17967 (  4%)], Train Loss: 1.95410\n","Epoch: 00 [  724/17967 (  4%)], Train Loss: 1.87663\n","Epoch: 00 [  764/17967 (  4%)], Train Loss: 1.80517\n","Epoch: 00 [  804/17967 (  4%)], Train Loss: 1.73736\n","Epoch: 00 [  844/17967 (  5%)], Train Loss: 1.67245\n","Epoch: 00 [  884/17967 (  5%)], Train Loss: 1.62122\n","Epoch: 00 [  924/17967 (  5%)], Train Loss: 1.57259\n","Epoch: 00 [  964/17967 (  5%)], Train Loss: 1.52251\n","Epoch: 00 [ 1004/17967 (  6%)], Train Loss: 1.47411\n","Epoch: 00 [ 1044/17967 (  6%)], Train Loss: 1.43306\n","Epoch: 00 [ 1084/17967 (  6%)], Train Loss: 1.39611\n","Epoch: 00 [ 1124/17967 (  6%)], Train Loss: 1.36017\n","Epoch: 00 [ 1164/17967 (  6%)], Train Loss: 1.32353\n","Epoch: 00 [ 1204/17967 (  7%)], Train Loss: 1.29779\n","Epoch: 00 [ 1244/17967 (  7%)], Train Loss: 1.27877\n","Epoch: 00 [ 1284/17967 (  7%)], Train Loss: 1.25958\n","Epoch: 00 [ 1324/17967 (  7%)], Train Loss: 1.23662\n","Epoch: 00 [ 1364/17967 (  8%)], Train Loss: 1.21246\n","Epoch: 00 [ 1404/17967 (  8%)], Train Loss: 1.18820\n","Epoch: 00 [ 1444/17967 (  8%)], Train Loss: 1.16751\n","Epoch: 00 [ 1484/17967 (  8%)], Train Loss: 1.14969\n","Epoch: 00 [ 1524/17967 (  8%)], Train Loss: 1.13358\n","Epoch: 00 [ 1564/17967 (  9%)], Train Loss: 1.11149\n","Epoch: 00 [ 1604/17967 (  9%)], Train Loss: 1.09041\n","Epoch: 00 [ 1644/17967 (  9%)], Train Loss: 1.07384\n","Epoch: 00 [ 1684/17967 (  9%)], Train Loss: 1.05846\n","Epoch: 00 [ 1724/17967 ( 10%)], Train Loss: 1.04319\n","Epoch: 00 [ 1764/17967 ( 10%)], Train Loss: 1.02670\n","Epoch: 00 [ 1804/17967 ( 10%)], Train Loss: 1.01459\n","Epoch: 00 [ 1844/17967 ( 10%)], Train Loss: 0.99736\n","Epoch: 00 [ 1884/17967 ( 10%)], Train Loss: 0.98295\n","Epoch: 00 [ 1924/17967 ( 11%)], Train Loss: 0.97076\n","Epoch: 00 [ 1964/17967 ( 11%)], Train Loss: 0.95722\n","Epoch: 00 [ 2004/17967 ( 11%)], Train Loss: 0.94539\n","Epoch: 00 [ 2044/17967 ( 11%)], Train Loss: 0.93092\n","Epoch: 00 [ 2084/17967 ( 12%)], Train Loss: 0.91706\n","Epoch: 00 [ 2124/17967 ( 12%)], Train Loss: 0.90949\n","Epoch: 00 [ 2164/17967 ( 12%)], Train Loss: 0.89782\n","Epoch: 00 [ 2204/17967 ( 12%)], Train Loss: 0.89093\n","Epoch: 00 [ 2244/17967 ( 12%)], Train Loss: 0.88028\n","Epoch: 00 [ 2284/17967 ( 13%)], Train Loss: 0.87143\n","Epoch: 00 [ 2324/17967 ( 13%)], Train Loss: 0.86050\n","Epoch: 00 [ 2364/17967 ( 13%)], Train Loss: 0.85257\n","Epoch: 00 [ 2404/17967 ( 13%)], Train Loss: 0.84401\n","Epoch: 00 [ 2444/17967 ( 14%)], Train Loss: 0.83905\n","Epoch: 00 [ 2484/17967 ( 14%)], Train Loss: 0.83051\n","Epoch: 00 [ 2524/17967 ( 14%)], Train Loss: 0.82322\n","Epoch: 00 [ 2564/17967 ( 14%)], Train Loss: 0.81456\n","Epoch: 00 [ 2604/17967 ( 14%)], Train Loss: 0.80836\n","Epoch: 00 [ 2644/17967 ( 15%)], Train Loss: 0.80062\n","Epoch: 00 [ 2684/17967 ( 15%)], Train Loss: 0.79690\n","Epoch: 00 [ 2724/17967 ( 15%)], Train Loss: 0.79143\n","Epoch: 00 [ 2764/17967 ( 15%)], Train Loss: 0.78428\n","Epoch: 00 [ 2804/17967 ( 16%)], Train Loss: 0.77779\n","Epoch: 00 [ 2844/17967 ( 16%)], Train Loss: 0.77325\n","Epoch: 00 [ 2884/17967 ( 16%)], Train Loss: 0.77116\n","Epoch: 00 [ 2924/17967 ( 16%)], Train Loss: 0.76734\n","Epoch: 00 [ 2964/17967 ( 16%)], Train Loss: 0.76247\n","Epoch: 00 [ 3004/17967 ( 17%)], Train Loss: 0.75854\n","Epoch: 00 [ 3044/17967 ( 17%)], Train Loss: 0.75169\n","Epoch: 00 [ 3084/17967 ( 17%)], Train Loss: 0.75111\n","Epoch: 00 [ 3124/17967 ( 17%)], Train Loss: 0.74738\n","Epoch: 00 [ 3164/17967 ( 18%)], Train Loss: 0.74462\n","Epoch: 00 [ 3204/17967 ( 18%)], Train Loss: 0.74040\n","Epoch: 00 [ 3244/17967 ( 18%)], Train Loss: 0.73673\n","Epoch: 00 [ 3284/17967 ( 18%)], Train Loss: 0.73144\n","Epoch: 00 [ 3324/17967 ( 19%)], Train Loss: 0.72848\n","Epoch: 00 [ 3364/17967 ( 19%)], Train Loss: 0.72546\n","Epoch: 00 [ 3404/17967 ( 19%)], Train Loss: 0.71985\n","Epoch: 00 [ 3444/17967 ( 19%)], Train Loss: 0.71571\n","Epoch: 00 [ 3484/17967 ( 19%)], Train Loss: 0.71359\n","Epoch: 00 [ 3524/17967 ( 20%)], Train Loss: 0.71107\n","Epoch: 00 [ 3564/17967 ( 20%)], Train Loss: 0.70990\n","Epoch: 00 [ 3604/17967 ( 20%)], Train Loss: 0.70709\n","Epoch: 00 [ 3644/17967 ( 20%)], Train Loss: 0.70389\n","Epoch: 00 [ 3684/17967 ( 21%)], Train Loss: 0.69840\n","Epoch: 00 [ 3724/17967 ( 21%)], Train Loss: 0.69468\n","Epoch: 00 [ 3764/17967 ( 21%)], Train Loss: 0.69182\n","Epoch: 00 [ 3804/17967 ( 21%)], Train Loss: 0.68818\n","Epoch: 00 [ 3844/17967 ( 21%)], Train Loss: 0.68448\n","Epoch: 00 [ 3884/17967 ( 22%)], Train Loss: 0.67956\n","Epoch: 00 [ 3924/17967 ( 22%)], Train Loss: 0.67545\n","Epoch: 00 [ 3964/17967 ( 22%)], Train Loss: 0.67230\n","Epoch: 00 [ 4004/17967 ( 22%)], Train Loss: 0.66861\n","Epoch: 00 [ 4044/17967 ( 23%)], Train Loss: 0.66432\n","Epoch: 00 [ 4084/17967 ( 23%)], Train Loss: 0.66217\n","Epoch: 00 [ 4124/17967 ( 23%)], Train Loss: 0.66076\n","Epoch: 00 [ 4164/17967 ( 23%)], Train Loss: 0.65872\n","Epoch: 00 [ 4204/17967 ( 23%)], Train Loss: 0.65517\n","Epoch: 00 [ 4244/17967 ( 24%)], Train Loss: 0.65265\n","Epoch: 00 [ 4284/17967 ( 24%)], Train Loss: 0.64885\n","Epoch: 00 [ 4324/17967 ( 24%)], Train Loss: 0.64738\n","Epoch: 00 [ 4364/17967 ( 24%)], Train Loss: 0.64547\n","Epoch: 00 [ 4404/17967 ( 25%)], Train Loss: 0.64400\n","Epoch: 00 [ 4444/17967 ( 25%)], Train Loss: 0.64146\n","Epoch: 00 [ 4484/17967 ( 25%)], Train Loss: 0.63927\n","Epoch: 00 [ 4524/17967 ( 25%)], Train Loss: 0.63647\n","Epoch: 00 [ 4564/17967 ( 25%)], Train Loss: 0.63276\n","Epoch: 00 [ 4604/17967 ( 26%)], Train Loss: 0.63023\n","Epoch: 00 [ 4644/17967 ( 26%)], Train Loss: 0.62713\n","Epoch: 00 [ 4684/17967 ( 26%)], Train Loss: 0.62462\n","Epoch: 00 [ 4724/17967 ( 26%)], Train Loss: 0.62329\n","Epoch: 00 [ 4764/17967 ( 27%)], Train Loss: 0.62112\n","Epoch: 00 [ 4804/17967 ( 27%)], Train Loss: 0.61990\n","Epoch: 00 [ 4844/17967 ( 27%)], Train Loss: 0.61883\n","Epoch: 00 [ 4884/17967 ( 27%)], Train Loss: 0.61638\n","Epoch: 00 [ 4924/17967 ( 27%)], Train Loss: 0.61570\n","Epoch: 00 [ 4964/17967 ( 28%)], Train Loss: 0.61430\n","Epoch: 00 [ 5004/17967 ( 28%)], Train Loss: 0.61330\n","Epoch: 00 [ 5044/17967 ( 28%)], Train Loss: 0.61254\n","Epoch: 00 [ 5084/17967 ( 28%)], Train Loss: 0.60964\n","Epoch: 00 [ 5124/17967 ( 29%)], Train Loss: 0.60831\n","Epoch: 00 [ 5164/17967 ( 29%)], Train Loss: 0.60810\n","Epoch: 00 [ 5204/17967 ( 29%)], Train Loss: 0.60641\n","Epoch: 00 [ 5244/17967 ( 29%)], Train Loss: 0.60531\n","Epoch: 00 [ 5284/17967 ( 29%)], Train Loss: 0.60342\n","Epoch: 00 [ 5324/17967 ( 30%)], Train Loss: 0.60214\n","Epoch: 00 [ 5364/17967 ( 30%)], Train Loss: 0.60123\n","Epoch: 00 [ 5404/17967 ( 30%)], Train Loss: 0.60074\n","Epoch: 00 [ 5444/17967 ( 30%)], Train Loss: 0.59870\n","Epoch: 00 [ 5484/17967 ( 31%)], Train Loss: 0.59682\n","Epoch: 00 [ 5524/17967 ( 31%)], Train Loss: 0.59457\n","Epoch: 00 [ 5564/17967 ( 31%)], Train Loss: 0.59196\n","Epoch: 00 [ 5604/17967 ( 31%)], Train Loss: 0.59000\n","Epoch: 00 [ 5644/17967 ( 31%)], Train Loss: 0.58930\n","Epoch: 00 [ 5684/17967 ( 32%)], Train Loss: 0.58785\n","Epoch: 00 [ 5724/17967 ( 32%)], Train Loss: 0.58632\n","Epoch: 00 [ 5764/17967 ( 32%)], Train Loss: 0.58508\n","Epoch: 00 [ 5804/17967 ( 32%)], Train Loss: 0.58381\n","Epoch: 00 [ 5844/17967 ( 33%)], Train Loss: 0.58225\n","Epoch: 00 [ 5884/17967 ( 33%)], Train Loss: 0.58106\n","Epoch: 00 [ 5924/17967 ( 33%)], Train Loss: 0.57916\n","Epoch: 00 [ 5964/17967 ( 33%)], Train Loss: 0.57727\n","Epoch: 00 [ 6004/17967 ( 33%)], Train Loss: 0.57486\n","Epoch: 00 [ 6044/17967 ( 34%)], Train Loss: 0.57361\n","Epoch: 00 [ 6084/17967 ( 34%)], Train Loss: 0.57243\n","Epoch: 00 [ 6124/17967 ( 34%)], Train Loss: 0.57099\n","Epoch: 00 [ 6164/17967 ( 34%)], Train Loss: 0.57031\n","Epoch: 00 [ 6204/17967 ( 35%)], Train Loss: 0.56965\n","Epoch: 00 [ 6244/17967 ( 35%)], Train Loss: 0.56915\n","Epoch: 00 [ 6284/17967 ( 35%)], Train Loss: 0.56816\n","Epoch: 00 [ 6324/17967 ( 35%)], Train Loss: 0.56657\n","Epoch: 00 [ 6364/17967 ( 35%)], Train Loss: 0.56608\n","Epoch: 00 [ 6404/17967 ( 36%)], Train Loss: 0.56507\n","Epoch: 00 [ 6444/17967 ( 36%)], Train Loss: 0.56498\n","Epoch: 00 [ 6484/17967 ( 36%)], Train Loss: 0.56342\n","Epoch: 00 [ 6524/17967 ( 36%)], Train Loss: 0.56179\n","Epoch: 00 [ 6564/17967 ( 37%)], Train Loss: 0.56125\n","Epoch: 00 [ 6604/17967 ( 37%)], Train Loss: 0.55929\n","Epoch: 00 [ 6644/17967 ( 37%)], Train Loss: 0.55771\n","Epoch: 00 [ 6684/17967 ( 37%)], Train Loss: 0.55564\n","Epoch: 00 [ 6724/17967 ( 37%)], Train Loss: 0.55447\n","Epoch: 00 [ 6764/17967 ( 38%)], Train Loss: 0.55319\n","Epoch: 00 [ 6804/17967 ( 38%)], Train Loss: 0.55203\n","Epoch: 00 [ 6844/17967 ( 38%)], Train Loss: 0.55098\n","Epoch: 00 [ 6884/17967 ( 38%)], Train Loss: 0.54957\n","Epoch: 00 [ 6924/17967 ( 39%)], Train Loss: 0.55022\n","Epoch: 00 [ 6964/17967 ( 39%)], Train Loss: 0.54879\n","Epoch: 00 [ 7004/17967 ( 39%)], Train Loss: 0.54778\n","Epoch: 00 [ 7044/17967 ( 39%)], Train Loss: 0.54671\n","Epoch: 00 [ 7084/17967 ( 39%)], Train Loss: 0.54463\n","Epoch: 00 [ 7124/17967 ( 40%)], Train Loss: 0.54308\n","Epoch: 00 [ 7164/17967 ( 40%)], Train Loss: 0.54181\n","Epoch: 00 [ 7204/17967 ( 40%)], Train Loss: 0.54122\n","Epoch: 00 [ 7244/17967 ( 40%)], Train Loss: 0.53986\n","Epoch: 00 [ 7284/17967 ( 41%)], Train Loss: 0.53901\n","Epoch: 00 [ 7324/17967 ( 41%)], Train Loss: 0.53742\n","Epoch: 00 [ 7364/17967 ( 41%)], Train Loss: 0.53649\n","Epoch: 00 [ 7404/17967 ( 41%)], Train Loss: 0.53587\n","Epoch: 00 [ 7444/17967 ( 41%)], Train Loss: 0.53522\n","Epoch: 00 [ 7484/17967 ( 42%)], Train Loss: 0.53352\n","Epoch: 00 [ 7524/17967 ( 42%)], Train Loss: 0.53253\n","Epoch: 00 [ 7564/17967 ( 42%)], Train Loss: 0.53204\n","Epoch: 00 [ 7604/17967 ( 42%)], Train Loss: 0.53090\n","Epoch: 00 [ 7644/17967 ( 43%)], Train Loss: 0.52998\n","Epoch: 00 [ 7684/17967 ( 43%)], Train Loss: 0.52960\n","Epoch: 00 [ 7724/17967 ( 43%)], Train Loss: 0.52775\n","Epoch: 00 [ 7764/17967 ( 43%)], Train Loss: 0.52701\n","Epoch: 00 [ 7804/17967 ( 43%)], Train Loss: 0.52667\n","Epoch: 00 [ 7844/17967 ( 44%)], Train Loss: 0.52549\n","Epoch: 00 [ 7884/17967 ( 44%)], Train Loss: 0.52370\n","Epoch: 00 [ 7924/17967 ( 44%)], Train Loss: 0.52297\n","Epoch: 00 [ 7964/17967 ( 44%)], Train Loss: 0.52282\n","Epoch: 00 [ 8004/17967 ( 45%)], Train Loss: 0.52227\n","Epoch: 00 [ 8044/17967 ( 45%)], Train Loss: 0.52218\n","Epoch: 00 [ 8084/17967 ( 45%)], Train Loss: 0.52158\n","Epoch: 00 [ 8124/17967 ( 45%)], Train Loss: 0.52119\n","Epoch: 00 [ 8164/17967 ( 45%)], Train Loss: 0.52091\n","Epoch: 00 [ 8204/17967 ( 46%)], Train Loss: 0.52017\n","Epoch: 00 [ 8244/17967 ( 46%)], Train Loss: 0.52061\n","Epoch: 00 [ 8284/17967 ( 46%)], Train Loss: 0.51881\n","Epoch: 00 [ 8324/17967 ( 46%)], Train Loss: 0.51777\n","Epoch: 00 [ 8364/17967 ( 47%)], Train Loss: 0.51741\n","Epoch: 00 [ 8404/17967 ( 47%)], Train Loss: 0.51625\n","Epoch: 00 [ 8444/17967 ( 47%)], Train Loss: 0.51470\n","Epoch: 00 [ 8484/17967 ( 47%)], Train Loss: 0.51371\n","Epoch: 00 [ 8524/17967 ( 47%)], Train Loss: 0.51324\n","Epoch: 00 [ 8564/17967 ( 48%)], Train Loss: 0.51295\n","Epoch: 00 [ 8604/17967 ( 48%)], Train Loss: 0.51244\n","Epoch: 00 [ 8644/17967 ( 48%)], Train Loss: 0.51141\n","Epoch: 00 [ 8684/17967 ( 48%)], Train Loss: 0.51086\n","Epoch: 00 [ 8724/17967 ( 49%)], Train Loss: 0.50998\n","Epoch: 00 [ 8764/17967 ( 49%)], Train Loss: 0.50966\n","Epoch: 00 [ 8804/17967 ( 49%)], Train Loss: 0.50821\n","Epoch: 00 [ 8844/17967 ( 49%)], Train Loss: 0.50740\n","Epoch: 00 [ 8884/17967 ( 49%)], Train Loss: 0.50629\n","Epoch: 00 [ 8924/17967 ( 50%)], Train Loss: 0.50610\n","Epoch: 00 [ 8964/17967 ( 50%)], Train Loss: 0.50575\n","Epoch: 00 [ 9004/17967 ( 50%)], Train Loss: 0.50505\n","Epoch: 00 [ 9044/17967 ( 50%)], Train Loss: 0.50369\n","Epoch: 00 [ 9084/17967 ( 51%)], Train Loss: 0.50367\n","Epoch: 00 [ 9124/17967 ( 51%)], Train Loss: 0.50343\n","Epoch: 00 [ 9164/17967 ( 51%)], Train Loss: 0.50387\n","Epoch: 00 [ 9204/17967 ( 51%)], Train Loss: 0.50278\n","Epoch: 00 [ 9244/17967 ( 51%)], Train Loss: 0.50184\n","Epoch: 00 [ 9284/17967 ( 52%)], Train Loss: 0.50137\n","Epoch: 00 [ 9324/17967 ( 52%)], Train Loss: 0.50065\n","Epoch: 00 [ 9364/17967 ( 52%)], Train Loss: 0.49940\n","Epoch: 00 [ 9404/17967 ( 52%)], Train Loss: 0.49907\n","Epoch: 00 [ 9444/17967 ( 53%)], Train Loss: 0.49900\n","Epoch: 00 [ 9484/17967 ( 53%)], Train Loss: 0.49798\n","Epoch: 00 [ 9524/17967 ( 53%)], Train Loss: 0.49876\n","Epoch: 00 [ 9564/17967 ( 53%)], Train Loss: 0.49762\n","Epoch: 00 [ 9604/17967 ( 53%)], Train Loss: 0.49706\n","Epoch: 00 [ 9644/17967 ( 54%)], Train Loss: 0.49641\n","Epoch: 00 [ 9684/17967 ( 54%)], Train Loss: 0.49583\n","Epoch: 00 [ 9724/17967 ( 54%)], Train Loss: 0.49513\n","Epoch: 00 [ 9764/17967 ( 54%)], Train Loss: 0.49455\n","Epoch: 00 [ 9804/17967 ( 55%)], Train Loss: 0.49366\n","Epoch: 00 [ 9844/17967 ( 55%)], Train Loss: 0.49247\n","Epoch: 00 [ 9884/17967 ( 55%)], Train Loss: 0.49146\n","Epoch: 00 [ 9924/17967 ( 55%)], Train Loss: 0.49105\n","Epoch: 00 [ 9964/17967 ( 55%)], Train Loss: 0.49112\n","Epoch: 00 [10004/17967 ( 56%)], Train Loss: 0.48999\n","Epoch: 00 [10044/17967 ( 56%)], Train Loss: 0.48993\n","Epoch: 00 [10084/17967 ( 56%)], Train Loss: 0.48941\n","Epoch: 00 [10124/17967 ( 56%)], Train Loss: 0.48896\n","Epoch: 00 [10164/17967 ( 57%)], Train Loss: 0.48837\n","Epoch: 00 [10204/17967 ( 57%)], Train Loss: 0.48763\n","Epoch: 00 [10244/17967 ( 57%)], Train Loss: 0.48671\n","Epoch: 00 [10284/17967 ( 57%)], Train Loss: 0.48591\n","Epoch: 00 [10324/17967 ( 57%)], Train Loss: 0.48509\n","Epoch: 00 [10364/17967 ( 58%)], Train Loss: 0.48435\n","Epoch: 00 [10404/17967 ( 58%)], Train Loss: 0.48351\n","Epoch: 00 [10444/17967 ( 58%)], Train Loss: 0.48391\n","Epoch: 00 [10484/17967 ( 58%)], Train Loss: 0.48323\n","Epoch: 00 [10524/17967 ( 59%)], Train Loss: 0.48270\n","Epoch: 00 [10564/17967 ( 59%)], Train Loss: 0.48221\n","Epoch: 00 [10604/17967 ( 59%)], Train Loss: 0.48197\n","Epoch: 00 [10644/17967 ( 59%)], Train Loss: 0.48180\n","Epoch: 00 [10684/17967 ( 59%)], Train Loss: 0.48087\n","Epoch: 00 [10724/17967 ( 60%)], Train Loss: 0.48004\n","Epoch: 00 [10764/17967 ( 60%)], Train Loss: 0.47986\n","Epoch: 00 [10804/17967 ( 60%)], Train Loss: 0.47948\n","Epoch: 00 [10844/17967 ( 60%)], Train Loss: 0.47891\n","Epoch: 00 [10884/17967 ( 61%)], Train Loss: 0.47849\n","Epoch: 00 [10924/17967 ( 61%)], Train Loss: 0.47863\n","Epoch: 00 [10964/17967 ( 61%)], Train Loss: 0.47840\n","Epoch: 00 [11004/17967 ( 61%)], Train Loss: 0.47795\n","Epoch: 00 [11044/17967 ( 61%)], Train Loss: 0.47790\n","Epoch: 00 [11084/17967 ( 62%)], Train Loss: 0.47787\n","Epoch: 00 [11124/17967 ( 62%)], Train Loss: 0.47723\n","Epoch: 00 [11164/17967 ( 62%)], Train Loss: 0.47592\n","Epoch: 00 [11204/17967 ( 62%)], Train Loss: 0.47561\n","Epoch: 00 [11244/17967 ( 63%)], Train Loss: 0.47535\n","Epoch: 00 [11284/17967 ( 63%)], Train Loss: 0.47463\n","Epoch: 00 [11324/17967 ( 63%)], Train Loss: 0.47522\n","Epoch: 00 [11364/17967 ( 63%)], Train Loss: 0.47482\n","Epoch: 00 [11404/17967 ( 63%)], Train Loss: 0.47435\n","Epoch: 00 [11444/17967 ( 64%)], Train Loss: 0.47384\n","Epoch: 00 [11484/17967 ( 64%)], Train Loss: 0.47256\n","Epoch: 00 [11524/17967 ( 64%)], Train Loss: 0.47258\n","Epoch: 00 [11564/17967 ( 64%)], Train Loss: 0.47226\n","Epoch: 00 [11604/17967 ( 65%)], Train Loss: 0.47236\n","Epoch: 00 [11644/17967 ( 65%)], Train Loss: 0.47135\n","Epoch: 00 [11684/17967 ( 65%)], Train Loss: 0.47065\n","Epoch: 00 [11724/17967 ( 65%)], Train Loss: 0.47027\n","Epoch: 00 [11764/17967 ( 65%)], Train Loss: 0.46936\n","Epoch: 00 [11804/17967 ( 66%)], Train Loss: 0.46924\n","Epoch: 00 [11844/17967 ( 66%)], Train Loss: 0.46855\n","Epoch: 00 [11884/17967 ( 66%)], Train Loss: 0.46795\n","Epoch: 00 [11924/17967 ( 66%)], Train Loss: 0.46759\n","Epoch: 00 [11964/17967 ( 67%)], Train Loss: 0.46813\n","Epoch: 00 [12004/17967 ( 67%)], Train Loss: 0.46789\n","Epoch: 00 [12044/17967 ( 67%)], Train Loss: 0.46704\n","Epoch: 00 [12084/17967 ( 67%)], Train Loss: 0.46677\n","Epoch: 00 [12124/17967 ( 67%)], Train Loss: 0.46595\n","Epoch: 00 [12164/17967 ( 68%)], Train Loss: 0.46585\n","Epoch: 00 [12204/17967 ( 68%)], Train Loss: 0.46611\n","Epoch: 00 [12244/17967 ( 68%)], Train Loss: 0.46513\n","Epoch: 00 [12284/17967 ( 68%)], Train Loss: 0.46455\n","Epoch: 00 [12324/17967 ( 69%)], Train Loss: 0.46406\n","Epoch: 00 [12364/17967 ( 69%)], Train Loss: 0.46323\n","Epoch: 00 [12404/17967 ( 69%)], Train Loss: 0.46268\n","Epoch: 00 [12444/17967 ( 69%)], Train Loss: 0.46263\n","Epoch: 00 [12484/17967 ( 69%)], Train Loss: 0.46201\n","Epoch: 00 [12524/17967 ( 70%)], Train Loss: 0.46186\n","Epoch: 00 [12564/17967 ( 70%)], Train Loss: 0.46176\n","Epoch: 00 [12604/17967 ( 70%)], Train Loss: 0.46165\n","Epoch: 00 [12644/17967 ( 70%)], Train Loss: 0.46136\n","Epoch: 00 [12684/17967 ( 71%)], Train Loss: 0.46068\n","Epoch: 00 [12724/17967 ( 71%)], Train Loss: 0.46002\n","Epoch: 00 [12764/17967 ( 71%)], Train Loss: 0.45993\n","Epoch: 00 [12804/17967 ( 71%)], Train Loss: 0.45958\n","Epoch: 00 [12844/17967 ( 71%)], Train Loss: 0.45911\n","Epoch: 00 [12884/17967 ( 72%)], Train Loss: 0.45871\n","Epoch: 00 [12924/17967 ( 72%)], Train Loss: 0.45831\n","Epoch: 00 [12964/17967 ( 72%)], Train Loss: 0.45814\n","Epoch: 00 [13004/17967 ( 72%)], Train Loss: 0.45816\n","Epoch: 00 [13044/17967 ( 73%)], Train Loss: 0.45779\n","Epoch: 00 [13084/17967 ( 73%)], Train Loss: 0.45721\n","Epoch: 00 [13124/17967 ( 73%)], Train Loss: 0.45681\n","Epoch: 00 [13164/17967 ( 73%)], Train Loss: 0.45674\n","Epoch: 00 [13204/17967 ( 73%)], Train Loss: 0.45655\n","Epoch: 00 [13244/17967 ( 74%)], Train Loss: 0.45583\n","Epoch: 00 [13284/17967 ( 74%)], Train Loss: 0.45551\n","Epoch: 00 [13324/17967 ( 74%)], Train Loss: 0.45492\n","Epoch: 00 [13364/17967 ( 74%)], Train Loss: 0.45486\n","Epoch: 00 [13404/17967 ( 75%)], Train Loss: 0.45435\n","Epoch: 00 [13444/17967 ( 75%)], Train Loss: 0.45383\n","Epoch: 00 [13484/17967 ( 75%)], Train Loss: 0.45370\n","Epoch: 00 [13524/17967 ( 75%)], Train Loss: 0.45385\n","Epoch: 00 [13564/17967 ( 75%)], Train Loss: 0.45407\n","Epoch: 00 [13604/17967 ( 76%)], Train Loss: 0.45333\n","Epoch: 00 [13644/17967 ( 76%)], Train Loss: 0.45347\n","Epoch: 00 [13684/17967 ( 76%)], Train Loss: 0.45319\n","Epoch: 00 [13724/17967 ( 76%)], Train Loss: 0.45391\n","Epoch: 00 [13764/17967 ( 77%)], Train Loss: 0.45301\n","Epoch: 00 [13804/17967 ( 77%)], Train Loss: 0.45244\n","Epoch: 00 [13844/17967 ( 77%)], Train Loss: 0.45203\n","Epoch: 00 [13884/17967 ( 77%)], Train Loss: 0.45135\n","Epoch: 00 [13924/17967 ( 77%)], Train Loss: 0.45056\n","Epoch: 00 [13964/17967 ( 78%)], Train Loss: 0.44995\n","Epoch: 00 [14004/17967 ( 78%)], Train Loss: 0.45000\n","Epoch: 00 [14044/17967 ( 78%)], Train Loss: 0.44970\n","Epoch: 00 [14084/17967 ( 78%)], Train Loss: 0.44947\n","Epoch: 00 [14124/17967 ( 79%)], Train Loss: 0.44910\n","Epoch: 00 [14164/17967 ( 79%)], Train Loss: 0.44855\n","Epoch: 00 [14204/17967 ( 79%)], Train Loss: 0.44754\n","Epoch: 00 [14244/17967 ( 79%)], Train Loss: 0.44780\n","Epoch: 00 [14284/17967 ( 80%)], Train Loss: 0.44756\n","Epoch: 00 [14324/17967 ( 80%)], Train Loss: 0.44702\n","Epoch: 00 [14364/17967 ( 80%)], Train Loss: 0.44661\n","Epoch: 00 [14404/17967 ( 80%)], Train Loss: 0.44592\n","Epoch: 00 [14444/17967 ( 80%)], Train Loss: 0.44541\n","Epoch: 00 [14484/17967 ( 81%)], Train Loss: 0.44556\n","Epoch: 00 [14524/17967 ( 81%)], Train Loss: 0.44572\n","Epoch: 00 [14564/17967 ( 81%)], Train Loss: 0.44505\n","Epoch: 00 [14604/17967 ( 81%)], Train Loss: 0.44481\n","Epoch: 00 [14644/17967 ( 82%)], Train Loss: 0.44418\n","Epoch: 00 [14684/17967 ( 82%)], Train Loss: 0.44352\n","Epoch: 00 [14724/17967 ( 82%)], Train Loss: 0.44318\n","Epoch: 00 [14764/17967 ( 82%)], Train Loss: 0.44231\n","Epoch: 00 [14804/17967 ( 82%)], Train Loss: 0.44193\n","Epoch: 00 [14844/17967 ( 83%)], Train Loss: 0.44099\n","Epoch: 00 [14884/17967 ( 83%)], Train Loss: 0.44186\n","Epoch: 00 [14924/17967 ( 83%)], Train Loss: 0.44183\n","Epoch: 00 [14964/17967 ( 83%)], Train Loss: 0.44156\n","Epoch: 00 [15004/17967 ( 84%)], Train Loss: 0.44146\n","Epoch: 00 [15044/17967 ( 84%)], Train Loss: 0.44107\n","Epoch: 00 [15084/17967 ( 84%)], Train Loss: 0.44062\n","Epoch: 00 [15124/17967 ( 84%)], Train Loss: 0.44010\n","Epoch: 00 [15164/17967 ( 84%)], Train Loss: 0.44009\n","Epoch: 00 [15204/17967 ( 85%)], Train Loss: 0.43993\n","Epoch: 00 [15244/17967 ( 85%)], Train Loss: 0.43974\n","Epoch: 00 [15284/17967 ( 85%)], Train Loss: 0.43944\n","Epoch: 00 [15324/17967 ( 85%)], Train Loss: 0.43918\n","Epoch: 00 [15364/17967 ( 86%)], Train Loss: 0.43878\n","Epoch: 00 [15404/17967 ( 86%)], Train Loss: 0.43865\n","Epoch: 00 [15444/17967 ( 86%)], Train Loss: 0.43844\n","Epoch: 00 [15484/17967 ( 86%)], Train Loss: 0.43813\n","Epoch: 00 [15524/17967 ( 86%)], Train Loss: 0.43760\n","Epoch: 00 [15564/17967 ( 87%)], Train Loss: 0.43742\n","Epoch: 00 [15604/17967 ( 87%)], Train Loss: 0.43737\n","Epoch: 00 [15644/17967 ( 87%)], Train Loss: 0.43727\n","Epoch: 00 [15684/17967 ( 87%)], Train Loss: 0.43747\n","Epoch: 00 [15724/17967 ( 88%)], Train Loss: 0.43705\n","Epoch: 00 [15764/17967 ( 88%)], Train Loss: 0.43668\n","Epoch: 00 [15804/17967 ( 88%)], Train Loss: 0.43609\n","Epoch: 00 [15844/17967 ( 88%)], Train Loss: 0.43540\n","Epoch: 00 [15884/17967 ( 88%)], Train Loss: 0.43524\n","Epoch: 00 [15924/17967 ( 89%)], Train Loss: 0.43502\n","Epoch: 00 [15964/17967 ( 89%)], Train Loss: 0.43438\n","Epoch: 00 [16004/17967 ( 89%)], Train Loss: 0.43408\n","Epoch: 00 [16044/17967 ( 89%)], Train Loss: 0.43441\n","Epoch: 00 [16084/17967 ( 90%)], Train Loss: 0.43452\n","Epoch: 00 [16124/17967 ( 90%)], Train Loss: 0.43421\n","Epoch: 00 [16164/17967 ( 90%)], Train Loss: 0.43397\n","Epoch: 00 [16204/17967 ( 90%)], Train Loss: 0.43397\n","Epoch: 00 [16244/17967 ( 90%)], Train Loss: 0.43367\n","Epoch: 00 [16284/17967 ( 91%)], Train Loss: 0.43345\n","Epoch: 00 [16324/17967 ( 91%)], Train Loss: 0.43308\n","Epoch: 00 [16364/17967 ( 91%)], Train Loss: 0.43284\n","Epoch: 00 [16404/17967 ( 91%)], Train Loss: 0.43256\n","Epoch: 00 [16444/17967 ( 92%)], Train Loss: 0.43180\n","Epoch: 00 [16484/17967 ( 92%)], Train Loss: 0.43125\n","Epoch: 00 [16524/17967 ( 92%)], Train Loss: 0.43096\n","Epoch: 00 [16564/17967 ( 92%)], Train Loss: 0.43075\n","Epoch: 00 [16604/17967 ( 92%)], Train Loss: 0.43076\n","Epoch: 00 [16644/17967 ( 93%)], Train Loss: 0.43052\n","Epoch: 00 [16684/17967 ( 93%)], Train Loss: 0.43065\n","Epoch: 00 [16724/17967 ( 93%)], Train Loss: 0.43020\n","Epoch: 00 [16764/17967 ( 93%)], Train Loss: 0.42943\n","Epoch: 00 [16804/17967 ( 94%)], Train Loss: 0.42922\n","Epoch: 00 [16844/17967 ( 94%)], Train Loss: 0.42905\n","Epoch: 00 [16884/17967 ( 94%)], Train Loss: 0.42889\n","Epoch: 00 [16924/17967 ( 94%)], Train Loss: 0.42921\n","Epoch: 00 [16964/17967 ( 94%)], Train Loss: 0.42886\n","Epoch: 00 [17004/17967 ( 95%)], Train Loss: 0.42882\n","Epoch: 00 [17044/17967 ( 95%)], Train Loss: 0.42834\n","Epoch: 00 [17084/17967 ( 95%)], Train Loss: 0.42778\n","Epoch: 00 [17124/17967 ( 95%)], Train Loss: 0.42742\n","Epoch: 00 [17164/17967 ( 96%)], Train Loss: 0.42714\n","Epoch: 00 [17204/17967 ( 96%)], Train Loss: 0.42689\n","Epoch: 00 [17244/17967 ( 96%)], Train Loss: 0.42666\n","Epoch: 00 [17284/17967 ( 96%)], Train Loss: 0.42633\n","Epoch: 00 [17324/17967 ( 96%)], Train Loss: 0.42584\n","Epoch: 00 [17364/17967 ( 97%)], Train Loss: 0.42541\n","Epoch: 00 [17404/17967 ( 97%)], Train Loss: 0.42514\n","Epoch: 00 [17444/17967 ( 97%)], Train Loss: 0.42481\n","Epoch: 00 [17484/17967 ( 97%)], Train Loss: 0.42492\n","Epoch: 00 [17524/17967 ( 98%)], Train Loss: 0.42445\n","Epoch: 00 [17564/17967 ( 98%)], Train Loss: 0.42410\n","Epoch: 00 [17604/17967 ( 98%)], Train Loss: 0.42364\n","Epoch: 00 [17644/17967 ( 98%)], Train Loss: 0.42338\n","Epoch: 00 [17684/17967 ( 98%)], Train Loss: 0.42320\n","Epoch: 00 [17724/17967 ( 99%)], Train Loss: 0.42277\n","Epoch: 00 [17764/17967 ( 99%)], Train Loss: 0.42228\n","Epoch: 00 [17804/17967 ( 99%)], Train Loss: 0.42193\n","Epoch: 00 [17844/17967 ( 99%)], Train Loss: 0.42176\n","Epoch: 00 [17884/17967 (100%)], Train Loss: 0.42137\n","Epoch: 00 [17924/17967 (100%)], Train Loss: 0.42126\n","Epoch: 00 [17964/17967 (100%)], Train Loss: 0.42098\n","Epoch: 00 [17967/17967 (100%)], Train Loss: 0.42099\n","----Validation Results Summary----\n","Epoch: [0] Valid Loss: 0.57144\n","0 Epoch, Best epoch was updated! Valid Loss: 0.57144\n","Saving model checkpoint to chaii-qa-5-fold-xlmroberta-torch-fit/checkpoint-fold-2.\n","\n","Epoch: 01 [    4/17967 (  0%)], Train Loss: 0.26037\n","Epoch: 01 [   44/17967 (  0%)], Train Loss: 0.42651\n","Epoch: 01 [   84/17967 (  0%)], Train Loss: 0.31449\n","Epoch: 01 [  124/17967 (  1%)], Train Loss: 0.30918\n","Epoch: 01 [  164/17967 (  1%)], Train Loss: 0.33189\n","Epoch: 01 [  204/17967 (  1%)], Train Loss: 0.31284\n","Epoch: 01 [  244/17967 (  1%)], Train Loss: 0.32528\n","Epoch: 01 [  284/17967 (  2%)], Train Loss: 0.34305\n","Epoch: 01 [  324/17967 (  2%)], Train Loss: 0.33040\n","Epoch: 01 [  364/17967 (  2%)], Train Loss: 0.33701\n","Epoch: 01 [  404/17967 (  2%)], Train Loss: 0.35034\n","Epoch: 01 [  444/17967 (  2%)], Train Loss: 0.33706\n","Epoch: 01 [  484/17967 (  3%)], Train Loss: 0.32852\n","Epoch: 01 [  524/17967 (  3%)], Train Loss: 0.32734\n","Epoch: 01 [  564/17967 (  3%)], Train Loss: 0.32029\n","Epoch: 01 [  604/17967 (  3%)], Train Loss: 0.30389\n","Epoch: 01 [  644/17967 (  4%)], Train Loss: 0.30206\n","Epoch: 01 [  684/17967 (  4%)], Train Loss: 0.30722\n","Epoch: 01 [  724/17967 (  4%)], Train Loss: 0.30384\n","Epoch: 01 [  764/17967 (  4%)], Train Loss: 0.30191\n","Epoch: 01 [  804/17967 (  4%)], Train Loss: 0.29232\n","Epoch: 01 [  844/17967 (  5%)], Train Loss: 0.28529\n","Epoch: 01 [  884/17967 (  5%)], Train Loss: 0.28221\n","Epoch: 01 [  924/17967 (  5%)], Train Loss: 0.28132\n","Epoch: 01 [  964/17967 (  5%)], Train Loss: 0.27852\n","Epoch: 01 [ 1004/17967 (  6%)], Train Loss: 0.27241\n","Epoch: 01 [ 1044/17967 (  6%)], Train Loss: 0.27007\n","Epoch: 01 [ 1084/17967 (  6%)], Train Loss: 0.26664\n","Epoch: 01 [ 1124/17967 (  6%)], Train Loss: 0.26254\n","Epoch: 01 [ 1164/17967 (  6%)], Train Loss: 0.25947\n","Epoch: 01 [ 1204/17967 (  7%)], Train Loss: 0.26190\n","Epoch: 01 [ 1244/17967 (  7%)], Train Loss: 0.26408\n","Epoch: 01 [ 1284/17967 (  7%)], Train Loss: 0.26995\n","Epoch: 01 [ 1324/17967 (  7%)], Train Loss: 0.26962\n","Epoch: 01 [ 1364/17967 (  8%)], Train Loss: 0.26772\n","Epoch: 01 [ 1404/17967 (  8%)], Train Loss: 0.26556\n","Epoch: 01 [ 1444/17967 (  8%)], Train Loss: 0.26228\n","Epoch: 01 [ 1484/17967 (  8%)], Train Loss: 0.26189\n","Epoch: 01 [ 1524/17967 (  8%)], Train Loss: 0.26157\n","Epoch: 01 [ 1564/17967 (  9%)], Train Loss: 0.25726\n","Epoch: 01 [ 1604/17967 (  9%)], Train Loss: 0.25500\n","Epoch: 01 [ 1644/17967 (  9%)], Train Loss: 0.25330\n","Epoch: 01 [ 1684/17967 (  9%)], Train Loss: 0.25072\n","Epoch: 01 [ 1724/17967 ( 10%)], Train Loss: 0.24893\n","Epoch: 01 [ 1764/17967 ( 10%)], Train Loss: 0.24520\n","Epoch: 01 [ 1804/17967 ( 10%)], Train Loss: 0.24387\n","Epoch: 01 [ 1844/17967 ( 10%)], Train Loss: 0.24089\n","Epoch: 01 [ 1884/17967 ( 10%)], Train Loss: 0.23928\n","Epoch: 01 [ 1924/17967 ( 11%)], Train Loss: 0.23719\n","Epoch: 01 [ 1964/17967 ( 11%)], Train Loss: 0.23477\n","Epoch: 01 [ 2004/17967 ( 11%)], Train Loss: 0.23256\n","Epoch: 01 [ 2044/17967 ( 11%)], Train Loss: 0.23042\n","Epoch: 01 [ 2084/17967 ( 12%)], Train Loss: 0.22678\n","Epoch: 01 [ 2124/17967 ( 12%)], Train Loss: 0.22804\n","Epoch: 01 [ 2164/17967 ( 12%)], Train Loss: 0.22507\n","Epoch: 01 [ 2204/17967 ( 12%)], Train Loss: 0.22571\n","Epoch: 01 [ 2244/17967 ( 12%)], Train Loss: 0.22405\n","Epoch: 01 [ 2284/17967 ( 13%)], Train Loss: 0.22378\n","Epoch: 01 [ 2324/17967 ( 13%)], Train Loss: 0.22227\n","Epoch: 01 [ 2364/17967 ( 13%)], Train Loss: 0.22127\n","Epoch: 01 [ 2404/17967 ( 13%)], Train Loss: 0.22060\n","Epoch: 01 [ 2444/17967 ( 14%)], Train Loss: 0.22044\n","Epoch: 01 [ 2484/17967 ( 14%)], Train Loss: 0.21862\n","Epoch: 01 [ 2524/17967 ( 14%)], Train Loss: 0.21783\n","Epoch: 01 [ 2564/17967 ( 14%)], Train Loss: 0.21616\n","Epoch: 01 [ 2604/17967 ( 14%)], Train Loss: 0.21576\n","Epoch: 01 [ 2644/17967 ( 15%)], Train Loss: 0.21394\n","Epoch: 01 [ 2684/17967 ( 15%)], Train Loss: 0.21384\n","Epoch: 01 [ 2724/17967 ( 15%)], Train Loss: 0.21367\n","Epoch: 01 [ 2764/17967 ( 15%)], Train Loss: 0.21137\n","Epoch: 01 [ 2804/17967 ( 16%)], Train Loss: 0.21013\n","Epoch: 01 [ 2844/17967 ( 16%)], Train Loss: 0.20928\n","Epoch: 01 [ 2884/17967 ( 16%)], Train Loss: 0.20922\n","Epoch: 01 [ 2924/17967 ( 16%)], Train Loss: 0.20851\n","Epoch: 01 [ 2964/17967 ( 16%)], Train Loss: 0.20739\n","Epoch: 01 [ 3004/17967 ( 17%)], Train Loss: 0.20603\n","Epoch: 01 [ 3044/17967 ( 17%)], Train Loss: 0.20449\n","Epoch: 01 [ 3084/17967 ( 17%)], Train Loss: 0.20547\n","Epoch: 01 [ 3124/17967 ( 17%)], Train Loss: 0.20480\n","Epoch: 01 [ 3164/17967 ( 18%)], Train Loss: 0.20641\n","Epoch: 01 [ 3204/17967 ( 18%)], Train Loss: 0.20564\n","Epoch: 01 [ 3244/17967 ( 18%)], Train Loss: 0.20568\n","Epoch: 01 [ 3284/17967 ( 18%)], Train Loss: 0.20401\n","Epoch: 01 [ 3324/17967 ( 19%)], Train Loss: 0.20294\n","Epoch: 01 [ 3364/17967 ( 19%)], Train Loss: 0.20243\n","Epoch: 01 [ 3404/17967 ( 19%)], Train Loss: 0.20109\n","Epoch: 01 [ 3444/17967 ( 19%)], Train Loss: 0.19971\n","Epoch: 01 [ 3484/17967 ( 19%)], Train Loss: 0.20000\n","Epoch: 01 [ 3524/17967 ( 20%)], Train Loss: 0.19951\n","Epoch: 01 [ 3564/17967 ( 20%)], Train Loss: 0.19932\n","Epoch: 01 [ 3604/17967 ( 20%)], Train Loss: 0.19841\n","Epoch: 01 [ 3644/17967 ( 20%)], Train Loss: 0.19743\n","Epoch: 01 [ 3684/17967 ( 21%)], Train Loss: 0.19558\n","Epoch: 01 [ 3724/17967 ( 21%)], Train Loss: 0.19422\n","Epoch: 01 [ 3764/17967 ( 21%)], Train Loss: 0.19396\n","Epoch: 01 [ 3804/17967 ( 21%)], Train Loss: 0.19341\n","Epoch: 01 [ 3844/17967 ( 21%)], Train Loss: 0.19319\n","Epoch: 01 [ 3884/17967 ( 22%)], Train Loss: 0.19167\n","Epoch: 01 [ 3924/17967 ( 22%)], Train Loss: 0.19012\n","Epoch: 01 [ 3964/17967 ( 22%)], Train Loss: 0.18868\n","Epoch: 01 [ 4004/17967 ( 22%)], Train Loss: 0.18783\n","Epoch: 01 [ 4044/17967 ( 23%)], Train Loss: 0.18715\n","Epoch: 01 [ 4084/17967 ( 23%)], Train Loss: 0.18643\n","Epoch: 01 [ 4124/17967 ( 23%)], Train Loss: 0.18618\n","Epoch: 01 [ 4164/17967 ( 23%)], Train Loss: 0.18521\n","Epoch: 01 [ 4204/17967 ( 23%)], Train Loss: 0.18407\n","Epoch: 01 [ 4244/17967 ( 24%)], Train Loss: 0.18343\n","Epoch: 01 [ 4284/17967 ( 24%)], Train Loss: 0.18238\n","Epoch: 01 [ 4324/17967 ( 24%)], Train Loss: 0.18156\n","Epoch: 01 [ 4364/17967 ( 24%)], Train Loss: 0.18136\n","Epoch: 01 [ 4404/17967 ( 25%)], Train Loss: 0.18046\n","Epoch: 01 [ 4444/17967 ( 25%)], Train Loss: 0.18060\n","Epoch: 01 [ 4484/17967 ( 25%)], Train Loss: 0.17961\n","Epoch: 01 [ 4524/17967 ( 25%)], Train Loss: 0.17885\n","Epoch: 01 [ 4564/17967 ( 25%)], Train Loss: 0.17798\n","Epoch: 01 [ 4604/17967 ( 26%)], Train Loss: 0.17792\n","Epoch: 01 [ 4644/17967 ( 26%)], Train Loss: 0.17705\n","Epoch: 01 [ 4684/17967 ( 26%)], Train Loss: 0.17680\n","Epoch: 01 [ 4724/17967 ( 26%)], Train Loss: 0.17657\n","Epoch: 01 [ 4764/17967 ( 27%)], Train Loss: 0.17577\n","Epoch: 01 [ 4804/17967 ( 27%)], Train Loss: 0.17533\n","Epoch: 01 [ 4844/17967 ( 27%)], Train Loss: 0.17479\n","Epoch: 01 [ 4884/17967 ( 27%)], Train Loss: 0.17450\n","Epoch: 01 [ 4924/17967 ( 27%)], Train Loss: 0.17447\n","Epoch: 01 [ 4964/17967 ( 28%)], Train Loss: 0.17396\n","Epoch: 01 [ 5004/17967 ( 28%)], Train Loss: 0.17373\n","Epoch: 01 [ 5044/17967 ( 28%)], Train Loss: 0.17359\n","Epoch: 01 [ 5084/17967 ( 28%)], Train Loss: 0.17302\n","Epoch: 01 [ 5124/17967 ( 29%)], Train Loss: 0.17308\n","Epoch: 01 [ 5164/17967 ( 29%)], Train Loss: 0.17395\n","Epoch: 01 [ 5204/17967 ( 29%)], Train Loss: 0.17344\n","Epoch: 01 [ 5244/17967 ( 29%)], Train Loss: 0.17315\n","Epoch: 01 [ 5284/17967 ( 29%)], Train Loss: 0.17255\n","Epoch: 01 [ 5324/17967 ( 30%)], Train Loss: 0.17214\n","Epoch: 01 [ 5364/17967 ( 30%)], Train Loss: 0.17151\n","Epoch: 01 [ 5404/17967 ( 30%)], Train Loss: 0.17158\n","Epoch: 01 [ 5444/17967 ( 30%)], Train Loss: 0.17096\n","Epoch: 01 [ 5484/17967 ( 31%)], Train Loss: 0.17022\n","Epoch: 01 [ 5524/17967 ( 31%)], Train Loss: 0.16941\n","Epoch: 01 [ 5564/17967 ( 31%)], Train Loss: 0.16922\n","Epoch: 01 [ 5604/17967 ( 31%)], Train Loss: 0.16831\n","Epoch: 01 [ 5644/17967 ( 31%)], Train Loss: 0.16847\n","Epoch: 01 [ 5684/17967 ( 32%)], Train Loss: 0.16769\n","Epoch: 01 [ 5724/17967 ( 32%)], Train Loss: 0.16726\n","Epoch: 01 [ 5764/17967 ( 32%)], Train Loss: 0.16642\n","Epoch: 01 [ 5804/17967 ( 32%)], Train Loss: 0.16603\n","Epoch: 01 [ 5844/17967 ( 33%)], Train Loss: 0.16568\n","Epoch: 01 [ 5884/17967 ( 33%)], Train Loss: 0.16577\n","Epoch: 01 [ 5924/17967 ( 33%)], Train Loss: 0.16496\n","Epoch: 01 [ 5964/17967 ( 33%)], Train Loss: 0.16414\n","Epoch: 01 [ 6004/17967 ( 33%)], Train Loss: 0.16338\n","Epoch: 01 [ 6044/17967 ( 34%)], Train Loss: 0.16273\n","Epoch: 01 [ 6084/17967 ( 34%)], Train Loss: 0.16204\n","Epoch: 01 [ 6124/17967 ( 34%)], Train Loss: 0.16174\n","Epoch: 01 [ 6164/17967 ( 34%)], Train Loss: 0.16227\n","Epoch: 01 [ 6204/17967 ( 35%)], Train Loss: 0.16252\n","Epoch: 01 [ 6244/17967 ( 35%)], Train Loss: 0.16250\n","Epoch: 01 [ 6284/17967 ( 35%)], Train Loss: 0.16248\n","Epoch: 01 [ 6324/17967 ( 35%)], Train Loss: 0.16196\n","Epoch: 01 [ 6364/17967 ( 35%)], Train Loss: 0.16199\n","Epoch: 01 [ 6404/17967 ( 36%)], Train Loss: 0.16204\n","Epoch: 01 [ 6444/17967 ( 36%)], Train Loss: 0.16242\n","Epoch: 01 [ 6484/17967 ( 36%)], Train Loss: 0.16183\n","Epoch: 01 [ 6524/17967 ( 36%)], Train Loss: 0.16149\n","Epoch: 01 [ 6564/17967 ( 37%)], Train Loss: 0.16106\n","Epoch: 01 [ 6604/17967 ( 37%)], Train Loss: 0.16026\n","Epoch: 01 [ 6644/17967 ( 37%)], Train Loss: 0.15996\n","Epoch: 01 [ 6684/17967 ( 37%)], Train Loss: 0.15912\n","Epoch: 01 [ 6724/17967 ( 37%)], Train Loss: 0.15854\n","Epoch: 01 [ 6764/17967 ( 38%)], Train Loss: 0.15811\n","Epoch: 01 [ 6804/17967 ( 38%)], Train Loss: 0.15755\n","Epoch: 01 [ 6844/17967 ( 38%)], Train Loss: 0.15710\n","Epoch: 01 [ 6884/17967 ( 38%)], Train Loss: 0.15636\n","Epoch: 01 [ 6924/17967 ( 39%)], Train Loss: 0.15671\n","Epoch: 01 [ 6964/17967 ( 39%)], Train Loss: 0.15614\n","Epoch: 01 [ 7004/17967 ( 39%)], Train Loss: 0.15549\n","Epoch: 01 [ 7044/17967 ( 39%)], Train Loss: 0.15530\n","Epoch: 01 [ 7084/17967 ( 39%)], Train Loss: 0.15460\n","Epoch: 01 [ 7124/17967 ( 40%)], Train Loss: 0.15386\n","Epoch: 01 [ 7164/17967 ( 40%)], Train Loss: 0.15366\n","Epoch: 01 [ 7204/17967 ( 40%)], Train Loss: 0.15345\n","Epoch: 01 [ 7244/17967 ( 40%)], Train Loss: 0.15293\n","Epoch: 01 [ 7284/17967 ( 41%)], Train Loss: 0.15282\n","Epoch: 01 [ 7324/17967 ( 41%)], Train Loss: 0.15268\n","Epoch: 01 [ 7364/17967 ( 41%)], Train Loss: 0.15261\n","Epoch: 01 [ 7404/17967 ( 41%)], Train Loss: 0.15267\n","Epoch: 01 [ 7444/17967 ( 41%)], Train Loss: 0.15241\n","Epoch: 01 [ 7484/17967 ( 42%)], Train Loss: 0.15183\n","Epoch: 01 [ 7524/17967 ( 42%)], Train Loss: 0.15153\n","Epoch: 01 [ 7564/17967 ( 42%)], Train Loss: 0.15146\n","Epoch: 01 [ 7604/17967 ( 42%)], Train Loss: 0.15108\n","Epoch: 01 [ 7644/17967 ( 43%)], Train Loss: 0.15087\n","Epoch: 01 [ 7684/17967 ( 43%)], Train Loss: 0.15100\n","Epoch: 01 [ 7724/17967 ( 43%)], Train Loss: 0.15047\n","Epoch: 01 [ 7764/17967 ( 43%)], Train Loss: 0.15017\n","Epoch: 01 [ 7804/17967 ( 43%)], Train Loss: 0.14984\n","Epoch: 01 [ 7844/17967 ( 44%)], Train Loss: 0.14955\n","Epoch: 01 [ 7884/17967 ( 44%)], Train Loss: 0.14899\n","Epoch: 01 [ 7924/17967 ( 44%)], Train Loss: 0.14872\n","Epoch: 01 [ 7964/17967 ( 44%)], Train Loss: 0.14844\n","Epoch: 01 [ 8004/17967 ( 45%)], Train Loss: 0.14818\n","Epoch: 01 [ 8044/17967 ( 45%)], Train Loss: 0.14820\n","Epoch: 01 [ 8084/17967 ( 45%)], Train Loss: 0.14793\n","Epoch: 01 [ 8124/17967 ( 45%)], Train Loss: 0.14825\n","Epoch: 01 [ 8164/17967 ( 45%)], Train Loss: 0.14808\n","Epoch: 01 [ 8204/17967 ( 46%)], Train Loss: 0.14823\n","Epoch: 01 [ 8244/17967 ( 46%)], Train Loss: 0.14849\n","Epoch: 01 [ 8284/17967 ( 46%)], Train Loss: 0.14793\n","Epoch: 01 [ 8324/17967 ( 46%)], Train Loss: 0.14774\n","Epoch: 01 [ 8364/17967 ( 47%)], Train Loss: 0.14760\n","Epoch: 01 [ 8404/17967 ( 47%)], Train Loss: 0.14732\n","Epoch: 01 [ 8444/17967 ( 47%)], Train Loss: 0.14692\n","Epoch: 01 [ 8484/17967 ( 47%)], Train Loss: 0.14674\n","Epoch: 01 [ 8524/17967 ( 47%)], Train Loss: 0.14668\n","Epoch: 01 [ 8564/17967 ( 48%)], Train Loss: 0.14654\n","Epoch: 01 [ 8604/17967 ( 48%)], Train Loss: 0.14661\n","Epoch: 01 [ 8644/17967 ( 48%)], Train Loss: 0.14639\n","Epoch: 01 [ 8684/17967 ( 48%)], Train Loss: 0.14610\n","Epoch: 01 [ 8724/17967 ( 49%)], Train Loss: 0.14596\n","Epoch: 01 [ 8764/17967 ( 49%)], Train Loss: 0.14597\n","Epoch: 01 [ 8804/17967 ( 49%)], Train Loss: 0.14559\n","Epoch: 01 [ 8844/17967 ( 49%)], Train Loss: 0.14526\n","Epoch: 01 [ 8884/17967 ( 49%)], Train Loss: 0.14486\n","Epoch: 01 [ 8924/17967 ( 50%)], Train Loss: 0.14484\n","Epoch: 01 [ 8964/17967 ( 50%)], Train Loss: 0.14488\n","Epoch: 01 [ 9004/17967 ( 50%)], Train Loss: 0.14467\n","Epoch: 01 [ 9044/17967 ( 50%)], Train Loss: 0.14432\n","Epoch: 01 [ 9084/17967 ( 51%)], Train Loss: 0.14461\n","Epoch: 01 [ 9124/17967 ( 51%)], Train Loss: 0.14448\n","Epoch: 01 [ 9164/17967 ( 51%)], Train Loss: 0.14437\n","Epoch: 01 [ 9204/17967 ( 51%)], Train Loss: 0.14395\n","Epoch: 01 [ 9244/17967 ( 51%)], Train Loss: 0.14355\n","Epoch: 01 [ 9284/17967 ( 52%)], Train Loss: 0.14333\n","Epoch: 01 [ 9324/17967 ( 52%)], Train Loss: 0.14291\n","Epoch: 01 [ 9364/17967 ( 52%)], Train Loss: 0.14243\n","Epoch: 01 [ 9404/17967 ( 52%)], Train Loss: 0.14232\n","Epoch: 01 [ 9444/17967 ( 53%)], Train Loss: 0.14231\n","Epoch: 01 [ 9484/17967 ( 53%)], Train Loss: 0.14186\n","Epoch: 01 [ 9524/17967 ( 53%)], Train Loss: 0.14185\n","Epoch: 01 [ 9564/17967 ( 53%)], Train Loss: 0.14153\n","Epoch: 01 [ 9604/17967 ( 53%)], Train Loss: 0.14129\n","Epoch: 01 [ 9644/17967 ( 54%)], Train Loss: 0.14093\n","Epoch: 01 [ 9684/17967 ( 54%)], Train Loss: 0.14054\n","Epoch: 01 [ 9724/17967 ( 54%)], Train Loss: 0.14032\n","Epoch: 01 [ 9764/17967 ( 54%)], Train Loss: 0.13992\n","Epoch: 01 [ 9804/17967 ( 55%)], Train Loss: 0.13953\n","Epoch: 01 [ 9844/17967 ( 55%)], Train Loss: 0.13908\n","Epoch: 01 [ 9884/17967 ( 55%)], Train Loss: 0.13869\n","Epoch: 01 [ 9924/17967 ( 55%)], Train Loss: 0.13873\n","Epoch: 01 [ 9964/17967 ( 55%)], Train Loss: 0.13875\n","Epoch: 01 [10004/17967 ( 56%)], Train Loss: 0.13846\n","Epoch: 01 [10044/17967 ( 56%)], Train Loss: 0.13856\n","Epoch: 01 [10084/17967 ( 56%)], Train Loss: 0.13853\n","Epoch: 01 [10124/17967 ( 56%)], Train Loss: 0.13847\n","Epoch: 01 [10164/17967 ( 57%)], Train Loss: 0.13842\n","Epoch: 01 [10204/17967 ( 57%)], Train Loss: 0.13824\n","Epoch: 01 [10244/17967 ( 57%)], Train Loss: 0.13787\n","Epoch: 01 [10284/17967 ( 57%)], Train Loss: 0.13764\n","Epoch: 01 [10324/17967 ( 57%)], Train Loss: 0.13724\n","Epoch: 01 [10364/17967 ( 58%)], Train Loss: 0.13695\n","Epoch: 01 [10404/17967 ( 58%)], Train Loss: 0.13668\n","Epoch: 01 [10444/17967 ( 58%)], Train Loss: 0.13724\n","Epoch: 01 [10484/17967 ( 58%)], Train Loss: 0.13689\n","Epoch: 01 [10524/17967 ( 59%)], Train Loss: 0.13686\n","Epoch: 01 [10564/17967 ( 59%)], Train Loss: 0.13650\n","Epoch: 01 [10604/17967 ( 59%)], Train Loss: 0.13617\n","Epoch: 01 [10644/17967 ( 59%)], Train Loss: 0.13607\n","Epoch: 01 [10684/17967 ( 59%)], Train Loss: 0.13572\n","Epoch: 01 [10724/17967 ( 60%)], Train Loss: 0.13570\n","Epoch: 01 [10764/17967 ( 60%)], Train Loss: 0.13557\n","Epoch: 01 [10804/17967 ( 60%)], Train Loss: 0.13564\n","Epoch: 01 [10844/17967 ( 60%)], Train Loss: 0.13548\n","Epoch: 01 [10884/17967 ( 61%)], Train Loss: 0.13530\n","Epoch: 01 [10924/17967 ( 61%)], Train Loss: 0.13569\n","Epoch: 01 [10964/17967 ( 61%)], Train Loss: 0.13595\n","Epoch: 01 [11004/17967 ( 61%)], Train Loss: 0.13562\n","Epoch: 01 [11044/17967 ( 61%)], Train Loss: 0.13545\n","Epoch: 01 [11084/17967 ( 62%)], Train Loss: 0.13566\n","Epoch: 01 [11124/17967 ( 62%)], Train Loss: 0.13550\n","Epoch: 01 [11164/17967 ( 62%)], Train Loss: 0.13508\n","Epoch: 01 [11204/17967 ( 62%)], Train Loss: 0.13485\n","Epoch: 01 [11244/17967 ( 63%)], Train Loss: 0.13475\n","Epoch: 01 [11284/17967 ( 63%)], Train Loss: 0.13465\n","Epoch: 01 [11324/17967 ( 63%)], Train Loss: 0.13475\n","Epoch: 01 [11364/17967 ( 63%)], Train Loss: 0.13462\n","Epoch: 01 [11404/17967 ( 63%)], Train Loss: 0.13475\n","Epoch: 01 [11444/17967 ( 64%)], Train Loss: 0.13465\n","Epoch: 01 [11484/17967 ( 64%)], Train Loss: 0.13426\n","Epoch: 01 [11524/17967 ( 64%)], Train Loss: 0.13434\n","Epoch: 01 [11564/17967 ( 64%)], Train Loss: 0.13413\n","Epoch: 01 [11604/17967 ( 65%)], Train Loss: 0.13411\n","Epoch: 01 [11644/17967 ( 65%)], Train Loss: 0.13377\n","Epoch: 01 [11684/17967 ( 65%)], Train Loss: 0.13363\n","Epoch: 01 [11724/17967 ( 65%)], Train Loss: 0.13353\n","Epoch: 01 [11764/17967 ( 65%)], Train Loss: 0.13336\n","Epoch: 01 [11804/17967 ( 66%)], Train Loss: 0.13329\n","Epoch: 01 [11844/17967 ( 66%)], Train Loss: 0.13302\n","Epoch: 01 [11884/17967 ( 66%)], Train Loss: 0.13290\n","Epoch: 01 [11924/17967 ( 66%)], Train Loss: 0.13256\n","Epoch: 01 [11964/17967 ( 67%)], Train Loss: 0.13273\n","Epoch: 01 [12004/17967 ( 67%)], Train Loss: 0.13315\n","Epoch: 01 [12044/17967 ( 67%)], Train Loss: 0.13290\n","Epoch: 01 [12084/17967 ( 67%)], Train Loss: 0.13289\n","Epoch: 01 [12124/17967 ( 67%)], Train Loss: 0.13270\n","Epoch: 01 [12164/17967 ( 68%)], Train Loss: 0.13255\n","Epoch: 01 [12204/17967 ( 68%)], Train Loss: 0.13306\n","Epoch: 01 [12244/17967 ( 68%)], Train Loss: 0.13280\n","Epoch: 01 [12284/17967 ( 68%)], Train Loss: 0.13248\n","Epoch: 01 [12324/17967 ( 69%)], Train Loss: 0.13227\n","Epoch: 01 [12364/17967 ( 69%)], Train Loss: 0.13218\n","Epoch: 01 [12404/17967 ( 69%)], Train Loss: 0.13196\n","Epoch: 01 [12444/17967 ( 69%)], Train Loss: 0.13202\n","Epoch: 01 [12484/17967 ( 69%)], Train Loss: 0.13181\n","Epoch: 01 [12524/17967 ( 70%)], Train Loss: 0.13173\n","Epoch: 01 [12564/17967 ( 70%)], Train Loss: 0.13172\n","Epoch: 01 [12604/17967 ( 70%)], Train Loss: 0.13193\n","Epoch: 01 [12644/17967 ( 70%)], Train Loss: 0.13180\n","Epoch: 01 [12684/17967 ( 71%)], Train Loss: 0.13169\n","Epoch: 01 [12724/17967 ( 71%)], Train Loss: 0.13143\n","Epoch: 01 [12764/17967 ( 71%)], Train Loss: 0.13157\n","Epoch: 01 [12804/17967 ( 71%)], Train Loss: 0.13161\n","Epoch: 01 [12844/17967 ( 71%)], Train Loss: 0.13138\n","Epoch: 01 [12884/17967 ( 72%)], Train Loss: 0.13122\n","Epoch: 01 [12924/17967 ( 72%)], Train Loss: 0.13122\n","Epoch: 01 [12964/17967 ( 72%)], Train Loss: 0.13111\n","Epoch: 01 [13004/17967 ( 72%)], Train Loss: 0.13117\n","Epoch: 01 [13044/17967 ( 73%)], Train Loss: 0.13140\n","Epoch: 01 [13084/17967 ( 73%)], Train Loss: 0.13120\n","Epoch: 01 [13124/17967 ( 73%)], Train Loss: 0.13110\n","Epoch: 01 [13164/17967 ( 73%)], Train Loss: 0.13113\n","Epoch: 01 [13204/17967 ( 73%)], Train Loss: 0.13114\n","Epoch: 01 [13244/17967 ( 74%)], Train Loss: 0.13085\n","Epoch: 01 [13284/17967 ( 74%)], Train Loss: 0.13069\n","Epoch: 01 [13324/17967 ( 74%)], Train Loss: 0.13042\n","Epoch: 01 [13364/17967 ( 74%)], Train Loss: 0.13038\n","Epoch: 01 [13404/17967 ( 75%)], Train Loss: 0.13025\n","Epoch: 01 [13444/17967 ( 75%)], Train Loss: 0.13006\n","Epoch: 01 [13484/17967 ( 75%)], Train Loss: 0.12998\n","Epoch: 01 [13524/17967 ( 75%)], Train Loss: 0.13008\n","Epoch: 01 [13564/17967 ( 75%)], Train Loss: 0.13034\n","Epoch: 01 [13604/17967 ( 76%)], Train Loss: 0.13004\n","Epoch: 01 [13644/17967 ( 76%)], Train Loss: 0.13019\n","Epoch: 01 [13684/17967 ( 76%)], Train Loss: 0.13000\n","Epoch: 01 [13724/17967 ( 76%)], Train Loss: 0.13036\n","Epoch: 01 [13764/17967 ( 77%)], Train Loss: 0.13014\n","Epoch: 01 [13804/17967 ( 77%)], Train Loss: 0.12996\n","Epoch: 01 [13844/17967 ( 77%)], Train Loss: 0.12985\n","Epoch: 01 [13884/17967 ( 77%)], Train Loss: 0.12976\n","Epoch: 01 [13924/17967 ( 77%)], Train Loss: 0.12960\n","Epoch: 01 [13964/17967 ( 78%)], Train Loss: 0.12942\n","Epoch: 01 [14004/17967 ( 78%)], Train Loss: 0.12946\n","Epoch: 01 [14044/17967 ( 78%)], Train Loss: 0.12919\n","Epoch: 01 [14084/17967 ( 78%)], Train Loss: 0.12901\n","Epoch: 01 [14124/17967 ( 79%)], Train Loss: 0.12878\n","Epoch: 01 [14164/17967 ( 79%)], Train Loss: 0.12859\n","Epoch: 01 [14204/17967 ( 79%)], Train Loss: 0.12831\n","Epoch: 01 [14244/17967 ( 79%)], Train Loss: 0.12844\n","Epoch: 01 [14284/17967 ( 80%)], Train Loss: 0.12852\n","Epoch: 01 [14324/17967 ( 80%)], Train Loss: 0.12835\n","Epoch: 01 [14364/17967 ( 80%)], Train Loss: 0.12816\n","Epoch: 01 [14404/17967 ( 80%)], Train Loss: 0.12798\n","Epoch: 01 [14444/17967 ( 80%)], Train Loss: 0.12774\n","Epoch: 01 [14484/17967 ( 81%)], Train Loss: 0.12789\n","Epoch: 01 [14524/17967 ( 81%)], Train Loss: 0.12801\n","Epoch: 01 [14564/17967 ( 81%)], Train Loss: 0.12776\n","Epoch: 01 [14604/17967 ( 81%)], Train Loss: 0.12759\n","Epoch: 01 [14644/17967 ( 82%)], Train Loss: 0.12741\n","Epoch: 01 [14684/17967 ( 82%)], Train Loss: 0.12725\n","Epoch: 01 [14724/17967 ( 82%)], Train Loss: 0.12711\n","Epoch: 01 [14764/17967 ( 82%)], Train Loss: 0.12687\n","Epoch: 01 [14804/17967 ( 82%)], Train Loss: 0.12668\n","Epoch: 01 [14844/17967 ( 83%)], Train Loss: 0.12646\n","Epoch: 01 [14884/17967 ( 83%)], Train Loss: 0.12676\n","Epoch: 01 [14924/17967 ( 83%)], Train Loss: 0.12658\n","Epoch: 01 [14964/17967 ( 83%)], Train Loss: 0.12654\n","Epoch: 01 [15004/17967 ( 84%)], Train Loss: 0.12647\n","Epoch: 01 [15044/17967 ( 84%)], Train Loss: 0.12639\n","Epoch: 01 [15084/17967 ( 84%)], Train Loss: 0.12615\n","Epoch: 01 [15124/17967 ( 84%)], Train Loss: 0.12588\n","Epoch: 01 [15164/17967 ( 84%)], Train Loss: 0.12571\n","Epoch: 01 [15204/17967 ( 85%)], Train Loss: 0.12576\n","Epoch: 01 [15244/17967 ( 85%)], Train Loss: 0.12572\n","Epoch: 01 [15284/17967 ( 85%)], Train Loss: 0.12563\n","Epoch: 01 [15324/17967 ( 85%)], Train Loss: 0.12568\n","Epoch: 01 [15364/17967 ( 86%)], Train Loss: 0.12566\n","Epoch: 01 [15404/17967 ( 86%)], Train Loss: 0.12560\n","Epoch: 01 [15444/17967 ( 86%)], Train Loss: 0.12558\n","Epoch: 01 [15484/17967 ( 86%)], Train Loss: 0.12545\n","Epoch: 01 [15524/17967 ( 86%)], Train Loss: 0.12530\n","Epoch: 01 [15564/17967 ( 87%)], Train Loss: 0.12545\n","Epoch: 01 [15604/17967 ( 87%)], Train Loss: 0.12534\n","Epoch: 01 [15644/17967 ( 87%)], Train Loss: 0.12528\n","Epoch: 01 [15684/17967 ( 87%)], Train Loss: 0.12534\n","Epoch: 01 [15724/17967 ( 88%)], Train Loss: 0.12520\n","Epoch: 01 [15764/17967 ( 88%)], Train Loss: 0.12506\n","Epoch: 01 [15804/17967 ( 88%)], Train Loss: 0.12478\n","Epoch: 01 [15844/17967 ( 88%)], Train Loss: 0.12460\n","Epoch: 01 [15884/17967 ( 88%)], Train Loss: 0.12451\n","Epoch: 01 [15924/17967 ( 89%)], Train Loss: 0.12452\n","Epoch: 01 [15964/17967 ( 89%)], Train Loss: 0.12434\n","Epoch: 01 [16004/17967 ( 89%)], Train Loss: 0.12419\n","Epoch: 01 [16044/17967 ( 89%)], Train Loss: 0.12429\n","Epoch: 01 [16084/17967 ( 90%)], Train Loss: 0.12432\n","Epoch: 01 [16124/17967 ( 90%)], Train Loss: 0.12423\n","Epoch: 01 [16164/17967 ( 90%)], Train Loss: 0.12417\n","Epoch: 01 [16204/17967 ( 90%)], Train Loss: 0.12433\n","Epoch: 01 [16244/17967 ( 90%)], Train Loss: 0.12444\n","Epoch: 01 [16284/17967 ( 91%)], Train Loss: 0.12451\n","Epoch: 01 [16324/17967 ( 91%)], Train Loss: 0.12443\n","Epoch: 01 [16364/17967 ( 91%)], Train Loss: 0.12436\n","Epoch: 01 [16404/17967 ( 91%)], Train Loss: 0.12431\n","Epoch: 01 [16444/17967 ( 92%)], Train Loss: 0.12407\n","Epoch: 01 [16484/17967 ( 92%)], Train Loss: 0.12396\n","Epoch: 01 [16524/17967 ( 92%)], Train Loss: 0.12386\n","Epoch: 01 [16564/17967 ( 92%)], Train Loss: 0.12384\n","Epoch: 01 [16604/17967 ( 92%)], Train Loss: 0.12384\n","Epoch: 01 [16644/17967 ( 93%)], Train Loss: 0.12380\n","Epoch: 01 [16684/17967 ( 93%)], Train Loss: 0.12371\n","Epoch: 01 [16724/17967 ( 93%)], Train Loss: 0.12358\n","Epoch: 01 [16764/17967 ( 93%)], Train Loss: 0.12335\n","Epoch: 01 [16804/17967 ( 94%)], Train Loss: 0.12336\n","Epoch: 01 [16844/17967 ( 94%)], Train Loss: 0.12333\n","Epoch: 01 [16884/17967 ( 94%)], Train Loss: 0.12331\n","Epoch: 01 [16924/17967 ( 94%)], Train Loss: 0.12343\n","Epoch: 01 [16964/17967 ( 94%)], Train Loss: 0.12329\n","Epoch: 01 [17004/17967 ( 95%)], Train Loss: 0.12332\n","Epoch: 01 [17044/17967 ( 95%)], Train Loss: 0.12316\n","Epoch: 01 [17084/17967 ( 95%)], Train Loss: 0.12300\n","Epoch: 01 [17124/17967 ( 95%)], Train Loss: 0.12283\n","Epoch: 01 [17164/17967 ( 96%)], Train Loss: 0.12265\n","Epoch: 01 [17204/17967 ( 96%)], Train Loss: 0.12254\n","Epoch: 01 [17244/17967 ( 96%)], Train Loss: 0.12252\n","Epoch: 01 [17284/17967 ( 96%)], Train Loss: 0.12239\n","Epoch: 01 [17324/17967 ( 96%)], Train Loss: 0.12218\n","Epoch: 01 [17364/17967 ( 97%)], Train Loss: 0.12203\n","Epoch: 01 [17404/17967 ( 97%)], Train Loss: 0.12194\n","Epoch: 01 [17444/17967 ( 97%)], Train Loss: 0.12175\n","Epoch: 01 [17484/17967 ( 97%)], Train Loss: 0.12165\n","Epoch: 01 [17524/17967 ( 98%)], Train Loss: 0.12142\n","Epoch: 01 [17564/17967 ( 98%)], Train Loss: 0.12123\n","Epoch: 01 [17604/17967 ( 98%)], Train Loss: 0.12109\n","Epoch: 01 [17644/17967 ( 98%)], Train Loss: 0.12098\n","Epoch: 01 [17684/17967 ( 98%)], Train Loss: 0.12096\n","Epoch: 01 [17724/17967 ( 99%)], Train Loss: 0.12078\n","Epoch: 01 [17764/17967 ( 99%)], Train Loss: 0.12068\n","Epoch: 01 [17804/17967 ( 99%)], Train Loss: 0.12070\n","Epoch: 01 [17844/17967 ( 99%)], Train Loss: 0.12057\n","Epoch: 01 [17884/17967 (100%)], Train Loss: 0.12034\n","Epoch: 01 [17924/17967 (100%)], Train Loss: 0.12019\n","Epoch: 01 [17964/17967 (100%)], Train Loss: 0.12007\n","Epoch: 01 [17967/17967 (100%)], Train Loss: 0.12006\n","----Validation Results Summary----\n","Epoch: [1] Valid Loss: 0.70074\n","\n","Total Training Time: 4996.380847454071secs, Average Training Time per Epoch: 2498.1904237270355secs.\n","Total Validation Time: 428.80840158462524secs, Average Validation Time per Epoch: 214.40420079231262secs.\n","\n","\n","--------------------------------------------------\n","FOLD: 3\n","--------------------------------------------------\n","Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n","Num examples Train= 18765, Num examples Valid=4304\n","Total Training Steps: 4692, Total Warmup Steps: 469\n","Epoch: 00 [    4/18765 (  0%)], Train Loss: 2.83550\n","Epoch: 00 [   44/18765 (  0%)], Train Loss: 2.92852\n","Epoch: 00 [   84/18765 (  0%)], Train Loss: 2.90194\n","Epoch: 00 [  124/18765 (  1%)], Train Loss: 2.87410\n","Epoch: 00 [  164/18765 (  1%)], Train Loss: 2.84246\n","Epoch: 00 [  204/18765 (  1%)], Train Loss: 2.80668\n","Epoch: 00 [  244/18765 (  1%)], Train Loss: 2.76898\n","Epoch: 00 [  284/18765 (  2%)], Train Loss: 2.72440\n","Epoch: 00 [  324/18765 (  2%)], Train Loss: 2.67066\n","Epoch: 00 [  364/18765 (  2%)], Train Loss: 2.61636\n","Epoch: 00 [  404/18765 (  2%)], Train Loss: 2.55794\n","Epoch: 00 [  444/18765 (  2%)], Train Loss: 2.48200\n","Epoch: 00 [  484/18765 (  3%)], Train Loss: 2.39168\n","Epoch: 00 [  524/18765 (  3%)], Train Loss: 2.31660\n","Epoch: 00 [  564/18765 (  3%)], Train Loss: 2.22942\n","Epoch: 00 [  604/18765 (  3%)], Train Loss: 2.13733\n","Epoch: 00 [  644/18765 (  3%)], Train Loss: 2.04818\n","Epoch: 00 [  684/18765 (  4%)], Train Loss: 1.97754\n","Epoch: 00 [  724/18765 (  4%)], Train Loss: 1.89376\n","Epoch: 00 [  764/18765 (  4%)], Train Loss: 1.81821\n","Epoch: 00 [  804/18765 (  4%)], Train Loss: 1.74872\n","Epoch: 00 [  844/18765 (  4%)], Train Loss: 1.69400\n","Epoch: 00 [  884/18765 (  5%)], Train Loss: 1.64207\n","Epoch: 00 [  924/18765 (  5%)], Train Loss: 1.58484\n","Epoch: 00 [  964/18765 (  5%)], Train Loss: 1.53914\n","Epoch: 00 [ 1004/18765 (  5%)], Train Loss: 1.48738\n","Epoch: 00 [ 1044/18765 (  6%)], Train Loss: 1.45159\n","Epoch: 00 [ 1084/18765 (  6%)], Train Loss: 1.40839\n","Epoch: 00 [ 1124/18765 (  6%)], Train Loss: 1.37867\n","Epoch: 00 [ 1164/18765 (  6%)], Train Loss: 1.34662\n","Epoch: 00 [ 1204/18765 (  6%)], Train Loss: 1.31877\n","Epoch: 00 [ 1244/18765 (  7%)], Train Loss: 1.29095\n","Epoch: 00 [ 1284/18765 (  7%)], Train Loss: 1.27347\n","Epoch: 00 [ 1324/18765 (  7%)], Train Loss: 1.24438\n","Epoch: 00 [ 1364/18765 (  7%)], Train Loss: 1.21592\n","Epoch: 00 [ 1404/18765 (  7%)], Train Loss: 1.18599\n","Epoch: 00 [ 1444/18765 (  8%)], Train Loss: 1.16405\n","Epoch: 00 [ 1484/18765 (  8%)], Train Loss: 1.14071\n","Epoch: 00 [ 1524/18765 (  8%)], Train Loss: 1.12284\n","Epoch: 00 [ 1564/18765 (  8%)], Train Loss: 1.10514\n","Epoch: 00 [ 1604/18765 (  9%)], Train Loss: 1.09121\n","Epoch: 00 [ 1644/18765 (  9%)], Train Loss: 1.07213\n","Epoch: 00 [ 1684/18765 (  9%)], Train Loss: 1.05507\n","Epoch: 00 [ 1724/18765 (  9%)], Train Loss: 1.04106\n","Epoch: 00 [ 1764/18765 (  9%)], Train Loss: 1.02645\n","Epoch: 00 [ 1804/18765 ( 10%)], Train Loss: 1.01260\n","Epoch: 00 [ 1844/18765 ( 10%)], Train Loss: 1.00034\n","Epoch: 00 [ 1884/18765 ( 10%)], Train Loss: 0.98818\n","Epoch: 00 [ 1924/18765 ( 10%)], Train Loss: 0.97392\n","Epoch: 00 [ 1964/18765 ( 10%)], Train Loss: 0.95965\n","Epoch: 00 [ 2004/18765 ( 11%)], Train Loss: 0.94743\n","Epoch: 00 [ 2044/18765 ( 11%)], Train Loss: 0.93491\n","Epoch: 00 [ 2084/18765 ( 11%)], Train Loss: 0.92242\n","Epoch: 00 [ 2124/18765 ( 11%)], Train Loss: 0.91678\n","Epoch: 00 [ 2164/18765 ( 12%)], Train Loss: 0.90715\n","Epoch: 00 [ 2204/18765 ( 12%)], Train Loss: 0.89720\n","Epoch: 00 [ 2244/18765 ( 12%)], Train Loss: 0.88700\n","Epoch: 00 [ 2284/18765 ( 12%)], Train Loss: 0.87916\n","Epoch: 00 [ 2324/18765 ( 12%)], Train Loss: 0.86987\n","Epoch: 00 [ 2364/18765 ( 13%)], Train Loss: 0.86142\n","Epoch: 00 [ 2404/18765 ( 13%)], Train Loss: 0.85358\n","Epoch: 00 [ 2444/18765 ( 13%)], Train Loss: 0.84630\n","Epoch: 00 [ 2484/18765 ( 13%)], Train Loss: 0.83918\n","Epoch: 00 [ 2524/18765 ( 13%)], Train Loss: 0.83052\n","Epoch: 00 [ 2564/18765 ( 14%)], Train Loss: 0.82281\n","Epoch: 00 [ 2604/18765 ( 14%)], Train Loss: 0.81373\n","Epoch: 00 [ 2644/18765 ( 14%)], Train Loss: 0.80806\n","Epoch: 00 [ 2684/18765 ( 14%)], Train Loss: 0.80060\n","Epoch: 00 [ 2724/18765 ( 15%)], Train Loss: 0.79214\n","Epoch: 00 [ 2764/18765 ( 15%)], Train Loss: 0.78705\n","Epoch: 00 [ 2804/18765 ( 15%)], Train Loss: 0.78087\n","Epoch: 00 [ 2844/18765 ( 15%)], Train Loss: 0.77362\n","Epoch: 00 [ 2884/18765 ( 15%)], Train Loss: 0.76797\n","Epoch: 00 [ 2924/18765 ( 16%)], Train Loss: 0.76474\n","Epoch: 00 [ 2964/18765 ( 16%)], Train Loss: 0.76054\n","Epoch: 00 [ 3004/18765 ( 16%)], Train Loss: 0.75750\n","Epoch: 00 [ 3044/18765 ( 16%)], Train Loss: 0.75359\n","Epoch: 00 [ 3084/18765 ( 16%)], Train Loss: 0.74870\n","Epoch: 00 [ 3124/18765 ( 17%)], Train Loss: 0.74456\n","Epoch: 00 [ 3164/18765 ( 17%)], Train Loss: 0.74184\n","Epoch: 00 [ 3204/18765 ( 17%)], Train Loss: 0.73620\n","Epoch: 00 [ 3244/18765 ( 17%)], Train Loss: 0.73132\n","Epoch: 00 [ 3284/18765 ( 18%)], Train Loss: 0.72763\n","Epoch: 00 [ 3324/18765 ( 18%)], Train Loss: 0.72223\n","Epoch: 00 [ 3364/18765 ( 18%)], Train Loss: 0.71664\n","Epoch: 00 [ 3404/18765 ( 18%)], Train Loss: 0.71211\n","Epoch: 00 [ 3444/18765 ( 18%)], Train Loss: 0.70703\n","Epoch: 00 [ 3484/18765 ( 19%)], Train Loss: 0.70194\n","Epoch: 00 [ 3524/18765 ( 19%)], Train Loss: 0.69692\n","Epoch: 00 [ 3564/18765 ( 19%)], Train Loss: 0.69415\n","Epoch: 00 [ 3604/18765 ( 19%)], Train Loss: 0.69195\n","Epoch: 00 [ 3644/18765 ( 19%)], Train Loss: 0.68708\n","Epoch: 00 [ 3684/18765 ( 20%)], Train Loss: 0.68400\n","Epoch: 00 [ 3724/18765 ( 20%)], Train Loss: 0.67997\n","Epoch: 00 [ 3764/18765 ( 20%)], Train Loss: 0.67864\n","Epoch: 00 [ 3804/18765 ( 20%)], Train Loss: 0.67533\n","Epoch: 00 [ 3844/18765 ( 20%)], Train Loss: 0.67147\n","Epoch: 00 [ 3884/18765 ( 21%)], Train Loss: 0.66850\n","Epoch: 00 [ 3924/18765 ( 21%)], Train Loss: 0.66461\n","Epoch: 00 [ 3964/18765 ( 21%)], Train Loss: 0.66142\n","Epoch: 00 [ 4004/18765 ( 21%)], Train Loss: 0.65931\n","Epoch: 00 [ 4044/18765 ( 22%)], Train Loss: 0.65507\n","Epoch: 00 [ 4084/18765 ( 22%)], Train Loss: 0.65156\n","Epoch: 00 [ 4124/18765 ( 22%)], Train Loss: 0.64735\n","Epoch: 00 [ 4164/18765 ( 22%)], Train Loss: 0.64426\n","Epoch: 00 [ 4204/18765 ( 22%)], Train Loss: 0.64139\n","Epoch: 00 [ 4244/18765 ( 23%)], Train Loss: 0.63844\n","Epoch: 00 [ 4284/18765 ( 23%)], Train Loss: 0.63607\n","Epoch: 00 [ 4324/18765 ( 23%)], Train Loss: 0.63318\n","Epoch: 00 [ 4364/18765 ( 23%)], Train Loss: 0.62967\n","Epoch: 00 [ 4404/18765 ( 23%)], Train Loss: 0.62789\n","Epoch: 00 [ 4444/18765 ( 24%)], Train Loss: 0.62514\n","Epoch: 00 [ 4484/18765 ( 24%)], Train Loss: 0.62450\n","Epoch: 00 [ 4524/18765 ( 24%)], Train Loss: 0.62143\n","Epoch: 00 [ 4564/18765 ( 24%)], Train Loss: 0.61935\n","Epoch: 00 [ 4604/18765 ( 25%)], Train Loss: 0.61687\n","Epoch: 00 [ 4644/18765 ( 25%)], Train Loss: 0.61303\n","Epoch: 00 [ 4684/18765 ( 25%)], Train Loss: 0.61273\n","Epoch: 00 [ 4724/18765 ( 25%)], Train Loss: 0.61168\n","Epoch: 00 [ 4764/18765 ( 25%)], Train Loss: 0.60884\n","Epoch: 00 [ 4804/18765 ( 26%)], Train Loss: 0.60867\n","Epoch: 00 [ 4844/18765 ( 26%)], Train Loss: 0.60693\n","Epoch: 00 [ 4884/18765 ( 26%)], Train Loss: 0.60515\n","Epoch: 00 [ 4924/18765 ( 26%)], Train Loss: 0.60296\n","Epoch: 00 [ 4964/18765 ( 26%)], Train Loss: 0.60343\n","Epoch: 00 [ 5004/18765 ( 27%)], Train Loss: 0.60202\n","Epoch: 00 [ 5044/18765 ( 27%)], Train Loss: 0.59970\n","Epoch: 00 [ 5084/18765 ( 27%)], Train Loss: 0.59799\n","Epoch: 00 [ 5124/18765 ( 27%)], Train Loss: 0.59589\n","Epoch: 00 [ 5164/18765 ( 28%)], Train Loss: 0.59407\n","Epoch: 00 [ 5204/18765 ( 28%)], Train Loss: 0.59137\n","Epoch: 00 [ 5244/18765 ( 28%)], Train Loss: 0.59021\n","Epoch: 00 [ 5284/18765 ( 28%)], Train Loss: 0.58937\n","Epoch: 00 [ 5324/18765 ( 28%)], Train Loss: 0.58755\n","Epoch: 00 [ 5364/18765 ( 29%)], Train Loss: 0.58661\n","Epoch: 00 [ 5404/18765 ( 29%)], Train Loss: 0.58643\n","Epoch: 00 [ 5444/18765 ( 29%)], Train Loss: 0.58503\n","Epoch: 00 [ 5484/18765 ( 29%)], Train Loss: 0.58327\n","Epoch: 00 [ 5524/18765 ( 29%)], Train Loss: 0.58149\n","Epoch: 00 [ 5564/18765 ( 30%)], Train Loss: 0.58064\n","Epoch: 00 [ 5604/18765 ( 30%)], Train Loss: 0.57820\n","Epoch: 00 [ 5644/18765 ( 30%)], Train Loss: 0.57568\n","Epoch: 00 [ 5684/18765 ( 30%)], Train Loss: 0.57398\n","Epoch: 00 [ 5724/18765 ( 31%)], Train Loss: 0.57280\n","Epoch: 00 [ 5764/18765 ( 31%)], Train Loss: 0.57220\n","Epoch: 00 [ 5804/18765 ( 31%)], Train Loss: 0.56992\n","Epoch: 00 [ 5844/18765 ( 31%)], Train Loss: 0.56809\n","Epoch: 00 [ 5884/18765 ( 31%)], Train Loss: 0.56540\n","Epoch: 00 [ 5924/18765 ( 32%)], Train Loss: 0.56380\n","Epoch: 00 [ 5964/18765 ( 32%)], Train Loss: 0.56245\n","Epoch: 00 [ 6004/18765 ( 32%)], Train Loss: 0.55989\n","Epoch: 00 [ 6044/18765 ( 32%)], Train Loss: 0.55938\n","Epoch: 00 [ 6084/18765 ( 32%)], Train Loss: 0.55771\n","Epoch: 00 [ 6124/18765 ( 33%)], Train Loss: 0.55562\n","Epoch: 00 [ 6164/18765 ( 33%)], Train Loss: 0.55499\n","Epoch: 00 [ 6204/18765 ( 33%)], Train Loss: 0.55320\n","Epoch: 00 [ 6244/18765 ( 33%)], Train Loss: 0.55301\n","Epoch: 00 [ 6284/18765 ( 33%)], Train Loss: 0.55304\n","Epoch: 00 [ 6324/18765 ( 34%)], Train Loss: 0.55175\n","Epoch: 00 [ 6364/18765 ( 34%)], Train Loss: 0.55109\n","Epoch: 00 [ 6404/18765 ( 34%)], Train Loss: 0.55005\n","Epoch: 00 [ 6444/18765 ( 34%)], Train Loss: 0.54819\n","Epoch: 00 [ 6484/18765 ( 35%)], Train Loss: 0.54717\n","Epoch: 00 [ 6524/18765 ( 35%)], Train Loss: 0.54571\n","Epoch: 00 [ 6564/18765 ( 35%)], Train Loss: 0.54457\n","Epoch: 00 [ 6604/18765 ( 35%)], Train Loss: 0.54358\n","Epoch: 00 [ 6644/18765 ( 35%)], Train Loss: 0.54184\n","Epoch: 00 [ 6684/18765 ( 36%)], Train Loss: 0.54022\n","Epoch: 00 [ 6724/18765 ( 36%)], Train Loss: 0.53993\n","Epoch: 00 [ 6764/18765 ( 36%)], Train Loss: 0.53896\n","Epoch: 00 [ 6804/18765 ( 36%)], Train Loss: 0.53880\n","Epoch: 00 [ 6844/18765 ( 36%)], Train Loss: 0.53846\n","Epoch: 00 [ 6884/18765 ( 37%)], Train Loss: 0.53728\n","Epoch: 00 [ 6924/18765 ( 37%)], Train Loss: 0.53594\n","Epoch: 00 [ 6964/18765 ( 37%)], Train Loss: 0.53607\n","Epoch: 00 [ 7004/18765 ( 37%)], Train Loss: 0.53487\n","Epoch: 00 [ 7044/18765 ( 38%)], Train Loss: 0.53378\n","Epoch: 00 [ 7084/18765 ( 38%)], Train Loss: 0.53240\n","Epoch: 00 [ 7124/18765 ( 38%)], Train Loss: 0.53035\n","Epoch: 00 [ 7164/18765 ( 38%)], Train Loss: 0.52838\n","Epoch: 00 [ 7204/18765 ( 38%)], Train Loss: 0.52655\n","Epoch: 00 [ 7244/18765 ( 39%)], Train Loss: 0.52630\n","Epoch: 00 [ 7284/18765 ( 39%)], Train Loss: 0.52467\n","Epoch: 00 [ 7324/18765 ( 39%)], Train Loss: 0.52342\n","Epoch: 00 [ 7364/18765 ( 39%)], Train Loss: 0.52336\n","Epoch: 00 [ 7404/18765 ( 39%)], Train Loss: 0.52241\n","Epoch: 00 [ 7444/18765 ( 40%)], Train Loss: 0.52125\n","Epoch: 00 [ 7484/18765 ( 40%)], Train Loss: 0.52039\n","Epoch: 00 [ 7524/18765 ( 40%)], Train Loss: 0.52032\n","Epoch: 00 [ 7564/18765 ( 40%)], Train Loss: 0.51844\n","Epoch: 00 [ 7604/18765 ( 41%)], Train Loss: 0.51766\n","Epoch: 00 [ 7644/18765 ( 41%)], Train Loss: 0.51606\n","Epoch: 00 [ 7684/18765 ( 41%)], Train Loss: 0.51487\n","Epoch: 00 [ 7724/18765 ( 41%)], Train Loss: 0.51433\n","Epoch: 00 [ 7764/18765 ( 41%)], Train Loss: 0.51460\n","Epoch: 00 [ 7804/18765 ( 42%)], Train Loss: 0.51474\n","Epoch: 00 [ 7844/18765 ( 42%)], Train Loss: 0.51364\n","Epoch: 00 [ 7884/18765 ( 42%)], Train Loss: 0.51265\n","Epoch: 00 [ 7924/18765 ( 42%)], Train Loss: 0.51215\n","Epoch: 00 [ 7964/18765 ( 42%)], Train Loss: 0.51165\n","Epoch: 00 [ 8004/18765 ( 43%)], Train Loss: 0.51080\n","Epoch: 00 [ 8044/18765 ( 43%)], Train Loss: 0.51041\n","Epoch: 00 [ 8084/18765 ( 43%)], Train Loss: 0.50976\n","Epoch: 00 [ 8124/18765 ( 43%)], Train Loss: 0.50939\n","Epoch: 00 [ 8164/18765 ( 44%)], Train Loss: 0.50871\n","Epoch: 00 [ 8204/18765 ( 44%)], Train Loss: 0.50887\n","Epoch: 00 [ 8244/18765 ( 44%)], Train Loss: 0.50776\n","Epoch: 00 [ 8284/18765 ( 44%)], Train Loss: 0.50648\n","Epoch: 00 [ 8324/18765 ( 44%)], Train Loss: 0.50696\n","Epoch: 00 [ 8364/18765 ( 45%)], Train Loss: 0.50563\n","Epoch: 00 [ 8404/18765 ( 45%)], Train Loss: 0.50489\n","Epoch: 00 [ 8444/18765 ( 45%)], Train Loss: 0.50473\n","Epoch: 00 [ 8484/18765 ( 45%)], Train Loss: 0.50418\n","Epoch: 00 [ 8524/18765 ( 45%)], Train Loss: 0.50331\n","Epoch: 00 [ 8564/18765 ( 46%)], Train Loss: 0.50179\n","Epoch: 00 [ 8604/18765 ( 46%)], Train Loss: 0.50056\n","Epoch: 00 [ 8644/18765 ( 46%)], Train Loss: 0.50026\n","Epoch: 00 [ 8684/18765 ( 46%)], Train Loss: 0.49925\n","Epoch: 00 [ 8724/18765 ( 46%)], Train Loss: 0.49842\n","Epoch: 00 [ 8764/18765 ( 47%)], Train Loss: 0.49801\n","Epoch: 00 [ 8804/18765 ( 47%)], Train Loss: 0.49693\n","Epoch: 00 [ 8844/18765 ( 47%)], Train Loss: 0.49726\n","Epoch: 00 [ 8884/18765 ( 47%)], Train Loss: 0.49689\n","Epoch: 00 [ 8924/18765 ( 48%)], Train Loss: 0.49588\n","Epoch: 00 [ 8964/18765 ( 48%)], Train Loss: 0.49595\n","Epoch: 00 [ 9004/18765 ( 48%)], Train Loss: 0.49624\n","Epoch: 00 [ 9044/18765 ( 48%)], Train Loss: 0.49518\n","Epoch: 00 [ 9084/18765 ( 48%)], Train Loss: 0.49400\n","Epoch: 00 [ 9124/18765 ( 49%)], Train Loss: 0.49381\n","Epoch: 00 [ 9164/18765 ( 49%)], Train Loss: 0.49283\n","Epoch: 00 [ 9204/18765 ( 49%)], Train Loss: 0.49170\n","Epoch: 00 [ 9244/18765 ( 49%)], Train Loss: 0.49142\n","Epoch: 00 [ 9284/18765 ( 49%)], Train Loss: 0.49153\n","Epoch: 00 [ 9324/18765 ( 50%)], Train Loss: 0.49099\n","Epoch: 00 [ 9364/18765 ( 50%)], Train Loss: 0.49021\n","Epoch: 00 [ 9404/18765 ( 50%)], Train Loss: 0.48963\n","Epoch: 00 [ 9444/18765 ( 50%)], Train Loss: 0.48909\n","Epoch: 00 [ 9484/18765 ( 51%)], Train Loss: 0.48856\n","Epoch: 00 [ 9524/18765 ( 51%)], Train Loss: 0.48892\n","Epoch: 00 [ 9564/18765 ( 51%)], Train Loss: 0.48783\n","Epoch: 00 [ 9604/18765 ( 51%)], Train Loss: 0.48701\n","Epoch: 00 [ 9644/18765 ( 51%)], Train Loss: 0.48580\n","Epoch: 00 [ 9684/18765 ( 52%)], Train Loss: 0.48546\n","Epoch: 00 [ 9724/18765 ( 52%)], Train Loss: 0.48537\n","Epoch: 00 [ 9764/18765 ( 52%)], Train Loss: 0.48443\n","Epoch: 00 [ 9804/18765 ( 52%)], Train Loss: 0.48360\n","Epoch: 00 [ 9844/18765 ( 52%)], Train Loss: 0.48301\n","Epoch: 00 [ 9884/18765 ( 53%)], Train Loss: 0.48294\n","Epoch: 00 [ 9924/18765 ( 53%)], Train Loss: 0.48243\n","Epoch: 00 [ 9964/18765 ( 53%)], Train Loss: 0.48150\n","Epoch: 00 [10004/18765 ( 53%)], Train Loss: 0.48089\n","Epoch: 00 [10044/18765 ( 54%)], Train Loss: 0.48087\n","Epoch: 00 [10084/18765 ( 54%)], Train Loss: 0.48030\n","Epoch: 00 [10124/18765 ( 54%)], Train Loss: 0.47890\n","Epoch: 00 [10164/18765 ( 54%)], Train Loss: 0.47777\n","Epoch: 00 [10204/18765 ( 54%)], Train Loss: 0.47654\n","Epoch: 00 [10244/18765 ( 55%)], Train Loss: 0.47607\n","Epoch: 00 [10284/18765 ( 55%)], Train Loss: 0.47624\n","Epoch: 00 [10324/18765 ( 55%)], Train Loss: 0.47542\n","Epoch: 00 [10364/18765 ( 55%)], Train Loss: 0.47436\n","Epoch: 00 [10404/18765 ( 55%)], Train Loss: 0.47482\n","Epoch: 00 [10444/18765 ( 56%)], Train Loss: 0.47447\n","Epoch: 00 [10484/18765 ( 56%)], Train Loss: 0.47354\n","Epoch: 00 [10524/18765 ( 56%)], Train Loss: 0.47391\n","Epoch: 00 [10564/18765 ( 56%)], Train Loss: 0.47391\n","Epoch: 00 [10604/18765 ( 57%)], Train Loss: 0.47305\n","Epoch: 00 [10644/18765 ( 57%)], Train Loss: 0.47255\n","Epoch: 00 [10684/18765 ( 57%)], Train Loss: 0.47167\n","Epoch: 00 [10724/18765 ( 57%)], Train Loss: 0.47180\n","Epoch: 00 [10764/18765 ( 57%)], Train Loss: 0.47113\n","Epoch: 00 [10804/18765 ( 58%)], Train Loss: 0.47060\n","Epoch: 00 [10844/18765 ( 58%)], Train Loss: 0.47002\n","Epoch: 00 [10884/18765 ( 58%)], Train Loss: 0.46946\n","Epoch: 00 [10924/18765 ( 58%)], Train Loss: 0.46896\n","Epoch: 00 [10964/18765 ( 58%)], Train Loss: 0.46824\n","Epoch: 00 [11004/18765 ( 59%)], Train Loss: 0.46883\n","Epoch: 00 [11044/18765 ( 59%)], Train Loss: 0.46867\n","Epoch: 00 [11084/18765 ( 59%)], Train Loss: 0.46825\n","Epoch: 00 [11124/18765 ( 59%)], Train Loss: 0.46755\n","Epoch: 00 [11164/18765 ( 59%)], Train Loss: 0.46763\n","Epoch: 00 [11204/18765 ( 60%)], Train Loss: 0.46689\n","Epoch: 00 [11244/18765 ( 60%)], Train Loss: 0.46641\n","Epoch: 00 [11284/18765 ( 60%)], Train Loss: 0.46589\n","Epoch: 00 [11324/18765 ( 60%)], Train Loss: 0.46501\n","Epoch: 00 [11364/18765 ( 61%)], Train Loss: 0.46506\n","Epoch: 00 [11404/18765 ( 61%)], Train Loss: 0.46489\n","Epoch: 00 [11444/18765 ( 61%)], Train Loss: 0.46432\n","Epoch: 00 [11484/18765 ( 61%)], Train Loss: 0.46393\n","Epoch: 00 [11524/18765 ( 61%)], Train Loss: 0.46358\n","Epoch: 00 [11564/18765 ( 62%)], Train Loss: 0.46318\n","Epoch: 00 [11604/18765 ( 62%)], Train Loss: 0.46332\n","Epoch: 00 [11644/18765 ( 62%)], Train Loss: 0.46262\n","Epoch: 00 [11684/18765 ( 62%)], Train Loss: 0.46189\n","Epoch: 00 [11724/18765 ( 62%)], Train Loss: 0.46184\n","Epoch: 00 [11764/18765 ( 63%)], Train Loss: 0.46214\n","Epoch: 00 [11804/18765 ( 63%)], Train Loss: 0.46202\n","Epoch: 00 [11844/18765 ( 63%)], Train Loss: 0.46221\n","Epoch: 00 [11884/18765 ( 63%)], Train Loss: 0.46223\n","Epoch: 00 [11924/18765 ( 64%)], Train Loss: 0.46176\n","Epoch: 00 [11964/18765 ( 64%)], Train Loss: 0.46131\n","Epoch: 00 [12004/18765 ( 64%)], Train Loss: 0.46066\n","Epoch: 00 [12044/18765 ( 64%)], Train Loss: 0.46018\n","Epoch: 00 [12084/18765 ( 64%)], Train Loss: 0.45930\n","Epoch: 00 [12124/18765 ( 65%)], Train Loss: 0.45845\n","Epoch: 00 [12164/18765 ( 65%)], Train Loss: 0.45750\n","Epoch: 00 [12204/18765 ( 65%)], Train Loss: 0.45733\n","Epoch: 00 [12244/18765 ( 65%)], Train Loss: 0.45756\n","Epoch: 00 [12284/18765 ( 65%)], Train Loss: 0.45686\n","Epoch: 00 [12324/18765 ( 66%)], Train Loss: 0.45656\n","Epoch: 00 [12364/18765 ( 66%)], Train Loss: 0.45617\n","Epoch: 00 [12404/18765 ( 66%)], Train Loss: 0.45575\n","Epoch: 00 [12444/18765 ( 66%)], Train Loss: 0.45587\n","Epoch: 00 [12484/18765 ( 67%)], Train Loss: 0.45615\n","Epoch: 00 [12524/18765 ( 67%)], Train Loss: 0.45513\n","Epoch: 00 [12564/18765 ( 67%)], Train Loss: 0.45483\n","Epoch: 00 [12604/18765 ( 67%)], Train Loss: 0.45381\n","Epoch: 00 [12644/18765 ( 67%)], Train Loss: 0.45419\n","Epoch: 00 [12684/18765 ( 68%)], Train Loss: 0.45364\n","Epoch: 00 [12724/18765 ( 68%)], Train Loss: 0.45298\n","Epoch: 00 [12764/18765 ( 68%)], Train Loss: 0.45286\n","Epoch: 00 [12804/18765 ( 68%)], Train Loss: 0.45256\n","Epoch: 00 [12844/18765 ( 68%)], Train Loss: 0.45220\n","Epoch: 00 [12884/18765 ( 69%)], Train Loss: 0.45149\n","Epoch: 00 [12924/18765 ( 69%)], Train Loss: 0.45116\n","Epoch: 00 [12964/18765 ( 69%)], Train Loss: 0.45035\n","Epoch: 00 [13004/18765 ( 69%)], Train Loss: 0.45020\n","Epoch: 00 [13044/18765 ( 70%)], Train Loss: 0.44959\n","Epoch: 00 [13084/18765 ( 70%)], Train Loss: 0.44922\n","Epoch: 00 [13124/18765 ( 70%)], Train Loss: 0.44883\n","Epoch: 00 [13164/18765 ( 70%)], Train Loss: 0.44813\n","Epoch: 00 [13204/18765 ( 70%)], Train Loss: 0.44789\n","Epoch: 00 [13244/18765 ( 71%)], Train Loss: 0.44757\n","Epoch: 00 [13284/18765 ( 71%)], Train Loss: 0.44721\n","Epoch: 00 [13324/18765 ( 71%)], Train Loss: 0.44735\n","Epoch: 00 [13364/18765 ( 71%)], Train Loss: 0.44683\n","Epoch: 00 [13404/18765 ( 71%)], Train Loss: 0.44681\n","Epoch: 00 [13444/18765 ( 72%)], Train Loss: 0.44640\n","Epoch: 00 [13484/18765 ( 72%)], Train Loss: 0.44604\n","Epoch: 00 [13524/18765 ( 72%)], Train Loss: 0.44587\n","Epoch: 00 [13564/18765 ( 72%)], Train Loss: 0.44558\n","Epoch: 00 [13604/18765 ( 72%)], Train Loss: 0.44565\n","Epoch: 00 [13644/18765 ( 73%)], Train Loss: 0.44476\n","Epoch: 00 [13684/18765 ( 73%)], Train Loss: 0.44454\n","Epoch: 00 [13724/18765 ( 73%)], Train Loss: 0.44380\n","Epoch: 00 [13764/18765 ( 73%)], Train Loss: 0.44387\n","Epoch: 00 [13804/18765 ( 74%)], Train Loss: 0.44342\n","Epoch: 00 [13844/18765 ( 74%)], Train Loss: 0.44310\n","Epoch: 00 [13884/18765 ( 74%)], Train Loss: 0.44258\n","Epoch: 00 [13924/18765 ( 74%)], Train Loss: 0.44235\n","Epoch: 00 [13964/18765 ( 74%)], Train Loss: 0.44190\n","Epoch: 00 [14004/18765 ( 75%)], Train Loss: 0.44167\n","Epoch: 00 [14044/18765 ( 75%)], Train Loss: 0.44111\n","Epoch: 00 [14084/18765 ( 75%)], Train Loss: 0.44062\n","Epoch: 00 [14124/18765 ( 75%)], Train Loss: 0.44066\n","Epoch: 00 [14164/18765 ( 75%)], Train Loss: 0.44007\n","Epoch: 00 [14204/18765 ( 76%)], Train Loss: 0.43963\n","Epoch: 00 [14244/18765 ( 76%)], Train Loss: 0.43895\n","Epoch: 00 [14284/18765 ( 76%)], Train Loss: 0.43845\n","Epoch: 00 [14324/18765 ( 76%)], Train Loss: 0.43812\n","Epoch: 00 [14364/18765 ( 77%)], Train Loss: 0.43755\n","Epoch: 00 [14404/18765 ( 77%)], Train Loss: 0.43715\n","Epoch: 00 [14444/18765 ( 77%)], Train Loss: 0.43664\n","Epoch: 00 [14484/18765 ( 77%)], Train Loss: 0.43667\n","Epoch: 00 [14524/18765 ( 77%)], Train Loss: 0.43681\n","Epoch: 00 [14564/18765 ( 78%)], Train Loss: 0.43665\n","Epoch: 00 [14604/18765 ( 78%)], Train Loss: 0.43621\n","Epoch: 00 [14644/18765 ( 78%)], Train Loss: 0.43622\n","Epoch: 00 [14684/18765 ( 78%)], Train Loss: 0.43641\n","Epoch: 00 [14724/18765 ( 78%)], Train Loss: 0.43599\n","Epoch: 00 [14764/18765 ( 79%)], Train Loss: 0.43588\n","Epoch: 00 [14804/18765 ( 79%)], Train Loss: 0.43570\n","Epoch: 00 [14844/18765 ( 79%)], Train Loss: 0.43540\n","Epoch: 00 [14884/18765 ( 79%)], Train Loss: 0.43480\n","Epoch: 00 [14924/18765 ( 80%)], Train Loss: 0.43425\n","Epoch: 00 [14964/18765 ( 80%)], Train Loss: 0.43366\n","Epoch: 00 [15004/18765 ( 80%)], Train Loss: 0.43339\n","Epoch: 00 [15044/18765 ( 80%)], Train Loss: 0.43278\n","Epoch: 00 [15084/18765 ( 80%)], Train Loss: 0.43249\n","Epoch: 00 [15124/18765 ( 81%)], Train Loss: 0.43244\n","Epoch: 00 [15164/18765 ( 81%)], Train Loss: 0.43248\n","Epoch: 00 [15204/18765 ( 81%)], Train Loss: 0.43305\n","Epoch: 00 [15244/18765 ( 81%)], Train Loss: 0.43252\n","Epoch: 00 [15284/18765 ( 81%)], Train Loss: 0.43305\n","Epoch: 00 [15324/18765 ( 82%)], Train Loss: 0.43264\n","Epoch: 00 [15364/18765 ( 82%)], Train Loss: 0.43209\n","Epoch: 00 [15404/18765 ( 82%)], Train Loss: 0.43192\n","Epoch: 00 [15444/18765 ( 82%)], Train Loss: 0.43161\n","Epoch: 00 [15484/18765 ( 83%)], Train Loss: 0.43150\n","Epoch: 00 [15524/18765 ( 83%)], Train Loss: 0.43172\n","Epoch: 00 [15564/18765 ( 83%)], Train Loss: 0.43188\n","Epoch: 00 [15604/18765 ( 83%)], Train Loss: 0.43136\n","Epoch: 00 [15644/18765 ( 83%)], Train Loss: 0.43101\n","Epoch: 00 [15684/18765 ( 84%)], Train Loss: 0.43068\n","Epoch: 00 [15724/18765 ( 84%)], Train Loss: 0.43128\n","Epoch: 00 [15764/18765 ( 84%)], Train Loss: 0.43106\n","Epoch: 00 [15804/18765 ( 84%)], Train Loss: 0.43082\n","Epoch: 00 [15844/18765 ( 84%)], Train Loss: 0.43040\n","Epoch: 00 [15884/18765 ( 85%)], Train Loss: 0.42999\n","Epoch: 00 [15924/18765 ( 85%)], Train Loss: 0.42956\n","Epoch: 00 [15964/18765 ( 85%)], Train Loss: 0.42960\n","Epoch: 00 [16004/18765 ( 85%)], Train Loss: 0.42934\n","Epoch: 00 [16044/18765 ( 85%)], Train Loss: 0.42928\n","Epoch: 00 [16084/18765 ( 86%)], Train Loss: 0.42929\n","Epoch: 00 [16124/18765 ( 86%)], Train Loss: 0.42857\n","Epoch: 00 [16164/18765 ( 86%)], Train Loss: 0.42839\n","Epoch: 00 [16204/18765 ( 86%)], Train Loss: 0.42780\n","Epoch: 00 [16244/18765 ( 87%)], Train Loss: 0.42753\n","Epoch: 00 [16284/18765 ( 87%)], Train Loss: 0.42780\n","Epoch: 00 [16324/18765 ( 87%)], Train Loss: 0.42742\n","Epoch: 00 [16364/18765 ( 87%)], Train Loss: 0.42692\n","Epoch: 00 [16404/18765 ( 87%)], Train Loss: 0.42680\n","Epoch: 00 [16444/18765 ( 88%)], Train Loss: 0.42645\n","Epoch: 00 [16484/18765 ( 88%)], Train Loss: 0.42609\n","Epoch: 00 [16524/18765 ( 88%)], Train Loss: 0.42620\n","Epoch: 00 [16564/18765 ( 88%)], Train Loss: 0.42619\n","Epoch: 00 [16604/18765 ( 88%)], Train Loss: 0.42559\n","Epoch: 00 [16644/18765 ( 89%)], Train Loss: 0.42490\n","Epoch: 00 [16684/18765 ( 89%)], Train Loss: 0.42445\n","Epoch: 00 [16724/18765 ( 89%)], Train Loss: 0.42433\n","Epoch: 00 [16764/18765 ( 89%)], Train Loss: 0.42389\n","Epoch: 00 [16804/18765 ( 90%)], Train Loss: 0.42353\n","Epoch: 00 [16844/18765 ( 90%)], Train Loss: 0.42330\n","Epoch: 00 [16884/18765 ( 90%)], Train Loss: 0.42326\n","Epoch: 00 [16924/18765 ( 90%)], Train Loss: 0.42270\n","Epoch: 00 [16964/18765 ( 90%)], Train Loss: 0.42232\n","Epoch: 00 [17004/18765 ( 91%)], Train Loss: 0.42209\n","Epoch: 00 [17044/18765 ( 91%)], Train Loss: 0.42164\n","Epoch: 00 [17084/18765 ( 91%)], Train Loss: 0.42135\n","Epoch: 00 [17124/18765 ( 91%)], Train Loss: 0.42121\n","Epoch: 00 [17164/18765 ( 91%)], Train Loss: 0.42067\n","Epoch: 00 [17204/18765 ( 92%)], Train Loss: 0.42039\n","Epoch: 00 [17244/18765 ( 92%)], Train Loss: 0.42038\n","Epoch: 00 [17284/18765 ( 92%)], Train Loss: 0.42048\n","Epoch: 00 [17324/18765 ( 92%)], Train Loss: 0.42055\n","Epoch: 00 [17364/18765 ( 93%)], Train Loss: 0.42020\n","Epoch: 00 [17404/18765 ( 93%)], Train Loss: 0.42038\n","Epoch: 00 [17444/18765 ( 93%)], Train Loss: 0.42013\n","Epoch: 00 [17484/18765 ( 93%)], Train Loss: 0.42016\n","Epoch: 00 [17524/18765 ( 93%)], Train Loss: 0.42003\n","Epoch: 00 [17564/18765 ( 94%)], Train Loss: 0.41985\n","Epoch: 00 [17604/18765 ( 94%)], Train Loss: 0.41954\n","Epoch: 00 [17644/18765 ( 94%)], Train Loss: 0.41902\n","Epoch: 00 [17684/18765 ( 94%)], Train Loss: 0.41891\n","Epoch: 00 [17724/18765 ( 94%)], Train Loss: 0.41854\n","Epoch: 00 [17764/18765 ( 95%)], Train Loss: 0.41863\n","Epoch: 00 [17804/18765 ( 95%)], Train Loss: 0.41835\n","Epoch: 00 [17844/18765 ( 95%)], Train Loss: 0.41796\n","Epoch: 00 [17884/18765 ( 95%)], Train Loss: 0.41757\n","Epoch: 00 [17924/18765 ( 96%)], Train Loss: 0.41729\n","Epoch: 00 [17964/18765 ( 96%)], Train Loss: 0.41718\n","Epoch: 00 [18004/18765 ( 96%)], Train Loss: 0.41678\n","Epoch: 00 [18044/18765 ( 96%)], Train Loss: 0.41629\n","Epoch: 00 [18084/18765 ( 96%)], Train Loss: 0.41576\n","Epoch: 00 [18124/18765 ( 97%)], Train Loss: 0.41529\n","Epoch: 00 [18164/18765 ( 97%)], Train Loss: 0.41520\n","Epoch: 00 [18204/18765 ( 97%)], Train Loss: 0.41477\n","Epoch: 00 [18244/18765 ( 97%)], Train Loss: 0.41428\n","Epoch: 00 [18284/18765 ( 97%)], Train Loss: 0.41364\n","Epoch: 00 [18324/18765 ( 98%)], Train Loss: 0.41342\n","Epoch: 00 [18364/18765 ( 98%)], Train Loss: 0.41345\n","Epoch: 00 [18404/18765 ( 98%)], Train Loss: 0.41354\n","Epoch: 00 [18444/18765 ( 98%)], Train Loss: 0.41364\n","Epoch: 00 [18484/18765 ( 99%)], Train Loss: 0.41371\n","Epoch: 00 [18524/18765 ( 99%)], Train Loss: 0.41350\n","Epoch: 00 [18564/18765 ( 99%)], Train Loss: 0.41315\n","Epoch: 00 [18604/18765 ( 99%)], Train Loss: 0.41261\n","Epoch: 00 [18644/18765 ( 99%)], Train Loss: 0.41220\n","Epoch: 00 [18684/18765 (100%)], Train Loss: 0.41230\n","Epoch: 00 [18724/18765 (100%)], Train Loss: 0.41242\n","Epoch: 00 [18764/18765 (100%)], Train Loss: 0.41273\n","Epoch: 00 [18765/18765 (100%)], Train Loss: 0.41282\n","----Validation Results Summary----\n","Epoch: [0] Valid Loss: 0.63953\n","0 Epoch, Best epoch was updated! Valid Loss: 0.63953\n","Saving model checkpoint to chaii-qa-5-fold-xlmroberta-torch-fit/checkpoint-fold-3.\n","\n","Epoch: 01 [    4/18765 (  0%)], Train Loss: 0.22857\n","Epoch: 01 [   44/18765 (  0%)], Train Loss: 0.38706\n","Epoch: 01 [   84/18765 (  0%)], Train Loss: 0.37724\n","Epoch: 01 [  124/18765 (  1%)], Train Loss: 0.33612\n","Epoch: 01 [  164/18765 (  1%)], Train Loss: 0.35310\n","Epoch: 01 [  204/18765 (  1%)], Train Loss: 0.34226\n","Epoch: 01 [  244/18765 (  1%)], Train Loss: 0.32037\n","Epoch: 01 [  284/18765 (  2%)], Train Loss: 0.32228\n","Epoch: 01 [  324/18765 (  2%)], Train Loss: 0.33525\n","Epoch: 01 [  364/18765 (  2%)], Train Loss: 0.33011\n","Epoch: 01 [  404/18765 (  2%)], Train Loss: 0.33162\n","Epoch: 01 [  444/18765 (  2%)], Train Loss: 0.31773\n","Epoch: 01 [  484/18765 (  3%)], Train Loss: 0.30560\n","Epoch: 01 [  524/18765 (  3%)], Train Loss: 0.31054\n","Epoch: 01 [  564/18765 (  3%)], Train Loss: 0.30690\n","Epoch: 01 [  604/18765 (  3%)], Train Loss: 0.29881\n","Epoch: 01 [  644/18765 (  3%)], Train Loss: 0.29762\n","Epoch: 01 [  684/18765 (  4%)], Train Loss: 0.29989\n","Epoch: 01 [  724/18765 (  4%)], Train Loss: 0.29576\n","Epoch: 01 [  764/18765 (  4%)], Train Loss: 0.29321\n","Epoch: 01 [  804/18765 (  4%)], Train Loss: 0.28387\n","Epoch: 01 [  844/18765 (  4%)], Train Loss: 0.28095\n","Epoch: 01 [  884/18765 (  5%)], Train Loss: 0.27863\n","Epoch: 01 [  924/18765 (  5%)], Train Loss: 0.26925\n","Epoch: 01 [  964/18765 (  5%)], Train Loss: 0.26548\n","Epoch: 01 [ 1004/18765 (  5%)], Train Loss: 0.25947\n","Epoch: 01 [ 1044/18765 (  6%)], Train Loss: 0.25500\n","Epoch: 01 [ 1084/18765 (  6%)], Train Loss: 0.24986\n","Epoch: 01 [ 1124/18765 (  6%)], Train Loss: 0.25070\n","Epoch: 01 [ 1164/18765 (  6%)], Train Loss: 0.24847\n","Epoch: 01 [ 1204/18765 (  6%)], Train Loss: 0.24950\n","Epoch: 01 [ 1244/18765 (  7%)], Train Loss: 0.24632\n","Epoch: 01 [ 1284/18765 (  7%)], Train Loss: 0.25314\n","Epoch: 01 [ 1324/18765 (  7%)], Train Loss: 0.25101\n","Epoch: 01 [ 1364/18765 (  7%)], Train Loss: 0.24714\n","Epoch: 01 [ 1404/18765 (  7%)], Train Loss: 0.24294\n","Epoch: 01 [ 1444/18765 (  8%)], Train Loss: 0.24014\n","Epoch: 01 [ 1484/18765 (  8%)], Train Loss: 0.23747\n","Epoch: 01 [ 1524/18765 (  8%)], Train Loss: 0.23872\n","Epoch: 01 [ 1564/18765 (  8%)], Train Loss: 0.23848\n","Epoch: 01 [ 1604/18765 (  9%)], Train Loss: 0.23841\n","Epoch: 01 [ 1644/18765 (  9%)], Train Loss: 0.23525\n","Epoch: 01 [ 1684/18765 (  9%)], Train Loss: 0.23292\n","Epoch: 01 [ 1724/18765 (  9%)], Train Loss: 0.23098\n","Epoch: 01 [ 1764/18765 (  9%)], Train Loss: 0.23097\n","Epoch: 01 [ 1804/18765 ( 10%)], Train Loss: 0.22936\n","Epoch: 01 [ 1844/18765 ( 10%)], Train Loss: 0.22775\n","Epoch: 01 [ 1884/18765 ( 10%)], Train Loss: 0.22882\n","Epoch: 01 [ 1924/18765 ( 10%)], Train Loss: 0.22657\n","Epoch: 01 [ 1964/18765 ( 10%)], Train Loss: 0.22469\n","Epoch: 01 [ 2004/18765 ( 11%)], Train Loss: 0.22301\n","Epoch: 01 [ 2044/18765 ( 11%)], Train Loss: 0.22110\n","Epoch: 01 [ 2084/18765 ( 11%)], Train Loss: 0.21943\n","Epoch: 01 [ 2124/18765 ( 11%)], Train Loss: 0.22235\n","Epoch: 01 [ 2164/18765 ( 12%)], Train Loss: 0.22062\n","Epoch: 01 [ 2204/18765 ( 12%)], Train Loss: 0.22112\n","Epoch: 01 [ 2244/18765 ( 12%)], Train Loss: 0.22041\n","Epoch: 01 [ 2284/18765 ( 12%)], Train Loss: 0.22048\n","Epoch: 01 [ 2324/18765 ( 12%)], Train Loss: 0.21868\n","Epoch: 01 [ 2364/18765 ( 13%)], Train Loss: 0.21904\n","Epoch: 01 [ 2404/18765 ( 13%)], Train Loss: 0.21758\n","Epoch: 01 [ 2444/18765 ( 13%)], Train Loss: 0.21691\n","Epoch: 01 [ 2484/18765 ( 13%)], Train Loss: 0.21637\n","Epoch: 01 [ 2524/18765 ( 13%)], Train Loss: 0.21395\n","Epoch: 01 [ 2564/18765 ( 14%)], Train Loss: 0.21321\n","Epoch: 01 [ 2604/18765 ( 14%)], Train Loss: 0.21156\n","Epoch: 01 [ 2644/18765 ( 14%)], Train Loss: 0.21101\n","Epoch: 01 [ 2684/18765 ( 14%)], Train Loss: 0.20924\n","Epoch: 01 [ 2724/18765 ( 15%)], Train Loss: 0.20683\n","Epoch: 01 [ 2764/18765 ( 15%)], Train Loss: 0.20593\n","Epoch: 01 [ 2804/18765 ( 15%)], Train Loss: 0.20408\n","Epoch: 01 [ 2844/18765 ( 15%)], Train Loss: 0.20317\n","Epoch: 01 [ 2884/18765 ( 15%)], Train Loss: 0.20159\n","Epoch: 01 [ 2924/18765 ( 16%)], Train Loss: 0.20190\n","Epoch: 01 [ 2964/18765 ( 16%)], Train Loss: 0.20092\n","Epoch: 01 [ 3004/18765 ( 16%)], Train Loss: 0.20078\n","Epoch: 01 [ 3044/18765 ( 16%)], Train Loss: 0.20056\n","Epoch: 01 [ 3084/18765 ( 16%)], Train Loss: 0.19953\n","Epoch: 01 [ 3124/18765 ( 17%)], Train Loss: 0.19836\n","Epoch: 01 [ 3164/18765 ( 17%)], Train Loss: 0.19841\n","Epoch: 01 [ 3204/18765 ( 17%)], Train Loss: 0.19716\n","Epoch: 01 [ 3244/18765 ( 17%)], Train Loss: 0.19561\n","Epoch: 01 [ 3284/18765 ( 18%)], Train Loss: 0.19482\n","Epoch: 01 [ 3324/18765 ( 18%)], Train Loss: 0.19344\n","Epoch: 01 [ 3364/18765 ( 18%)], Train Loss: 0.19203\n","Epoch: 01 [ 3404/18765 ( 18%)], Train Loss: 0.19093\n","Epoch: 01 [ 3444/18765 ( 18%)], Train Loss: 0.18935\n","Epoch: 01 [ 3484/18765 ( 19%)], Train Loss: 0.18779\n","Epoch: 01 [ 3524/18765 ( 19%)], Train Loss: 0.18686\n","Epoch: 01 [ 3564/18765 ( 19%)], Train Loss: 0.18684\n","Epoch: 01 [ 3604/18765 ( 19%)], Train Loss: 0.18713\n","Epoch: 01 [ 3644/18765 ( 19%)], Train Loss: 0.18540\n","Epoch: 01 [ 3684/18765 ( 20%)], Train Loss: 0.18423\n","Epoch: 01 [ 3724/18765 ( 20%)], Train Loss: 0.18359\n","Epoch: 01 [ 3764/18765 ( 20%)], Train Loss: 0.18293\n","Epoch: 01 [ 3804/18765 ( 20%)], Train Loss: 0.18180\n","Epoch: 01 [ 3844/18765 ( 20%)], Train Loss: 0.18106\n","Epoch: 01 [ 3884/18765 ( 21%)], Train Loss: 0.18136\n","Epoch: 01 [ 3924/18765 ( 21%)], Train Loss: 0.18055\n","Epoch: 01 [ 3964/18765 ( 21%)], Train Loss: 0.17929\n","Epoch: 01 [ 4004/18765 ( 21%)], Train Loss: 0.17885\n","Epoch: 01 [ 4044/18765 ( 22%)], Train Loss: 0.17759\n","Epoch: 01 [ 4084/18765 ( 22%)], Train Loss: 0.17647\n","Epoch: 01 [ 4124/18765 ( 22%)], Train Loss: 0.17566\n","Epoch: 01 [ 4164/18765 ( 22%)], Train Loss: 0.17441\n","Epoch: 01 [ 4204/18765 ( 22%)], Train Loss: 0.17381\n","Epoch: 01 [ 4244/18765 ( 23%)], Train Loss: 0.17316\n","Epoch: 01 [ 4284/18765 ( 23%)], Train Loss: 0.17247\n","Epoch: 01 [ 4324/18765 ( 23%)], Train Loss: 0.17150\n","Epoch: 01 [ 4364/18765 ( 23%)], Train Loss: 0.17063\n","Epoch: 01 [ 4404/18765 ( 23%)], Train Loss: 0.17037\n","Epoch: 01 [ 4444/18765 ( 24%)], Train Loss: 0.16951\n","Epoch: 01 [ 4484/18765 ( 24%)], Train Loss: 0.16957\n","Epoch: 01 [ 4524/18765 ( 24%)], Train Loss: 0.16856\n","Epoch: 01 [ 4564/18765 ( 24%)], Train Loss: 0.16773\n","Epoch: 01 [ 4604/18765 ( 25%)], Train Loss: 0.16732\n","Epoch: 01 [ 4644/18765 ( 25%)], Train Loss: 0.16610\n","Epoch: 01 [ 4684/18765 ( 25%)], Train Loss: 0.16562\n","Epoch: 01 [ 4724/18765 ( 25%)], Train Loss: 0.16520\n","Epoch: 01 [ 4764/18765 ( 25%)], Train Loss: 0.16411\n","Epoch: 01 [ 4804/18765 ( 26%)], Train Loss: 0.16532\n","Epoch: 01 [ 4844/18765 ( 26%)], Train Loss: 0.16506\n","Epoch: 01 [ 4884/18765 ( 26%)], Train Loss: 0.16467\n","Epoch: 01 [ 4924/18765 ( 26%)], Train Loss: 0.16385\n","Epoch: 01 [ 4964/18765 ( 26%)], Train Loss: 0.16393\n","Epoch: 01 [ 5004/18765 ( 27%)], Train Loss: 0.16430\n","Epoch: 01 [ 5044/18765 ( 27%)], Train Loss: 0.16385\n","Epoch: 01 [ 5084/18765 ( 27%)], Train Loss: 0.16338\n","Epoch: 01 [ 5124/18765 ( 27%)], Train Loss: 0.16322\n","Epoch: 01 [ 5164/18765 ( 28%)], Train Loss: 0.16292\n","Epoch: 01 [ 5204/18765 ( 28%)], Train Loss: 0.16228\n","Epoch: 01 [ 5244/18765 ( 28%)], Train Loss: 0.16199\n","Epoch: 01 [ 5284/18765 ( 28%)], Train Loss: 0.16244\n","Epoch: 01 [ 5324/18765 ( 28%)], Train Loss: 0.16171\n","Epoch: 01 [ 5364/18765 ( 29%)], Train Loss: 0.16141\n","Epoch: 01 [ 5404/18765 ( 29%)], Train Loss: 0.16138\n","Epoch: 01 [ 5444/18765 ( 29%)], Train Loss: 0.16138\n","Epoch: 01 [ 5484/18765 ( 29%)], Train Loss: 0.16076\n","Epoch: 01 [ 5524/18765 ( 29%)], Train Loss: 0.16026\n","Epoch: 01 [ 5564/18765 ( 30%)], Train Loss: 0.16116\n","Epoch: 01 [ 5604/18765 ( 30%)], Train Loss: 0.16042\n","Epoch: 01 [ 5644/18765 ( 30%)], Train Loss: 0.15961\n","Epoch: 01 [ 5684/18765 ( 30%)], Train Loss: 0.15950\n","Epoch: 01 [ 5724/18765 ( 31%)], Train Loss: 0.15924\n","Epoch: 01 [ 5764/18765 ( 31%)], Train Loss: 0.15918\n","Epoch: 01 [ 5804/18765 ( 31%)], Train Loss: 0.15833\n","Epoch: 01 [ 5844/18765 ( 31%)], Train Loss: 0.15784\n","Epoch: 01 [ 5884/18765 ( 31%)], Train Loss: 0.15703\n","Epoch: 01 [ 5924/18765 ( 32%)], Train Loss: 0.15669\n","Epoch: 01 [ 5964/18765 ( 32%)], Train Loss: 0.15648\n","Epoch: 01 [ 6004/18765 ( 32%)], Train Loss: 0.15573\n","Epoch: 01 [ 6044/18765 ( 32%)], Train Loss: 0.15622\n","Epoch: 01 [ 6084/18765 ( 32%)], Train Loss: 0.15577\n","Epoch: 01 [ 6124/18765 ( 33%)], Train Loss: 0.15520\n","Epoch: 01 [ 6164/18765 ( 33%)], Train Loss: 0.15560\n","Epoch: 01 [ 6204/18765 ( 33%)], Train Loss: 0.15499\n","Epoch: 01 [ 6244/18765 ( 33%)], Train Loss: 0.15536\n","Epoch: 01 [ 6284/18765 ( 33%)], Train Loss: 0.15507\n","Epoch: 01 [ 6324/18765 ( 34%)], Train Loss: 0.15498\n","Epoch: 01 [ 6364/18765 ( 34%)], Train Loss: 0.15492\n","Epoch: 01 [ 6404/18765 ( 34%)], Train Loss: 0.15477\n","Epoch: 01 [ 6444/18765 ( 34%)], Train Loss: 0.15412\n","Epoch: 01 [ 6484/18765 ( 35%)], Train Loss: 0.15403\n","Epoch: 01 [ 6524/18765 ( 35%)], Train Loss: 0.15361\n","Epoch: 01 [ 6564/18765 ( 35%)], Train Loss: 0.15386\n","Epoch: 01 [ 6604/18765 ( 35%)], Train Loss: 0.15373\n","Epoch: 01 [ 6644/18765 ( 35%)], Train Loss: 0.15324\n","Epoch: 01 [ 6684/18765 ( 36%)], Train Loss: 0.15271\n","Epoch: 01 [ 6724/18765 ( 36%)], Train Loss: 0.15265\n","Epoch: 01 [ 6764/18765 ( 36%)], Train Loss: 0.15226\n","Epoch: 01 [ 6804/18765 ( 36%)], Train Loss: 0.15213\n","Epoch: 01 [ 6844/18765 ( 36%)], Train Loss: 0.15217\n","Epoch: 01 [ 6884/18765 ( 37%)], Train Loss: 0.15169\n","Epoch: 01 [ 6924/18765 ( 37%)], Train Loss: 0.15143\n","Epoch: 01 [ 6964/18765 ( 37%)], Train Loss: 0.15145\n","Epoch: 01 [ 7004/18765 ( 37%)], Train Loss: 0.15114\n","Epoch: 01 [ 7044/18765 ( 38%)], Train Loss: 0.15090\n","Epoch: 01 [ 7084/18765 ( 38%)], Train Loss: 0.15047\n","Epoch: 01 [ 7124/18765 ( 38%)], Train Loss: 0.14991\n","Epoch: 01 [ 7164/18765 ( 38%)], Train Loss: 0.14929\n","Epoch: 01 [ 7204/18765 ( 38%)], Train Loss: 0.14905\n","Epoch: 01 [ 7244/18765 ( 39%)], Train Loss: 0.14899\n","Epoch: 01 [ 7284/18765 ( 39%)], Train Loss: 0.14849\n","Epoch: 01 [ 7324/18765 ( 39%)], Train Loss: 0.14802\n","Epoch: 01 [ 7364/18765 ( 39%)], Train Loss: 0.14787\n","Epoch: 01 [ 7404/18765 ( 39%)], Train Loss: 0.14778\n","Epoch: 01 [ 7444/18765 ( 40%)], Train Loss: 0.14799\n","Epoch: 01 [ 7484/18765 ( 40%)], Train Loss: 0.14768\n","Epoch: 01 [ 7524/18765 ( 40%)], Train Loss: 0.14776\n","Epoch: 01 [ 7564/18765 ( 40%)], Train Loss: 0.14731\n","Epoch: 01 [ 7604/18765 ( 41%)], Train Loss: 0.14697\n","Epoch: 01 [ 7644/18765 ( 41%)], Train Loss: 0.14661\n","Epoch: 01 [ 7684/18765 ( 41%)], Train Loss: 0.14625\n","Epoch: 01 [ 7724/18765 ( 41%)], Train Loss: 0.14612\n","Epoch: 01 [ 7764/18765 ( 41%)], Train Loss: 0.14637\n","Epoch: 01 [ 7804/18765 ( 42%)], Train Loss: 0.14632\n","Epoch: 01 [ 7844/18765 ( 42%)], Train Loss: 0.14621\n","Epoch: 01 [ 7884/18765 ( 42%)], Train Loss: 0.14658\n","Epoch: 01 [ 7924/18765 ( 42%)], Train Loss: 0.14670\n","Epoch: 01 [ 7964/18765 ( 42%)], Train Loss: 0.14651\n","Epoch: 01 [ 8004/18765 ( 43%)], Train Loss: 0.14635\n","Epoch: 01 [ 8044/18765 ( 43%)], Train Loss: 0.14608\n","Epoch: 01 [ 8084/18765 ( 43%)], Train Loss: 0.14577\n","Epoch: 01 [ 8124/18765 ( 43%)], Train Loss: 0.14552\n","Epoch: 01 [ 8164/18765 ( 44%)], Train Loss: 0.14506\n","Epoch: 01 [ 8204/18765 ( 44%)], Train Loss: 0.14532\n","Epoch: 01 [ 8244/18765 ( 44%)], Train Loss: 0.14492\n","Epoch: 01 [ 8284/18765 ( 44%)], Train Loss: 0.14462\n","Epoch: 01 [ 8324/18765 ( 44%)], Train Loss: 0.14445\n","Epoch: 01 [ 8364/18765 ( 45%)], Train Loss: 0.14424\n","Epoch: 01 [ 8404/18765 ( 45%)], Train Loss: 0.14387\n","Epoch: 01 [ 8444/18765 ( 45%)], Train Loss: 0.14362\n","Epoch: 01 [ 8484/18765 ( 45%)], Train Loss: 0.14349\n","Epoch: 01 [ 8524/18765 ( 45%)], Train Loss: 0.14350\n","Epoch: 01 [ 8564/18765 ( 46%)], Train Loss: 0.14305\n","Epoch: 01 [ 8604/18765 ( 46%)], Train Loss: 0.14272\n","Epoch: 01 [ 8644/18765 ( 46%)], Train Loss: 0.14249\n","Epoch: 01 [ 8684/18765 ( 46%)], Train Loss: 0.14200\n","Epoch: 01 [ 8724/18765 ( 46%)], Train Loss: 0.14159\n","Epoch: 01 [ 8764/18765 ( 47%)], Train Loss: 0.14150\n","Epoch: 01 [ 8804/18765 ( 47%)], Train Loss: 0.14130\n","Epoch: 01 [ 8844/18765 ( 47%)], Train Loss: 0.14193\n","Epoch: 01 [ 8884/18765 ( 47%)], Train Loss: 0.14193\n","Epoch: 01 [ 8924/18765 ( 48%)], Train Loss: 0.14155\n","Epoch: 01 [ 8964/18765 ( 48%)], Train Loss: 0.14167\n","Epoch: 01 [ 9004/18765 ( 48%)], Train Loss: 0.14184\n","Epoch: 01 [ 9044/18765 ( 48%)], Train Loss: 0.14149\n","Epoch: 01 [ 9084/18765 ( 48%)], Train Loss: 0.14111\n","Epoch: 01 [ 9124/18765 ( 49%)], Train Loss: 0.14104\n","Epoch: 01 [ 9164/18765 ( 49%)], Train Loss: 0.14086\n","Epoch: 01 [ 9204/18765 ( 49%)], Train Loss: 0.14042\n","Epoch: 01 [ 9244/18765 ( 49%)], Train Loss: 0.14055\n","Epoch: 01 [ 9284/18765 ( 49%)], Train Loss: 0.14025\n","Epoch: 01 [ 9324/18765 ( 50%)], Train Loss: 0.13994\n","Epoch: 01 [ 9364/18765 ( 50%)], Train Loss: 0.13971\n","Epoch: 01 [ 9404/18765 ( 50%)], Train Loss: 0.13953\n","Epoch: 01 [ 9444/18765 ( 50%)], Train Loss: 0.13946\n","Epoch: 01 [ 9484/18765 ( 51%)], Train Loss: 0.13904\n","Epoch: 01 [ 9524/18765 ( 51%)], Train Loss: 0.13953\n","Epoch: 01 [ 9564/18765 ( 51%)], Train Loss: 0.13907\n","Epoch: 01 [ 9604/18765 ( 51%)], Train Loss: 0.13877\n","Epoch: 01 [ 9644/18765 ( 51%)], Train Loss: 0.13833\n","Epoch: 01 [ 9684/18765 ( 52%)], Train Loss: 0.13809\n","Epoch: 01 [ 9724/18765 ( 52%)], Train Loss: 0.13862\n","Epoch: 01 [ 9764/18765 ( 52%)], Train Loss: 0.13821\n","Epoch: 01 [ 9804/18765 ( 52%)], Train Loss: 0.13788\n","Epoch: 01 [ 9844/18765 ( 52%)], Train Loss: 0.13758\n","Epoch: 01 [ 9884/18765 ( 53%)], Train Loss: 0.13780\n","Epoch: 01 [ 9924/18765 ( 53%)], Train Loss: 0.13760\n","Epoch: 01 [ 9964/18765 ( 53%)], Train Loss: 0.13723\n","Epoch: 01 [10004/18765 ( 53%)], Train Loss: 0.13723\n","Epoch: 01 [10044/18765 ( 54%)], Train Loss: 0.13710\n","Epoch: 01 [10084/18765 ( 54%)], Train Loss: 0.13677\n","Epoch: 01 [10124/18765 ( 54%)], Train Loss: 0.13636\n","Epoch: 01 [10164/18765 ( 54%)], Train Loss: 0.13618\n","Epoch: 01 [10204/18765 ( 54%)], Train Loss: 0.13580\n","Epoch: 01 [10244/18765 ( 55%)], Train Loss: 0.13547\n","Epoch: 01 [10284/18765 ( 55%)], Train Loss: 0.13532\n","Epoch: 01 [10324/18765 ( 55%)], Train Loss: 0.13511\n","Epoch: 01 [10364/18765 ( 55%)], Train Loss: 0.13496\n","Epoch: 01 [10404/18765 ( 55%)], Train Loss: 0.13533\n","Epoch: 01 [10444/18765 ( 56%)], Train Loss: 0.13528\n","Epoch: 01 [10484/18765 ( 56%)], Train Loss: 0.13504\n","Epoch: 01 [10524/18765 ( 56%)], Train Loss: 0.13537\n","Epoch: 01 [10564/18765 ( 56%)], Train Loss: 0.13534\n","Epoch: 01 [10604/18765 ( 57%)], Train Loss: 0.13515\n","Epoch: 01 [10644/18765 ( 57%)], Train Loss: 0.13497\n","Epoch: 01 [10684/18765 ( 57%)], Train Loss: 0.13469\n","Epoch: 01 [10724/18765 ( 57%)], Train Loss: 0.13473\n","Epoch: 01 [10764/18765 ( 57%)], Train Loss: 0.13448\n","Epoch: 01 [10804/18765 ( 58%)], Train Loss: 0.13465\n","Epoch: 01 [10844/18765 ( 58%)], Train Loss: 0.13453\n","Epoch: 01 [10884/18765 ( 58%)], Train Loss: 0.13427\n","Epoch: 01 [10924/18765 ( 58%)], Train Loss: 0.13401\n","Epoch: 01 [10964/18765 ( 58%)], Train Loss: 0.13378\n","Epoch: 01 [11004/18765 ( 59%)], Train Loss: 0.13412\n","Epoch: 01 [11044/18765 ( 59%)], Train Loss: 0.13424\n","Epoch: 01 [11084/18765 ( 59%)], Train Loss: 0.13421\n","Epoch: 01 [11124/18765 ( 59%)], Train Loss: 0.13429\n","Epoch: 01 [11164/18765 ( 59%)], Train Loss: 0.13426\n","Epoch: 01 [11204/18765 ( 60%)], Train Loss: 0.13421\n","Epoch: 01 [11244/18765 ( 60%)], Train Loss: 0.13401\n","Epoch: 01 [11284/18765 ( 60%)], Train Loss: 0.13381\n","Epoch: 01 [11324/18765 ( 60%)], Train Loss: 0.13353\n","Epoch: 01 [11364/18765 ( 61%)], Train Loss: 0.13361\n","Epoch: 01 [11404/18765 ( 61%)], Train Loss: 0.13360\n","Epoch: 01 [11444/18765 ( 61%)], Train Loss: 0.13357\n","Epoch: 01 [11484/18765 ( 61%)], Train Loss: 0.13334\n","Epoch: 01 [11524/18765 ( 61%)], Train Loss: 0.13348\n","Epoch: 01 [11564/18765 ( 62%)], Train Loss: 0.13339\n","Epoch: 01 [11604/18765 ( 62%)], Train Loss: 0.13352\n","Epoch: 01 [11644/18765 ( 62%)], Train Loss: 0.13337\n","Epoch: 01 [11684/18765 ( 62%)], Train Loss: 0.13329\n","Epoch: 01 [11724/18765 ( 62%)], Train Loss: 0.13310\n","Epoch: 01 [11764/18765 ( 63%)], Train Loss: 0.13309\n","Epoch: 01 [11804/18765 ( 63%)], Train Loss: 0.13285\n","Epoch: 01 [11844/18765 ( 63%)], Train Loss: 0.13280\n","Epoch: 01 [11884/18765 ( 63%)], Train Loss: 0.13290\n","Epoch: 01 [11924/18765 ( 64%)], Train Loss: 0.13288\n","Epoch: 01 [11964/18765 ( 64%)], Train Loss: 0.13283\n","Epoch: 01 [12004/18765 ( 64%)], Train Loss: 0.13271\n","Epoch: 01 [12044/18765 ( 64%)], Train Loss: 0.13259\n","Epoch: 01 [12084/18765 ( 64%)], Train Loss: 0.13240\n","Epoch: 01 [12124/18765 ( 65%)], Train Loss: 0.13222\n","Epoch: 01 [12164/18765 ( 65%)], Train Loss: 0.13217\n","Epoch: 01 [12204/18765 ( 65%)], Train Loss: 0.13225\n","Epoch: 01 [12244/18765 ( 65%)], Train Loss: 0.13232\n","Epoch: 01 [12284/18765 ( 65%)], Train Loss: 0.13215\n","Epoch: 01 [12324/18765 ( 66%)], Train Loss: 0.13199\n","Epoch: 01 [12364/18765 ( 66%)], Train Loss: 0.13178\n","Epoch: 01 [12404/18765 ( 66%)], Train Loss: 0.13153\n","Epoch: 01 [12444/18765 ( 66%)], Train Loss: 0.13169\n","Epoch: 01 [12484/18765 ( 67%)], Train Loss: 0.13167\n","Epoch: 01 [12524/18765 ( 67%)], Train Loss: 0.13134\n","Epoch: 01 [12564/18765 ( 67%)], Train Loss: 0.13114\n","Epoch: 01 [12604/18765 ( 67%)], Train Loss: 0.13095\n","Epoch: 01 [12644/18765 ( 67%)], Train Loss: 0.13102\n","Epoch: 01 [12684/18765 ( 68%)], Train Loss: 0.13077\n","Epoch: 01 [12724/18765 ( 68%)], Train Loss: 0.13051\n","Epoch: 01 [12764/18765 ( 68%)], Train Loss: 0.13037\n","Epoch: 01 [12804/18765 ( 68%)], Train Loss: 0.13025\n","Epoch: 01 [12844/18765 ( 68%)], Train Loss: 0.13013\n","Epoch: 01 [12884/18765 ( 69%)], Train Loss: 0.12997\n","Epoch: 01 [12924/18765 ( 69%)], Train Loss: 0.12975\n","Epoch: 01 [12964/18765 ( 69%)], Train Loss: 0.12955\n","Epoch: 01 [13004/18765 ( 69%)], Train Loss: 0.12936\n","Epoch: 01 [13044/18765 ( 70%)], Train Loss: 0.12924\n","Epoch: 01 [13084/18765 ( 70%)], Train Loss: 0.12909\n","Epoch: 01 [13124/18765 ( 70%)], Train Loss: 0.12923\n","Epoch: 01 [13164/18765 ( 70%)], Train Loss: 0.12895\n","Epoch: 01 [13204/18765 ( 70%)], Train Loss: 0.12881\n","Epoch: 01 [13244/18765 ( 71%)], Train Loss: 0.12872\n","Epoch: 01 [13284/18765 ( 71%)], Train Loss: 0.12873\n","Epoch: 01 [13324/18765 ( 71%)], Train Loss: 0.12902\n","Epoch: 01 [13364/18765 ( 71%)], Train Loss: 0.12883\n","Epoch: 01 [13404/18765 ( 71%)], Train Loss: 0.12866\n","Epoch: 01 [13444/18765 ( 72%)], Train Loss: 0.12845\n","Epoch: 01 [13484/18765 ( 72%)], Train Loss: 0.12849\n","Epoch: 01 [13524/18765 ( 72%)], Train Loss: 0.12831\n","Epoch: 01 [13564/18765 ( 72%)], Train Loss: 0.12824\n","Epoch: 01 [13604/18765 ( 72%)], Train Loss: 0.12824\n","Epoch: 01 [13644/18765 ( 73%)], Train Loss: 0.12798\n","Epoch: 01 [13684/18765 ( 73%)], Train Loss: 0.12787\n","Epoch: 01 [13724/18765 ( 73%)], Train Loss: 0.12766\n","Epoch: 01 [13764/18765 ( 73%)], Train Loss: 0.12785\n","Epoch: 01 [13804/18765 ( 74%)], Train Loss: 0.12779\n","Epoch: 01 [13844/18765 ( 74%)], Train Loss: 0.12766\n","Epoch: 01 [13884/18765 ( 74%)], Train Loss: 0.12746\n","Epoch: 01 [13924/18765 ( 74%)], Train Loss: 0.12730\n","Epoch: 01 [13964/18765 ( 74%)], Train Loss: 0.12705\n","Epoch: 01 [14004/18765 ( 75%)], Train Loss: 0.12689\n","Epoch: 01 [14044/18765 ( 75%)], Train Loss: 0.12668\n","Epoch: 01 [14084/18765 ( 75%)], Train Loss: 0.12642\n","Epoch: 01 [14124/18765 ( 75%)], Train Loss: 0.12639\n","Epoch: 01 [14164/18765 ( 75%)], Train Loss: 0.12618\n","Epoch: 01 [14204/18765 ( 76%)], Train Loss: 0.12602\n","Epoch: 01 [14244/18765 ( 76%)], Train Loss: 0.12577\n","Epoch: 01 [14284/18765 ( 76%)], Train Loss: 0.12565\n","Epoch: 01 [14324/18765 ( 76%)], Train Loss: 0.12548\n","Epoch: 01 [14364/18765 ( 77%)], Train Loss: 0.12528\n","Epoch: 01 [14404/18765 ( 77%)], Train Loss: 0.12512\n","Epoch: 01 [14444/18765 ( 77%)], Train Loss: 0.12485\n","Epoch: 01 [14484/18765 ( 77%)], Train Loss: 0.12513\n","Epoch: 01 [14524/18765 ( 77%)], Train Loss: 0.12525\n","Epoch: 01 [14564/18765 ( 78%)], Train Loss: 0.12510\n","Epoch: 01 [14604/18765 ( 78%)], Train Loss: 0.12498\n","Epoch: 01 [14644/18765 ( 78%)], Train Loss: 0.12489\n","Epoch: 01 [14684/18765 ( 78%)], Train Loss: 0.12498\n","Epoch: 01 [14724/18765 ( 78%)], Train Loss: 0.12485\n","Epoch: 01 [14764/18765 ( 79%)], Train Loss: 0.12479\n","Epoch: 01 [14804/18765 ( 79%)], Train Loss: 0.12463\n","Epoch: 01 [14844/18765 ( 79%)], Train Loss: 0.12457\n","Epoch: 01 [14884/18765 ( 79%)], Train Loss: 0.12437\n","Epoch: 01 [14924/18765 ( 80%)], Train Loss: 0.12419\n","Epoch: 01 [14964/18765 ( 80%)], Train Loss: 0.12407\n","Epoch: 01 [15004/18765 ( 80%)], Train Loss: 0.12384\n","Epoch: 01 [15044/18765 ( 80%)], Train Loss: 0.12365\n","Epoch: 01 [15084/18765 ( 80%)], Train Loss: 0.12345\n","Epoch: 01 [15124/18765 ( 81%)], Train Loss: 0.12352\n","Epoch: 01 [15164/18765 ( 81%)], Train Loss: 0.12352\n","Epoch: 01 [15204/18765 ( 81%)], Train Loss: 0.12392\n","Epoch: 01 [15244/18765 ( 81%)], Train Loss: 0.12368\n","Epoch: 01 [15284/18765 ( 81%)], Train Loss: 0.12404\n","Epoch: 01 [15324/18765 ( 82%)], Train Loss: 0.12381\n","Epoch: 01 [15364/18765 ( 82%)], Train Loss: 0.12366\n","Epoch: 01 [15404/18765 ( 82%)], Train Loss: 0.12374\n","Epoch: 01 [15444/18765 ( 82%)], Train Loss: 0.12363\n","Epoch: 01 [15484/18765 ( 83%)], Train Loss: 0.12362\n","Epoch: 01 [15524/18765 ( 83%)], Train Loss: 0.12365\n","Epoch: 01 [15564/18765 ( 83%)], Train Loss: 0.12389\n","Epoch: 01 [15604/18765 ( 83%)], Train Loss: 0.12374\n","Epoch: 01 [15644/18765 ( 83%)], Train Loss: 0.12358\n","Epoch: 01 [15684/18765 ( 84%)], Train Loss: 0.12343\n","Epoch: 01 [15724/18765 ( 84%)], Train Loss: 0.12404\n","Epoch: 01 [15764/18765 ( 84%)], Train Loss: 0.12387\n","Epoch: 01 [15804/18765 ( 84%)], Train Loss: 0.12380\n","Epoch: 01 [15844/18765 ( 84%)], Train Loss: 0.12370\n","Epoch: 01 [15884/18765 ( 85%)], Train Loss: 0.12350\n","Epoch: 01 [15924/18765 ( 85%)], Train Loss: 0.12336\n","Epoch: 01 [15964/18765 ( 85%)], Train Loss: 0.12331\n","Epoch: 01 [16004/18765 ( 85%)], Train Loss: 0.12336\n","Epoch: 01 [16044/18765 ( 85%)], Train Loss: 0.12322\n","Epoch: 01 [16084/18765 ( 86%)], Train Loss: 0.12328\n","Epoch: 01 [16124/18765 ( 86%)], Train Loss: 0.12309\n","Epoch: 01 [16164/18765 ( 86%)], Train Loss: 0.12308\n","Epoch: 01 [16204/18765 ( 86%)], Train Loss: 0.12288\n","Epoch: 01 [16244/18765 ( 87%)], Train Loss: 0.12287\n","Epoch: 01 [16284/18765 ( 87%)], Train Loss: 0.12301\n","Epoch: 01 [16324/18765 ( 87%)], Train Loss: 0.12288\n","Epoch: 01 [16364/18765 ( 87%)], Train Loss: 0.12274\n","Epoch: 01 [16404/18765 ( 87%)], Train Loss: 0.12284\n","Epoch: 01 [16444/18765 ( 88%)], Train Loss: 0.12267\n","Epoch: 01 [16484/18765 ( 88%)], Train Loss: 0.12261\n","Epoch: 01 [16524/18765 ( 88%)], Train Loss: 0.12259\n","Epoch: 01 [16564/18765 ( 88%)], Train Loss: 0.12254\n","Epoch: 01 [16604/18765 ( 88%)], Train Loss: 0.12231\n","Epoch: 01 [16644/18765 ( 89%)], Train Loss: 0.12211\n","Epoch: 01 [16684/18765 ( 89%)], Train Loss: 0.12202\n","Epoch: 01 [16724/18765 ( 89%)], Train Loss: 0.12196\n","Epoch: 01 [16764/18765 ( 89%)], Train Loss: 0.12181\n","Epoch: 01 [16804/18765 ( 90%)], Train Loss: 0.12181\n","Epoch: 01 [16844/18765 ( 90%)], Train Loss: 0.12187\n","Epoch: 01 [16884/18765 ( 90%)], Train Loss: 0.12176\n","Epoch: 01 [16924/18765 ( 90%)], Train Loss: 0.12162\n","Epoch: 01 [16964/18765 ( 90%)], Train Loss: 0.12151\n","Epoch: 01 [17004/18765 ( 91%)], Train Loss: 0.12140\n","Epoch: 01 [17044/18765 ( 91%)], Train Loss: 0.12123\n","Epoch: 01 [17084/18765 ( 91%)], Train Loss: 0.12115\n","Epoch: 01 [17124/18765 ( 91%)], Train Loss: 0.12112\n","Epoch: 01 [17164/18765 ( 91%)], Train Loss: 0.12093\n","Epoch: 01 [17204/18765 ( 92%)], Train Loss: 0.12081\n","Epoch: 01 [17244/18765 ( 92%)], Train Loss: 0.12078\n","Epoch: 01 [17284/18765 ( 92%)], Train Loss: 0.12088\n","Epoch: 01 [17324/18765 ( 92%)], Train Loss: 0.12093\n","Epoch: 01 [17364/18765 ( 93%)], Train Loss: 0.12085\n","Epoch: 01 [17404/18765 ( 93%)], Train Loss: 0.12101\n","Epoch: 01 [17444/18765 ( 93%)], Train Loss: 0.12102\n","Epoch: 01 [17484/18765 ( 93%)], Train Loss: 0.12114\n","Epoch: 01 [17524/18765 ( 93%)], Train Loss: 0.12112\n","Epoch: 01 [17564/18765 ( 94%)], Train Loss: 0.12096\n","Epoch: 01 [17604/18765 ( 94%)], Train Loss: 0.12075\n","Epoch: 01 [17644/18765 ( 94%)], Train Loss: 0.12059\n","Epoch: 01 [17684/18765 ( 94%)], Train Loss: 0.12058\n","Epoch: 01 [17724/18765 ( 94%)], Train Loss: 0.12041\n","Epoch: 01 [17764/18765 ( 95%)], Train Loss: 0.12051\n","Epoch: 01 [17804/18765 ( 95%)], Train Loss: 0.12032\n","Epoch: 01 [17844/18765 ( 95%)], Train Loss: 0.12025\n","Epoch: 01 [17884/18765 ( 95%)], Train Loss: 0.12008\n","Epoch: 01 [17924/18765 ( 96%)], Train Loss: 0.11999\n","Epoch: 01 [17964/18765 ( 96%)], Train Loss: 0.11991\n","Epoch: 01 [18004/18765 ( 96%)], Train Loss: 0.11987\n","Epoch: 01 [18044/18765 ( 96%)], Train Loss: 0.11983\n","Epoch: 01 [18084/18765 ( 96%)], Train Loss: 0.11976\n","Epoch: 01 [18124/18765 ( 97%)], Train Loss: 0.11968\n","Epoch: 01 [18164/18765 ( 97%)], Train Loss: 0.11960\n","Epoch: 01 [18204/18765 ( 97%)], Train Loss: 0.11944\n","Epoch: 01 [18244/18765 ( 97%)], Train Loss: 0.11928\n","Epoch: 01 [18284/18765 ( 97%)], Train Loss: 0.11909\n","Epoch: 01 [18324/18765 ( 98%)], Train Loss: 0.11892\n","Epoch: 01 [18364/18765 ( 98%)], Train Loss: 0.11912\n","Epoch: 01 [18404/18765 ( 98%)], Train Loss: 0.11929\n","Epoch: 01 [18444/18765 ( 98%)], Train Loss: 0.11928\n","Epoch: 01 [18484/18765 ( 99%)], Train Loss: 0.11938\n","Epoch: 01 [18524/18765 ( 99%)], Train Loss: 0.11920\n","Epoch: 01 [18564/18765 ( 99%)], Train Loss: 0.11919\n","Epoch: 01 [18604/18765 ( 99%)], Train Loss: 0.11911\n","Epoch: 01 [18644/18765 ( 99%)], Train Loss: 0.11901\n","Epoch: 01 [18684/18765 (100%)], Train Loss: 0.11911\n","Epoch: 01 [18724/18765 (100%)], Train Loss: 0.11923\n","Epoch: 01 [18764/18765 (100%)], Train Loss: 0.11931\n","Epoch: 01 [18765/18765 (100%)], Train Loss: 0.11931\n","----Validation Results Summary----\n","Epoch: [1] Valid Loss: 0.77846\n","\n","Total Training Time: 5220.622580289841secs, Average Training Time per Epoch: 2610.3112901449203secs.\n","Total Validation Time: 362.4424283504486secs, Average Validation Time per Epoch: 181.2212141752243secs.\n","\n","\n","--------------------------------------------------\n","FOLD: 4\n","--------------------------------------------------\n","Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n","Num examples Train= 18701, Num examples Valid=4368\n","Total Training Steps: 4676, Total Warmup Steps: 467\n","Epoch: 00 [    4/18701 (  0%)], Train Loss: 2.97787\n","Epoch: 00 [   44/18701 (  0%)], Train Loss: 2.91058\n","Epoch: 00 [   84/18701 (  0%)], Train Loss: 2.89761\n","Epoch: 00 [  124/18701 (  1%)], Train Loss: 2.87980\n","Epoch: 00 [  164/18701 (  1%)], Train Loss: 2.86090\n","Epoch: 00 [  204/18701 (  1%)], Train Loss: 2.82690\n","Epoch: 00 [  244/18701 (  1%)], Train Loss: 2.78489\n","Epoch: 00 [  284/18701 (  2%)], Train Loss: 2.73766\n","Epoch: 00 [  324/18701 (  2%)], Train Loss: 2.67345\n","Epoch: 00 [  364/18701 (  2%)], Train Loss: 2.61037\n","Epoch: 00 [  404/18701 (  2%)], Train Loss: 2.54493\n","Epoch: 00 [  444/18701 (  2%)], Train Loss: 2.47859\n","Epoch: 00 [  484/18701 (  3%)], Train Loss: 2.37554\n","Epoch: 00 [  524/18701 (  3%)], Train Loss: 2.28691\n","Epoch: 00 [  564/18701 (  3%)], Train Loss: 2.19727\n","Epoch: 00 [  604/18701 (  3%)], Train Loss: 2.11281\n","Epoch: 00 [  644/18701 (  3%)], Train Loss: 2.02436\n","Epoch: 00 [  684/18701 (  4%)], Train Loss: 1.95827\n","Epoch: 00 [  724/18701 (  4%)], Train Loss: 1.89970\n","Epoch: 00 [  764/18701 (  4%)], Train Loss: 1.84070\n","Epoch: 00 [  804/18701 (  4%)], Train Loss: 1.78341\n","Epoch: 00 [  844/18701 (  5%)], Train Loss: 1.72713\n","Epoch: 00 [  884/18701 (  5%)], Train Loss: 1.67369\n","Epoch: 00 [  924/18701 (  5%)], Train Loss: 1.63236\n","Epoch: 00 [  964/18701 (  5%)], Train Loss: 1.59308\n","Epoch: 00 [ 1004/18701 (  5%)], Train Loss: 1.54020\n","Epoch: 00 [ 1044/18701 (  6%)], Train Loss: 1.49647\n","Epoch: 00 [ 1084/18701 (  6%)], Train Loss: 1.46184\n","Epoch: 00 [ 1124/18701 (  6%)], Train Loss: 1.43166\n","Epoch: 00 [ 1164/18701 (  6%)], Train Loss: 1.39769\n","Epoch: 00 [ 1204/18701 (  6%)], Train Loss: 1.36736\n","Epoch: 00 [ 1244/18701 (  7%)], Train Loss: 1.34278\n","Epoch: 00 [ 1284/18701 (  7%)], Train Loss: 1.31674\n","Epoch: 00 [ 1324/18701 (  7%)], Train Loss: 1.29092\n","Epoch: 00 [ 1364/18701 (  7%)], Train Loss: 1.26468\n","Epoch: 00 [ 1404/18701 (  8%)], Train Loss: 1.24115\n","Epoch: 00 [ 1444/18701 (  8%)], Train Loss: 1.22328\n","Epoch: 00 [ 1484/18701 (  8%)], Train Loss: 1.19907\n","Epoch: 00 [ 1524/18701 (  8%)], Train Loss: 1.17805\n","Epoch: 00 [ 1564/18701 (  8%)], Train Loss: 1.16116\n","Epoch: 00 [ 1604/18701 (  9%)], Train Loss: 1.14555\n","Epoch: 00 [ 1644/18701 (  9%)], Train Loss: 1.12649\n","Epoch: 00 [ 1684/18701 (  9%)], Train Loss: 1.11060\n","Epoch: 00 [ 1724/18701 (  9%)], Train Loss: 1.09249\n","Epoch: 00 [ 1764/18701 (  9%)], Train Loss: 1.08269\n","Epoch: 00 [ 1804/18701 ( 10%)], Train Loss: 1.06602\n","Epoch: 00 [ 1844/18701 ( 10%)], Train Loss: 1.05163\n","Epoch: 00 [ 1884/18701 ( 10%)], Train Loss: 1.04194\n","Epoch: 00 [ 1924/18701 ( 10%)], Train Loss: 1.03045\n","Epoch: 00 [ 1964/18701 ( 11%)], Train Loss: 1.01695\n","Epoch: 00 [ 2004/18701 ( 11%)], Train Loss: 1.00267\n","Epoch: 00 [ 2044/18701 ( 11%)], Train Loss: 0.99294\n","Epoch: 00 [ 2084/18701 ( 11%)], Train Loss: 0.98153\n","Epoch: 00 [ 2124/18701 ( 11%)], Train Loss: 0.96742\n","Epoch: 00 [ 2164/18701 ( 12%)], Train Loss: 0.95963\n","Epoch: 00 [ 2204/18701 ( 12%)], Train Loss: 0.94592\n","Epoch: 00 [ 2244/18701 ( 12%)], Train Loss: 0.93786\n","Epoch: 00 [ 2284/18701 ( 12%)], Train Loss: 0.92549\n","Epoch: 00 [ 2324/18701 ( 12%)], Train Loss: 0.91426\n","Epoch: 00 [ 2364/18701 ( 13%)], Train Loss: 0.90603\n","Epoch: 00 [ 2404/18701 ( 13%)], Train Loss: 0.89796\n","Epoch: 00 [ 2444/18701 ( 13%)], Train Loss: 0.88626\n","Epoch: 00 [ 2484/18701 ( 13%)], Train Loss: 0.87920\n","Epoch: 00 [ 2524/18701 ( 13%)], Train Loss: 0.87339\n","Epoch: 00 [ 2564/18701 ( 14%)], Train Loss: 0.86709\n","Epoch: 00 [ 2604/18701 ( 14%)], Train Loss: 0.85996\n","Epoch: 00 [ 2644/18701 ( 14%)], Train Loss: 0.85055\n","Epoch: 00 [ 2684/18701 ( 14%)], Train Loss: 0.84478\n","Epoch: 00 [ 2724/18701 ( 15%)], Train Loss: 0.83667\n","Epoch: 00 [ 2764/18701 ( 15%)], Train Loss: 0.82928\n","Epoch: 00 [ 2804/18701 ( 15%)], Train Loss: 0.82393\n","Epoch: 00 [ 2844/18701 ( 15%)], Train Loss: 0.82373\n","Epoch: 00 [ 2884/18701 ( 15%)], Train Loss: 0.81736\n","Epoch: 00 [ 2924/18701 ( 16%)], Train Loss: 0.81111\n","Epoch: 00 [ 2964/18701 ( 16%)], Train Loss: 0.80628\n","Epoch: 00 [ 3004/18701 ( 16%)], Train Loss: 0.79913\n","Epoch: 00 [ 3044/18701 ( 16%)], Train Loss: 0.79388\n","Epoch: 00 [ 3084/18701 ( 16%)], Train Loss: 0.78792\n","Epoch: 00 [ 3124/18701 ( 17%)], Train Loss: 0.78174\n","Epoch: 00 [ 3164/18701 ( 17%)], Train Loss: 0.77727\n","Epoch: 00 [ 3204/18701 ( 17%)], Train Loss: 0.76943\n","Epoch: 00 [ 3244/18701 ( 17%)], Train Loss: 0.76229\n","Epoch: 00 [ 3284/18701 ( 18%)], Train Loss: 0.75656\n","Epoch: 00 [ 3324/18701 ( 18%)], Train Loss: 0.75241\n","Epoch: 00 [ 3364/18701 ( 18%)], Train Loss: 0.74956\n","Epoch: 00 [ 3404/18701 ( 18%)], Train Loss: 0.74316\n","Epoch: 00 [ 3444/18701 ( 18%)], Train Loss: 0.74024\n","Epoch: 00 [ 3484/18701 ( 19%)], Train Loss: 0.73621\n","Epoch: 00 [ 3524/18701 ( 19%)], Train Loss: 0.73323\n","Epoch: 00 [ 3564/18701 ( 19%)], Train Loss: 0.73115\n","Epoch: 00 [ 3604/18701 ( 19%)], Train Loss: 0.72576\n","Epoch: 00 [ 3644/18701 ( 19%)], Train Loss: 0.71986\n","Epoch: 00 [ 3684/18701 ( 20%)], Train Loss: 0.71946\n","Epoch: 00 [ 3724/18701 ( 20%)], Train Loss: 0.71618\n","Epoch: 00 [ 3764/18701 ( 20%)], Train Loss: 0.71232\n","Epoch: 00 [ 3804/18701 ( 20%)], Train Loss: 0.70819\n","Epoch: 00 [ 3844/18701 ( 21%)], Train Loss: 0.70406\n","Epoch: 00 [ 3884/18701 ( 21%)], Train Loss: 0.69952\n","Epoch: 00 [ 3924/18701 ( 21%)], Train Loss: 0.69668\n","Epoch: 00 [ 3964/18701 ( 21%)], Train Loss: 0.69444\n","Epoch: 00 [ 4004/18701 ( 21%)], Train Loss: 0.69210\n","Epoch: 00 [ 4044/18701 ( 22%)], Train Loss: 0.68670\n","Epoch: 00 [ 4084/18701 ( 22%)], Train Loss: 0.68382\n","Epoch: 00 [ 4124/18701 ( 22%)], Train Loss: 0.67990\n","Epoch: 00 [ 4164/18701 ( 22%)], Train Loss: 0.67594\n","Epoch: 00 [ 4204/18701 ( 22%)], Train Loss: 0.67354\n","Epoch: 00 [ 4244/18701 ( 23%)], Train Loss: 0.67057\n","Epoch: 00 [ 4284/18701 ( 23%)], Train Loss: 0.66901\n","Epoch: 00 [ 4324/18701 ( 23%)], Train Loss: 0.66470\n","Epoch: 00 [ 4364/18701 ( 23%)], Train Loss: 0.66216\n","Epoch: 00 [ 4404/18701 ( 24%)], Train Loss: 0.65923\n","Epoch: 00 [ 4444/18701 ( 24%)], Train Loss: 0.65467\n","Epoch: 00 [ 4484/18701 ( 24%)], Train Loss: 0.65369\n","Epoch: 00 [ 4524/18701 ( 24%)], Train Loss: 0.65300\n","Epoch: 00 [ 4564/18701 ( 24%)], Train Loss: 0.65019\n","Epoch: 00 [ 4604/18701 ( 25%)], Train Loss: 0.64709\n","Epoch: 00 [ 4644/18701 ( 25%)], Train Loss: 0.64475\n","Epoch: 00 [ 4684/18701 ( 25%)], Train Loss: 0.64079\n","Epoch: 00 [ 4724/18701 ( 25%)], Train Loss: 0.63863\n","Epoch: 00 [ 4764/18701 ( 25%)], Train Loss: 0.63749\n","Epoch: 00 [ 4804/18701 ( 26%)], Train Loss: 0.63521\n","Epoch: 00 [ 4844/18701 ( 26%)], Train Loss: 0.63253\n","Epoch: 00 [ 4884/18701 ( 26%)], Train Loss: 0.63013\n","Epoch: 00 [ 4924/18701 ( 26%)], Train Loss: 0.62799\n","Epoch: 00 [ 4964/18701 ( 27%)], Train Loss: 0.62569\n","Epoch: 00 [ 5004/18701 ( 27%)], Train Loss: 0.62345\n","Epoch: 00 [ 5044/18701 ( 27%)], Train Loss: 0.62384\n","Epoch: 00 [ 5084/18701 ( 27%)], Train Loss: 0.62216\n","Epoch: 00 [ 5124/18701 ( 27%)], Train Loss: 0.62017\n","Epoch: 00 [ 5164/18701 ( 28%)], Train Loss: 0.61747\n","Epoch: 00 [ 5204/18701 ( 28%)], Train Loss: 0.61544\n","Epoch: 00 [ 5244/18701 ( 28%)], Train Loss: 0.61443\n","Epoch: 00 [ 5284/18701 ( 28%)], Train Loss: 0.61218\n","Epoch: 00 [ 5324/18701 ( 28%)], Train Loss: 0.61239\n","Epoch: 00 [ 5364/18701 ( 29%)], Train Loss: 0.61093\n","Epoch: 00 [ 5404/18701 ( 29%)], Train Loss: 0.60905\n","Epoch: 00 [ 5444/18701 ( 29%)], Train Loss: 0.60603\n","Epoch: 00 [ 5484/18701 ( 29%)], Train Loss: 0.60376\n","Epoch: 00 [ 5524/18701 ( 30%)], Train Loss: 0.60089\n","Epoch: 00 [ 5564/18701 ( 30%)], Train Loss: 0.59993\n","Epoch: 00 [ 5604/18701 ( 30%)], Train Loss: 0.59890\n","Epoch: 00 [ 5644/18701 ( 30%)], Train Loss: 0.59682\n","Epoch: 00 [ 5684/18701 ( 30%)], Train Loss: 0.59527\n","Epoch: 00 [ 5724/18701 ( 31%)], Train Loss: 0.59373\n","Epoch: 00 [ 5764/18701 ( 31%)], Train Loss: 0.59229\n","Epoch: 00 [ 5804/18701 ( 31%)], Train Loss: 0.59103\n","Epoch: 00 [ 5844/18701 ( 31%)], Train Loss: 0.58937\n","Epoch: 00 [ 5884/18701 ( 31%)], Train Loss: 0.58708\n","Epoch: 00 [ 5924/18701 ( 32%)], Train Loss: 0.58509\n","Epoch: 00 [ 5964/18701 ( 32%)], Train Loss: 0.58283\n","Epoch: 00 [ 6004/18701 ( 32%)], Train Loss: 0.58216\n","Epoch: 00 [ 6044/18701 ( 32%)], Train Loss: 0.58183\n","Epoch: 00 [ 6084/18701 ( 33%)], Train Loss: 0.58061\n","Epoch: 00 [ 6124/18701 ( 33%)], Train Loss: 0.57826\n","Epoch: 00 [ 6164/18701 ( 33%)], Train Loss: 0.57705\n","Epoch: 00 [ 6204/18701 ( 33%)], Train Loss: 0.57538\n","Epoch: 00 [ 6244/18701 ( 33%)], Train Loss: 0.57389\n","Epoch: 00 [ 6284/18701 ( 34%)], Train Loss: 0.57239\n","Epoch: 00 [ 6324/18701 ( 34%)], Train Loss: 0.57148\n","Epoch: 00 [ 6364/18701 ( 34%)], Train Loss: 0.56887\n","Epoch: 00 [ 6404/18701 ( 34%)], Train Loss: 0.56807\n","Epoch: 00 [ 6444/18701 ( 34%)], Train Loss: 0.56664\n","Epoch: 00 [ 6484/18701 ( 35%)], Train Loss: 0.56529\n","Epoch: 00 [ 6524/18701 ( 35%)], Train Loss: 0.56453\n","Epoch: 00 [ 6564/18701 ( 35%)], Train Loss: 0.56446\n","Epoch: 00 [ 6604/18701 ( 35%)], Train Loss: 0.56260\n","Epoch: 00 [ 6644/18701 ( 36%)], Train Loss: 0.56317\n","Epoch: 00 [ 6684/18701 ( 36%)], Train Loss: 0.56150\n","Epoch: 00 [ 6724/18701 ( 36%)], Train Loss: 0.55972\n","Epoch: 00 [ 6764/18701 ( 36%)], Train Loss: 0.55847\n","Epoch: 00 [ 6804/18701 ( 36%)], Train Loss: 0.55704\n","Epoch: 00 [ 6844/18701 ( 37%)], Train Loss: 0.55530\n","Epoch: 00 [ 6884/18701 ( 37%)], Train Loss: 0.55446\n","Epoch: 00 [ 6924/18701 ( 37%)], Train Loss: 0.55337\n","Epoch: 00 [ 6964/18701 ( 37%)], Train Loss: 0.55339\n","Epoch: 00 [ 7004/18701 ( 37%)], Train Loss: 0.55309\n","Epoch: 00 [ 7044/18701 ( 38%)], Train Loss: 0.55214\n","Epoch: 00 [ 7084/18701 ( 38%)], Train Loss: 0.55109\n","Epoch: 00 [ 7124/18701 ( 38%)], Train Loss: 0.54962\n","Epoch: 00 [ 7164/18701 ( 38%)], Train Loss: 0.54961\n","Epoch: 00 [ 7204/18701 ( 39%)], Train Loss: 0.54934\n","Epoch: 00 [ 7244/18701 ( 39%)], Train Loss: 0.54989\n","Epoch: 00 [ 7284/18701 ( 39%)], Train Loss: 0.54912\n","Epoch: 00 [ 7324/18701 ( 39%)], Train Loss: 0.54854\n","Epoch: 00 [ 7364/18701 ( 39%)], Train Loss: 0.54738\n","Epoch: 00 [ 7404/18701 ( 40%)], Train Loss: 0.54590\n","Epoch: 00 [ 7444/18701 ( 40%)], Train Loss: 0.54398\n","Epoch: 00 [ 7484/18701 ( 40%)], Train Loss: 0.54300\n","Epoch: 00 [ 7524/18701 ( 40%)], Train Loss: 0.54259\n","Epoch: 00 [ 7564/18701 ( 40%)], Train Loss: 0.54156\n","Epoch: 00 [ 7604/18701 ( 41%)], Train Loss: 0.54093\n","Epoch: 00 [ 7644/18701 ( 41%)], Train Loss: 0.54012\n","Epoch: 00 [ 7684/18701 ( 41%)], Train Loss: 0.53799\n","Epoch: 00 [ 7724/18701 ( 41%)], Train Loss: 0.53712\n","Epoch: 00 [ 7764/18701 ( 42%)], Train Loss: 0.53665\n","Epoch: 00 [ 7804/18701 ( 42%)], Train Loss: 0.53522\n","Epoch: 00 [ 7844/18701 ( 42%)], Train Loss: 0.53434\n","Epoch: 00 [ 7884/18701 ( 42%)], Train Loss: 0.53313\n","Epoch: 00 [ 7924/18701 ( 42%)], Train Loss: 0.53343\n","Epoch: 00 [ 7964/18701 ( 43%)], Train Loss: 0.53316\n","Epoch: 00 [ 8004/18701 ( 43%)], Train Loss: 0.53167\n","Epoch: 00 [ 8044/18701 ( 43%)], Train Loss: 0.53079\n","Epoch: 00 [ 8084/18701 ( 43%)], Train Loss: 0.52919\n","Epoch: 00 [ 8124/18701 ( 43%)], Train Loss: 0.52902\n","Epoch: 00 [ 8164/18701 ( 44%)], Train Loss: 0.52789\n","Epoch: 00 [ 8204/18701 ( 44%)], Train Loss: 0.52687\n","Epoch: 00 [ 8244/18701 ( 44%)], Train Loss: 0.52658\n","Epoch: 00 [ 8284/18701 ( 44%)], Train Loss: 0.52666\n","Epoch: 00 [ 8324/18701 ( 45%)], Train Loss: 0.52569\n","Epoch: 00 [ 8364/18701 ( 45%)], Train Loss: 0.52443\n","Epoch: 00 [ 8404/18701 ( 45%)], Train Loss: 0.52406\n","Epoch: 00 [ 8444/18701 ( 45%)], Train Loss: 0.52288\n","Epoch: 00 [ 8484/18701 ( 45%)], Train Loss: 0.52088\n","Epoch: 00 [ 8524/18701 ( 46%)], Train Loss: 0.52007\n","Epoch: 00 [ 8564/18701 ( 46%)], Train Loss: 0.52009\n","Epoch: 00 [ 8604/18701 ( 46%)], Train Loss: 0.51937\n","Epoch: 00 [ 8644/18701 ( 46%)], Train Loss: 0.51783\n","Epoch: 00 [ 8684/18701 ( 46%)], Train Loss: 0.51788\n","Epoch: 00 [ 8724/18701 ( 47%)], Train Loss: 0.51773\n","Epoch: 00 [ 8764/18701 ( 47%)], Train Loss: 0.51649\n","Epoch: 00 [ 8804/18701 ( 47%)], Train Loss: 0.51535\n","Epoch: 00 [ 8844/18701 ( 47%)], Train Loss: 0.51457\n","Epoch: 00 [ 8884/18701 ( 48%)], Train Loss: 0.51304\n","Epoch: 00 [ 8924/18701 ( 48%)], Train Loss: 0.51207\n","Epoch: 00 [ 8964/18701 ( 48%)], Train Loss: 0.51142\n","Epoch: 00 [ 9004/18701 ( 48%)], Train Loss: 0.51101\n","Epoch: 00 [ 9044/18701 ( 48%)], Train Loss: 0.51020\n","Epoch: 00 [ 9084/18701 ( 49%)], Train Loss: 0.51008\n","Epoch: 00 [ 9124/18701 ( 49%)], Train Loss: 0.50918\n","Epoch: 00 [ 9164/18701 ( 49%)], Train Loss: 0.50835\n","Epoch: 00 [ 9204/18701 ( 49%)], Train Loss: 0.50802\n","Epoch: 00 [ 9244/18701 ( 49%)], Train Loss: 0.50751\n","Epoch: 00 [ 9284/18701 ( 50%)], Train Loss: 0.50705\n","Epoch: 00 [ 9324/18701 ( 50%)], Train Loss: 0.50653\n","Epoch: 00 [ 9364/18701 ( 50%)], Train Loss: 0.50585\n","Epoch: 00 [ 9404/18701 ( 50%)], Train Loss: 0.50548\n","Epoch: 00 [ 9444/18701 ( 50%)], Train Loss: 0.50498\n","Epoch: 00 [ 9484/18701 ( 51%)], Train Loss: 0.50461\n","Epoch: 00 [ 9524/18701 ( 51%)], Train Loss: 0.50471\n","Epoch: 00 [ 9564/18701 ( 51%)], Train Loss: 0.50434\n","Epoch: 00 [ 9604/18701 ( 51%)], Train Loss: 0.50318\n","Epoch: 00 [ 9644/18701 ( 52%)], Train Loss: 0.50241\n","Epoch: 00 [ 9684/18701 ( 52%)], Train Loss: 0.50147\n","Epoch: 00 [ 9724/18701 ( 52%)], Train Loss: 0.50125\n","Epoch: 00 [ 9764/18701 ( 52%)], Train Loss: 0.50057\n","Epoch: 00 [ 9804/18701 ( 52%)], Train Loss: 0.49982\n","Epoch: 00 [ 9844/18701 ( 53%)], Train Loss: 0.49936\n","Epoch: 00 [ 9884/18701 ( 53%)], Train Loss: 0.49853\n","Epoch: 00 [ 9924/18701 ( 53%)], Train Loss: 0.49818\n","Epoch: 00 [ 9964/18701 ( 53%)], Train Loss: 0.49789\n","Epoch: 00 [10004/18701 ( 53%)], Train Loss: 0.49744\n","Epoch: 00 [10044/18701 ( 54%)], Train Loss: 0.49735\n","Epoch: 00 [10084/18701 ( 54%)], Train Loss: 0.49692\n","Epoch: 00 [10124/18701 ( 54%)], Train Loss: 0.49655\n","Epoch: 00 [10164/18701 ( 54%)], Train Loss: 0.49594\n","Epoch: 00 [10204/18701 ( 55%)], Train Loss: 0.49523\n","Epoch: 00 [10244/18701 ( 55%)], Train Loss: 0.49496\n","Epoch: 00 [10284/18701 ( 55%)], Train Loss: 0.49440\n","Epoch: 00 [10324/18701 ( 55%)], Train Loss: 0.49396\n","Epoch: 00 [10364/18701 ( 55%)], Train Loss: 0.49316\n","Epoch: 00 [10404/18701 ( 56%)], Train Loss: 0.49294\n","Epoch: 00 [10444/18701 ( 56%)], Train Loss: 0.49254\n","Epoch: 00 [10484/18701 ( 56%)], Train Loss: 0.49171\n","Epoch: 00 [10524/18701 ( 56%)], Train Loss: 0.49115\n","Epoch: 00 [10564/18701 ( 56%)], Train Loss: 0.49052\n","Epoch: 00 [10604/18701 ( 57%)], Train Loss: 0.48976\n","Epoch: 00 [10644/18701 ( 57%)], Train Loss: 0.48923\n","Epoch: 00 [10684/18701 ( 57%)], Train Loss: 0.48846\n","Epoch: 00 [10724/18701 ( 57%)], Train Loss: 0.48879\n","Epoch: 00 [10764/18701 ( 58%)], Train Loss: 0.48827\n","Epoch: 00 [10804/18701 ( 58%)], Train Loss: 0.48788\n","Epoch: 00 [10844/18701 ( 58%)], Train Loss: 0.48752\n","Epoch: 00 [10884/18701 ( 58%)], Train Loss: 0.48678\n","Epoch: 00 [10924/18701 ( 58%)], Train Loss: 0.48664\n","Epoch: 00 [10964/18701 ( 59%)], Train Loss: 0.48659\n","Epoch: 00 [11004/18701 ( 59%)], Train Loss: 0.48622\n","Epoch: 00 [11044/18701 ( 59%)], Train Loss: 0.48606\n","Epoch: 00 [11084/18701 ( 59%)], Train Loss: 0.48549\n","Epoch: 00 [11124/18701 ( 59%)], Train Loss: 0.48521\n","Epoch: 00 [11164/18701 ( 60%)], Train Loss: 0.48456\n","Epoch: 00 [11204/18701 ( 60%)], Train Loss: 0.48349\n","Epoch: 00 [11244/18701 ( 60%)], Train Loss: 0.48249\n","Epoch: 00 [11284/18701 ( 60%)], Train Loss: 0.48132\n","Epoch: 00 [11324/18701 ( 61%)], Train Loss: 0.48168\n","Epoch: 00 [11364/18701 ( 61%)], Train Loss: 0.48149\n","Epoch: 00 [11404/18701 ( 61%)], Train Loss: 0.48091\n","Epoch: 00 [11444/18701 ( 61%)], Train Loss: 0.48048\n","Epoch: 00 [11484/18701 ( 61%)], Train Loss: 0.48037\n","Epoch: 00 [11524/18701 ( 62%)], Train Loss: 0.47969\n","Epoch: 00 [11564/18701 ( 62%)], Train Loss: 0.47964\n","Epoch: 00 [11604/18701 ( 62%)], Train Loss: 0.47890\n","Epoch: 00 [11644/18701 ( 62%)], Train Loss: 0.47828\n","Epoch: 00 [11684/18701 ( 62%)], Train Loss: 0.47737\n","Epoch: 00 [11724/18701 ( 63%)], Train Loss: 0.47710\n","Epoch: 00 [11764/18701 ( 63%)], Train Loss: 0.47652\n","Epoch: 00 [11804/18701 ( 63%)], Train Loss: 0.47599\n","Epoch: 00 [11844/18701 ( 63%)], Train Loss: 0.47527\n","Epoch: 00 [11884/18701 ( 64%)], Train Loss: 0.47495\n","Epoch: 00 [11924/18701 ( 64%)], Train Loss: 0.47452\n","Epoch: 00 [11964/18701 ( 64%)], Train Loss: 0.47368\n","Epoch: 00 [12004/18701 ( 64%)], Train Loss: 0.47327\n","Epoch: 00 [12044/18701 ( 64%)], Train Loss: 0.47245\n","Epoch: 00 [12084/18701 ( 65%)], Train Loss: 0.47185\n","Epoch: 00 [12124/18701 ( 65%)], Train Loss: 0.47146\n","Epoch: 00 [12164/18701 ( 65%)], Train Loss: 0.47117\n","Epoch: 00 [12204/18701 ( 65%)], Train Loss: 0.47038\n","Epoch: 00 [12244/18701 ( 65%)], Train Loss: 0.46988\n","Epoch: 00 [12284/18701 ( 66%)], Train Loss: 0.46952\n","Epoch: 00 [12324/18701 ( 66%)], Train Loss: 0.46918\n","Epoch: 00 [12364/18701 ( 66%)], Train Loss: 0.46887\n","Epoch: 00 [12404/18701 ( 66%)], Train Loss: 0.46832\n","Epoch: 00 [12444/18701 ( 67%)], Train Loss: 0.46744\n","Epoch: 00 [12484/18701 ( 67%)], Train Loss: 0.46676\n","Epoch: 00 [12524/18701 ( 67%)], Train Loss: 0.46619\n","Epoch: 00 [12564/18701 ( 67%)], Train Loss: 0.46556\n","Epoch: 00 [12604/18701 ( 67%)], Train Loss: 0.46505\n","Epoch: 00 [12644/18701 ( 68%)], Train Loss: 0.46461\n","Epoch: 00 [12684/18701 ( 68%)], Train Loss: 0.46465\n","Epoch: 00 [12724/18701 ( 68%)], Train Loss: 0.46435\n","Epoch: 00 [12764/18701 ( 68%)], Train Loss: 0.46406\n","Epoch: 00 [12804/18701 ( 68%)], Train Loss: 0.46333\n","Epoch: 00 [12844/18701 ( 69%)], Train Loss: 0.46291\n","Epoch: 00 [12884/18701 ( 69%)], Train Loss: 0.46275\n","Epoch: 00 [12924/18701 ( 69%)], Train Loss: 0.46228\n","Epoch: 00 [12964/18701 ( 69%)], Train Loss: 0.46194\n","Epoch: 00 [13004/18701 ( 70%)], Train Loss: 0.46143\n","Epoch: 00 [13044/18701 ( 70%)], Train Loss: 0.46097\n","Epoch: 00 [13084/18701 ( 70%)], Train Loss: 0.46079\n","Epoch: 00 [13124/18701 ( 70%)], Train Loss: 0.46034\n","Epoch: 00 [13164/18701 ( 70%)], Train Loss: 0.46008\n","Epoch: 00 [13204/18701 ( 71%)], Train Loss: 0.45972\n","Epoch: 00 [13244/18701 ( 71%)], Train Loss: 0.45889\n","Epoch: 00 [13284/18701 ( 71%)], Train Loss: 0.45821\n","Epoch: 00 [13324/18701 ( 71%)], Train Loss: 0.45752\n","Epoch: 00 [13364/18701 ( 71%)], Train Loss: 0.45701\n","Epoch: 00 [13404/18701 ( 72%)], Train Loss: 0.45661\n","Epoch: 00 [13444/18701 ( 72%)], Train Loss: 0.45600\n","Epoch: 00 [13484/18701 ( 72%)], Train Loss: 0.45564\n","Epoch: 00 [13524/18701 ( 72%)], Train Loss: 0.45489\n","Epoch: 00 [13564/18701 ( 73%)], Train Loss: 0.45454\n","Epoch: 00 [13604/18701 ( 73%)], Train Loss: 0.45441\n","Epoch: 00 [13644/18701 ( 73%)], Train Loss: 0.45383\n","Epoch: 00 [13684/18701 ( 73%)], Train Loss: 0.45330\n","Epoch: 00 [13724/18701 ( 73%)], Train Loss: 0.45303\n","Epoch: 00 [13764/18701 ( 74%)], Train Loss: 0.45257\n","Epoch: 00 [13804/18701 ( 74%)], Train Loss: 0.45220\n","Epoch: 00 [13844/18701 ( 74%)], Train Loss: 0.45191\n","Epoch: 00 [13884/18701 ( 74%)], Train Loss: 0.45160\n","Epoch: 00 [13924/18701 ( 74%)], Train Loss: 0.45070\n","Epoch: 00 [13964/18701 ( 75%)], Train Loss: 0.45137\n","Epoch: 00 [14004/18701 ( 75%)], Train Loss: 0.45057\n","Epoch: 00 [14044/18701 ( 75%)], Train Loss: 0.44997\n","Epoch: 00 [14084/18701 ( 75%)], Train Loss: 0.44938\n","Epoch: 00 [14124/18701 ( 76%)], Train Loss: 0.44903\n","Epoch: 00 [14164/18701 ( 76%)], Train Loss: 0.44837\n","Epoch: 00 [14204/18701 ( 76%)], Train Loss: 0.44821\n","Epoch: 00 [14244/18701 ( 76%)], Train Loss: 0.44766\n","Epoch: 00 [14284/18701 ( 76%)], Train Loss: 0.44754\n","Epoch: 00 [14324/18701 ( 77%)], Train Loss: 0.44724\n","Epoch: 00 [14364/18701 ( 77%)], Train Loss: 0.44704\n","Epoch: 00 [14404/18701 ( 77%)], Train Loss: 0.44625\n","Epoch: 00 [14444/18701 ( 77%)], Train Loss: 0.44580\n","Epoch: 00 [14484/18701 ( 77%)], Train Loss: 0.44633\n","Epoch: 00 [14524/18701 ( 78%)], Train Loss: 0.44610\n","Epoch: 00 [14564/18701 ( 78%)], Train Loss: 0.44592\n","Epoch: 00 [14604/18701 ( 78%)], Train Loss: 0.44533\n","Epoch: 00 [14644/18701 ( 78%)], Train Loss: 0.44481\n","Epoch: 00 [14684/18701 ( 79%)], Train Loss: 0.44450\n","Epoch: 00 [14724/18701 ( 79%)], Train Loss: 0.44434\n","Epoch: 00 [14764/18701 ( 79%)], Train Loss: 0.44403\n","Epoch: 00 [14804/18701 ( 79%)], Train Loss: 0.44366\n","Epoch: 00 [14844/18701 ( 79%)], Train Loss: 0.44347\n","Epoch: 00 [14884/18701 ( 80%)], Train Loss: 0.44330\n","Epoch: 00 [14924/18701 ( 80%)], Train Loss: 0.44372\n","Epoch: 00 [14964/18701 ( 80%)], Train Loss: 0.44359\n","Epoch: 00 [15004/18701 ( 80%)], Train Loss: 0.44319\n","Epoch: 00 [15044/18701 ( 80%)], Train Loss: 0.44241\n","Epoch: 00 [15084/18701 ( 81%)], Train Loss: 0.44197\n","Epoch: 00 [15124/18701 ( 81%)], Train Loss: 0.44176\n","Epoch: 00 [15164/18701 ( 81%)], Train Loss: 0.44109\n","Epoch: 00 [15204/18701 ( 81%)], Train Loss: 0.44083\n","Epoch: 00 [15244/18701 ( 82%)], Train Loss: 0.44044\n","Epoch: 00 [15284/18701 ( 82%)], Train Loss: 0.44011\n","Epoch: 00 [15324/18701 ( 82%)], Train Loss: 0.44008\n","Epoch: 00 [15364/18701 ( 82%)], Train Loss: 0.44052\n","Epoch: 00 [15404/18701 ( 82%)], Train Loss: 0.44005\n","Epoch: 00 [15444/18701 ( 83%)], Train Loss: 0.44016\n","Epoch: 00 [15484/18701 ( 83%)], Train Loss: 0.44036\n","Epoch: 00 [15524/18701 ( 83%)], Train Loss: 0.43984\n","Epoch: 00 [15564/18701 ( 83%)], Train Loss: 0.43937\n","Epoch: 00 [15604/18701 ( 83%)], Train Loss: 0.43883\n","Epoch: 00 [15644/18701 ( 84%)], Train Loss: 0.43861\n","Epoch: 00 [15684/18701 ( 84%)], Train Loss: 0.43836\n","Epoch: 00 [15724/18701 ( 84%)], Train Loss: 0.43824\n","Epoch: 00 [15764/18701 ( 84%)], Train Loss: 0.43810\n","Epoch: 00 [15804/18701 ( 85%)], Train Loss: 0.43816\n","Epoch: 00 [15844/18701 ( 85%)], Train Loss: 0.43756\n","Epoch: 00 [15884/18701 ( 85%)], Train Loss: 0.43726\n","Epoch: 00 [15924/18701 ( 85%)], Train Loss: 0.43696\n","Epoch: 00 [15964/18701 ( 85%)], Train Loss: 0.43654\n","Epoch: 00 [16004/18701 ( 86%)], Train Loss: 0.43644\n","Epoch: 00 [16044/18701 ( 86%)], Train Loss: 0.43657\n","Epoch: 00 [16084/18701 ( 86%)], Train Loss: 0.43659\n","Epoch: 00 [16124/18701 ( 86%)], Train Loss: 0.43648\n","Epoch: 00 [16164/18701 ( 86%)], Train Loss: 0.43646\n","Epoch: 00 [16204/18701 ( 87%)], Train Loss: 0.43642\n","Epoch: 00 [16244/18701 ( 87%)], Train Loss: 0.43606\n","Epoch: 00 [16284/18701 ( 87%)], Train Loss: 0.43576\n","Epoch: 00 [16324/18701 ( 87%)], Train Loss: 0.43537\n","Epoch: 00 [16364/18701 ( 88%)], Train Loss: 0.43562\n","Epoch: 00 [16404/18701 ( 88%)], Train Loss: 0.43551\n","Epoch: 00 [16444/18701 ( 88%)], Train Loss: 0.43528\n","Epoch: 00 [16484/18701 ( 88%)], Train Loss: 0.43509\n","Epoch: 00 [16524/18701 ( 88%)], Train Loss: 0.43484\n","Epoch: 00 [16564/18701 ( 89%)], Train Loss: 0.43483\n","Epoch: 00 [16604/18701 ( 89%)], Train Loss: 0.43443\n","Epoch: 00 [16644/18701 ( 89%)], Train Loss: 0.43476\n","Epoch: 00 [16684/18701 ( 89%)], Train Loss: 0.43480\n","Epoch: 00 [16724/18701 ( 89%)], Train Loss: 0.43474\n","Epoch: 00 [16764/18701 ( 90%)], Train Loss: 0.43486\n","Epoch: 00 [16804/18701 ( 90%)], Train Loss: 0.43437\n","Epoch: 00 [16844/18701 ( 90%)], Train Loss: 0.43413\n","Epoch: 00 [16884/18701 ( 90%)], Train Loss: 0.43387\n","Epoch: 00 [16924/18701 ( 90%)], Train Loss: 0.43368\n","Epoch: 00 [16964/18701 ( 91%)], Train Loss: 0.43351\n","Epoch: 00 [17004/18701 ( 91%)], Train Loss: 0.43287\n","Epoch: 00 [17044/18701 ( 91%)], Train Loss: 0.43344\n","Epoch: 00 [17084/18701 ( 91%)], Train Loss: 0.43300\n","Epoch: 00 [17124/18701 ( 92%)], Train Loss: 0.43272\n","Epoch: 00 [17164/18701 ( 92%)], Train Loss: 0.43246\n","Epoch: 00 [17204/18701 ( 92%)], Train Loss: 0.43202\n","Epoch: 00 [17244/18701 ( 92%)], Train Loss: 0.43201\n","Epoch: 00 [17284/18701 ( 92%)], Train Loss: 0.43178\n","Epoch: 00 [17324/18701 ( 93%)], Train Loss: 0.43122\n","Epoch: 00 [17364/18701 ( 93%)], Train Loss: 0.43081\n","Epoch: 00 [17404/18701 ( 93%)], Train Loss: 0.43081\n","Epoch: 00 [17444/18701 ( 93%)], Train Loss: 0.43062\n","Epoch: 00 [17484/18701 ( 93%)], Train Loss: 0.43036\n","Epoch: 00 [17524/18701 ( 94%)], Train Loss: 0.42977\n","Epoch: 00 [17564/18701 ( 94%)], Train Loss: 0.42966\n","Epoch: 00 [17604/18701 ( 94%)], Train Loss: 0.42968\n","Epoch: 00 [17644/18701 ( 94%)], Train Loss: 0.42916\n","Epoch: 00 [17684/18701 ( 95%)], Train Loss: 0.42861\n","Epoch: 00 [17724/18701 ( 95%)], Train Loss: 0.42825\n","Epoch: 00 [17764/18701 ( 95%)], Train Loss: 0.42803\n","Epoch: 00 [17804/18701 ( 95%)], Train Loss: 0.42792\n","Epoch: 00 [17844/18701 ( 95%)], Train Loss: 0.42766\n","Epoch: 00 [17884/18701 ( 96%)], Train Loss: 0.42727\n","Epoch: 00 [17924/18701 ( 96%)], Train Loss: 0.42669\n","Epoch: 00 [17964/18701 ( 96%)], Train Loss: 0.42599\n","Epoch: 00 [18004/18701 ( 96%)], Train Loss: 0.42561\n","Epoch: 00 [18044/18701 ( 96%)], Train Loss: 0.42510\n","Epoch: 00 [18084/18701 ( 97%)], Train Loss: 0.42459\n","Epoch: 00 [18124/18701 ( 97%)], Train Loss: 0.42419\n","Epoch: 00 [18164/18701 ( 97%)], Train Loss: 0.42408\n","Epoch: 00 [18204/18701 ( 97%)], Train Loss: 0.42364\n","Epoch: 00 [18244/18701 ( 98%)], Train Loss: 0.42394\n","Epoch: 00 [18284/18701 ( 98%)], Train Loss: 0.42365\n","Epoch: 00 [18324/18701 ( 98%)], Train Loss: 0.42377\n","Epoch: 00 [18364/18701 ( 98%)], Train Loss: 0.42365\n","Epoch: 00 [18404/18701 ( 98%)], Train Loss: 0.42361\n","Epoch: 00 [18444/18701 ( 99%)], Train Loss: 0.42351\n","Epoch: 00 [18484/18701 ( 99%)], Train Loss: 0.42343\n","Epoch: 00 [18524/18701 ( 99%)], Train Loss: 0.42344\n","Epoch: 00 [18564/18701 ( 99%)], Train Loss: 0.42298\n","Epoch: 00 [18604/18701 ( 99%)], Train Loss: 0.42279\n","Epoch: 00 [18644/18701 (100%)], Train Loss: 0.42237\n","Epoch: 00 [18684/18701 (100%)], Train Loss: 0.42237\n","Epoch: 00 [18701/18701 (100%)], Train Loss: 0.42232\n","----Validation Results Summary----\n","Epoch: [0] Valid Loss: 0.62968\n","0 Epoch, Best epoch was updated! Valid Loss: 0.62968\n","Saving model checkpoint to chaii-qa-5-fold-xlmroberta-torch-fit/checkpoint-fold-4.\n","\n","Epoch: 01 [    4/18701 (  0%)], Train Loss: 0.46519\n","Epoch: 01 [   44/18701 (  0%)], Train Loss: 0.30222\n","Epoch: 01 [   84/18701 (  0%)], Train Loss: 0.29111\n","Epoch: 01 [  124/18701 (  1%)], Train Loss: 0.27550\n","Epoch: 01 [  164/18701 (  1%)], Train Loss: 0.32000\n","Epoch: 01 [  204/18701 (  1%)], Train Loss: 0.31568\n","Epoch: 01 [  244/18701 (  1%)], Train Loss: 0.30434\n","Epoch: 01 [  284/18701 (  2%)], Train Loss: 0.28363\n","Epoch: 01 [  324/18701 (  2%)], Train Loss: 0.26326\n","Epoch: 01 [  364/18701 (  2%)], Train Loss: 0.27652\n","Epoch: 01 [  404/18701 (  2%)], Train Loss: 0.28305\n","Epoch: 01 [  444/18701 (  2%)], Train Loss: 0.28865\n","Epoch: 01 [  484/18701 (  3%)], Train Loss: 0.27890\n","Epoch: 01 [  524/18701 (  3%)], Train Loss: 0.28040\n","Epoch: 01 [  564/18701 (  3%)], Train Loss: 0.27579\n","Epoch: 01 [  604/18701 (  3%)], Train Loss: 0.28582\n","Epoch: 01 [  644/18701 (  3%)], Train Loss: 0.27977\n","Epoch: 01 [  684/18701 (  4%)], Train Loss: 0.28133\n","Epoch: 01 [  724/18701 (  4%)], Train Loss: 0.29061\n","Epoch: 01 [  764/18701 (  4%)], Train Loss: 0.29126\n","Epoch: 01 [  804/18701 (  4%)], Train Loss: 0.29160\n","Epoch: 01 [  844/18701 (  5%)], Train Loss: 0.29424\n","Epoch: 01 [  884/18701 (  5%)], Train Loss: 0.29539\n","Epoch: 01 [  924/18701 (  5%)], Train Loss: 0.29640\n","Epoch: 01 [  964/18701 (  5%)], Train Loss: 0.30318\n","Epoch: 01 [ 1004/18701 (  5%)], Train Loss: 0.29571\n","Epoch: 01 [ 1044/18701 (  6%)], Train Loss: 0.29626\n","Epoch: 01 [ 1084/18701 (  6%)], Train Loss: 0.29504\n","Epoch: 01 [ 1124/18701 (  6%)], Train Loss: 0.29453\n","Epoch: 01 [ 1164/18701 (  6%)], Train Loss: 0.29209\n","Epoch: 01 [ 1204/18701 (  6%)], Train Loss: 0.29161\n","Epoch: 01 [ 1244/18701 (  7%)], Train Loss: 0.29086\n","Epoch: 01 [ 1284/18701 (  7%)], Train Loss: 0.29036\n","Epoch: 01 [ 1324/18701 (  7%)], Train Loss: 0.28713\n","Epoch: 01 [ 1364/18701 (  7%)], Train Loss: 0.28577\n","Epoch: 01 [ 1404/18701 (  8%)], Train Loss: 0.28236\n","Epoch: 01 [ 1444/18701 (  8%)], Train Loss: 0.28327\n","Epoch: 01 [ 1484/18701 (  8%)], Train Loss: 0.28178\n","Epoch: 01 [ 1524/18701 (  8%)], Train Loss: 0.27947\n","Epoch: 01 [ 1564/18701 (  8%)], Train Loss: 0.27920\n","Epoch: 01 [ 1604/18701 (  9%)], Train Loss: 0.27774\n","Epoch: 01 [ 1644/18701 (  9%)], Train Loss: 0.27695\n","Epoch: 01 [ 1684/18701 (  9%)], Train Loss: 0.27632\n","Epoch: 01 [ 1724/18701 (  9%)], Train Loss: 0.27397\n","Epoch: 01 [ 1764/18701 (  9%)], Train Loss: 0.27777\n","Epoch: 01 [ 1804/18701 ( 10%)], Train Loss: 0.27620\n","Epoch: 01 [ 1844/18701 ( 10%)], Train Loss: 0.27458\n","Epoch: 01 [ 1884/18701 ( 10%)], Train Loss: 0.27563\n","Epoch: 01 [ 1924/18701 ( 10%)], Train Loss: 0.27512\n","Epoch: 01 [ 1964/18701 ( 11%)], Train Loss: 0.27180\n","Epoch: 01 [ 2004/18701 ( 11%)], Train Loss: 0.26979\n","Epoch: 01 [ 2044/18701 ( 11%)], Train Loss: 0.26780\n","Epoch: 01 [ 2084/18701 ( 11%)], Train Loss: 0.26529\n","Epoch: 01 [ 2124/18701 ( 11%)], Train Loss: 0.26278\n","Epoch: 01 [ 2164/18701 ( 12%)], Train Loss: 0.26207\n","Epoch: 01 [ 2204/18701 ( 12%)], Train Loss: 0.25967\n","Epoch: 01 [ 2244/18701 ( 12%)], Train Loss: 0.25912\n","Epoch: 01 [ 2284/18701 ( 12%)], Train Loss: 0.25561\n","Epoch: 01 [ 2324/18701 ( 12%)], Train Loss: 0.25347\n","Epoch: 01 [ 2364/18701 ( 13%)], Train Loss: 0.25270\n","Epoch: 01 [ 2404/18701 ( 13%)], Train Loss: 0.25150\n","Epoch: 01 [ 2444/18701 ( 13%)], Train Loss: 0.24821\n","Epoch: 01 [ 2484/18701 ( 13%)], Train Loss: 0.24709\n","Epoch: 01 [ 2524/18701 ( 13%)], Train Loss: 0.24679\n","Epoch: 01 [ 2564/18701 ( 14%)], Train Loss: 0.24489\n","Epoch: 01 [ 2604/18701 ( 14%)], Train Loss: 0.24352\n","Epoch: 01 [ 2644/18701 ( 14%)], Train Loss: 0.24041\n","Epoch: 01 [ 2684/18701 ( 14%)], Train Loss: 0.23976\n","Epoch: 01 [ 2724/18701 ( 15%)], Train Loss: 0.23709\n","Epoch: 01 [ 2764/18701 ( 15%)], Train Loss: 0.23596\n","Epoch: 01 [ 2804/18701 ( 15%)], Train Loss: 0.23410\n","Epoch: 01 [ 2844/18701 ( 15%)], Train Loss: 0.23664\n","Epoch: 01 [ 2884/18701 ( 15%)], Train Loss: 0.23554\n","Epoch: 01 [ 2924/18701 ( 16%)], Train Loss: 0.23415\n","Epoch: 01 [ 2964/18701 ( 16%)], Train Loss: 0.23336\n","Epoch: 01 [ 3004/18701 ( 16%)], Train Loss: 0.23119\n","Epoch: 01 [ 3044/18701 ( 16%)], Train Loss: 0.23096\n","Epoch: 01 [ 3084/18701 ( 16%)], Train Loss: 0.22951\n","Epoch: 01 [ 3124/18701 ( 17%)], Train Loss: 0.22885\n","Epoch: 01 [ 3164/18701 ( 17%)], Train Loss: 0.22781\n","Epoch: 01 [ 3204/18701 ( 17%)], Train Loss: 0.22539\n","Epoch: 01 [ 3244/18701 ( 17%)], Train Loss: 0.22305\n","Epoch: 01 [ 3284/18701 ( 18%)], Train Loss: 0.22154\n","Epoch: 01 [ 3324/18701 ( 18%)], Train Loss: 0.22096\n","Epoch: 01 [ 3364/18701 ( 18%)], Train Loss: 0.21965\n","Epoch: 01 [ 3404/18701 ( 18%)], Train Loss: 0.21748\n","Epoch: 01 [ 3444/18701 ( 18%)], Train Loss: 0.21793\n","Epoch: 01 [ 3484/18701 ( 19%)], Train Loss: 0.21673\n","Epoch: 01 [ 3524/18701 ( 19%)], Train Loss: 0.21553\n","Epoch: 01 [ 3564/18701 ( 19%)], Train Loss: 0.21532\n","Epoch: 01 [ 3604/18701 ( 19%)], Train Loss: 0.21409\n","Epoch: 01 [ 3644/18701 ( 19%)], Train Loss: 0.21207\n","Epoch: 01 [ 3684/18701 ( 20%)], Train Loss: 0.21324\n","Epoch: 01 [ 3724/18701 ( 20%)], Train Loss: 0.21217\n","Epoch: 01 [ 3764/18701 ( 20%)], Train Loss: 0.21137\n","Epoch: 01 [ 3804/18701 ( 20%)], Train Loss: 0.20990\n","Epoch: 01 [ 3844/18701 ( 21%)], Train Loss: 0.20873\n","Epoch: 01 [ 3884/18701 ( 21%)], Train Loss: 0.20801\n","Epoch: 01 [ 3924/18701 ( 21%)], Train Loss: 0.20686\n","Epoch: 01 [ 3964/18701 ( 21%)], Train Loss: 0.20637\n","Epoch: 01 [ 4004/18701 ( 21%)], Train Loss: 0.20532\n","Epoch: 01 [ 4044/18701 ( 22%)], Train Loss: 0.20379\n","Epoch: 01 [ 4084/18701 ( 22%)], Train Loss: 0.20325\n","Epoch: 01 [ 4124/18701 ( 22%)], Train Loss: 0.20162\n","Epoch: 01 [ 4164/18701 ( 22%)], Train Loss: 0.20036\n","Epoch: 01 [ 4204/18701 ( 22%)], Train Loss: 0.19873\n","Epoch: 01 [ 4244/18701 ( 23%)], Train Loss: 0.19808\n","Epoch: 01 [ 4284/18701 ( 23%)], Train Loss: 0.19857\n","Epoch: 01 [ 4324/18701 ( 23%)], Train Loss: 0.19741\n","Epoch: 01 [ 4364/18701 ( 23%)], Train Loss: 0.19637\n","Epoch: 01 [ 4404/18701 ( 24%)], Train Loss: 0.19563\n","Epoch: 01 [ 4444/18701 ( 24%)], Train Loss: 0.19444\n","Epoch: 01 [ 4484/18701 ( 24%)], Train Loss: 0.19400\n","Epoch: 01 [ 4524/18701 ( 24%)], Train Loss: 0.19421\n","Epoch: 01 [ 4564/18701 ( 24%)], Train Loss: 0.19342\n","Epoch: 01 [ 4604/18701 ( 25%)], Train Loss: 0.19239\n","Epoch: 01 [ 4644/18701 ( 25%)], Train Loss: 0.19152\n","Epoch: 01 [ 4684/18701 ( 25%)], Train Loss: 0.19074\n","Epoch: 01 [ 4724/18701 ( 25%)], Train Loss: 0.19006\n","Epoch: 01 [ 4764/18701 ( 25%)], Train Loss: 0.18960\n","Epoch: 01 [ 4804/18701 ( 26%)], Train Loss: 0.18971\n","Epoch: 01 [ 4844/18701 ( 26%)], Train Loss: 0.18891\n","Epoch: 01 [ 4884/18701 ( 26%)], Train Loss: 0.18823\n","Epoch: 01 [ 4924/18701 ( 26%)], Train Loss: 0.18779\n","Epoch: 01 [ 4964/18701 ( 27%)], Train Loss: 0.18718\n","Epoch: 01 [ 5004/18701 ( 27%)], Train Loss: 0.18685\n","Epoch: 01 [ 5044/18701 ( 27%)], Train Loss: 0.18737\n","Epoch: 01 [ 5084/18701 ( 27%)], Train Loss: 0.18647\n","Epoch: 01 [ 5124/18701 ( 27%)], Train Loss: 0.18591\n","Epoch: 01 [ 5164/18701 ( 28%)], Train Loss: 0.18504\n","Epoch: 01 [ 5204/18701 ( 28%)], Train Loss: 0.18422\n","Epoch: 01 [ 5244/18701 ( 28%)], Train Loss: 0.18516\n","Epoch: 01 [ 5284/18701 ( 28%)], Train Loss: 0.18428\n","Epoch: 01 [ 5324/18701 ( 28%)], Train Loss: 0.18458\n","Epoch: 01 [ 5364/18701 ( 29%)], Train Loss: 0.18438\n","Epoch: 01 [ 5404/18701 ( 29%)], Train Loss: 0.18359\n","Epoch: 01 [ 5444/18701 ( 29%)], Train Loss: 0.18282\n","Epoch: 01 [ 5484/18701 ( 29%)], Train Loss: 0.18194\n","Epoch: 01 [ 5524/18701 ( 30%)], Train Loss: 0.18106\n","Epoch: 01 [ 5564/18701 ( 30%)], Train Loss: 0.18098\n","Epoch: 01 [ 5604/18701 ( 30%)], Train Loss: 0.18076\n","Epoch: 01 [ 5644/18701 ( 30%)], Train Loss: 0.18051\n","Epoch: 01 [ 5684/18701 ( 30%)], Train Loss: 0.17979\n","Epoch: 01 [ 5724/18701 ( 31%)], Train Loss: 0.17935\n","Epoch: 01 [ 5764/18701 ( 31%)], Train Loss: 0.17864\n","Epoch: 01 [ 5804/18701 ( 31%)], Train Loss: 0.17871\n","Epoch: 01 [ 5844/18701 ( 31%)], Train Loss: 0.17792\n","Epoch: 01 [ 5884/18701 ( 31%)], Train Loss: 0.17709\n","Epoch: 01 [ 5924/18701 ( 32%)], Train Loss: 0.17634\n","Epoch: 01 [ 5964/18701 ( 32%)], Train Loss: 0.17530\n","Epoch: 01 [ 6004/18701 ( 32%)], Train Loss: 0.17538\n","Epoch: 01 [ 6044/18701 ( 32%)], Train Loss: 0.17527\n","Epoch: 01 [ 6084/18701 ( 33%)], Train Loss: 0.17481\n","Epoch: 01 [ 6124/18701 ( 33%)], Train Loss: 0.17410\n","Epoch: 01 [ 6164/18701 ( 33%)], Train Loss: 0.17382\n","Epoch: 01 [ 6204/18701 ( 33%)], Train Loss: 0.17325\n","Epoch: 01 [ 6244/18701 ( 33%)], Train Loss: 0.17283\n","Epoch: 01 [ 6284/18701 ( 34%)], Train Loss: 0.17245\n","Epoch: 01 [ 6324/18701 ( 34%)], Train Loss: 0.17248\n","Epoch: 01 [ 6364/18701 ( 34%)], Train Loss: 0.17148\n","Epoch: 01 [ 6404/18701 ( 34%)], Train Loss: 0.17133\n","Epoch: 01 [ 6444/18701 ( 34%)], Train Loss: 0.17117\n","Epoch: 01 [ 6484/18701 ( 35%)], Train Loss: 0.17083\n","Epoch: 01 [ 6524/18701 ( 35%)], Train Loss: 0.17085\n","Epoch: 01 [ 6564/18701 ( 35%)], Train Loss: 0.17112\n","Epoch: 01 [ 6604/18701 ( 35%)], Train Loss: 0.17060\n","Epoch: 01 [ 6644/18701 ( 36%)], Train Loss: 0.17081\n","Epoch: 01 [ 6684/18701 ( 36%)], Train Loss: 0.17028\n","Epoch: 01 [ 6724/18701 ( 36%)], Train Loss: 0.16952\n","Epoch: 01 [ 6764/18701 ( 36%)], Train Loss: 0.16912\n","Epoch: 01 [ 6804/18701 ( 36%)], Train Loss: 0.16874\n","Epoch: 01 [ 6844/18701 ( 37%)], Train Loss: 0.16818\n","Epoch: 01 [ 6884/18701 ( 37%)], Train Loss: 0.16785\n","Epoch: 01 [ 6924/18701 ( 37%)], Train Loss: 0.16782\n","Epoch: 01 [ 6964/18701 ( 37%)], Train Loss: 0.16786\n","Epoch: 01 [ 7004/18701 ( 37%)], Train Loss: 0.16804\n","Epoch: 01 [ 7044/18701 ( 38%)], Train Loss: 0.16756\n","Epoch: 01 [ 7084/18701 ( 38%)], Train Loss: 0.16717\n","Epoch: 01 [ 7124/18701 ( 38%)], Train Loss: 0.16660\n","Epoch: 01 [ 7164/18701 ( 38%)], Train Loss: 0.16651\n","Epoch: 01 [ 7204/18701 ( 39%)], Train Loss: 0.16629\n","Epoch: 01 [ 7244/18701 ( 39%)], Train Loss: 0.16584\n","Epoch: 01 [ 7284/18701 ( 39%)], Train Loss: 0.16528\n","Epoch: 01 [ 7324/18701 ( 39%)], Train Loss: 0.16500\n","Epoch: 01 [ 7364/18701 ( 39%)], Train Loss: 0.16461\n","Epoch: 01 [ 7404/18701 ( 40%)], Train Loss: 0.16407\n","Epoch: 01 [ 7444/18701 ( 40%)], Train Loss: 0.16343\n","Epoch: 01 [ 7484/18701 ( 40%)], Train Loss: 0.16306\n","Epoch: 01 [ 7524/18701 ( 40%)], Train Loss: 0.16303\n","Epoch: 01 [ 7564/18701 ( 40%)], Train Loss: 0.16255\n","Epoch: 01 [ 7604/18701 ( 41%)], Train Loss: 0.16268\n","Epoch: 01 [ 7644/18701 ( 41%)], Train Loss: 0.16228\n","Epoch: 01 [ 7684/18701 ( 41%)], Train Loss: 0.16161\n","Epoch: 01 [ 7724/18701 ( 41%)], Train Loss: 0.16131\n","Epoch: 01 [ 7764/18701 ( 42%)], Train Loss: 0.16164\n","Epoch: 01 [ 7804/18701 ( 42%)], Train Loss: 0.16103\n","Epoch: 01 [ 7844/18701 ( 42%)], Train Loss: 0.16062\n","Epoch: 01 [ 7884/18701 ( 42%)], Train Loss: 0.16004\n","Epoch: 01 [ 7924/18701 ( 42%)], Train Loss: 0.16032\n","Epoch: 01 [ 7964/18701 ( 43%)], Train Loss: 0.16020\n","Epoch: 01 [ 8004/18701 ( 43%)], Train Loss: 0.15965\n","Epoch: 01 [ 8044/18701 ( 43%)], Train Loss: 0.15947\n","Epoch: 01 [ 8084/18701 ( 43%)], Train Loss: 0.15920\n","Epoch: 01 [ 8124/18701 ( 43%)], Train Loss: 0.15950\n","Epoch: 01 [ 8164/18701 ( 44%)], Train Loss: 0.15901\n","Epoch: 01 [ 8204/18701 ( 44%)], Train Loss: 0.15863\n","Epoch: 01 [ 8244/18701 ( 44%)], Train Loss: 0.15855\n","Epoch: 01 [ 8284/18701 ( 44%)], Train Loss: 0.15911\n","Epoch: 01 [ 8324/18701 ( 45%)], Train Loss: 0.15882\n","Epoch: 01 [ 8364/18701 ( 45%)], Train Loss: 0.15842\n","Epoch: 01 [ 8404/18701 ( 45%)], Train Loss: 0.15796\n","Epoch: 01 [ 8444/18701 ( 45%)], Train Loss: 0.15738\n","Epoch: 01 [ 8484/18701 ( 45%)], Train Loss: 0.15668\n","Epoch: 01 [ 8524/18701 ( 46%)], Train Loss: 0.15622\n","Epoch: 01 [ 8564/18701 ( 46%)], Train Loss: 0.15620\n","Epoch: 01 [ 8604/18701 ( 46%)], Train Loss: 0.15603\n","Epoch: 01 [ 8644/18701 ( 46%)], Train Loss: 0.15575\n","Epoch: 01 [ 8684/18701 ( 46%)], Train Loss: 0.15581\n","Epoch: 01 [ 8724/18701 ( 47%)], Train Loss: 0.15551\n","Epoch: 01 [ 8764/18701 ( 47%)], Train Loss: 0.15544\n","Epoch: 01 [ 8804/18701 ( 47%)], Train Loss: 0.15519\n","Epoch: 01 [ 8844/18701 ( 47%)], Train Loss: 0.15474\n","Epoch: 01 [ 8884/18701 ( 48%)], Train Loss: 0.15430\n","Epoch: 01 [ 8924/18701 ( 48%)], Train Loss: 0.15400\n","Epoch: 01 [ 8964/18701 ( 48%)], Train Loss: 0.15398\n","Epoch: 01 [ 9004/18701 ( 48%)], Train Loss: 0.15355\n","Epoch: 01 [ 9044/18701 ( 48%)], Train Loss: 0.15341\n","Epoch: 01 [ 9084/18701 ( 49%)], Train Loss: 0.15314\n","Epoch: 01 [ 9124/18701 ( 49%)], Train Loss: 0.15291\n","Epoch: 01 [ 9164/18701 ( 49%)], Train Loss: 0.15245\n","Epoch: 01 [ 9204/18701 ( 49%)], Train Loss: 0.15227\n","Epoch: 01 [ 9244/18701 ( 49%)], Train Loss: 0.15232\n","Epoch: 01 [ 9284/18701 ( 50%)], Train Loss: 0.15226\n","Epoch: 01 [ 9324/18701 ( 50%)], Train Loss: 0.15223\n","Epoch: 01 [ 9364/18701 ( 50%)], Train Loss: 0.15202\n","Epoch: 01 [ 9404/18701 ( 50%)], Train Loss: 0.15175\n","Epoch: 01 [ 9444/18701 ( 50%)], Train Loss: 0.15155\n","Epoch: 01 [ 9484/18701 ( 51%)], Train Loss: 0.15151\n","Epoch: 01 [ 9524/18701 ( 51%)], Train Loss: 0.15176\n","Epoch: 01 [ 9564/18701 ( 51%)], Train Loss: 0.15145\n","Epoch: 01 [ 9604/18701 ( 51%)], Train Loss: 0.15104\n","Epoch: 01 [ 9644/18701 ( 52%)], Train Loss: 0.15071\n","Epoch: 01 [ 9684/18701 ( 52%)], Train Loss: 0.15043\n","Epoch: 01 [ 9724/18701 ( 52%)], Train Loss: 0.15052\n","Epoch: 01 [ 9764/18701 ( 52%)], Train Loss: 0.15034\n","Epoch: 01 [ 9804/18701 ( 52%)], Train Loss: 0.15011\n","Epoch: 01 [ 9844/18701 ( 53%)], Train Loss: 0.14986\n","Epoch: 01 [ 9884/18701 ( 53%)], Train Loss: 0.14994\n","Epoch: 01 [ 9924/18701 ( 53%)], Train Loss: 0.15011\n","Epoch: 01 [ 9964/18701 ( 53%)], Train Loss: 0.15009\n","Epoch: 01 [10004/18701 ( 53%)], Train Loss: 0.15001\n","Epoch: 01 [10044/18701 ( 54%)], Train Loss: 0.15012\n","Epoch: 01 [10084/18701 ( 54%)], Train Loss: 0.14991\n","Epoch: 01 [10124/18701 ( 54%)], Train Loss: 0.14977\n","Epoch: 01 [10164/18701 ( 54%)], Train Loss: 0.14953\n","Epoch: 01 [10204/18701 ( 55%)], Train Loss: 0.14931\n","Epoch: 01 [10244/18701 ( 55%)], Train Loss: 0.14937\n","Epoch: 01 [10284/18701 ( 55%)], Train Loss: 0.14919\n","Epoch: 01 [10324/18701 ( 55%)], Train Loss: 0.14919\n","Epoch: 01 [10364/18701 ( 55%)], Train Loss: 0.14890\n","Epoch: 01 [10404/18701 ( 56%)], Train Loss: 0.14885\n","Epoch: 01 [10444/18701 ( 56%)], Train Loss: 0.14857\n","Epoch: 01 [10484/18701 ( 56%)], Train Loss: 0.14837\n","Epoch: 01 [10524/18701 ( 56%)], Train Loss: 0.14815\n","Epoch: 01 [10564/18701 ( 56%)], Train Loss: 0.14779\n","Epoch: 01 [10604/18701 ( 57%)], Train Loss: 0.14764\n","Epoch: 01 [10644/18701 ( 57%)], Train Loss: 0.14739\n","Epoch: 01 [10684/18701 ( 57%)], Train Loss: 0.14730\n","Epoch: 01 [10724/18701 ( 57%)], Train Loss: 0.14723\n","Epoch: 01 [10764/18701 ( 58%)], Train Loss: 0.14701\n","Epoch: 01 [10804/18701 ( 58%)], Train Loss: 0.14712\n","Epoch: 01 [10844/18701 ( 58%)], Train Loss: 0.14679\n","Epoch: 01 [10884/18701 ( 58%)], Train Loss: 0.14661\n","Epoch: 01 [10924/18701 ( 58%)], Train Loss: 0.14631\n","Epoch: 01 [10964/18701 ( 59%)], Train Loss: 0.14603\n","Epoch: 01 [11004/18701 ( 59%)], Train Loss: 0.14601\n","Epoch: 01 [11044/18701 ( 59%)], Train Loss: 0.14584\n","Epoch: 01 [11084/18701 ( 59%)], Train Loss: 0.14579\n","Epoch: 01 [11124/18701 ( 59%)], Train Loss: 0.14556\n","Epoch: 01 [11164/18701 ( 60%)], Train Loss: 0.14536\n","Epoch: 01 [11204/18701 ( 60%)], Train Loss: 0.14498\n","Epoch: 01 [11244/18701 ( 60%)], Train Loss: 0.14466\n","Epoch: 01 [11284/18701 ( 60%)], Train Loss: 0.14426\n","Epoch: 01 [11324/18701 ( 61%)], Train Loss: 0.14452\n","Epoch: 01 [11364/18701 ( 61%)], Train Loss: 0.14488\n","Epoch: 01 [11404/18701 ( 61%)], Train Loss: 0.14476\n","Epoch: 01 [11444/18701 ( 61%)], Train Loss: 0.14481\n","Epoch: 01 [11484/18701 ( 61%)], Train Loss: 0.14482\n","Epoch: 01 [11524/18701 ( 62%)], Train Loss: 0.14456\n","Epoch: 01 [11564/18701 ( 62%)], Train Loss: 0.14443\n","Epoch: 01 [11604/18701 ( 62%)], Train Loss: 0.14414\n","Epoch: 01 [11644/18701 ( 62%)], Train Loss: 0.14381\n","Epoch: 01 [11684/18701 ( 62%)], Train Loss: 0.14355\n","Epoch: 01 [11724/18701 ( 63%)], Train Loss: 0.14322\n","Epoch: 01 [11764/18701 ( 63%)], Train Loss: 0.14296\n","Epoch: 01 [11804/18701 ( 63%)], Train Loss: 0.14275\n","Epoch: 01 [11844/18701 ( 63%)], Train Loss: 0.14253\n","Epoch: 01 [11884/18701 ( 64%)], Train Loss: 0.14230\n","Epoch: 01 [11924/18701 ( 64%)], Train Loss: 0.14232\n","Epoch: 01 [11964/18701 ( 64%)], Train Loss: 0.14202\n","Epoch: 01 [12004/18701 ( 64%)], Train Loss: 0.14191\n","Epoch: 01 [12044/18701 ( 64%)], Train Loss: 0.14162\n","Epoch: 01 [12084/18701 ( 65%)], Train Loss: 0.14140\n","Epoch: 01 [12124/18701 ( 65%)], Train Loss: 0.14124\n","Epoch: 01 [12164/18701 ( 65%)], Train Loss: 0.14110\n","Epoch: 01 [12204/18701 ( 65%)], Train Loss: 0.14085\n","Epoch: 01 [12244/18701 ( 65%)], Train Loss: 0.14059\n","Epoch: 01 [12284/18701 ( 66%)], Train Loss: 0.14041\n","Epoch: 01 [12324/18701 ( 66%)], Train Loss: 0.14027\n","Epoch: 01 [12364/18701 ( 66%)], Train Loss: 0.14019\n","Epoch: 01 [12404/18701 ( 66%)], Train Loss: 0.13997\n","Epoch: 01 [12444/18701 ( 67%)], Train Loss: 0.13973\n","Epoch: 01 [12484/18701 ( 67%)], Train Loss: 0.13954\n","Epoch: 01 [12524/18701 ( 67%)], Train Loss: 0.13931\n","Epoch: 01 [12564/18701 ( 67%)], Train Loss: 0.13912\n","Epoch: 01 [12604/18701 ( 67%)], Train Loss: 0.13913\n","Epoch: 01 [12644/18701 ( 68%)], Train Loss: 0.13900\n","Epoch: 01 [12684/18701 ( 68%)], Train Loss: 0.13898\n","Epoch: 01 [12724/18701 ( 68%)], Train Loss: 0.13872\n","Epoch: 01 [12764/18701 ( 68%)], Train Loss: 0.13862\n","Epoch: 01 [12804/18701 ( 68%)], Train Loss: 0.13834\n","Epoch: 01 [12844/18701 ( 69%)], Train Loss: 0.13825\n","Epoch: 01 [12884/18701 ( 69%)], Train Loss: 0.13810\n","Epoch: 01 [12924/18701 ( 69%)], Train Loss: 0.13784\n","Epoch: 01 [12964/18701 ( 69%)], Train Loss: 0.13786\n","Epoch: 01 [13004/18701 ( 70%)], Train Loss: 0.13789\n","Epoch: 01 [13044/18701 ( 70%)], Train Loss: 0.13770\n","Epoch: 01 [13084/18701 ( 70%)], Train Loss: 0.13766\n","Epoch: 01 [13124/18701 ( 70%)], Train Loss: 0.13755\n","Epoch: 01 [13164/18701 ( 70%)], Train Loss: 0.13762\n","Epoch: 01 [13204/18701 ( 71%)], Train Loss: 0.13752\n","Epoch: 01 [13244/18701 ( 71%)], Train Loss: 0.13727\n","Epoch: 01 [13284/18701 ( 71%)], Train Loss: 0.13700\n","Epoch: 01 [13324/18701 ( 71%)], Train Loss: 0.13667\n","Epoch: 01 [13364/18701 ( 71%)], Train Loss: 0.13634\n","Epoch: 01 [13404/18701 ( 72%)], Train Loss: 0.13606\n","Epoch: 01 [13444/18701 ( 72%)], Train Loss: 0.13587\n","Epoch: 01 [13484/18701 ( 72%)], Train Loss: 0.13588\n","Epoch: 01 [13524/18701 ( 72%)], Train Loss: 0.13554\n","Epoch: 01 [13564/18701 ( 73%)], Train Loss: 0.13533\n","Epoch: 01 [13604/18701 ( 73%)], Train Loss: 0.13554\n","Epoch: 01 [13644/18701 ( 73%)], Train Loss: 0.13536\n","Epoch: 01 [13684/18701 ( 73%)], Train Loss: 0.13520\n","Epoch: 01 [13724/18701 ( 73%)], Train Loss: 0.13509\n","Epoch: 01 [13764/18701 ( 74%)], Train Loss: 0.13486\n","Epoch: 01 [13804/18701 ( 74%)], Train Loss: 0.13469\n","Epoch: 01 [13844/18701 ( 74%)], Train Loss: 0.13459\n","Epoch: 01 [13884/18701 ( 74%)], Train Loss: 0.13441\n","Epoch: 01 [13924/18701 ( 74%)], Train Loss: 0.13413\n","Epoch: 01 [13964/18701 ( 75%)], Train Loss: 0.13399\n","Epoch: 01 [14004/18701 ( 75%)], Train Loss: 0.13378\n","Epoch: 01 [14044/18701 ( 75%)], Train Loss: 0.13362\n","Epoch: 01 [14084/18701 ( 75%)], Train Loss: 0.13336\n","Epoch: 01 [14124/18701 ( 76%)], Train Loss: 0.13331\n","Epoch: 01 [14164/18701 ( 76%)], Train Loss: 0.13303\n","Epoch: 01 [14204/18701 ( 76%)], Train Loss: 0.13310\n","Epoch: 01 [14244/18701 ( 76%)], Train Loss: 0.13285\n","Epoch: 01 [14284/18701 ( 76%)], Train Loss: 0.13271\n","Epoch: 01 [14324/18701 ( 77%)], Train Loss: 0.13260\n","Epoch: 01 [14364/18701 ( 77%)], Train Loss: 0.13254\n","Epoch: 01 [14404/18701 ( 77%)], Train Loss: 0.13224\n","Epoch: 01 [14444/18701 ( 77%)], Train Loss: 0.13204\n","Epoch: 01 [14484/18701 ( 77%)], Train Loss: 0.13215\n","Epoch: 01 [14524/18701 ( 78%)], Train Loss: 0.13215\n","Epoch: 01 [14564/18701 ( 78%)], Train Loss: 0.13206\n","Epoch: 01 [14604/18701 ( 78%)], Train Loss: 0.13188\n","Epoch: 01 [14644/18701 ( 78%)], Train Loss: 0.13187\n","Epoch: 01 [14684/18701 ( 79%)], Train Loss: 0.13194\n","Epoch: 01 [14724/18701 ( 79%)], Train Loss: 0.13199\n","Epoch: 01 [14764/18701 ( 79%)], Train Loss: 0.13216\n","Epoch: 01 [14804/18701 ( 79%)], Train Loss: 0.13206\n","Epoch: 01 [14844/18701 ( 79%)], Train Loss: 0.13209\n","Epoch: 01 [14884/18701 ( 80%)], Train Loss: 0.13214\n","Epoch: 01 [14924/18701 ( 80%)], Train Loss: 0.13213\n","Epoch: 01 [14964/18701 ( 80%)], Train Loss: 0.13223\n","Epoch: 01 [15004/18701 ( 80%)], Train Loss: 0.13211\n","Epoch: 01 [15044/18701 ( 80%)], Train Loss: 0.13190\n","Epoch: 01 [15084/18701 ( 81%)], Train Loss: 0.13176\n","Epoch: 01 [15124/18701 ( 81%)], Train Loss: 0.13159\n","Epoch: 01 [15164/18701 ( 81%)], Train Loss: 0.13139\n","Epoch: 01 [15204/18701 ( 81%)], Train Loss: 0.13127\n","Epoch: 01 [15244/18701 ( 82%)], Train Loss: 0.13116\n","Epoch: 01 [15284/18701 ( 82%)], Train Loss: 0.13088\n","Epoch: 01 [15324/18701 ( 82%)], Train Loss: 0.13088\n","Epoch: 01 [15364/18701 ( 82%)], Train Loss: 0.13094\n","Epoch: 01 [15404/18701 ( 82%)], Train Loss: 0.13085\n","Epoch: 01 [15444/18701 ( 83%)], Train Loss: 0.13116\n","Epoch: 01 [15484/18701 ( 83%)], Train Loss: 0.13131\n","Epoch: 01 [15524/18701 ( 83%)], Train Loss: 0.13135\n","Epoch: 01 [15564/18701 ( 83%)], Train Loss: 0.13123\n","Epoch: 01 [15604/18701 ( 83%)], Train Loss: 0.13102\n","Epoch: 01 [15644/18701 ( 84%)], Train Loss: 0.13102\n","Epoch: 01 [15684/18701 ( 84%)], Train Loss: 0.13090\n","Epoch: 01 [15724/18701 ( 84%)], Train Loss: 0.13106\n","Epoch: 01 [15764/18701 ( 84%)], Train Loss: 0.13126\n","Epoch: 01 [15804/18701 ( 85%)], Train Loss: 0.13138\n","Epoch: 01 [15844/18701 ( 85%)], Train Loss: 0.13117\n","Epoch: 01 [15884/18701 ( 85%)], Train Loss: 0.13122\n","Epoch: 01 [15924/18701 ( 85%)], Train Loss: 0.13115\n","Epoch: 01 [15964/18701 ( 85%)], Train Loss: 0.13110\n","Epoch: 01 [16004/18701 ( 86%)], Train Loss: 0.13111\n","Epoch: 01 [16044/18701 ( 86%)], Train Loss: 0.13106\n","Epoch: 01 [16084/18701 ( 86%)], Train Loss: 0.13121\n","Epoch: 01 [16124/18701 ( 86%)], Train Loss: 0.13130\n","Epoch: 01 [16164/18701 ( 86%)], Train Loss: 0.13141\n","Epoch: 01 [16204/18701 ( 87%)], Train Loss: 0.13165\n","Epoch: 01 [16244/18701 ( 87%)], Train Loss: 0.13151\n","Epoch: 01 [16284/18701 ( 87%)], Train Loss: 0.13139\n","Epoch: 01 [16324/18701 ( 87%)], Train Loss: 0.13132\n","Epoch: 01 [16364/18701 ( 88%)], Train Loss: 0.13175\n","Epoch: 01 [16404/18701 ( 88%)], Train Loss: 0.13157\n","Epoch: 01 [16444/18701 ( 88%)], Train Loss: 0.13139\n","Epoch: 01 [16484/18701 ( 88%)], Train Loss: 0.13133\n","Epoch: 01 [16524/18701 ( 88%)], Train Loss: 0.13126\n","Epoch: 01 [16564/18701 ( 89%)], Train Loss: 0.13124\n","Epoch: 01 [16604/18701 ( 89%)], Train Loss: 0.13125\n","Epoch: 01 [16644/18701 ( 89%)], Train Loss: 0.13137\n","Epoch: 01 [16684/18701 ( 89%)], Train Loss: 0.13147\n","Epoch: 01 [16724/18701 ( 89%)], Train Loss: 0.13185\n","Epoch: 01 [16764/18701 ( 90%)], Train Loss: 0.13194\n","Epoch: 01 [16804/18701 ( 90%)], Train Loss: 0.13178\n","Epoch: 01 [16844/18701 ( 90%)], Train Loss: 0.13175\n","Epoch: 01 [16884/18701 ( 90%)], Train Loss: 0.13162\n","Epoch: 01 [16924/18701 ( 90%)], Train Loss: 0.13155\n","Epoch: 01 [16964/18701 ( 91%)], Train Loss: 0.13134\n","Epoch: 01 [17004/18701 ( 91%)], Train Loss: 0.13112\n","Epoch: 01 [17044/18701 ( 91%)], Train Loss: 0.13122\n","Epoch: 01 [17084/18701 ( 91%)], Train Loss: 0.13099\n","Epoch: 01 [17124/18701 ( 92%)], Train Loss: 0.13089\n","Epoch: 01 [17164/18701 ( 92%)], Train Loss: 0.13091\n","Epoch: 01 [17204/18701 ( 92%)], Train Loss: 0.13073\n","Epoch: 01 [17244/18701 ( 92%)], Train Loss: 0.13062\n","Epoch: 01 [17284/18701 ( 92%)], Train Loss: 0.13055\n","Epoch: 01 [17324/18701 ( 93%)], Train Loss: 0.13030\n","Epoch: 01 [17364/18701 ( 93%)], Train Loss: 0.13017\n","Epoch: 01 [17404/18701 ( 93%)], Train Loss: 0.13009\n","Epoch: 01 [17444/18701 ( 93%)], Train Loss: 0.13009\n","Epoch: 01 [17484/18701 ( 93%)], Train Loss: 0.12993\n","Epoch: 01 [17524/18701 ( 94%)], Train Loss: 0.12968\n","Epoch: 01 [17564/18701 ( 94%)], Train Loss: 0.12975\n","Epoch: 01 [17604/18701 ( 94%)], Train Loss: 0.12978\n","Epoch: 01 [17644/18701 ( 94%)], Train Loss: 0.12959\n","Epoch: 01 [17684/18701 ( 95%)], Train Loss: 0.12936\n","Epoch: 01 [17724/18701 ( 95%)], Train Loss: 0.12919\n","Epoch: 01 [17764/18701 ( 95%)], Train Loss: 0.12916\n","Epoch: 01 [17804/18701 ( 95%)], Train Loss: 0.12909\n","Epoch: 01 [17844/18701 ( 95%)], Train Loss: 0.12901\n","Epoch: 01 [17884/18701 ( 96%)], Train Loss: 0.12900\n","Epoch: 01 [17924/18701 ( 96%)], Train Loss: 0.12875\n","Epoch: 01 [17964/18701 ( 96%)], Train Loss: 0.12853\n","Epoch: 01 [18004/18701 ( 96%)], Train Loss: 0.12841\n","Epoch: 01 [18044/18701 ( 96%)], Train Loss: 0.12826\n","Epoch: 01 [18084/18701 ( 97%)], Train Loss: 0.12813\n","Epoch: 01 [18124/18701 ( 97%)], Train Loss: 0.12796\n","Epoch: 01 [18164/18701 ( 97%)], Train Loss: 0.12789\n","Epoch: 01 [18204/18701 ( 97%)], Train Loss: 0.12774\n","Epoch: 01 [18244/18701 ( 98%)], Train Loss: 0.12795\n","Epoch: 01 [18284/18701 ( 98%)], Train Loss: 0.12786\n","Epoch: 01 [18324/18701 ( 98%)], Train Loss: 0.12803\n","Epoch: 01 [18364/18701 ( 98%)], Train Loss: 0.12801\n","Epoch: 01 [18404/18701 ( 98%)], Train Loss: 0.12804\n","Epoch: 01 [18444/18701 ( 99%)], Train Loss: 0.12806\n","Epoch: 01 [18484/18701 ( 99%)], Train Loss: 0.12813\n","Epoch: 01 [18524/18701 ( 99%)], Train Loss: 0.12844\n","Epoch: 01 [18564/18701 ( 99%)], Train Loss: 0.12829\n","Epoch: 01 [18604/18701 ( 99%)], Train Loss: 0.12822\n","Epoch: 01 [18644/18701 (100%)], Train Loss: 0.12798\n","Epoch: 01 [18684/18701 (100%)], Train Loss: 0.12810\n","Epoch: 01 [18701/18701 (100%)], Train Loss: 0.12807\n","----Validation Results Summary----\n","Epoch: [1] Valid Loss: 0.73707\n","\n","Total Training Time: 5202.850752353668secs, Average Training Time per Epoch: 2601.425376176834secs.\n","Total Validation Time: 367.7928156852722secs, Average Validation Time per Epoch: 183.8964078426361secs.\n","\n","\n","--------------------------------------------------\n","FOLD: 5\n","--------------------------------------------------\n","Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n","Num examples Train= 23069, Num examples Valid=0\n","Total Training Steps: 5768, Total Warmup Steps: 576\n","Epoch: 00 [    4/23069 (  0%)], Train Loss: 2.92595\n","Epoch: 00 [   44/23069 (  0%)], Train Loss: 2.90352\n","Epoch: 00 [   84/23069 (  0%)], Train Loss: 2.88832\n","Epoch: 00 [  124/23069 (  1%)], Train Loss: 2.87314\n","Epoch: 00 [  164/23069 (  1%)], Train Loss: 2.85557\n","Epoch: 00 [  204/23069 (  1%)], Train Loss: 2.83108\n","Epoch: 00 [  244/23069 (  1%)], Train Loss: 2.80374\n","Epoch: 00 [  284/23069 (  1%)], Train Loss: 2.76398\n","Epoch: 00 [  324/23069 (  1%)], Train Loss: 2.71738\n","Epoch: 00 [  364/23069 (  2%)], Train Loss: 2.67853\n","Epoch: 00 [  404/23069 (  2%)], Train Loss: 2.62464\n","Epoch: 00 [  444/23069 (  2%)], Train Loss: 2.56502\n","Epoch: 00 [  484/23069 (  2%)], Train Loss: 2.50247\n","Epoch: 00 [  524/23069 (  2%)], Train Loss: 2.42261\n","Epoch: 00 [  564/23069 (  2%)], Train Loss: 2.33578\n","Epoch: 00 [  604/23069 (  3%)], Train Loss: 2.24390\n","Epoch: 00 [  644/23069 (  3%)], Train Loss: 2.16632\n","Epoch: 00 [  684/23069 (  3%)], Train Loss: 2.07835\n","Epoch: 00 [  724/23069 (  3%)], Train Loss: 2.00598\n","Epoch: 00 [  764/23069 (  3%)], Train Loss: 1.93635\n","Epoch: 00 [  804/23069 (  3%)], Train Loss: 1.86607\n","Epoch: 00 [  844/23069 (  4%)], Train Loss: 1.80132\n","Epoch: 00 [  884/23069 (  4%)], Train Loss: 1.73996\n","Epoch: 00 [  924/23069 (  4%)], Train Loss: 1.69416\n","Epoch: 00 [  964/23069 (  4%)], Train Loss: 1.64435\n","Epoch: 00 [ 1004/23069 (  4%)], Train Loss: 1.60258\n","Epoch: 00 [ 1044/23069 (  5%)], Train Loss: 1.55508\n","Epoch: 00 [ 1084/23069 (  5%)], Train Loss: 1.52063\n","Epoch: 00 [ 1124/23069 (  5%)], Train Loss: 1.48706\n","Epoch: 00 [ 1164/23069 (  5%)], Train Loss: 1.44940\n","Epoch: 00 [ 1204/23069 (  5%)], Train Loss: 1.41583\n","Epoch: 00 [ 1244/23069 (  5%)], Train Loss: 1.38191\n","Epoch: 00 [ 1284/23069 (  6%)], Train Loss: 1.35297\n","Epoch: 00 [ 1324/23069 (  6%)], Train Loss: 1.32417\n","Epoch: 00 [ 1364/23069 (  6%)], Train Loss: 1.29945\n","Epoch: 00 [ 1404/23069 (  6%)], Train Loss: 1.27488\n","Epoch: 00 [ 1444/23069 (  6%)], Train Loss: 1.25641\n","Epoch: 00 [ 1484/23069 (  6%)], Train Loss: 1.23107\n","Epoch: 00 [ 1524/23069 (  7%)], Train Loss: 1.21296\n","Epoch: 00 [ 1564/23069 (  7%)], Train Loss: 1.19438\n","Epoch: 00 [ 1604/23069 (  7%)], Train Loss: 1.17864\n","Epoch: 00 [ 1644/23069 (  7%)], Train Loss: 1.16095\n","Epoch: 00 [ 1684/23069 (  7%)], Train Loss: 1.14270\n","Epoch: 00 [ 1724/23069 (  7%)], Train Loss: 1.12982\n","Epoch: 00 [ 1764/23069 (  8%)], Train Loss: 1.11547\n","Epoch: 00 [ 1804/23069 (  8%)], Train Loss: 1.09910\n","Epoch: 00 [ 1844/23069 (  8%)], Train Loss: 1.08510\n","Epoch: 00 [ 1884/23069 (  8%)], Train Loss: 1.07277\n","Epoch: 00 [ 1924/23069 (  8%)], Train Loss: 1.05882\n","Epoch: 00 [ 1964/23069 (  9%)], Train Loss: 1.04546\n","Epoch: 00 [ 2004/23069 (  9%)], Train Loss: 1.03444\n","Epoch: 00 [ 2044/23069 (  9%)], Train Loss: 1.02180\n","Epoch: 00 [ 2084/23069 (  9%)], Train Loss: 1.01204\n","Epoch: 00 [ 2124/23069 (  9%)], Train Loss: 1.00396\n","Epoch: 00 [ 2164/23069 (  9%)], Train Loss: 0.99703\n","Epoch: 00 [ 2204/23069 ( 10%)], Train Loss: 0.99129\n","Epoch: 00 [ 2244/23069 ( 10%)], Train Loss: 0.98505\n","Epoch: 00 [ 2284/23069 ( 10%)], Train Loss: 0.97847\n","Epoch: 00 [ 2324/23069 ( 10%)], Train Loss: 0.96898\n","Epoch: 00 [ 2364/23069 ( 10%)], Train Loss: 0.95931\n","Epoch: 00 [ 2404/23069 ( 10%)], Train Loss: 0.94938\n","Epoch: 00 [ 2444/23069 ( 11%)], Train Loss: 0.94176\n","Epoch: 00 [ 2484/23069 ( 11%)], Train Loss: 0.93377\n","Epoch: 00 [ 2524/23069 ( 11%)], Train Loss: 0.92494\n","Epoch: 00 [ 2564/23069 ( 11%)], Train Loss: 0.91544\n","Epoch: 00 [ 2604/23069 ( 11%)], Train Loss: 0.90519\n","Epoch: 00 [ 2644/23069 ( 11%)], Train Loss: 0.89866\n","Epoch: 00 [ 2684/23069 ( 12%)], Train Loss: 0.89107\n","Epoch: 00 [ 2724/23069 ( 12%)], Train Loss: 0.88268\n","Epoch: 00 [ 2764/23069 ( 12%)], Train Loss: 0.87617\n","Epoch: 00 [ 2804/23069 ( 12%)], Train Loss: 0.86852\n","Epoch: 00 [ 2844/23069 ( 12%)], Train Loss: 0.86161\n","Epoch: 00 [ 2884/23069 ( 13%)], Train Loss: 0.85417\n","Epoch: 00 [ 2924/23069 ( 13%)], Train Loss: 0.84686\n","Epoch: 00 [ 2964/23069 ( 13%)], Train Loss: 0.83899\n","Epoch: 00 [ 3004/23069 ( 13%)], Train Loss: 0.83483\n","Epoch: 00 [ 3044/23069 ( 13%)], Train Loss: 0.82979\n","Epoch: 00 [ 3084/23069 ( 13%)], Train Loss: 0.82165\n","Epoch: 00 [ 3124/23069 ( 14%)], Train Loss: 0.81374\n","Epoch: 00 [ 3164/23069 ( 14%)], Train Loss: 0.80631\n","Epoch: 00 [ 3204/23069 ( 14%)], Train Loss: 0.79918\n","Epoch: 00 [ 3244/23069 ( 14%)], Train Loss: 0.79545\n","Epoch: 00 [ 3284/23069 ( 14%)], Train Loss: 0.79210\n","Epoch: 00 [ 3324/23069 ( 14%)], Train Loss: 0.78654\n","Epoch: 00 [ 3364/23069 ( 15%)], Train Loss: 0.78180\n","Epoch: 00 [ 3404/23069 ( 15%)], Train Loss: 0.77842\n","Epoch: 00 [ 3444/23069 ( 15%)], Train Loss: 0.77318\n","Epoch: 00 [ 3484/23069 ( 15%)], Train Loss: 0.76869\n","Epoch: 00 [ 3524/23069 ( 15%)], Train Loss: 0.76517\n","Epoch: 00 [ 3564/23069 ( 15%)], Train Loss: 0.76263\n","Epoch: 00 [ 3604/23069 ( 16%)], Train Loss: 0.76095\n","Epoch: 00 [ 3644/23069 ( 16%)], Train Loss: 0.75845\n","Epoch: 00 [ 3684/23069 ( 16%)], Train Loss: 0.75544\n","Epoch: 00 [ 3724/23069 ( 16%)], Train Loss: 0.75157\n","Epoch: 00 [ 3764/23069 ( 16%)], Train Loss: 0.74826\n","Epoch: 00 [ 3804/23069 ( 16%)], Train Loss: 0.74391\n","Epoch: 00 [ 3844/23069 ( 17%)], Train Loss: 0.73899\n","Epoch: 00 [ 3884/23069 ( 17%)], Train Loss: 0.73713\n","Epoch: 00 [ 3924/23069 ( 17%)], Train Loss: 0.73427\n","Epoch: 00 [ 3964/23069 ( 17%)], Train Loss: 0.73038\n","Epoch: 00 [ 4004/23069 ( 17%)], Train Loss: 0.72657\n","Epoch: 00 [ 4044/23069 ( 18%)], Train Loss: 0.72413\n","Epoch: 00 [ 4084/23069 ( 18%)], Train Loss: 0.72125\n","Epoch: 00 [ 4124/23069 ( 18%)], Train Loss: 0.71863\n","Epoch: 00 [ 4164/23069 ( 18%)], Train Loss: 0.71568\n","Epoch: 00 [ 4204/23069 ( 18%)], Train Loss: 0.71195\n","Epoch: 00 [ 4244/23069 ( 18%)], Train Loss: 0.71004\n","Epoch: 00 [ 4284/23069 ( 19%)], Train Loss: 0.70700\n","Epoch: 00 [ 4324/23069 ( 19%)], Train Loss: 0.70375\n","Epoch: 00 [ 4364/23069 ( 19%)], Train Loss: 0.69982\n","Epoch: 00 [ 4404/23069 ( 19%)], Train Loss: 0.69729\n","Epoch: 00 [ 4444/23069 ( 19%)], Train Loss: 0.69457\n","Epoch: 00 [ 4484/23069 ( 19%)], Train Loss: 0.69039\n","Epoch: 00 [ 4524/23069 ( 20%)], Train Loss: 0.68716\n","Epoch: 00 [ 4564/23069 ( 20%)], Train Loss: 0.68317\n","Epoch: 00 [ 4604/23069 ( 20%)], Train Loss: 0.68144\n","Epoch: 00 [ 4644/23069 ( 20%)], Train Loss: 0.67803\n","Epoch: 00 [ 4684/23069 ( 20%)], Train Loss: 0.67535\n","Epoch: 00 [ 4724/23069 ( 20%)], Train Loss: 0.67260\n","Epoch: 00 [ 4764/23069 ( 21%)], Train Loss: 0.66975\n","Epoch: 00 [ 4804/23069 ( 21%)], Train Loss: 0.66691\n","Epoch: 00 [ 4844/23069 ( 21%)], Train Loss: 0.66293\n","Epoch: 00 [ 4884/23069 ( 21%)], Train Loss: 0.65883\n","Epoch: 00 [ 4924/23069 ( 21%)], Train Loss: 0.65645\n","Epoch: 00 [ 4964/23069 ( 22%)], Train Loss: 0.65357\n","Epoch: 00 [ 5004/23069 ( 22%)], Train Loss: 0.65131\n","Epoch: 00 [ 5044/23069 ( 22%)], Train Loss: 0.64993\n","Epoch: 00 [ 5084/23069 ( 22%)], Train Loss: 0.64834\n","Epoch: 00 [ 5124/23069 ( 22%)], Train Loss: 0.64575\n","Epoch: 00 [ 5164/23069 ( 22%)], Train Loss: 0.64490\n","Epoch: 00 [ 5204/23069 ( 23%)], Train Loss: 0.64258\n","Epoch: 00 [ 5244/23069 ( 23%)], Train Loss: 0.64076\n","Epoch: 00 [ 5284/23069 ( 23%)], Train Loss: 0.63826\n","Epoch: 00 [ 5324/23069 ( 23%)], Train Loss: 0.63507\n","Epoch: 00 [ 5364/23069 ( 23%)], Train Loss: 0.63203\n","Epoch: 00 [ 5404/23069 ( 23%)], Train Loss: 0.62982\n","Epoch: 00 [ 5444/23069 ( 24%)], Train Loss: 0.62780\n","Epoch: 00 [ 5484/23069 ( 24%)], Train Loss: 0.62770\n","Epoch: 00 [ 5524/23069 ( 24%)], Train Loss: 0.62618\n","Epoch: 00 [ 5564/23069 ( 24%)], Train Loss: 0.62452\n","Epoch: 00 [ 5604/23069 ( 24%)], Train Loss: 0.62300\n","Epoch: 00 [ 5644/23069 ( 24%)], Train Loss: 0.62024\n","Epoch: 00 [ 5684/23069 ( 25%)], Train Loss: 0.61953\n","Epoch: 00 [ 5724/23069 ( 25%)], Train Loss: 0.61784\n","Epoch: 00 [ 5764/23069 ( 25%)], Train Loss: 0.61672\n","Epoch: 00 [ 5804/23069 ( 25%)], Train Loss: 0.61635\n","Epoch: 00 [ 5844/23069 ( 25%)], Train Loss: 0.61403\n","Epoch: 00 [ 5884/23069 ( 26%)], Train Loss: 0.61280\n","Epoch: 00 [ 5924/23069 ( 26%)], Train Loss: 0.61110\n","Epoch: 00 [ 5964/23069 ( 26%)], Train Loss: 0.60936\n","Epoch: 00 [ 6004/23069 ( 26%)], Train Loss: 0.60742\n","Epoch: 00 [ 6044/23069 ( 26%)], Train Loss: 0.60571\n","Epoch: 00 [ 6084/23069 ( 26%)], Train Loss: 0.60357\n","Epoch: 00 [ 6124/23069 ( 27%)], Train Loss: 0.60112\n","Epoch: 00 [ 6164/23069 ( 27%)], Train Loss: 0.60004\n","Epoch: 00 [ 6204/23069 ( 27%)], Train Loss: 0.59822\n","Epoch: 00 [ 6244/23069 ( 27%)], Train Loss: 0.59677\n","Epoch: 00 [ 6284/23069 ( 27%)], Train Loss: 0.59462\n","Epoch: 00 [ 6324/23069 ( 27%)], Train Loss: 0.59174\n","Epoch: 00 [ 6364/23069 ( 28%)], Train Loss: 0.59030\n","Epoch: 00 [ 6404/23069 ( 28%)], Train Loss: 0.59103\n","Epoch: 00 [ 6444/23069 ( 28%)], Train Loss: 0.58891\n","Epoch: 00 [ 6484/23069 ( 28%)], Train Loss: 0.58827\n","Epoch: 00 [ 6524/23069 ( 28%)], Train Loss: 0.58654\n","Epoch: 00 [ 6564/23069 ( 28%)], Train Loss: 0.58532\n","Epoch: 00 [ 6604/23069 ( 29%)], Train Loss: 0.58397\n","Epoch: 00 [ 6644/23069 ( 29%)], Train Loss: 0.58233\n","Epoch: 00 [ 6684/23069 ( 29%)], Train Loss: 0.58186\n","Epoch: 00 [ 6724/23069 ( 29%)], Train Loss: 0.58076\n","Epoch: 00 [ 6764/23069 ( 29%)], Train Loss: 0.57925\n","Epoch: 00 [ 6804/23069 ( 29%)], Train Loss: 0.57842\n","Epoch: 00 [ 6844/23069 ( 30%)], Train Loss: 0.57687\n","Epoch: 00 [ 6884/23069 ( 30%)], Train Loss: 0.57550\n","Epoch: 00 [ 6924/23069 ( 30%)], Train Loss: 0.57347\n","Epoch: 00 [ 6964/23069 ( 30%)], Train Loss: 0.57161\n","Epoch: 00 [ 7004/23069 ( 30%)], Train Loss: 0.57008\n","Epoch: 00 [ 7044/23069 ( 31%)], Train Loss: 0.56768\n","Epoch: 00 [ 7084/23069 ( 31%)], Train Loss: 0.56619\n","Epoch: 00 [ 7124/23069 ( 31%)], Train Loss: 0.56434\n","Epoch: 00 [ 7164/23069 ( 31%)], Train Loss: 0.56236\n","Epoch: 00 [ 7204/23069 ( 31%)], Train Loss: 0.56141\n","Epoch: 00 [ 7244/23069 ( 31%)], Train Loss: 0.56015\n","Epoch: 00 [ 7284/23069 ( 32%)], Train Loss: 0.55845\n","Epoch: 00 [ 7324/23069 ( 32%)], Train Loss: 0.55769\n","Epoch: 00 [ 7364/23069 ( 32%)], Train Loss: 0.55721\n","Epoch: 00 [ 7404/23069 ( 32%)], Train Loss: 0.55575\n","Epoch: 00 [ 7444/23069 ( 32%)], Train Loss: 0.55548\n","Epoch: 00 [ 7484/23069 ( 32%)], Train Loss: 0.55386\n","Epoch: 00 [ 7524/23069 ( 33%)], Train Loss: 0.55281\n","Epoch: 00 [ 7564/23069 ( 33%)], Train Loss: 0.55115\n","Epoch: 00 [ 7604/23069 ( 33%)], Train Loss: 0.54923\n","Epoch: 00 [ 7644/23069 ( 33%)], Train Loss: 0.54759\n","Epoch: 00 [ 7684/23069 ( 33%)], Train Loss: 0.54762\n","Epoch: 00 [ 7724/23069 ( 33%)], Train Loss: 0.54620\n","Epoch: 00 [ 7764/23069 ( 34%)], Train Loss: 0.54605\n","Epoch: 00 [ 7804/23069 ( 34%)], Train Loss: 0.54589\n","Epoch: 00 [ 7844/23069 ( 34%)], Train Loss: 0.54465\n","Epoch: 00 [ 7884/23069 ( 34%)], Train Loss: 0.54357\n","Epoch: 00 [ 7924/23069 ( 34%)], Train Loss: 0.54398\n","Epoch: 00 [ 7964/23069 ( 35%)], Train Loss: 0.54375\n","Epoch: 00 [ 8004/23069 ( 35%)], Train Loss: 0.54382\n","Epoch: 00 [ 8044/23069 ( 35%)], Train Loss: 0.54261\n","Epoch: 00 [ 8084/23069 ( 35%)], Train Loss: 0.54256\n","Epoch: 00 [ 8124/23069 ( 35%)], Train Loss: 0.54247\n","Epoch: 00 [ 8164/23069 ( 35%)], Train Loss: 0.54240\n","Epoch: 00 [ 8204/23069 ( 36%)], Train Loss: 0.54150\n","Epoch: 00 [ 8244/23069 ( 36%)], Train Loss: 0.53998\n","Epoch: 00 [ 8284/23069 ( 36%)], Train Loss: 0.53892\n","Epoch: 00 [ 8324/23069 ( 36%)], Train Loss: 0.53785\n","Epoch: 00 [ 8364/23069 ( 36%)], Train Loss: 0.53627\n","Epoch: 00 [ 8404/23069 ( 36%)], Train Loss: 0.53586\n","Epoch: 00 [ 8444/23069 ( 37%)], Train Loss: 0.53488\n","Epoch: 00 [ 8484/23069 ( 37%)], Train Loss: 0.53439\n","Epoch: 00 [ 8524/23069 ( 37%)], Train Loss: 0.53420\n","Epoch: 00 [ 8564/23069 ( 37%)], Train Loss: 0.53337\n","Epoch: 00 [ 8604/23069 ( 37%)], Train Loss: 0.53265\n","Epoch: 00 [ 8644/23069 ( 37%)], Train Loss: 0.53234\n","Epoch: 00 [ 8684/23069 ( 38%)], Train Loss: 0.53121\n","Epoch: 00 [ 8724/23069 ( 38%)], Train Loss: 0.53051\n","Epoch: 00 [ 8764/23069 ( 38%)], Train Loss: 0.52987\n","Epoch: 00 [ 8804/23069 ( 38%)], Train Loss: 0.52871\n","Epoch: 00 [ 8844/23069 ( 38%)], Train Loss: 0.52844\n","Epoch: 00 [ 8884/23069 ( 39%)], Train Loss: 0.52790\n","Epoch: 00 [ 8924/23069 ( 39%)], Train Loss: 0.52679\n","Epoch: 00 [ 8964/23069 ( 39%)], Train Loss: 0.52601\n","Epoch: 00 [ 9004/23069 ( 39%)], Train Loss: 0.52520\n","Epoch: 00 [ 9044/23069 ( 39%)], Train Loss: 0.52436\n","Epoch: 00 [ 9084/23069 ( 39%)], Train Loss: 0.52398\n","Epoch: 00 [ 9124/23069 ( 40%)], Train Loss: 0.52281\n","Epoch: 00 [ 9164/23069 ( 40%)], Train Loss: 0.52218\n","Epoch: 00 [ 9204/23069 ( 40%)], Train Loss: 0.52190\n","Epoch: 00 [ 9244/23069 ( 40%)], Train Loss: 0.52185\n","Epoch: 00 [ 9284/23069 ( 40%)], Train Loss: 0.52110\n","Epoch: 00 [ 9324/23069 ( 40%)], Train Loss: 0.52032\n","Epoch: 00 [ 9364/23069 ( 41%)], Train Loss: 0.51896\n","Epoch: 00 [ 9404/23069 ( 41%)], Train Loss: 0.51879\n","Epoch: 00 [ 9444/23069 ( 41%)], Train Loss: 0.51842\n","Epoch: 00 [ 9484/23069 ( 41%)], Train Loss: 0.51759\n","Epoch: 00 [ 9524/23069 ( 41%)], Train Loss: 0.51680\n","Epoch: 00 [ 9564/23069 ( 41%)], Train Loss: 0.51544\n","Epoch: 00 [ 9604/23069 ( 42%)], Train Loss: 0.51462\n","Epoch: 00 [ 9644/23069 ( 42%)], Train Loss: 0.51335\n","Epoch: 00 [ 9684/23069 ( 42%)], Train Loss: 0.51256\n","Epoch: 00 [ 9724/23069 ( 42%)], Train Loss: 0.51217\n","Epoch: 00 [ 9764/23069 ( 42%)], Train Loss: 0.51097\n","Epoch: 00 [ 9804/23069 ( 42%)], Train Loss: 0.50975\n","Epoch: 00 [ 9844/23069 ( 43%)], Train Loss: 0.50827\n","Epoch: 00 [ 9884/23069 ( 43%)], Train Loss: 0.50736\n","Epoch: 00 [ 9924/23069 ( 43%)], Train Loss: 0.50694\n","Epoch: 00 [ 9964/23069 ( 43%)], Train Loss: 0.50709\n","Epoch: 00 [10004/23069 ( 43%)], Train Loss: 0.50685\n","Epoch: 00 [10044/23069 ( 44%)], Train Loss: 0.50597\n","Epoch: 00 [10084/23069 ( 44%)], Train Loss: 0.50565\n","Epoch: 00 [10124/23069 ( 44%)], Train Loss: 0.50466\n","Epoch: 00 [10164/23069 ( 44%)], Train Loss: 0.50438\n","Epoch: 00 [10204/23069 ( 44%)], Train Loss: 0.50443\n","Epoch: 00 [10244/23069 ( 44%)], Train Loss: 0.50351\n","Epoch: 00 [10284/23069 ( 45%)], Train Loss: 0.50267\n","Epoch: 00 [10324/23069 ( 45%)], Train Loss: 0.50137\n","Epoch: 00 [10364/23069 ( 45%)], Train Loss: 0.50176\n","Epoch: 00 [10404/23069 ( 45%)], Train Loss: 0.50126\n","Epoch: 00 [10444/23069 ( 45%)], Train Loss: 0.50163\n","Epoch: 00 [10484/23069 ( 45%)], Train Loss: 0.50105\n","Epoch: 00 [10524/23069 ( 46%)], Train Loss: 0.50060\n","Epoch: 00 [10564/23069 ( 46%)], Train Loss: 0.50044\n","Epoch: 00 [10604/23069 ( 46%)], Train Loss: 0.49938\n","Epoch: 00 [10644/23069 ( 46%)], Train Loss: 0.49875\n","Epoch: 00 [10684/23069 ( 46%)], Train Loss: 0.49745\n","Epoch: 00 [10724/23069 ( 46%)], Train Loss: 0.49715\n","Epoch: 00 [10764/23069 ( 47%)], Train Loss: 0.49663\n","Epoch: 00 [10804/23069 ( 47%)], Train Loss: 0.49627\n","Epoch: 00 [10844/23069 ( 47%)], Train Loss: 0.49632\n","Epoch: 00 [10884/23069 ( 47%)], Train Loss: 0.49641\n","Epoch: 00 [10924/23069 ( 47%)], Train Loss: 0.49603\n","Epoch: 00 [10964/23069 ( 48%)], Train Loss: 0.49547\n","Epoch: 00 [11004/23069 ( 48%)], Train Loss: 0.49497\n","Epoch: 00 [11044/23069 ( 48%)], Train Loss: 0.49477\n","Epoch: 00 [11084/23069 ( 48%)], Train Loss: 0.49449\n","Epoch: 00 [11124/23069 ( 48%)], Train Loss: 0.49416\n","Epoch: 00 [11164/23069 ( 48%)], Train Loss: 0.49335\n","Epoch: 00 [11204/23069 ( 49%)], Train Loss: 0.49303\n","Epoch: 00 [11244/23069 ( 49%)], Train Loss: 0.49201\n","Epoch: 00 [11284/23069 ( 49%)], Train Loss: 0.49172\n","Epoch: 00 [11324/23069 ( 49%)], Train Loss: 0.49161\n","Epoch: 00 [11364/23069 ( 49%)], Train Loss: 0.49134\n","Epoch: 00 [11404/23069 ( 49%)], Train Loss: 0.49127\n","Epoch: 00 [11444/23069 ( 50%)], Train Loss: 0.49065\n","Epoch: 00 [11484/23069 ( 50%)], Train Loss: 0.49037\n","Epoch: 00 [11524/23069 ( 50%)], Train Loss: 0.48955\n","Epoch: 00 [11564/23069 ( 50%)], Train Loss: 0.48859\n","Epoch: 00 [11604/23069 ( 50%)], Train Loss: 0.48840\n","Epoch: 00 [11644/23069 ( 50%)], Train Loss: 0.48844\n","Epoch: 00 [11684/23069 ( 51%)], Train Loss: 0.48858\n","Epoch: 00 [11724/23069 ( 51%)], Train Loss: 0.48796\n","Epoch: 00 [11764/23069 ( 51%)], Train Loss: 0.48764\n","Epoch: 00 [11804/23069 ( 51%)], Train Loss: 0.48652\n","Epoch: 00 [11844/23069 ( 51%)], Train Loss: 0.48564\n","Epoch: 00 [11884/23069 ( 52%)], Train Loss: 0.48472\n","Epoch: 00 [11924/23069 ( 52%)], Train Loss: 0.48397\n","Epoch: 00 [11964/23069 ( 52%)], Train Loss: 0.48323\n","Epoch: 00 [12004/23069 ( 52%)], Train Loss: 0.48332\n","Epoch: 00 [12044/23069 ( 52%)], Train Loss: 0.48240\n","Epoch: 00 [12084/23069 ( 52%)], Train Loss: 0.48159\n","Epoch: 00 [12124/23069 ( 53%)], Train Loss: 0.48131\n","Epoch: 00 [12164/23069 ( 53%)], Train Loss: 0.48124\n","Epoch: 00 [12204/23069 ( 53%)], Train Loss: 0.48058\n","Epoch: 00 [12244/23069 ( 53%)], Train Loss: 0.48015\n","Epoch: 00 [12284/23069 ( 53%)], Train Loss: 0.47982\n","Epoch: 00 [12324/23069 ( 53%)], Train Loss: 0.47912\n","Epoch: 00 [12364/23069 ( 54%)], Train Loss: 0.47883\n","Epoch: 00 [12404/23069 ( 54%)], Train Loss: 0.47849\n","Epoch: 00 [12444/23069 ( 54%)], Train Loss: 0.47829\n","Epoch: 00 [12484/23069 ( 54%)], Train Loss: 0.47738\n","Epoch: 00 [12524/23069 ( 54%)], Train Loss: 0.47734\n","Epoch: 00 [12564/23069 ( 54%)], Train Loss: 0.47668\n","Epoch: 00 [12604/23069 ( 55%)], Train Loss: 0.47672\n","Epoch: 00 [12644/23069 ( 55%)], Train Loss: 0.47654\n","Epoch: 00 [12684/23069 ( 55%)], Train Loss: 0.47593\n","Epoch: 00 [12724/23069 ( 55%)], Train Loss: 0.47558\n","Epoch: 00 [12764/23069 ( 55%)], Train Loss: 0.47516\n","Epoch: 00 [12804/23069 ( 56%)], Train Loss: 0.47475\n","Epoch: 00 [12844/23069 ( 56%)], Train Loss: 0.47416\n","Epoch: 00 [12884/23069 ( 56%)], Train Loss: 0.47375\n","Epoch: 00 [12924/23069 ( 56%)], Train Loss: 0.47342\n","Epoch: 00 [12964/23069 ( 56%)], Train Loss: 0.47276\n","Epoch: 00 [13004/23069 ( 56%)], Train Loss: 0.47267\n","Epoch: 00 [13044/23069 ( 57%)], Train Loss: 0.47259\n","Epoch: 00 [13084/23069 ( 57%)], Train Loss: 0.47296\n","Epoch: 00 [13124/23069 ( 57%)], Train Loss: 0.47250\n","Epoch: 00 [13164/23069 ( 57%)], Train Loss: 0.47205\n","Epoch: 00 [13204/23069 ( 57%)], Train Loss: 0.47182\n","Epoch: 00 [13244/23069 ( 57%)], Train Loss: 0.47104\n","Epoch: 00 [13284/23069 ( 58%)], Train Loss: 0.47068\n","Epoch: 00 [13324/23069 ( 58%)], Train Loss: 0.47027\n","Epoch: 00 [13364/23069 ( 58%)], Train Loss: 0.47017\n","Epoch: 00 [13404/23069 ( 58%)], Train Loss: 0.47002\n","Epoch: 00 [13444/23069 ( 58%)], Train Loss: 0.46964\n","Epoch: 00 [13484/23069 ( 58%)], Train Loss: 0.46943\n","Epoch: 00 [13524/23069 ( 59%)], Train Loss: 0.46897\n","Epoch: 00 [13564/23069 ( 59%)], Train Loss: 0.46881\n","Epoch: 00 [13604/23069 ( 59%)], Train Loss: 0.46858\n","Epoch: 00 [13644/23069 ( 59%)], Train Loss: 0.46824\n","Epoch: 00 [13684/23069 ( 59%)], Train Loss: 0.46785\n","Epoch: 00 [13724/23069 ( 59%)], Train Loss: 0.46690\n","Epoch: 00 [13764/23069 ( 60%)], Train Loss: 0.46631\n","Epoch: 00 [13804/23069 ( 60%)], Train Loss: 0.46567\n","Epoch: 00 [13844/23069 ( 60%)], Train Loss: 0.46465\n","Epoch: 00 [13884/23069 ( 60%)], Train Loss: 0.46480\n","Epoch: 00 [13924/23069 ( 60%)], Train Loss: 0.46473\n","Epoch: 00 [13964/23069 ( 61%)], Train Loss: 0.46470\n","Epoch: 00 [14004/23069 ( 61%)], Train Loss: 0.46409\n","Epoch: 00 [14044/23069 ( 61%)], Train Loss: 0.46347\n","Epoch: 00 [14084/23069 ( 61%)], Train Loss: 0.46348\n","Epoch: 00 [14124/23069 ( 61%)], Train Loss: 0.46278\n","Epoch: 00 [14164/23069 ( 61%)], Train Loss: 0.46279\n","Epoch: 00 [14204/23069 ( 62%)], Train Loss: 0.46197\n","Epoch: 00 [14244/23069 ( 62%)], Train Loss: 0.46125\n","Epoch: 00 [14284/23069 ( 62%)], Train Loss: 0.46074\n","Epoch: 00 [14324/23069 ( 62%)], Train Loss: 0.46099\n","Epoch: 00 [14364/23069 ( 62%)], Train Loss: 0.46048\n","Epoch: 00 [14404/23069 ( 62%)], Train Loss: 0.46004\n","Epoch: 00 [14444/23069 ( 63%)], Train Loss: 0.46010\n","Epoch: 00 [14484/23069 ( 63%)], Train Loss: 0.45951\n","Epoch: 00 [14524/23069 ( 63%)], Train Loss: 0.45930\n","Epoch: 00 [14564/23069 ( 63%)], Train Loss: 0.45875\n","Epoch: 00 [14604/23069 ( 63%)], Train Loss: 0.45826\n","Epoch: 00 [14644/23069 ( 63%)], Train Loss: 0.45797\n","Epoch: 00 [14684/23069 ( 64%)], Train Loss: 0.45751\n","Epoch: 00 [14724/23069 ( 64%)], Train Loss: 0.45738\n","Epoch: 00 [14764/23069 ( 64%)], Train Loss: 0.45700\n","Epoch: 00 [14804/23069 ( 64%)], Train Loss: 0.45676\n","Epoch: 00 [14844/23069 ( 64%)], Train Loss: 0.45672\n","Epoch: 00 [14884/23069 ( 65%)], Train Loss: 0.45680\n","Epoch: 00 [14924/23069 ( 65%)], Train Loss: 0.45645\n","Epoch: 00 [14964/23069 ( 65%)], Train Loss: 0.45604\n","Epoch: 00 [15004/23069 ( 65%)], Train Loss: 0.45577\n","Epoch: 00 [15044/23069 ( 65%)], Train Loss: 0.45519\n","Epoch: 00 [15084/23069 ( 65%)], Train Loss: 0.45534\n","Epoch: 00 [15124/23069 ( 66%)], Train Loss: 0.45501\n","Epoch: 00 [15164/23069 ( 66%)], Train Loss: 0.45442\n","Epoch: 00 [15204/23069 ( 66%)], Train Loss: 0.45430\n","Epoch: 00 [15244/23069 ( 66%)], Train Loss: 0.45347\n","Epoch: 00 [15284/23069 ( 66%)], Train Loss: 0.45325\n","Epoch: 00 [15324/23069 ( 66%)], Train Loss: 0.45354\n","Epoch: 00 [15364/23069 ( 67%)], Train Loss: 0.45344\n","Epoch: 00 [15404/23069 ( 67%)], Train Loss: 0.45314\n","Epoch: 00 [15444/23069 ( 67%)], Train Loss: 0.45295\n","Epoch: 00 [15484/23069 ( 67%)], Train Loss: 0.45264\n","Epoch: 00 [15524/23069 ( 67%)], Train Loss: 0.45198\n","Epoch: 00 [15564/23069 ( 67%)], Train Loss: 0.45141\n","Epoch: 00 [15604/23069 ( 68%)], Train Loss: 0.45121\n","Epoch: 00 [15644/23069 ( 68%)], Train Loss: 0.45047\n","Epoch: 00 [15684/23069 ( 68%)], Train Loss: 0.45007\n","Epoch: 00 [15724/23069 ( 68%)], Train Loss: 0.45015\n","Epoch: 00 [15764/23069 ( 68%)], Train Loss: 0.45002\n","Epoch: 00 [15804/23069 ( 69%)], Train Loss: 0.44972\n","Epoch: 00 [15844/23069 ( 69%)], Train Loss: 0.44937\n","Epoch: 00 [15884/23069 ( 69%)], Train Loss: 0.44926\n","Epoch: 00 [15924/23069 ( 69%)], Train Loss: 0.44932\n","Epoch: 00 [15964/23069 ( 69%)], Train Loss: 0.44888\n","Epoch: 00 [16004/23069 ( 69%)], Train Loss: 0.44869\n","Epoch: 00 [16044/23069 ( 70%)], Train Loss: 0.44820\n","Epoch: 00 [16084/23069 ( 70%)], Train Loss: 0.44772\n","Epoch: 00 [16124/23069 ( 70%)], Train Loss: 0.44718\n","Epoch: 00 [16164/23069 ( 70%)], Train Loss: 0.44727\n","Epoch: 00 [16204/23069 ( 70%)], Train Loss: 0.44662\n","Epoch: 00 [16244/23069 ( 70%)], Train Loss: 0.44610\n","Epoch: 00 [16284/23069 ( 71%)], Train Loss: 0.44599\n","Epoch: 00 [16324/23069 ( 71%)], Train Loss: 0.44581\n","Epoch: 00 [16364/23069 ( 71%)], Train Loss: 0.44544\n","Epoch: 00 [16404/23069 ( 71%)], Train Loss: 0.44483\n","Epoch: 00 [16444/23069 ( 71%)], Train Loss: 0.44440\n","Epoch: 00 [16484/23069 ( 71%)], Train Loss: 0.44409\n","Epoch: 00 [16524/23069 ( 72%)], Train Loss: 0.44389\n","Epoch: 00 [16564/23069 ( 72%)], Train Loss: 0.44348\n","Epoch: 00 [16604/23069 ( 72%)], Train Loss: 0.44294\n","Epoch: 00 [16644/23069 ( 72%)], Train Loss: 0.44295\n","Epoch: 00 [16684/23069 ( 72%)], Train Loss: 0.44261\n","Epoch: 00 [16724/23069 ( 72%)], Train Loss: 0.44208\n","Epoch: 00 [16764/23069 ( 73%)], Train Loss: 0.44140\n","Epoch: 00 [16804/23069 ( 73%)], Train Loss: 0.44091\n","Epoch: 00 [16844/23069 ( 73%)], Train Loss: 0.44067\n","Epoch: 00 [16884/23069 ( 73%)], Train Loss: 0.44045\n","Epoch: 00 [16924/23069 ( 73%)], Train Loss: 0.44024\n","Epoch: 00 [16964/23069 ( 74%)], Train Loss: 0.44022\n","Epoch: 00 [17004/23069 ( 74%)], Train Loss: 0.43966\n","Epoch: 00 [17044/23069 ( 74%)], Train Loss: 0.43940\n","Epoch: 00 [17084/23069 ( 74%)], Train Loss: 0.43883\n","Epoch: 00 [17124/23069 ( 74%)], Train Loss: 0.43839\n","Epoch: 00 [17164/23069 ( 74%)], Train Loss: 0.43782\n","Epoch: 00 [17204/23069 ( 75%)], Train Loss: 0.43709\n","Epoch: 00 [17244/23069 ( 75%)], Train Loss: 0.43673\n","Epoch: 00 [17284/23069 ( 75%)], Train Loss: 0.43638\n","Epoch: 00 [17324/23069 ( 75%)], Train Loss: 0.43633\n","Epoch: 00 [17364/23069 ( 75%)], Train Loss: 0.43652\n","Epoch: 00 [17404/23069 ( 75%)], Train Loss: 0.43610\n","Epoch: 00 [17444/23069 ( 76%)], Train Loss: 0.43572\n","Epoch: 00 [17484/23069 ( 76%)], Train Loss: 0.43553\n","Epoch: 00 [17524/23069 ( 76%)], Train Loss: 0.43526\n","Epoch: 00 [17564/23069 ( 76%)], Train Loss: 0.43512\n","Epoch: 00 [17604/23069 ( 76%)], Train Loss: 0.43464\n","Epoch: 00 [17644/23069 ( 76%)], Train Loss: 0.43463\n","Epoch: 00 [17684/23069 ( 77%)], Train Loss: 0.43451\n","Epoch: 00 [17724/23069 ( 77%)], Train Loss: 0.43419\n","Epoch: 00 [17764/23069 ( 77%)], Train Loss: 0.43444\n","Epoch: 00 [17804/23069 ( 77%)], Train Loss: 0.43407\n","Epoch: 00 [17844/23069 ( 77%)], Train Loss: 0.43426\n","Epoch: 00 [17884/23069 ( 78%)], Train Loss: 0.43392\n","Epoch: 00 [17924/23069 ( 78%)], Train Loss: 0.43345\n","Epoch: 00 [17964/23069 ( 78%)], Train Loss: 0.43297\n","Epoch: 00 [18004/23069 ( 78%)], Train Loss: 0.43293\n","Epoch: 00 [18044/23069 ( 78%)], Train Loss: 0.43236\n","Epoch: 00 [18084/23069 ( 78%)], Train Loss: 0.43211\n","Epoch: 00 [18124/23069 ( 79%)], Train Loss: 0.43155\n","Epoch: 00 [18164/23069 ( 79%)], Train Loss: 0.43130\n","Epoch: 00 [18204/23069 ( 79%)], Train Loss: 0.43139\n","Epoch: 00 [18244/23069 ( 79%)], Train Loss: 0.43089\n","Epoch: 00 [18284/23069 ( 79%)], Train Loss: 0.43037\n","Epoch: 00 [18324/23069 ( 79%)], Train Loss: 0.43027\n","Epoch: 00 [18364/23069 ( 80%)], Train Loss: 0.42979\n","Epoch: 00 [18404/23069 ( 80%)], Train Loss: 0.42947\n","Epoch: 00 [18444/23069 ( 80%)], Train Loss: 0.42946\n","Epoch: 00 [18484/23069 ( 80%)], Train Loss: 0.42918\n","Epoch: 00 [18524/23069 ( 80%)], Train Loss: 0.42882\n","Epoch: 00 [18564/23069 ( 80%)], Train Loss: 0.42834\n","Epoch: 00 [18604/23069 ( 81%)], Train Loss: 0.42855\n","Epoch: 00 [18644/23069 ( 81%)], Train Loss: 0.42820\n","Epoch: 00 [18684/23069 ( 81%)], Train Loss: 0.42774\n","Epoch: 00 [18724/23069 ( 81%)], Train Loss: 0.42749\n","Epoch: 00 [18764/23069 ( 81%)], Train Loss: 0.42729\n","Epoch: 00 [18804/23069 ( 82%)], Train Loss: 0.42708\n","Epoch: 00 [18844/23069 ( 82%)], Train Loss: 0.42703\n","Epoch: 00 [18884/23069 ( 82%)], Train Loss: 0.42675\n","Epoch: 00 [18924/23069 ( 82%)], Train Loss: 0.42623\n","Epoch: 00 [18964/23069 ( 82%)], Train Loss: 0.42594\n","Epoch: 00 [19004/23069 ( 82%)], Train Loss: 0.42583\n","Epoch: 00 [19044/23069 ( 83%)], Train Loss: 0.42568\n","Epoch: 00 [19084/23069 ( 83%)], Train Loss: 0.42519\n","Epoch: 00 [19124/23069 ( 83%)], Train Loss: 0.42474\n","Epoch: 00 [19164/23069 ( 83%)], Train Loss: 0.42471\n","Epoch: 00 [19204/23069 ( 83%)], Train Loss: 0.42444\n","Epoch: 00 [19244/23069 ( 83%)], Train Loss: 0.42393\n","Epoch: 00 [19284/23069 ( 84%)], Train Loss: 0.42349\n","Epoch: 00 [19324/23069 ( 84%)], Train Loss: 0.42343\n","Epoch: 00 [19364/23069 ( 84%)], Train Loss: 0.42343\n","Epoch: 00 [19404/23069 ( 84%)], Train Loss: 0.42328\n","Epoch: 00 [19444/23069 ( 84%)], Train Loss: 0.42333\n","Epoch: 00 [19484/23069 ( 84%)], Train Loss: 0.42286\n","Epoch: 00 [19524/23069 ( 85%)], Train Loss: 0.42278\n","Epoch: 00 [19564/23069 ( 85%)], Train Loss: 0.42248\n","Epoch: 00 [19604/23069 ( 85%)], Train Loss: 0.42200\n","Epoch: 00 [19644/23069 ( 85%)], Train Loss: 0.42160\n","Epoch: 00 [19684/23069 ( 85%)], Train Loss: 0.42140\n","Epoch: 00 [19724/23069 ( 86%)], Train Loss: 0.42126\n","Epoch: 00 [19764/23069 ( 86%)], Train Loss: 0.42103\n","Epoch: 00 [19804/23069 ( 86%)], Train Loss: 0.42097\n","Epoch: 00 [19844/23069 ( 86%)], Train Loss: 0.42083\n","Epoch: 00 [19884/23069 ( 86%)], Train Loss: 0.42042\n","Epoch: 00 [19924/23069 ( 86%)], Train Loss: 0.42055\n","Epoch: 00 [19964/23069 ( 87%)], Train Loss: 0.42011\n","Epoch: 00 [20004/23069 ( 87%)], Train Loss: 0.41994\n","Epoch: 00 [20044/23069 ( 87%)], Train Loss: 0.41977\n","Epoch: 00 [20084/23069 ( 87%)], Train Loss: 0.41963\n","Epoch: 00 [20124/23069 ( 87%)], Train Loss: 0.41955\n","Epoch: 00 [20164/23069 ( 87%)], Train Loss: 0.41953\n","Epoch: 00 [20204/23069 ( 88%)], Train Loss: 0.41953\n","Epoch: 00 [20244/23069 ( 88%)], Train Loss: 0.41935\n","Epoch: 00 [20284/23069 ( 88%)], Train Loss: 0.41910\n","Epoch: 00 [20324/23069 ( 88%)], Train Loss: 0.41910\n","Epoch: 00 [20364/23069 ( 88%)], Train Loss: 0.41873\n","Epoch: 00 [20404/23069 ( 88%)], Train Loss: 0.41835\n","Epoch: 00 [20444/23069 ( 89%)], Train Loss: 0.41813\n","Epoch: 00 [20484/23069 ( 89%)], Train Loss: 0.41786\n","Epoch: 00 [20524/23069 ( 89%)], Train Loss: 0.41797\n","Epoch: 00 [20564/23069 ( 89%)], Train Loss: 0.41796\n","Epoch: 00 [20604/23069 ( 89%)], Train Loss: 0.41779\n","Epoch: 00 [20644/23069 ( 89%)], Train Loss: 0.41747\n","Epoch: 00 [20684/23069 ( 90%)], Train Loss: 0.41687\n","Epoch: 00 [20724/23069 ( 90%)], Train Loss: 0.41654\n","Epoch: 00 [20764/23069 ( 90%)], Train Loss: 0.41612\n","Epoch: 00 [20804/23069 ( 90%)], Train Loss: 0.41615\n","Epoch: 00 [20844/23069 ( 90%)], Train Loss: 0.41612\n","Epoch: 00 [20884/23069 ( 91%)], Train Loss: 0.41591\n","Epoch: 00 [20924/23069 ( 91%)], Train Loss: 0.41566\n","Epoch: 00 [20964/23069 ( 91%)], Train Loss: 0.41541\n","Epoch: 00 [21004/23069 ( 91%)], Train Loss: 0.41496\n","Epoch: 00 [21044/23069 ( 91%)], Train Loss: 0.41477\n","Epoch: 00 [21084/23069 ( 91%)], Train Loss: 0.41478\n","Epoch: 00 [21124/23069 ( 92%)], Train Loss: 0.41482\n","Epoch: 00 [21164/23069 ( 92%)], Train Loss: 0.41475\n","Epoch: 00 [21204/23069 ( 92%)], Train Loss: 0.41453\n","Epoch: 00 [21244/23069 ( 92%)], Train Loss: 0.41432\n","Epoch: 00 [21284/23069 ( 92%)], Train Loss: 0.41392\n","Epoch: 00 [21324/23069 ( 92%)], Train Loss: 0.41343\n","Epoch: 00 [21364/23069 ( 93%)], Train Loss: 0.41326\n","Epoch: 00 [21404/23069 ( 93%)], Train Loss: 0.41348\n","Epoch: 00 [21444/23069 ( 93%)], Train Loss: 0.41311\n","Epoch: 00 [21484/23069 ( 93%)], Train Loss: 0.41288\n","Epoch: 00 [21524/23069 ( 93%)], Train Loss: 0.41262\n","Epoch: 00 [21564/23069 ( 93%)], Train Loss: 0.41218\n","Epoch: 00 [21604/23069 ( 94%)], Train Loss: 0.41177\n","Epoch: 00 [21644/23069 ( 94%)], Train Loss: 0.41143\n","Epoch: 00 [21684/23069 ( 94%)], Train Loss: 0.41133\n","Epoch: 00 [21724/23069 ( 94%)], Train Loss: 0.41103\n","Epoch: 00 [21764/23069 ( 94%)], Train Loss: 0.41064\n","Epoch: 00 [21804/23069 ( 95%)], Train Loss: 0.41063\n","Epoch: 00 [21844/23069 ( 95%)], Train Loss: 0.41038\n","Epoch: 00 [21884/23069 ( 95%)], Train Loss: 0.41033\n","Epoch: 00 [21924/23069 ( 95%)], Train Loss: 0.41005\n","Epoch: 00 [21964/23069 ( 95%)], Train Loss: 0.41003\n","Epoch: 00 [22004/23069 ( 95%)], Train Loss: 0.40965\n","Epoch: 00 [22044/23069 ( 96%)], Train Loss: 0.40923\n","Epoch: 00 [22084/23069 ( 96%)], Train Loss: 0.40909\n","Epoch: 00 [22124/23069 ( 96%)], Train Loss: 0.40855\n","Epoch: 00 [22164/23069 ( 96%)], Train Loss: 0.40845\n","Epoch: 00 [22204/23069 ( 96%)], Train Loss: 0.40842\n","Epoch: 00 [22244/23069 ( 96%)], Train Loss: 0.40808\n","Epoch: 00 [22284/23069 ( 97%)], Train Loss: 0.40800\n","Epoch: 00 [22324/23069 ( 97%)], Train Loss: 0.40766\n","Epoch: 00 [22364/23069 ( 97%)], Train Loss: 0.40740\n","Epoch: 00 [22404/23069 ( 97%)], Train Loss: 0.40741\n","Epoch: 00 [22444/23069 ( 97%)], Train Loss: 0.40711\n","Epoch: 00 [22484/23069 ( 97%)], Train Loss: 0.40666\n","Epoch: 00 [22524/23069 ( 98%)], Train Loss: 0.40650\n","Epoch: 00 [22564/23069 ( 98%)], Train Loss: 0.40604\n","Epoch: 00 [22604/23069 ( 98%)], Train Loss: 0.40593\n","Epoch: 00 [22644/23069 ( 98%)], Train Loss: 0.40582\n","Epoch: 00 [22684/23069 ( 98%)], Train Loss: 0.40568\n","Epoch: 00 [22724/23069 ( 99%)], Train Loss: 0.40554\n","Epoch: 00 [22764/23069 ( 99%)], Train Loss: 0.40555\n","Epoch: 00 [22804/23069 ( 99%)], Train Loss: 0.40512\n","Epoch: 00 [22844/23069 ( 99%)], Train Loss: 0.40509\n","Epoch: 00 [22884/23069 ( 99%)], Train Loss: 0.40495\n","Epoch: 00 [22924/23069 ( 99%)], Train Loss: 0.40467\n","Epoch: 00 [22964/23069 (100%)], Train Loss: 0.40434\n","Epoch: 00 [23004/23069 (100%)], Train Loss: 0.40397\n","Epoch: 00 [23044/23069 (100%)], Train Loss: 0.40391\n","Epoch: 00 [23069/23069 (100%)], Train Loss: 0.40388\n","----Validation Results Summary----\n","Epoch: [0] Valid Loss: 0.00000\n","0 Epoch, Best epoch was updated! Valid Loss: 0.00000\n","Saving model checkpoint to chaii-qa-5-fold-xlmroberta-torch-fit/checkpoint-fold-5.\n","\n","Epoch: 01 [    4/23069 (  0%)], Train Loss: 0.50494\n","Epoch: 01 [   44/23069 (  0%)], Train Loss: 0.27655\n","Epoch: 01 [   84/23069 (  0%)], Train Loss: 0.19858\n","Epoch: 01 [  124/23069 (  1%)], Train Loss: 0.25071\n","Epoch: 01 [  164/23069 (  1%)], Train Loss: 0.27531\n","Epoch: 01 [  204/23069 (  1%)], Train Loss: 0.25286\n","Epoch: 01 [  244/23069 (  1%)], Train Loss: 0.27387\n","Epoch: 01 [  284/23069 (  1%)], Train Loss: 0.25539\n","Epoch: 01 [  324/23069 (  1%)], Train Loss: 0.26999\n","Epoch: 01 [  364/23069 (  2%)], Train Loss: 0.27738\n","Epoch: 01 [  404/23069 (  2%)], Train Loss: 0.27406\n","Epoch: 01 [  444/23069 (  2%)], Train Loss: 0.28372\n","Epoch: 01 [  484/23069 (  2%)], Train Loss: 0.29425\n","Epoch: 01 [  524/23069 (  2%)], Train Loss: 0.28561\n","Epoch: 01 [  564/23069 (  2%)], Train Loss: 0.27708\n","Epoch: 01 [  604/23069 (  3%)], Train Loss: 0.27132\n","Epoch: 01 [  644/23069 (  3%)], Train Loss: 0.27061\n","Epoch: 01 [  684/23069 (  3%)], Train Loss: 0.27168\n","Epoch: 01 [  724/23069 (  3%)], Train Loss: 0.27442\n","Epoch: 01 [  764/23069 (  3%)], Train Loss: 0.27237\n","Epoch: 01 [  804/23069 (  3%)], Train Loss: 0.27224\n","Epoch: 01 [  844/23069 (  4%)], Train Loss: 0.26999\n","Epoch: 01 [  884/23069 (  4%)], Train Loss: 0.26190\n","Epoch: 01 [  924/23069 (  4%)], Train Loss: 0.26815\n","Epoch: 01 [  964/23069 (  4%)], Train Loss: 0.26298\n","Epoch: 01 [ 1004/23069 (  4%)], Train Loss: 0.26494\n","Epoch: 01 [ 1044/23069 (  5%)], Train Loss: 0.25961\n","Epoch: 01 [ 1084/23069 (  5%)], Train Loss: 0.25890\n","Epoch: 01 [ 1124/23069 (  5%)], Train Loss: 0.26014\n","Epoch: 01 [ 1164/23069 (  5%)], Train Loss: 0.25510\n","Epoch: 01 [ 1204/23069 (  5%)], Train Loss: 0.25754\n","Epoch: 01 [ 1244/23069 (  5%)], Train Loss: 0.25152\n","Epoch: 01 [ 1284/23069 (  6%)], Train Loss: 0.25073\n","Epoch: 01 [ 1324/23069 (  6%)], Train Loss: 0.24988\n","Epoch: 01 [ 1364/23069 (  6%)], Train Loss: 0.24955\n","Epoch: 01 [ 1404/23069 (  6%)], Train Loss: 0.24980\n","Epoch: 01 [ 1444/23069 (  6%)], Train Loss: 0.25171\n","Epoch: 01 [ 1484/23069 (  6%)], Train Loss: 0.25025\n","Epoch: 01 [ 1524/23069 (  7%)], Train Loss: 0.25113\n","Epoch: 01 [ 1564/23069 (  7%)], Train Loss: 0.25208\n","Epoch: 01 [ 1604/23069 (  7%)], Train Loss: 0.25454\n","Epoch: 01 [ 1644/23069 (  7%)], Train Loss: 0.25467\n","Epoch: 01 [ 1684/23069 (  7%)], Train Loss: 0.25231\n","Epoch: 01 [ 1724/23069 (  7%)], Train Loss: 0.25292\n","Epoch: 01 [ 1764/23069 (  8%)], Train Loss: 0.25421\n","Epoch: 01 [ 1804/23069 (  8%)], Train Loss: 0.25270\n","Epoch: 01 [ 1844/23069 (  8%)], Train Loss: 0.25272\n","Epoch: 01 [ 1884/23069 (  8%)], Train Loss: 0.25309\n","Epoch: 01 [ 1924/23069 (  8%)], Train Loss: 0.25276\n","Epoch: 01 [ 1964/23069 (  9%)], Train Loss: 0.24941\n","Epoch: 01 [ 2004/23069 (  9%)], Train Loss: 0.24928\n","Epoch: 01 [ 2044/23069 (  9%)], Train Loss: 0.24715\n","Epoch: 01 [ 2084/23069 (  9%)], Train Loss: 0.24755\n","Epoch: 01 [ 2124/23069 (  9%)], Train Loss: 0.24843\n","Epoch: 01 [ 2164/23069 (  9%)], Train Loss: 0.25076\n","Epoch: 01 [ 2204/23069 ( 10%)], Train Loss: 0.25378\n","Epoch: 01 [ 2244/23069 ( 10%)], Train Loss: 0.25654\n","Epoch: 01 [ 2284/23069 ( 10%)], Train Loss: 0.25836\n","Epoch: 01 [ 2324/23069 ( 10%)], Train Loss: 0.25841\n","Epoch: 01 [ 2364/23069 ( 10%)], Train Loss: 0.25669\n","Epoch: 01 [ 2404/23069 ( 10%)], Train Loss: 0.25476\n","Epoch: 01 [ 2444/23069 ( 11%)], Train Loss: 0.25472\n","Epoch: 01 [ 2484/23069 ( 11%)], Train Loss: 0.25505\n","Epoch: 01 [ 2524/23069 ( 11%)], Train Loss: 0.25430\n","Epoch: 01 [ 2564/23069 ( 11%)], Train Loss: 0.25210\n","Epoch: 01 [ 2604/23069 ( 11%)], Train Loss: 0.24990\n","Epoch: 01 [ 2644/23069 ( 11%)], Train Loss: 0.24776\n","Epoch: 01 [ 2684/23069 ( 12%)], Train Loss: 0.24602\n","Epoch: 01 [ 2724/23069 ( 12%)], Train Loss: 0.24458\n","Epoch: 01 [ 2764/23069 ( 12%)], Train Loss: 0.24296\n","Epoch: 01 [ 2804/23069 ( 12%)], Train Loss: 0.24144\n","Epoch: 01 [ 2844/23069 ( 12%)], Train Loss: 0.24085\n","Epoch: 01 [ 2884/23069 ( 13%)], Train Loss: 0.23919\n","Epoch: 01 [ 2924/23069 ( 13%)], Train Loss: 0.23747\n","Epoch: 01 [ 2964/23069 ( 13%)], Train Loss: 0.23605\n","Epoch: 01 [ 3004/23069 ( 13%)], Train Loss: 0.23698\n","Epoch: 01 [ 3044/23069 ( 13%)], Train Loss: 0.23589\n","Epoch: 01 [ 3084/23069 ( 13%)], Train Loss: 0.23428\n","Epoch: 01 [ 3124/23069 ( 14%)], Train Loss: 0.23252\n","Epoch: 01 [ 3164/23069 ( 14%)], Train Loss: 0.23151\n","Epoch: 01 [ 3204/23069 ( 14%)], Train Loss: 0.23005\n","Epoch: 01 [ 3244/23069 ( 14%)], Train Loss: 0.22836\n","Epoch: 01 [ 3284/23069 ( 14%)], Train Loss: 0.22733\n","Epoch: 01 [ 3324/23069 ( 14%)], Train Loss: 0.22645\n","Epoch: 01 [ 3364/23069 ( 15%)], Train Loss: 0.22584\n","Epoch: 01 [ 3404/23069 ( 15%)], Train Loss: 0.22515\n","Epoch: 01 [ 3444/23069 ( 15%)], Train Loss: 0.22433\n","Epoch: 01 [ 3484/23069 ( 15%)], Train Loss: 0.22261\n","Epoch: 01 [ 3524/23069 ( 15%)], Train Loss: 0.22204\n","Epoch: 01 [ 3564/23069 ( 15%)], Train Loss: 0.22160\n","Epoch: 01 [ 3604/23069 ( 16%)], Train Loss: 0.22116\n","Epoch: 01 [ 3644/23069 ( 16%)], Train Loss: 0.22205\n","Epoch: 01 [ 3684/23069 ( 16%)], Train Loss: 0.22108\n","Epoch: 01 [ 3724/23069 ( 16%)], Train Loss: 0.22163\n","Epoch: 01 [ 3764/23069 ( 16%)], Train Loss: 0.22103\n","Epoch: 01 [ 3804/23069 ( 16%)], Train Loss: 0.22073\n","Epoch: 01 [ 3844/23069 ( 17%)], Train Loss: 0.21975\n","Epoch: 01 [ 3884/23069 ( 17%)], Train Loss: 0.21950\n","Epoch: 01 [ 3924/23069 ( 17%)], Train Loss: 0.21853\n","Epoch: 01 [ 3964/23069 ( 17%)], Train Loss: 0.21784\n","Epoch: 01 [ 4004/23069 ( 17%)], Train Loss: 0.21723\n","Epoch: 01 [ 4044/23069 ( 18%)], Train Loss: 0.21592\n","Epoch: 01 [ 4084/23069 ( 18%)], Train Loss: 0.21587\n","Epoch: 01 [ 4124/23069 ( 18%)], Train Loss: 0.21533\n","Epoch: 01 [ 4164/23069 ( 18%)], Train Loss: 0.21468\n","Epoch: 01 [ 4204/23069 ( 18%)], Train Loss: 0.21377\n","Epoch: 01 [ 4244/23069 ( 18%)], Train Loss: 0.21295\n","Epoch: 01 [ 4284/23069 ( 19%)], Train Loss: 0.21168\n","Epoch: 01 [ 4324/23069 ( 19%)], Train Loss: 0.21087\n","Epoch: 01 [ 4364/23069 ( 19%)], Train Loss: 0.20956\n","Epoch: 01 [ 4404/23069 ( 19%)], Train Loss: 0.20905\n","Epoch: 01 [ 4444/23069 ( 19%)], Train Loss: 0.20913\n","Epoch: 01 [ 4484/23069 ( 19%)], Train Loss: 0.20798\n","Epoch: 01 [ 4524/23069 ( 20%)], Train Loss: 0.20677\n","Epoch: 01 [ 4564/23069 ( 20%)], Train Loss: 0.20550\n","Epoch: 01 [ 4604/23069 ( 20%)], Train Loss: 0.20489\n","Epoch: 01 [ 4644/23069 ( 20%)], Train Loss: 0.20433\n","Epoch: 01 [ 4684/23069 ( 20%)], Train Loss: 0.20393\n","Epoch: 01 [ 4724/23069 ( 20%)], Train Loss: 0.20255\n","Epoch: 01 [ 4764/23069 ( 21%)], Train Loss: 0.20193\n","Epoch: 01 [ 4804/23069 ( 21%)], Train Loss: 0.20132\n","Epoch: 01 [ 4844/23069 ( 21%)], Train Loss: 0.20011\n","Epoch: 01 [ 4884/23069 ( 21%)], Train Loss: 0.19892\n","Epoch: 01 [ 4924/23069 ( 21%)], Train Loss: 0.19820\n","Epoch: 01 [ 4964/23069 ( 22%)], Train Loss: 0.19692\n","Epoch: 01 [ 5004/23069 ( 22%)], Train Loss: 0.19575\n","Epoch: 01 [ 5044/23069 ( 22%)], Train Loss: 0.19475\n","Epoch: 01 [ 5084/23069 ( 22%)], Train Loss: 0.19499\n","Epoch: 01 [ 5124/23069 ( 22%)], Train Loss: 0.19404\n","Epoch: 01 [ 5164/23069 ( 22%)], Train Loss: 0.19421\n","Epoch: 01 [ 5204/23069 ( 23%)], Train Loss: 0.19362\n","Epoch: 01 [ 5244/23069 ( 23%)], Train Loss: 0.19374\n","Epoch: 01 [ 5284/23069 ( 23%)], Train Loss: 0.19293\n","Epoch: 01 [ 5324/23069 ( 23%)], Train Loss: 0.19219\n","Epoch: 01 [ 5364/23069 ( 23%)], Train Loss: 0.19107\n","Epoch: 01 [ 5404/23069 ( 23%)], Train Loss: 0.19007\n","Epoch: 01 [ 5444/23069 ( 24%)], Train Loss: 0.18953\n","Epoch: 01 [ 5484/23069 ( 24%)], Train Loss: 0.18956\n","Epoch: 01 [ 5524/23069 ( 24%)], Train Loss: 0.18907\n","Epoch: 01 [ 5564/23069 ( 24%)], Train Loss: 0.18867\n","Epoch: 01 [ 5604/23069 ( 24%)], Train Loss: 0.18798\n","Epoch: 01 [ 5644/23069 ( 24%)], Train Loss: 0.18698\n","Epoch: 01 [ 5684/23069 ( 25%)], Train Loss: 0.18675\n","Epoch: 01 [ 5724/23069 ( 25%)], Train Loss: 0.18614\n","Epoch: 01 [ 5764/23069 ( 25%)], Train Loss: 0.18625\n","Epoch: 01 [ 5804/23069 ( 25%)], Train Loss: 0.18610\n","Epoch: 01 [ 5844/23069 ( 25%)], Train Loss: 0.18553\n","Epoch: 01 [ 5884/23069 ( 26%)], Train Loss: 0.18495\n","Epoch: 01 [ 5924/23069 ( 26%)], Train Loss: 0.18446\n","Epoch: 01 [ 5964/23069 ( 26%)], Train Loss: 0.18438\n","Epoch: 01 [ 6004/23069 ( 26%)], Train Loss: 0.18437\n","Epoch: 01 [ 6044/23069 ( 26%)], Train Loss: 0.18432\n","Epoch: 01 [ 6084/23069 ( 26%)], Train Loss: 0.18361\n","Epoch: 01 [ 6124/23069 ( 27%)], Train Loss: 0.18278\n","Epoch: 01 [ 6164/23069 ( 27%)], Train Loss: 0.18228\n","Epoch: 01 [ 6204/23069 ( 27%)], Train Loss: 0.18180\n","Epoch: 01 [ 6244/23069 ( 27%)], Train Loss: 0.18142\n","Epoch: 01 [ 6284/23069 ( 27%)], Train Loss: 0.18060\n","Epoch: 01 [ 6324/23069 ( 27%)], Train Loss: 0.17959\n","Epoch: 01 [ 6364/23069 ( 28%)], Train Loss: 0.17938\n","Epoch: 01 [ 6404/23069 ( 28%)], Train Loss: 0.18088\n","Epoch: 01 [ 6444/23069 ( 28%)], Train Loss: 0.18000\n","Epoch: 01 [ 6484/23069 ( 28%)], Train Loss: 0.17980\n","Epoch: 01 [ 6524/23069 ( 28%)], Train Loss: 0.17964\n","Epoch: 01 [ 6564/23069 ( 28%)], Train Loss: 0.17964\n","Epoch: 01 [ 6604/23069 ( 29%)], Train Loss: 0.17896\n","Epoch: 01 [ 6644/23069 ( 29%)], Train Loss: 0.17868\n","Epoch: 01 [ 6684/23069 ( 29%)], Train Loss: 0.17949\n","Epoch: 01 [ 6724/23069 ( 29%)], Train Loss: 0.17930\n","Epoch: 01 [ 6764/23069 ( 29%)], Train Loss: 0.17927\n","Epoch: 01 [ 6804/23069 ( 29%)], Train Loss: 0.17923\n","Epoch: 01 [ 6844/23069 ( 30%)], Train Loss: 0.17896\n","Epoch: 01 [ 6884/23069 ( 30%)], Train Loss: 0.17842\n","Epoch: 01 [ 6924/23069 ( 30%)], Train Loss: 0.17782\n","Epoch: 01 [ 6964/23069 ( 30%)], Train Loss: 0.17737\n","Epoch: 01 [ 7004/23069 ( 30%)], Train Loss: 0.17664\n","Epoch: 01 [ 7044/23069 ( 31%)], Train Loss: 0.17584\n","Epoch: 01 [ 7084/23069 ( 31%)], Train Loss: 0.17524\n","Epoch: 01 [ 7124/23069 ( 31%)], Train Loss: 0.17488\n","Epoch: 01 [ 7164/23069 ( 31%)], Train Loss: 0.17432\n","Epoch: 01 [ 7204/23069 ( 31%)], Train Loss: 0.17383\n","Epoch: 01 [ 7244/23069 ( 31%)], Train Loss: 0.17337\n","Epoch: 01 [ 7284/23069 ( 32%)], Train Loss: 0.17280\n","Epoch: 01 [ 7324/23069 ( 32%)], Train Loss: 0.17293\n","Epoch: 01 [ 7364/23069 ( 32%)], Train Loss: 0.17278\n","Epoch: 01 [ 7404/23069 ( 32%)], Train Loss: 0.17211\n","Epoch: 01 [ 7444/23069 ( 32%)], Train Loss: 0.17192\n","Epoch: 01 [ 7484/23069 ( 32%)], Train Loss: 0.17123\n","Epoch: 01 [ 7524/23069 ( 33%)], Train Loss: 0.17105\n","Epoch: 01 [ 7564/23069 ( 33%)], Train Loss: 0.17052\n","Epoch: 01 [ 7604/23069 ( 33%)], Train Loss: 0.16984\n","Epoch: 01 [ 7644/23069 ( 33%)], Train Loss: 0.16929\n","Epoch: 01 [ 7684/23069 ( 33%)], Train Loss: 0.16931\n","Epoch: 01 [ 7724/23069 ( 33%)], Train Loss: 0.16901\n","Epoch: 01 [ 7764/23069 ( 34%)], Train Loss: 0.16934\n","Epoch: 01 [ 7804/23069 ( 34%)], Train Loss: 0.16954\n","Epoch: 01 [ 7844/23069 ( 34%)], Train Loss: 0.16899\n","Epoch: 01 [ 7884/23069 ( 34%)], Train Loss: 0.16888\n","Epoch: 01 [ 7924/23069 ( 34%)], Train Loss: 0.16937\n","Epoch: 01 [ 7964/23069 ( 35%)], Train Loss: 0.16920\n","Epoch: 01 [ 8004/23069 ( 35%)], Train Loss: 0.16985\n","Epoch: 01 [ 8044/23069 ( 35%)], Train Loss: 0.16919\n","Epoch: 01 [ 8084/23069 ( 35%)], Train Loss: 0.16952\n","Epoch: 01 [ 8124/23069 ( 35%)], Train Loss: 0.16955\n","Epoch: 01 [ 8164/23069 ( 35%)], Train Loss: 0.16969\n","Epoch: 01 [ 8204/23069 ( 36%)], Train Loss: 0.16924\n","Epoch: 01 [ 8244/23069 ( 36%)], Train Loss: 0.16860\n","Epoch: 01 [ 8284/23069 ( 36%)], Train Loss: 0.16826\n","Epoch: 01 [ 8324/23069 ( 36%)], Train Loss: 0.16781\n","Epoch: 01 [ 8364/23069 ( 36%)], Train Loss: 0.16757\n","Epoch: 01 [ 8404/23069 ( 36%)], Train Loss: 0.16710\n","Epoch: 01 [ 8444/23069 ( 37%)], Train Loss: 0.16676\n","Epoch: 01 [ 8484/23069 ( 37%)], Train Loss: 0.16632\n","Epoch: 01 [ 8524/23069 ( 37%)], Train Loss: 0.16621\n","Epoch: 01 [ 8564/23069 ( 37%)], Train Loss: 0.16616\n","Epoch: 01 [ 8604/23069 ( 37%)], Train Loss: 0.16585\n","Epoch: 01 [ 8644/23069 ( 37%)], Train Loss: 0.16582\n","Epoch: 01 [ 8684/23069 ( 38%)], Train Loss: 0.16548\n","Epoch: 01 [ 8724/23069 ( 38%)], Train Loss: 0.16569\n","Epoch: 01 [ 8764/23069 ( 38%)], Train Loss: 0.16531\n","Epoch: 01 [ 8804/23069 ( 38%)], Train Loss: 0.16488\n","Epoch: 01 [ 8844/23069 ( 38%)], Train Loss: 0.16509\n","Epoch: 01 [ 8884/23069 ( 39%)], Train Loss: 0.16448\n","Epoch: 01 [ 8924/23069 ( 39%)], Train Loss: 0.16424\n","Epoch: 01 [ 8964/23069 ( 39%)], Train Loss: 0.16394\n","Epoch: 01 [ 9004/23069 ( 39%)], Train Loss: 0.16365\n","Epoch: 01 [ 9044/23069 ( 39%)], Train Loss: 0.16362\n","Epoch: 01 [ 9084/23069 ( 39%)], Train Loss: 0.16349\n","Epoch: 01 [ 9124/23069 ( 40%)], Train Loss: 0.16294\n","Epoch: 01 [ 9164/23069 ( 40%)], Train Loss: 0.16256\n","Epoch: 01 [ 9204/23069 ( 40%)], Train Loss: 0.16249\n","Epoch: 01 [ 9244/23069 ( 40%)], Train Loss: 0.16248\n","Epoch: 01 [ 9284/23069 ( 40%)], Train Loss: 0.16230\n","Epoch: 01 [ 9324/23069 ( 40%)], Train Loss: 0.16204\n","Epoch: 01 [ 9364/23069 ( 41%)], Train Loss: 0.16172\n","Epoch: 01 [ 9404/23069 ( 41%)], Train Loss: 0.16159\n","Epoch: 01 [ 9444/23069 ( 41%)], Train Loss: 0.16136\n","Epoch: 01 [ 9484/23069 ( 41%)], Train Loss: 0.16096\n","Epoch: 01 [ 9524/23069 ( 41%)], Train Loss: 0.16048\n","Epoch: 01 [ 9564/23069 ( 41%)], Train Loss: 0.15994\n","Epoch: 01 [ 9604/23069 ( 42%)], Train Loss: 0.15983\n","Epoch: 01 [ 9644/23069 ( 42%)], Train Loss: 0.15933\n","Epoch: 01 [ 9684/23069 ( 42%)], Train Loss: 0.15889\n","Epoch: 01 [ 9724/23069 ( 42%)], Train Loss: 0.15884\n","Epoch: 01 [ 9764/23069 ( 42%)], Train Loss: 0.15834\n","Epoch: 01 [ 9804/23069 ( 42%)], Train Loss: 0.15799\n","Epoch: 01 [ 9844/23069 ( 43%)], Train Loss: 0.15756\n","Epoch: 01 [ 9884/23069 ( 43%)], Train Loss: 0.15719\n","Epoch: 01 [ 9924/23069 ( 43%)], Train Loss: 0.15695\n","Epoch: 01 [ 9964/23069 ( 43%)], Train Loss: 0.15662\n","Epoch: 01 [10004/23069 ( 43%)], Train Loss: 0.15655\n","Epoch: 01 [10044/23069 ( 44%)], Train Loss: 0.15628\n","Epoch: 01 [10084/23069 ( 44%)], Train Loss: 0.15645\n","Epoch: 01 [10124/23069 ( 44%)], Train Loss: 0.15613\n","Epoch: 01 [10164/23069 ( 44%)], Train Loss: 0.15626\n","Epoch: 01 [10204/23069 ( 44%)], Train Loss: 0.15640\n","Epoch: 01 [10244/23069 ( 44%)], Train Loss: 0.15609\n","Epoch: 01 [10284/23069 ( 45%)], Train Loss: 0.15586\n","Epoch: 01 [10324/23069 ( 45%)], Train Loss: 0.15549\n","Epoch: 01 [10364/23069 ( 45%)], Train Loss: 0.15560\n","Epoch: 01 [10404/23069 ( 45%)], Train Loss: 0.15520\n","Epoch: 01 [10444/23069 ( 45%)], Train Loss: 0.15538\n","Epoch: 01 [10484/23069 ( 45%)], Train Loss: 0.15501\n","Epoch: 01 [10524/23069 ( 46%)], Train Loss: 0.15473\n","Epoch: 01 [10564/23069 ( 46%)], Train Loss: 0.15482\n","Epoch: 01 [10604/23069 ( 46%)], Train Loss: 0.15456\n","Epoch: 01 [10644/23069 ( 46%)], Train Loss: 0.15434\n","Epoch: 01 [10684/23069 ( 46%)], Train Loss: 0.15389\n","Epoch: 01 [10724/23069 ( 46%)], Train Loss: 0.15364\n","Epoch: 01 [10764/23069 ( 47%)], Train Loss: 0.15347\n","Epoch: 01 [10804/23069 ( 47%)], Train Loss: 0.15342\n","Epoch: 01 [10844/23069 ( 47%)], Train Loss: 0.15335\n","Epoch: 01 [10884/23069 ( 47%)], Train Loss: 0.15340\n","Epoch: 01 [10924/23069 ( 47%)], Train Loss: 0.15331\n","Epoch: 01 [10964/23069 ( 48%)], Train Loss: 0.15299\n","Epoch: 01 [11004/23069 ( 48%)], Train Loss: 0.15299\n","Epoch: 01 [11044/23069 ( 48%)], Train Loss: 0.15292\n","Epoch: 01 [11084/23069 ( 48%)], Train Loss: 0.15289\n","Epoch: 01 [11124/23069 ( 48%)], Train Loss: 0.15266\n","Epoch: 01 [11164/23069 ( 48%)], Train Loss: 0.15233\n","Epoch: 01 [11204/23069 ( 49%)], Train Loss: 0.15212\n","Epoch: 01 [11244/23069 ( 49%)], Train Loss: 0.15173\n","Epoch: 01 [11284/23069 ( 49%)], Train Loss: 0.15137\n","Epoch: 01 [11324/23069 ( 49%)], Train Loss: 0.15125\n","Epoch: 01 [11364/23069 ( 49%)], Train Loss: 0.15110\n","Epoch: 01 [11404/23069 ( 49%)], Train Loss: 0.15109\n","Epoch: 01 [11444/23069 ( 50%)], Train Loss: 0.15069\n","Epoch: 01 [11484/23069 ( 50%)], Train Loss: 0.15078\n","Epoch: 01 [11524/23069 ( 50%)], Train Loss: 0.15041\n","Epoch: 01 [11564/23069 ( 50%)], Train Loss: 0.15003\n","Epoch: 01 [11604/23069 ( 50%)], Train Loss: 0.14979\n","Epoch: 01 [11644/23069 ( 50%)], Train Loss: 0.14979\n","Epoch: 01 [11684/23069 ( 51%)], Train Loss: 0.14987\n","Epoch: 01 [11724/23069 ( 51%)], Train Loss: 0.14955\n","Epoch: 01 [11764/23069 ( 51%)], Train Loss: 0.14933\n","Epoch: 01 [11804/23069 ( 51%)], Train Loss: 0.14903\n","Epoch: 01 [11844/23069 ( 51%)], Train Loss: 0.14876\n","Epoch: 01 [11884/23069 ( 52%)], Train Loss: 0.14832\n","Epoch: 01 [11924/23069 ( 52%)], Train Loss: 0.14791\n","Epoch: 01 [11964/23069 ( 52%)], Train Loss: 0.14771\n","Epoch: 01 [12004/23069 ( 52%)], Train Loss: 0.14749\n","Epoch: 01 [12044/23069 ( 52%)], Train Loss: 0.14717\n","Epoch: 01 [12084/23069 ( 52%)], Train Loss: 0.14699\n","Epoch: 01 [12124/23069 ( 53%)], Train Loss: 0.14683\n","Epoch: 01 [12164/23069 ( 53%)], Train Loss: 0.14680\n","Epoch: 01 [12204/23069 ( 53%)], Train Loss: 0.14658\n","Epoch: 01 [12244/23069 ( 53%)], Train Loss: 0.14658\n","Epoch: 01 [12284/23069 ( 53%)], Train Loss: 0.14628\n","Epoch: 01 [12324/23069 ( 53%)], Train Loss: 0.14609\n","Epoch: 01 [12364/23069 ( 54%)], Train Loss: 0.14591\n","Epoch: 01 [12404/23069 ( 54%)], Train Loss: 0.14584\n","Epoch: 01 [12444/23069 ( 54%)], Train Loss: 0.14569\n","Epoch: 01 [12484/23069 ( 54%)], Train Loss: 0.14534\n","Epoch: 01 [12524/23069 ( 54%)], Train Loss: 0.14525\n","Epoch: 01 [12564/23069 ( 54%)], Train Loss: 0.14492\n","Epoch: 01 [12604/23069 ( 55%)], Train Loss: 0.14526\n","Epoch: 01 [12644/23069 ( 55%)], Train Loss: 0.14517\n","Epoch: 01 [12684/23069 ( 55%)], Train Loss: 0.14500\n","Epoch: 01 [12724/23069 ( 55%)], Train Loss: 0.14507\n","Epoch: 01 [12764/23069 ( 55%)], Train Loss: 0.14477\n","Epoch: 01 [12804/23069 ( 56%)], Train Loss: 0.14482\n","Epoch: 01 [12844/23069 ( 56%)], Train Loss: 0.14481\n","Epoch: 01 [12884/23069 ( 56%)], Train Loss: 0.14447\n","Epoch: 01 [12924/23069 ( 56%)], Train Loss: 0.14433\n","Epoch: 01 [12964/23069 ( 56%)], Train Loss: 0.14400\n","Epoch: 01 [13004/23069 ( 56%)], Train Loss: 0.14382\n","Epoch: 01 [13044/23069 ( 57%)], Train Loss: 0.14384\n","Epoch: 01 [13084/23069 ( 57%)], Train Loss: 0.14383\n","Epoch: 01 [13124/23069 ( 57%)], Train Loss: 0.14369\n","Epoch: 01 [13164/23069 ( 57%)], Train Loss: 0.14373\n","Epoch: 01 [13204/23069 ( 57%)], Train Loss: 0.14372\n","Epoch: 01 [13244/23069 ( 57%)], Train Loss: 0.14345\n","Epoch: 01 [13284/23069 ( 58%)], Train Loss: 0.14337\n","Epoch: 01 [13324/23069 ( 58%)], Train Loss: 0.14316\n","Epoch: 01 [13364/23069 ( 58%)], Train Loss: 0.14289\n","Epoch: 01 [13404/23069 ( 58%)], Train Loss: 0.14287\n","Epoch: 01 [13444/23069 ( 58%)], Train Loss: 0.14274\n","Epoch: 01 [13484/23069 ( 58%)], Train Loss: 0.14272\n","Epoch: 01 [13524/23069 ( 59%)], Train Loss: 0.14251\n","Epoch: 01 [13564/23069 ( 59%)], Train Loss: 0.14243\n","Epoch: 01 [13604/23069 ( 59%)], Train Loss: 0.14238\n","Epoch: 01 [13644/23069 ( 59%)], Train Loss: 0.14216\n","Epoch: 01 [13684/23069 ( 59%)], Train Loss: 0.14197\n","Epoch: 01 [13724/23069 ( 59%)], Train Loss: 0.14166\n","Epoch: 01 [13764/23069 ( 60%)], Train Loss: 0.14152\n","Epoch: 01 [13804/23069 ( 60%)], Train Loss: 0.14136\n","Epoch: 01 [13844/23069 ( 60%)], Train Loss: 0.14103\n","Epoch: 01 [13884/23069 ( 60%)], Train Loss: 0.14097\n","Epoch: 01 [13924/23069 ( 60%)], Train Loss: 0.14078\n","Epoch: 01 [13964/23069 ( 61%)], Train Loss: 0.14062\n","Epoch: 01 [14004/23069 ( 61%)], Train Loss: 0.14039\n","Epoch: 01 [14044/23069 ( 61%)], Train Loss: 0.14013\n","Epoch: 01 [14084/23069 ( 61%)], Train Loss: 0.13999\n","Epoch: 01 [14124/23069 ( 61%)], Train Loss: 0.13983\n","Epoch: 01 [14164/23069 ( 61%)], Train Loss: 0.14011\n","Epoch: 01 [14204/23069 ( 62%)], Train Loss: 0.13978\n","Epoch: 01 [14244/23069 ( 62%)], Train Loss: 0.13958\n","Epoch: 01 [14284/23069 ( 62%)], Train Loss: 0.13941\n","Epoch: 01 [14324/23069 ( 62%)], Train Loss: 0.13948\n","Epoch: 01 [14364/23069 ( 62%)], Train Loss: 0.13945\n","Epoch: 01 [14404/23069 ( 62%)], Train Loss: 0.13930\n","Epoch: 01 [14444/23069 ( 63%)], Train Loss: 0.13936\n","Epoch: 01 [14484/23069 ( 63%)], Train Loss: 0.13910\n","Epoch: 01 [14524/23069 ( 63%)], Train Loss: 0.13906\n","Epoch: 01 [14564/23069 ( 63%)], Train Loss: 0.13884\n","Epoch: 01 [14604/23069 ( 63%)], Train Loss: 0.13859\n","Epoch: 01 [14644/23069 ( 63%)], Train Loss: 0.13850\n","Epoch: 01 [14684/23069 ( 64%)], Train Loss: 0.13832\n","Epoch: 01 [14724/23069 ( 64%)], Train Loss: 0.13817\n","Epoch: 01 [14764/23069 ( 64%)], Train Loss: 0.13807\n","Epoch: 01 [14804/23069 ( 64%)], Train Loss: 0.13787\n","Epoch: 01 [14844/23069 ( 64%)], Train Loss: 0.13778\n","Epoch: 01 [14884/23069 ( 65%)], Train Loss: 0.13784\n","Epoch: 01 [14924/23069 ( 65%)], Train Loss: 0.13760\n","Epoch: 01 [14964/23069 ( 65%)], Train Loss: 0.13741\n","Epoch: 01 [15004/23069 ( 65%)], Train Loss: 0.13741\n","Epoch: 01 [15044/23069 ( 65%)], Train Loss: 0.13720\n","Epoch: 01 [15084/23069 ( 65%)], Train Loss: 0.13726\n","Epoch: 01 [15124/23069 ( 66%)], Train Loss: 0.13704\n","Epoch: 01 [15164/23069 ( 66%)], Train Loss: 0.13686\n","Epoch: 01 [15204/23069 ( 66%)], Train Loss: 0.13684\n","Epoch: 01 [15244/23069 ( 66%)], Train Loss: 0.13659\n","Epoch: 01 [15284/23069 ( 66%)], Train Loss: 0.13652\n","Epoch: 01 [15324/23069 ( 66%)], Train Loss: 0.13665\n","Epoch: 01 [15364/23069 ( 67%)], Train Loss: 0.13664\n","Epoch: 01 [15404/23069 ( 67%)], Train Loss: 0.13644\n","Epoch: 01 [15444/23069 ( 67%)], Train Loss: 0.13638\n","Epoch: 01 [15484/23069 ( 67%)], Train Loss: 0.13624\n","Epoch: 01 [15524/23069 ( 67%)], Train Loss: 0.13606\n","Epoch: 01 [15564/23069 ( 67%)], Train Loss: 0.13592\n","Epoch: 01 [15604/23069 ( 68%)], Train Loss: 0.13585\n","Epoch: 01 [15644/23069 ( 68%)], Train Loss: 0.13567\n","Epoch: 01 [15684/23069 ( 68%)], Train Loss: 0.13549\n","Epoch: 01 [15724/23069 ( 68%)], Train Loss: 0.13563\n","Epoch: 01 [15764/23069 ( 68%)], Train Loss: 0.13560\n","Epoch: 01 [15804/23069 ( 69%)], Train Loss: 0.13554\n","Epoch: 01 [15844/23069 ( 69%)], Train Loss: 0.13537\n","Epoch: 01 [15884/23069 ( 69%)], Train Loss: 0.13523\n","Epoch: 01 [15924/23069 ( 69%)], Train Loss: 0.13517\n","Epoch: 01 [15964/23069 ( 69%)], Train Loss: 0.13505\n","Epoch: 01 [16004/23069 ( 69%)], Train Loss: 0.13498\n","Epoch: 01 [16044/23069 ( 70%)], Train Loss: 0.13476\n","Epoch: 01 [16084/23069 ( 70%)], Train Loss: 0.13465\n","Epoch: 01 [16124/23069 ( 70%)], Train Loss: 0.13440\n","Epoch: 01 [16164/23069 ( 70%)], Train Loss: 0.13422\n","Epoch: 01 [16204/23069 ( 70%)], Train Loss: 0.13405\n","Epoch: 01 [16244/23069 ( 70%)], Train Loss: 0.13397\n","Epoch: 01 [16284/23069 ( 71%)], Train Loss: 0.13399\n","Epoch: 01 [16324/23069 ( 71%)], Train Loss: 0.13411\n","Epoch: 01 [16364/23069 ( 71%)], Train Loss: 0.13409\n","Epoch: 01 [16404/23069 ( 71%)], Train Loss: 0.13407\n","Epoch: 01 [16444/23069 ( 71%)], Train Loss: 0.13383\n","Epoch: 01 [16484/23069 ( 71%)], Train Loss: 0.13384\n","Epoch: 01 [16524/23069 ( 72%)], Train Loss: 0.13378\n","Epoch: 01 [16564/23069 ( 72%)], Train Loss: 0.13364\n","Epoch: 01 [16604/23069 ( 72%)], Train Loss: 0.13351\n","Epoch: 01 [16644/23069 ( 72%)], Train Loss: 0.13355\n","Epoch: 01 [16684/23069 ( 72%)], Train Loss: 0.13336\n","Epoch: 01 [16724/23069 ( 72%)], Train Loss: 0.13311\n","Epoch: 01 [16764/23069 ( 73%)], Train Loss: 0.13290\n","Epoch: 01 [16804/23069 ( 73%)], Train Loss: 0.13273\n","Epoch: 01 [16844/23069 ( 73%)], Train Loss: 0.13282\n","Epoch: 01 [16884/23069 ( 73%)], Train Loss: 0.13271\n","Epoch: 01 [16924/23069 ( 73%)], Train Loss: 0.13266\n","Epoch: 01 [16964/23069 ( 74%)], Train Loss: 0.13258\n","Epoch: 01 [17004/23069 ( 74%)], Train Loss: 0.13252\n","Epoch: 01 [17044/23069 ( 74%)], Train Loss: 0.13247\n","Epoch: 01 [17084/23069 ( 74%)], Train Loss: 0.13224\n","Epoch: 01 [17124/23069 ( 74%)], Train Loss: 0.13219\n","Epoch: 01 [17164/23069 ( 74%)], Train Loss: 0.13197\n","Epoch: 01 [17204/23069 ( 75%)], Train Loss: 0.13176\n","Epoch: 01 [17244/23069 ( 75%)], Train Loss: 0.13161\n","Epoch: 01 [17284/23069 ( 75%)], Train Loss: 0.13151\n","Epoch: 01 [17324/23069 ( 75%)], Train Loss: 0.13143\n","Epoch: 01 [17364/23069 ( 75%)], Train Loss: 0.13173\n","Epoch: 01 [17404/23069 ( 75%)], Train Loss: 0.13162\n","Epoch: 01 [17444/23069 ( 76%)], Train Loss: 0.13150\n","Epoch: 01 [17484/23069 ( 76%)], Train Loss: 0.13151\n","Epoch: 01 [17524/23069 ( 76%)], Train Loss: 0.13138\n","Epoch: 01 [17564/23069 ( 76%)], Train Loss: 0.13135\n","Epoch: 01 [17604/23069 ( 76%)], Train Loss: 0.13113\n","Epoch: 01 [17644/23069 ( 76%)], Train Loss: 0.13122\n","Epoch: 01 [17684/23069 ( 77%)], Train Loss: 0.13118\n","Epoch: 01 [17724/23069 ( 77%)], Train Loss: 0.13104\n","Epoch: 01 [17764/23069 ( 77%)], Train Loss: 0.13118\n","Epoch: 01 [17804/23069 ( 77%)], Train Loss: 0.13106\n","Epoch: 01 [17844/23069 ( 77%)], Train Loss: 0.13117\n","Epoch: 01 [17884/23069 ( 78%)], Train Loss: 0.13110\n","Epoch: 01 [17924/23069 ( 78%)], Train Loss: 0.13100\n","Epoch: 01 [17964/23069 ( 78%)], Train Loss: 0.13080\n","Epoch: 01 [18004/23069 ( 78%)], Train Loss: 0.13082\n","Epoch: 01 [18044/23069 ( 78%)], Train Loss: 0.13058\n","Epoch: 01 [18084/23069 ( 78%)], Train Loss: 0.13050\n","Epoch: 01 [18124/23069 ( 79%)], Train Loss: 0.13037\n","Epoch: 01 [18164/23069 ( 79%)], Train Loss: 0.13024\n","Epoch: 01 [18204/23069 ( 79%)], Train Loss: 0.13027\n","Epoch: 01 [18244/23069 ( 79%)], Train Loss: 0.13020\n","Epoch: 01 [18284/23069 ( 79%)], Train Loss: 0.13004\n","Epoch: 01 [18324/23069 ( 79%)], Train Loss: 0.12994\n","Epoch: 01 [18364/23069 ( 80%)], Train Loss: 0.12979\n","Epoch: 01 [18404/23069 ( 80%)], Train Loss: 0.12962\n","Epoch: 01 [18444/23069 ( 80%)], Train Loss: 0.12963\n","Epoch: 01 [18484/23069 ( 80%)], Train Loss: 0.12961\n","Epoch: 01 [18524/23069 ( 80%)], Train Loss: 0.12953\n","Epoch: 01 [18564/23069 ( 80%)], Train Loss: 0.12938\n","Epoch: 01 [18604/23069 ( 81%)], Train Loss: 0.12928\n","Epoch: 01 [18644/23069 ( 81%)], Train Loss: 0.12911\n","Epoch: 01 [18684/23069 ( 81%)], Train Loss: 0.12889\n","Epoch: 01 [18724/23069 ( 81%)], Train Loss: 0.12888\n","Epoch: 01 [18764/23069 ( 81%)], Train Loss: 0.12870\n","Epoch: 01 [18804/23069 ( 82%)], Train Loss: 0.12860\n","Epoch: 01 [18844/23069 ( 82%)], Train Loss: 0.12867\n","Epoch: 01 [18884/23069 ( 82%)], Train Loss: 0.12846\n","Epoch: 01 [18924/23069 ( 82%)], Train Loss: 0.12833\n","Epoch: 01 [18964/23069 ( 82%)], Train Loss: 0.12827\n","Epoch: 01 [19004/23069 ( 82%)], Train Loss: 0.12810\n","Epoch: 01 [19044/23069 ( 83%)], Train Loss: 0.12817\n","Epoch: 01 [19084/23069 ( 83%)], Train Loss: 0.12806\n","Epoch: 01 [19124/23069 ( 83%)], Train Loss: 0.12797\n","Epoch: 01 [19164/23069 ( 83%)], Train Loss: 0.12793\n","Epoch: 01 [19204/23069 ( 83%)], Train Loss: 0.12776\n","Epoch: 01 [19244/23069 ( 83%)], Train Loss: 0.12757\n","Epoch: 01 [19284/23069 ( 84%)], Train Loss: 0.12746\n","Epoch: 01 [19324/23069 ( 84%)], Train Loss: 0.12737\n","Epoch: 01 [19364/23069 ( 84%)], Train Loss: 0.12726\n","Epoch: 01 [19404/23069 ( 84%)], Train Loss: 0.12727\n","Epoch: 01 [19444/23069 ( 84%)], Train Loss: 0.12723\n","Epoch: 01 [19484/23069 ( 84%)], Train Loss: 0.12705\n","Epoch: 01 [19524/23069 ( 85%)], Train Loss: 0.12689\n","Epoch: 01 [19564/23069 ( 85%)], Train Loss: 0.12673\n","Epoch: 01 [19604/23069 ( 85%)], Train Loss: 0.12660\n","Epoch: 01 [19644/23069 ( 85%)], Train Loss: 0.12645\n","Epoch: 01 [19684/23069 ( 85%)], Train Loss: 0.12633\n","Epoch: 01 [19724/23069 ( 86%)], Train Loss: 0.12620\n","Epoch: 01 [19764/23069 ( 86%)], Train Loss: 0.12613\n","Epoch: 01 [19804/23069 ( 86%)], Train Loss: 0.12627\n","Epoch: 01 [19844/23069 ( 86%)], Train Loss: 0.12634\n","Epoch: 01 [19884/23069 ( 86%)], Train Loss: 0.12616\n","Epoch: 01 [19924/23069 ( 86%)], Train Loss: 0.12635\n","Epoch: 01 [19964/23069 ( 87%)], Train Loss: 0.12621\n","Epoch: 01 [20004/23069 ( 87%)], Train Loss: 0.12629\n","Epoch: 01 [20044/23069 ( 87%)], Train Loss: 0.12619\n","Epoch: 01 [20084/23069 ( 87%)], Train Loss: 0.12620\n","Epoch: 01 [20124/23069 ( 87%)], Train Loss: 0.12615\n","Epoch: 01 [20164/23069 ( 87%)], Train Loss: 0.12628\n","Epoch: 01 [20204/23069 ( 88%)], Train Loss: 0.12623\n","Epoch: 01 [20244/23069 ( 88%)], Train Loss: 0.12611\n","Epoch: 01 [20284/23069 ( 88%)], Train Loss: 0.12608\n","Epoch: 01 [20324/23069 ( 88%)], Train Loss: 0.12616\n","Epoch: 01 [20364/23069 ( 88%)], Train Loss: 0.12610\n","Epoch: 01 [20404/23069 ( 88%)], Train Loss: 0.12592\n","Epoch: 01 [20444/23069 ( 89%)], Train Loss: 0.12594\n","Epoch: 01 [20484/23069 ( 89%)], Train Loss: 0.12589\n","Epoch: 01 [20524/23069 ( 89%)], Train Loss: 0.12589\n","Epoch: 01 [20564/23069 ( 89%)], Train Loss: 0.12590\n","Epoch: 01 [20604/23069 ( 89%)], Train Loss: 0.12586\n","Epoch: 01 [20644/23069 ( 89%)], Train Loss: 0.12574\n","Epoch: 01 [20684/23069 ( 90%)], Train Loss: 0.12555\n","Epoch: 01 [20724/23069 ( 90%)], Train Loss: 0.12540\n","Epoch: 01 [20764/23069 ( 90%)], Train Loss: 0.12520\n","Epoch: 01 [20804/23069 ( 90%)], Train Loss: 0.12515\n","Epoch: 01 [20844/23069 ( 90%)], Train Loss: 0.12504\n","Epoch: 01 [20884/23069 ( 91%)], Train Loss: 0.12491\n","Epoch: 01 [20924/23069 ( 91%)], Train Loss: 0.12489\n","Epoch: 01 [20964/23069 ( 91%)], Train Loss: 0.12481\n","Epoch: 01 [21004/23069 ( 91%)], Train Loss: 0.12468\n","Epoch: 01 [21044/23069 ( 91%)], Train Loss: 0.12458\n","Epoch: 01 [21084/23069 ( 91%)], Train Loss: 0.12453\n","Epoch: 01 [21124/23069 ( 92%)], Train Loss: 0.12449\n","Epoch: 01 [21164/23069 ( 92%)], Train Loss: 0.12452\n","Epoch: 01 [21204/23069 ( 92%)], Train Loss: 0.12444\n","Epoch: 01 [21244/23069 ( 92%)], Train Loss: 0.12433\n","Epoch: 01 [21284/23069 ( 92%)], Train Loss: 0.12420\n","Epoch: 01 [21324/23069 ( 92%)], Train Loss: 0.12400\n","Epoch: 01 [21364/23069 ( 93%)], Train Loss: 0.12394\n","Epoch: 01 [21404/23069 ( 93%)], Train Loss: 0.12404\n","Epoch: 01 [21444/23069 ( 93%)], Train Loss: 0.12395\n","Epoch: 01 [21484/23069 ( 93%)], Train Loss: 0.12390\n","Epoch: 01 [21524/23069 ( 93%)], Train Loss: 0.12376\n","Epoch: 01 [21564/23069 ( 93%)], Train Loss: 0.12365\n","Epoch: 01 [21604/23069 ( 94%)], Train Loss: 0.12351\n","Epoch: 01 [21644/23069 ( 94%)], Train Loss: 0.12342\n","Epoch: 01 [21684/23069 ( 94%)], Train Loss: 0.12346\n","Epoch: 01 [21724/23069 ( 94%)], Train Loss: 0.12345\n","Epoch: 01 [21764/23069 ( 94%)], Train Loss: 0.12327\n","Epoch: 01 [21804/23069 ( 95%)], Train Loss: 0.12318\n","Epoch: 01 [21844/23069 ( 95%)], Train Loss: 0.12307\n","Epoch: 01 [21884/23069 ( 95%)], Train Loss: 0.12309\n","Epoch: 01 [21924/23069 ( 95%)], Train Loss: 0.12300\n","Epoch: 01 [21964/23069 ( 95%)], Train Loss: 0.12303\n","Epoch: 01 [22004/23069 ( 95%)], Train Loss: 0.12291\n","Epoch: 01 [22044/23069 ( 96%)], Train Loss: 0.12277\n","Epoch: 01 [22084/23069 ( 96%)], Train Loss: 0.12271\n","Epoch: 01 [22124/23069 ( 96%)], Train Loss: 0.12252\n","Epoch: 01 [22164/23069 ( 96%)], Train Loss: 0.12242\n","Epoch: 01 [22204/23069 ( 96%)], Train Loss: 0.12245\n","Epoch: 01 [22244/23069 ( 96%)], Train Loss: 0.12234\n","Epoch: 01 [22284/23069 ( 97%)], Train Loss: 0.12238\n","Epoch: 01 [22324/23069 ( 97%)], Train Loss: 0.12231\n","Epoch: 01 [22364/23069 ( 97%)], Train Loss: 0.12234\n","Epoch: 01 [22404/23069 ( 97%)], Train Loss: 0.12233\n","Epoch: 01 [22444/23069 ( 97%)], Train Loss: 0.12226\n","Epoch: 01 [22484/23069 ( 97%)], Train Loss: 0.12212\n","Epoch: 01 [22524/23069 ( 98%)], Train Loss: 0.12200\n","Epoch: 01 [22564/23069 ( 98%)], Train Loss: 0.12185\n","Epoch: 01 [22604/23069 ( 98%)], Train Loss: 0.12179\n","Epoch: 01 [22644/23069 ( 98%)], Train Loss: 0.12186\n","Epoch: 01 [22684/23069 ( 98%)], Train Loss: 0.12174\n","Epoch: 01 [22724/23069 ( 99%)], Train Loss: 0.12167\n","Epoch: 01 [22764/23069 ( 99%)], Train Loss: 0.12167\n","Epoch: 01 [22804/23069 ( 99%)], Train Loss: 0.12153\n","Epoch: 01 [22844/23069 ( 99%)], Train Loss: 0.12156\n","Epoch: 01 [22884/23069 ( 99%)], Train Loss: 0.12145\n","Epoch: 01 [22924/23069 ( 99%)], Train Loss: 0.12132\n","Epoch: 01 [22964/23069 (100%)], Train Loss: 0.12124\n","Epoch: 01 [23004/23069 (100%)], Train Loss: 0.12108\n","Epoch: 01 [23044/23069 (100%)], Train Loss: 0.12110\n","Epoch: 01 [23069/23069 (100%)], Train Loss: 0.12099\n","----Validation Results Summary----\n","Epoch: [1] Valid Loss: 0.00000\n","\n","Total Training Time: 6415.890786409378secs, Average Training Time per Epoch: 3207.945393204689secs.\n","Total Validation Time: 0.8882436752319336secs, Average Validation Time per Epoch: 0.4441218376159668secs.\n"]}]},{"cell_type":"code","metadata":{"id":"DkjRIhdbwjHx","execution":{"iopub.status.busy":"2021-08-17T20:36:49.722586Z","iopub.execute_input":"2021-08-17T20:36:49.723011Z","iopub.status.idle":"2021-08-17T20:36:49.727659Z","shell.execute_reply.started":"2021-08-17T20:36:49.722978Z","shell.execute_reply":"2021-08-17T20:36:49.726609Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1634474787109,"user_tz":-540,"elapsed":5,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["# example for training second fold\n","\n","# for fold in range(1, 2):\n","#     print();print()\n","#     print('-'*50)\n","#     print(f'FOLD: {fold}')\n","#     print('-'*50)\n","#     run(train, fold)"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7uIwhUECP4iP"},"source":["### Thanks and please do Upvote!"]}]}