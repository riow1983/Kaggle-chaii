{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"name":"chaii-qa-5-fold-xlmroberta-torch-fit.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"1ed761b35d894a36a13c1986f9d837a3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_1e1ecde8ac064c57ac087b29b06fb979","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_8f2bd6a7231040968d8bdedecde490f3","IPY_MODEL_f29dd3deb91f409591ec9e5aa0dba32c","IPY_MODEL_e56d0468a5b24381add3b36cd9ae92aa"]}},"1e1ecde8ac064c57ac087b29b06fb979":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8f2bd6a7231040968d8bdedecde490f3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0c88ec9eb5b7460ca3a98e28c5b81cce","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_75f284b37dc84699be5c3d0aa6e13829"}},"f29dd3deb91f409591ec9e5aa0dba32c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_a3c36816782e4d88b451a45475beaa2f","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":606,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":606,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_cab351b3db6e477e804fd7ded192504c"}},"e56d0468a5b24381add3b36cd9ae92aa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4742a4c2606442ec8006cb006b64c16a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 606/606 [00:00&lt;00:00, 23.1kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fdcc7b8d44b047a2ac009c676be81cd5"}},"0c88ec9eb5b7460ca3a98e28c5b81cce":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"75f284b37dc84699be5c3d0aa6e13829":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a3c36816782e4d88b451a45475beaa2f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"cab351b3db6e477e804fd7ded192504c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4742a4c2606442ec8006cb006b64c16a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"fdcc7b8d44b047a2ac009c676be81cd5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9540e562331a4998a283cf307cfebaf5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_5eab18ad529a40579e05d6e2962e38e9","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4507666b269b412b8d2ee8c5fcc1867d","IPY_MODEL_18b3ba1f830f42f68dd5b7fbf01c4b4a","IPY_MODEL_f38f5aa16dbd499d9f916fc1df444f3b"]}},"5eab18ad529a40579e05d6e2962e38e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4507666b269b412b8d2ee8c5fcc1867d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_114aac99910943bf9fc1e91eb3c0d445","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_cd72891e782f47c0ada163ddad2ff643"}},"18b3ba1f830f42f68dd5b7fbf01c4b4a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_1579d6dc429f46dbba3b880e1f932f6a","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":179,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":179,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0d7853852cd84783adb9d2bbe53e4975"}},"f38f5aa16dbd499d9f916fc1df444f3b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4953261ee00049c9b46836dc1974f09a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 179/179 [00:00&lt;00:00, 6.45kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6cbd253c4d144bcea0217711da6f5b3e"}},"114aac99910943bf9fc1e91eb3c0d445":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"cd72891e782f47c0ada163ddad2ff643":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1579d6dc429f46dbba3b880e1f932f6a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0d7853852cd84783adb9d2bbe53e4975":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4953261ee00049c9b46836dc1974f09a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6cbd253c4d144bcea0217711da6f5b3e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c67f52a3927044a9aaf51c68fe3719e4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_1a89cf47828e40e09eef5c45dffbc714","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_31bdba3c546241a0ae5195eebaa4c794","IPY_MODEL_033b9dc011d349dcb88a935f9f8272d0","IPY_MODEL_f9355da31910408ca235969111c560fb"]}},"1a89cf47828e40e09eef5c45dffbc714":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"31bdba3c546241a0ae5195eebaa4c794":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_246dcbdfd95a4700bbdc076043c60d46","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5273bde35b274c21bfad5f00c1655783"}},"033b9dc011d349dcb88a935f9f8272d0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_f569cc05edad456d84ff74e0dc76baab","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":5069051,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":5069051,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b9e623d28a21481bbd96254d1a58eb14"}},"f9355da31910408ca235969111c560fb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_67873897abe04b8e9b206e0654925b78","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 4.83M/4.83M [00:01&lt;00:00, 4.93MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d984a8ebb3264584b51df7844e871948"}},"246dcbdfd95a4700bbdc076043c60d46":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5273bde35b274c21bfad5f00c1655783":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f569cc05edad456d84ff74e0dc76baab":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"b9e623d28a21481bbd96254d1a58eb14":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"67873897abe04b8e9b206e0654925b78":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d984a8ebb3264584b51df7844e871948":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3e618e44d092437c87ba6de016c3f718":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d7e48be142274ea1abaefb57b6dc4a5d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_0c051f29b1384525b9faa022a6d27985","IPY_MODEL_ccb769c80c4b4ca789d26535416f88d4","IPY_MODEL_59d5054f1b164cdc9f9893caf0cb22b0"]}},"d7e48be142274ea1abaefb57b6dc4a5d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0c051f29b1384525b9faa022a6d27985":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b4e79f62594f4864bb143ddbb2b5b99d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7870f91887d64f46a08250b4fa9ada9f"}},"ccb769c80c4b4ca789d26535416f88d4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_3acf9ba8808941f1907f460e338c289b","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":150,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":150,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_582d6f7f4cf54d4a80967d07da1d4211"}},"59d5054f1b164cdc9f9893caf0cb22b0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c9079ed3fb4c4dc9920dc60bb059d308","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 150/150 [00:00&lt;00:00, 6.26kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_290f5e7b64514ebfa57558bc32d8aac4"}},"b4e79f62594f4864bb143ddbb2b5b99d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7870f91887d64f46a08250b4fa9ada9f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3acf9ba8808941f1907f460e338c289b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"582d6f7f4cf54d4a80967d07da1d4211":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c9079ed3fb4c4dc9920dc60bb059d308":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"290f5e7b64514ebfa57558bc32d8aac4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4f399f2a8c574689981ff074ec39bb38":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a7549a037ce2468f95f5364bb36dfa6c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_2629affb557744cfbac1a9b9c7f19fc5","IPY_MODEL_007f2dfb6aa946af83d44f97c7551df7","IPY_MODEL_78a20a72bca349bc9b3e977ff96f1519"]}},"a7549a037ce2468f95f5364bb36dfa6c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2629affb557744cfbac1a9b9c7f19fc5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1df40521bc5d465681b3ab61ec9a1d1c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d27f09d18b85487cb6cf5bb8270f2d19"}},"007f2dfb6aa946af83d44f97c7551df7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5866ae6a39fa46fc8a3044937a976d15","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":2239666418,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":2239666418,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e2fdce5401134b458f901a7b90349bd6"}},"78a20a72bca349bc9b3e977ff96f1519":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_01e6a4ce3a994f318577a02090ec9054","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 2.09G/2.09G [00:40&lt;00:00, 37.9MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e58188acc732452c8a11ff3a0ca7a02c"}},"1df40521bc5d465681b3ab61ec9a1d1c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d27f09d18b85487cb6cf5bb8270f2d19":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5866ae6a39fa46fc8a3044937a976d15":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e2fdce5401134b458f901a7b90349bd6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"01e6a4ce3a994f318577a02090ec9054":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e58188acc732452c8a11ff3a0ca7a02c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RkuY4GVbQ1k4","executionInfo":{"status":"ok","timestamp":1634288930981,"user_tz":-540,"elapsed":1325,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"1f3ab441-4eb6-4f37-c5e7-f4df4c9333d7"},"source":["# Kaggle or Colab\n","import sys\n","import os\n","if 'kaggle_web_client' in sys.modules:\n","    # Do something\n","    pass\n","elif 'google.colab' in sys.modules:\n","    # Do something\n","    from google.colab import drive\n","    drive.mount(\"/content/drive\")\n","\n","    comp_name_official = \"chaii-hindi-and-tamil-question-answering\"\n","    comp_name_local = \"Kaggle-chaii\"\n","\n","    !pip install --upgrade --force-reinstall --no-deps kaggle\n","    import json\n","    f = open(\"/content/drive/MyDrive/colab_notebooks/kaggle/kaggle.json\", \"r\")\n","    json_data = json.load(f)\n","    os.environ[\"KAGGLE_USERNAME\"] = json_data[\"username\"]\n","    os.environ[\"KAGGLE_KEY\"] = json_data[\"key\"]\n","\n","    %cd /content/drive/MyDrive/colab_notebooks/kaggle/{comp_name_local}/notebooks\n","\n","    dname = \"chaii-qa-5-fold-xlmroberta-torch-fit\"\n","    !mkdir {dname}\n","    #%cd /content/drive/MyDrive/colab_notebooks/kaggle/{comp_name_local}/notebooks/{dname}"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Collecting kaggle\n","  Using cached kaggle-1.5.12-py3-none-any.whl\n","Installing collected packages: kaggle\n","  Attempting uninstall: kaggle\n","    Found existing installation: kaggle 1.5.12\n","    Uninstalling kaggle-1.5.12:\n","      Successfully uninstalled kaggle-1.5.12\n","Successfully installed kaggle-1.5.12\n","/content/drive/MyDrive/colab_notebooks/kaggle/Kaggle-chaii/notebooks\n","mkdir: cannot create directory ‘chaii-qa-5-fold-xlmroberta-torch-fit’: File exists\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a0xZ62k7SB1g","executionInfo":{"status":"ok","timestamp":1634288933630,"user_tz":-540,"elapsed":2654,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"424c077b-b950-440b-d7b2-b0918c69a7b5"},"source":["if 'kaggle_web_client' in sys.modules:\n","    # Do something\n","    pass\n","elif 'google.colab' in sys.modules:\n","    #!pip install transformers\n","    !pip install transformers[sentencepiece]\n","\n","    # import yaml\n","    # with open(f'./config_notebook/config.yaml') as file:\n","    #     cfg = yaml.load(file, Loader=yaml.FullLoader) # Loader is recommended\n","    # print(\"Config:\\n\", cfg)"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.7/dist-packages (4.11.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.3.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (4.8.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (6.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (0.0.46)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (1.19.5)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2.23.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (21.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (4.62.3)\n","Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (0.0.19)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (0.10.3)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.17.3)\n","Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (0.1.96)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers[sentencepiece]) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers[sentencepiece]) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers[sentencepiece]) (3.6.0)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers[sentencepiece]) (1.15.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (2.10)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[sentencepiece]) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[sentencepiece]) (7.1.2)\n"]}]},{"cell_type":"markdown","metadata":{"id":"uQdNDkrBP4hj"},"source":["<h2>chaii QA - 5 Fold XLMRoberta Finetuning in Torch w/o Trainer API</h2>\n","    \n","<h3><span \"style: color=#444\">Introduction</span></h3>\n","\n","The kernel implements 5-Fold XLMRoberta QA Model without using the Trainer API from HuggingFace.\n","\n","This is a three part kernel,\n","\n","- [External Data - MLQA, XQUAD Preprocessing](https://www.kaggle.com/rhtsingh/external-data-mlqa-xquad-preprocessing) which preprocesses the Hindi Corpus of MLQA and XQUAD. I have used these data for training.\n","\n","- [chaii QA - 5 Fold XLMRoberta Torch | FIT](https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-fit/edit) This kernel is the current kernel and used for Finetuning (FIT) on competition + external data.\n","\n","- [chaii QA - 5 Fold XLMRoberta Torch | Infer](https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-infer) The Inference kernel where we ensemble our 5 Fold XLMRoberta Models and do the submission.\n","\n","<h3><span \"style: color=#444\">Techniques</span></h3>\n","\n","The kernel has implementation for below techniques, click on the links to learn more -\n","\n"," - [External Data Preprocessing + Training](https://www.kaggle.com/rhtsingh/external-data-mlqa-xquad-preprocessing)\n"," \n"," - [Mixed Precision Training using APEX](https://www.kaggle.com/rhtsingh/swa-apex-amp-interpreting-transformers-in-torch)\n"," \n"," - [Gradient Accumulation](https://www.kaggle.com/rhtsingh/speeding-up-transformer-w-optimization-strategies)\n"," \n"," - [Gradient Clipping](https://www.kaggle.com/rhtsingh/swa-apex-amp-interpreting-transformers-in-torch)\n"," \n"," - [Grouped Layerwise Learning Rate Decay](https://www.kaggle.com/rhtsingh/guide-to-huggingface-schedulers-differential-lrs)\n"," \n"," - [Utilizing Intermediate Transformer Representations](https://www.kaggle.com/rhtsingh/utilizing-transformer-representations-efficiently)\n"," \n"," - [Layer Initialization](https://www.kaggle.com/rhtsingh/on-stability-of-few-sample-transformer-fine-tuning)\n"," \n"," - [5-Fold Training](https://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-fit)\n"," \n"," - etc.\n"," \n","<h3><span \"style: color=#444\">References</span></h3>\n","I would like to acknowledge below kagglers work and resources without whom this kernel wouldn't have been possible,  \n","\n","- [roberta inference 5 folds by Abhishek](https://www.kaggle.com/abhishek/roberta-inference-5-folds)\n","\n","- [ChAII - EDA & Baseline by Darek](https://www.kaggle.com/thedrcat/chaii-eda-baseline) due to whom i found the great notebook below from HuggingFace,\n","\n","- [How to fine-tune a model on question answering](https://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb)\n","\n","- [HuggingFace QA Examples](https://github.com/huggingface/transformers/tree/master/examples/pytorch/question-answering)\n","\n","- [TensorFlow 2.0 Question Answering - Collections of gold solutions](https://www.kaggle.com/c/tensorflow2-question-answering/discussion/127409) these solutions are gold and highly recommend everyone to give a detailed read."]},{"cell_type":"markdown","metadata":{"id":"vI-eiJRPP4hw"},"source":["<h3><span style=\"color=#444\">Note</span></h3>\n","\n","The below points are worth noting,\n","\n"," - I haven't used FP16 because due to some reason this fails and model never starts training.\n"," - These are the original hyperparamters and setting that I have used for training my models.\n"," - I tried few pooling layers but none of them performed better than simple one.\n"," - Gradient clipping reduces model performance.\n"," - <span style=\"color:#2E86C1\">Training 5 Folds at once will give OOM issue on Kaggle and Colab. I have trained one fold at a time i.e. After 1st fold is completed I save the model as a dataset then train second fold. Repeat this process until all 5 folds are completed. Training each fold takes around 50 minuts on Kaggle.</span>\n"," - <span style=\"color:#2E86C1\">I have modified the preprocessing code a lot from original used in HuggingFace notebook since I am not using HuggingFace Datasets API as well. So I had to handle the sequence-ids specially since they contain None values which torch doesn't support.</span>"]},{"cell_type":"markdown","metadata":{"id":"8ykS_qCDP4hz"},"source":["### Install APEX"]},{"cell_type":"code","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","id":"aCY6yvR6ET3s","execution":{"iopub.status.busy":"2021-08-17T20:22:25.164246Z","iopub.execute_input":"2021-08-17T20:22:25.164729Z","iopub.status.idle":"2021-08-17T20:22:25.170948Z","shell.execute_reply.started":"2021-08-17T20:22:25.164627Z","shell.execute_reply":"2021-08-17T20:22:25.169352Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1634288933631,"user_tz":-540,"elapsed":14,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["# %%writefile setup.sh\n","# export CUDA_HOME=/usr/local/cuda-10.1\n","# git clone https://github.com/NVIDIA/apex\n","# cd apex\n","# pip install -v --disable-pip-version-check --no-cache-dir ./"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"-l2Jsav9ET3v","execution":{"iopub.status.busy":"2021-08-17T20:22:26.935257Z","iopub.execute_input":"2021-08-17T20:22:26.935919Z","iopub.status.idle":"2021-08-17T20:22:26.939334Z","shell.execute_reply.started":"2021-08-17T20:22:26.935881Z","shell.execute_reply":"2021-08-17T20:22:26.938488Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1634288933633,"user_tz":-540,"elapsed":14,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["# %%capture\n","# !sh setup.sh"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xFbbPlPgP4h7"},"source":["### Import Dependencies"]},{"cell_type":"code","metadata":{"id":"E4l6PirHET3x","execution":{"iopub.status.busy":"2021-08-17T20:23:50.232396Z","iopub.execute_input":"2021-08-17T20:23:50.232892Z","iopub.status.idle":"2021-08-17T20:24:01.076652Z","shell.execute_reply.started":"2021-08-17T20:23:50.232861Z","shell.execute_reply":"2021-08-17T20:24:01.075658Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634288936315,"user_tz":-540,"elapsed":2693,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"3054e035-dba6-46d0-c60f-e49a0305be58"},"source":["#import os\n","import gc\n","gc.enable()\n","import math\n","#mport json\n","import time\n","import random\n","import multiprocessing\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=UserWarning)\n","\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm, trange\n","from sklearn import model_selection\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn import Parameter\n","import torch.optim as optim\n","from torch.utils.data import (\n","    Dataset, DataLoader,\n","    SequentialSampler, RandomSampler\n",")\n","from torch.utils.data.distributed import DistributedSampler\n","\n","try:\n","    from apex import amp\n","    APEX_INSTALLED = True\n","except ImportError:\n","    APEX_INSTALLED = False\n","\n","import transformers\n","from transformers import (\n","    WEIGHTS_NAME,\n","    AdamW,\n","    AutoConfig,\n","    AutoModel,\n","    AutoTokenizer,\n","    get_cosine_schedule_with_warmup,\n","    get_linear_schedule_with_warmup,\n","    logging,\n","    MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n",")\n","logging.set_verbosity_warning()\n","logging.set_verbosity_error()\n","\n","def fix_all_seeds(seed):\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","def optimal_num_of_loader_workers():\n","    num_cpus = multiprocessing.cpu_count()\n","    num_gpus = torch.cuda.device_count()\n","    optimal_value = min(num_cpus, num_gpus*4) if num_gpus else num_cpus - 1\n","    return optimal_value\n","\n","print(f\"Apex AMP Installed :: {APEX_INSTALLED}\")\n","MODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())\n","MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Apex AMP Installed :: False\n"]}]},{"cell_type":"markdown","metadata":{"id":"y0OvcnMaP4h9"},"source":["### Training Configuration"]},{"cell_type":"code","metadata":{"id":"LUx5XplNET3y","execution":{"iopub.status.busy":"2021-08-17T20:24:13.391322Z","iopub.execute_input":"2021-08-17T20:24:13.391696Z","iopub.status.idle":"2021-08-17T20:24:13.398397Z","shell.execute_reply.started":"2021-08-17T20:24:13.391662Z","shell.execute_reply":"2021-08-17T20:24:13.39732Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1634288936316,"user_tz":-540,"elapsed":13,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["class Config:\n","    # model\n","    model_type = 'xlm_roberta'\n","    model_name_or_path = \"deepset/xlm-roberta-large-squad2\"\n","    config_name = \"deepset/xlm-roberta-large-squad2\"\n","    fp16 = True if APEX_INSTALLED else False\n","    fp16_opt_level = \"O1\"\n","    gradient_accumulation_steps = 2\n","\n","    # tokenizer\n","    tokenizer_name = \"deepset/xlm-roberta-large-squad2\"\n","    max_seq_length = 384 #512 #384\n","    doc_stride = 128 #80 #128\n","\n","    # train\n","    epochs = 1 #7 #1\n","    train_batch_size = 4 #2 #4\n","    eval_batch_size = 8 #4 #8\n","\n","    # optimizer\n","    optimizer_type = 'AdamW'\n","    learning_rate = 1.5e-5\n","    weight_decay = 1e-2\n","    epsilon = 1e-8\n","    max_grad_norm = 1.0\n","\n","    # scheduler\n","    decay_name = 'linear-warmup'\n","    warmup_ratio = 0.1\n","\n","    # logging\n","    logging_steps = 10\n","\n","    # evaluate\n","    output_dir = dname #'output'\n","    seed = 2021"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HYno9o1OP4h-"},"source":["### Data Factory"]},{"cell_type":"code","metadata":{"id":"X_eRZQrzET3z","execution":{"iopub.status.busy":"2021-08-17T20:24:26.939105Z","iopub.execute_input":"2021-08-17T20:24:26.939437Z","iopub.status.idle":"2021-08-17T20:24:27.776039Z","shell.execute_reply.started":"2021-08-17T20:24:26.939409Z","shell.execute_reply":"2021-08-17T20:24:27.775204Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1634288936717,"user_tz":-540,"elapsed":413,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["train = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/train.csv')\n","test = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/test.csv')\n","external_mlqa = pd.read_csv('../input/mlqa-hindi-processed/mlqa_hindi.csv')\n","external_xquad = pd.read_csv('../input/mlqa-hindi-processed/xquad.csv')\n","external_train = pd.concat([external_mlqa, external_xquad])\n","\n","def create_folds(data, num_splits):\n","    data[\"kfold\"] = -1\n","    kf = model_selection.StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=2021)\n","    for f, (t_, v_) in enumerate(kf.split(X=data, y=data['language'])):\n","        data.loc[v_, 'kfold'] = f\n","    return data\n","\n","#### RIOW\n","# train = create_folds(train, num_splits=5)\n","# external_train[\"kfold\"] = -1\n","# external_train['id'] = list(np.arange(1, len(external_train)+1))\n","# train = pd.concat([train, external_train]).reset_index(drop=True)\n","\n","external_train['id'] = list(np.arange(1, len(external_train)+1))\n","train = pd.concat([train, external_train]).reset_index(drop=True)\n","train = create_folds(train, num_splits=5)\n","#### RIOWRIOW\n","\n","def convert_answers(row):\n","    return {'answer_start': [row[0]], 'text': [row[1]]}\n","\n","train['answers'] = train[['answer_start', 'answer_text']].apply(convert_answers, axis=1)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dAoAJUVaP4iA"},"source":["### Covert Examples to Features (Preprocess)"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-12T15:50:26.947214Z","iopub.execute_input":"2021-08-12T15:50:26.947589Z","iopub.status.idle":"2021-08-12T15:50:26.960916Z","shell.execute_reply.started":"2021-08-12T15:50:26.947551Z","shell.execute_reply":"2021-08-12T15:50:26.959064Z"},"id":"dxbZdct1ET3z","trusted":true,"executionInfo":{"status":"ok","timestamp":1634288936718,"user_tz":-540,"elapsed":16,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["def prepare_train_features(args, example, tokenizer):\n","    example[\"question\"] = example[\"question\"].lstrip()\n","    tokenized_example = tokenizer(\n","        example[\"question\"],\n","        example[\"context\"],\n","        truncation=\"only_second\",\n","        max_length=args.max_seq_length,\n","        stride=args.doc_stride,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True,\n","        padding=\"max_length\",\n","    )\n","\n","    sample_mapping = tokenized_example.pop(\"overflow_to_sample_mapping\")\n","    offset_mapping = tokenized_example.pop(\"offset_mapping\")\n","\n","    features = []\n","    for i, offsets in enumerate(offset_mapping):\n","        feature = {}\n","\n","        input_ids = tokenized_example[\"input_ids\"][i]\n","        attention_mask = tokenized_example[\"attention_mask\"][i]\n","\n","        feature['input_ids'] = input_ids\n","        feature['attention_mask'] = attention_mask\n","        feature['offset_mapping'] = offsets\n","\n","        cls_index = input_ids.index(tokenizer.cls_token_id)\n","        sequence_ids = tokenized_example.sequence_ids(i)\n","\n","        sample_index = sample_mapping[i]\n","        answers = example[\"answers\"]\n","\n","        if len(answers[\"answer_start\"]) == 0:\n","            feature[\"start_position\"] = cls_index\n","            feature[\"end_position\"] = cls_index\n","        else:\n","            start_char = answers[\"answer_start\"][0]\n","            end_char = start_char + len(answers[\"text\"][0])\n","\n","            token_start_index = 0\n","            while sequence_ids[token_start_index] != 1:\n","                token_start_index += 1\n","\n","            token_end_index = len(input_ids) - 1\n","            while sequence_ids[token_end_index] != 1:\n","                token_end_index -= 1\n","\n","            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n","                feature[\"start_position\"] = cls_index\n","                feature[\"end_position\"] = cls_index\n","            else:\n","                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n","                    token_start_index += 1\n","                feature[\"start_position\"] = token_start_index - 1\n","                while offsets[token_end_index][1] >= end_char:\n","                    token_end_index -= 1\n","                feature[\"end_position\"] = token_end_index + 1\n","\n","        features.append(feature)\n","    return features"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a3SIS_xAP4iC"},"source":["### Dataset Retriever"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-12T15:50:26.962738Z","iopub.execute_input":"2021-08-12T15:50:26.963118Z","iopub.status.idle":"2021-08-12T15:50:26.97542Z","shell.execute_reply.started":"2021-08-12T15:50:26.963075Z","shell.execute_reply":"2021-08-12T15:50:26.974431Z"},"id":"6TuzHdjmET30","trusted":true,"executionInfo":{"status":"ok","timestamp":1634288936718,"user_tz":-540,"elapsed":16,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["class DatasetRetriever(Dataset):\n","    def __init__(self, features, mode='train'):\n","        super(DatasetRetriever, self).__init__()\n","        self.features = features\n","        self.mode = mode\n","        \n","    def __len__(self):\n","        return len(self.features)\n","    \n","    def __getitem__(self, item):   \n","        feature = self.features[item]\n","        if self.mode == 'train':\n","            return {\n","                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n","                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n","                'offset_mapping':torch.tensor(feature['offset_mapping'], dtype=torch.long),\n","                'start_position':torch.tensor(feature['start_position'], dtype=torch.long),\n","                'end_position':torch.tensor(feature['end_position'], dtype=torch.long)\n","            }\n","        else:\n","            return {\n","                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n","                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n","                'offset_mapping':feature['offset_mapping'],\n","                'sequence_ids':feature['sequence_ids'],\n","                'id':feature['example_id'],\n","                'context': feature['context'],\n","                'question': feature['question']\n","            }"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tpH-2nPqP4iE"},"source":["### Model"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-12T15:50:26.976977Z","iopub.execute_input":"2021-08-12T15:50:26.977627Z","iopub.status.idle":"2021-08-12T15:50:26.990227Z","shell.execute_reply.started":"2021-08-12T15:50:26.97747Z","shell.execute_reply":"2021-08-12T15:50:26.989443Z"},"id":"9OxhKqxcET31","trusted":true,"executionInfo":{"status":"ok","timestamp":1634288936719,"user_tz":-540,"elapsed":15,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["class Model(nn.Module):\n","    def __init__(self, modelname_or_path, config):\n","        super(Model, self).__init__()\n","        self.config = config\n","        self.xlm_roberta = AutoModel.from_pretrained(modelname_or_path, config=config)\n","        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","        self._init_weights(self.qa_outputs)\n","        \n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","\n","    def forward(\n","        self, \n","        input_ids, \n","        attention_mask=None, \n","        # token_type_ids=None\n","    ):\n","        outputs = self.xlm_roberta(\n","            input_ids,\n","            attention_mask=attention_mask,\n","        )\n","\n","        sequence_output = outputs[0]\n","        pooled_output = outputs[1]\n","        \n","        # sequence_output = self.dropout(sequence_output)\n","        qa_logits = self.qa_outputs(sequence_output)\n","        \n","        start_logits, end_logits = qa_logits.split(1, dim=-1)\n","        start_logits = start_logits.squeeze(-1)\n","        end_logits = end_logits.squeeze(-1)\n","    \n","        return start_logits, end_logits"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xdMx6plqP4iG"},"source":["### Loss"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-12T15:50:26.991891Z","iopub.execute_input":"2021-08-12T15:50:26.99237Z","iopub.status.idle":"2021-08-12T15:50:27.000188Z","shell.execute_reply.started":"2021-08-12T15:50:26.992334Z","shell.execute_reply":"2021-08-12T15:50:26.999374Z"},"id":"SxuNrJqqET32","trusted":true,"executionInfo":{"status":"ok","timestamp":1634288936720,"user_tz":-540,"elapsed":16,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["def loss_fn(preds, labels):\n","    start_preds, end_preds = preds\n","    start_labels, end_labels = labels\n","    \n","    start_loss = nn.CrossEntropyLoss(ignore_index=-1)(start_preds, start_labels)\n","    end_loss = nn.CrossEntropyLoss(ignore_index=-1)(end_preds, end_labels)\n","    total_loss = (start_loss + end_loss) / 2\n","    return total_loss"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jUMtX08cP4iH"},"source":["### Grouped Layerwise Learning Rate Decay"]},{"cell_type":"code","metadata":{"id":"vf6HVcu2ET34","execution":{"iopub.status.busy":"2021-08-17T20:25:36.36033Z","iopub.execute_input":"2021-08-17T20:25:36.361058Z","iopub.status.idle":"2021-08-17T20:25:36.381524Z","shell.execute_reply.started":"2021-08-17T20:25:36.361009Z","shell.execute_reply":"2021-08-17T20:25:36.380328Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1634288936721,"user_tz":-540,"elapsed":16,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["def get_optimizer_grouped_parameters(args, model):\n","    no_decay = [\"bias\", \"LayerNorm.weight\"]\n","    group1=['layer.0.','layer.1.','layer.2.','layer.3.']\n","    group2=['layer.4.','layer.5.','layer.6.','layer.7.']    \n","    group3=['layer.8.','layer.9.','layer.10.','layer.11.']\n","    group_all=['layer.0.','layer.1.','layer.2.','layer.3.','layer.4.','layer.5.','layer.6.','layer.7.','layer.8.','layer.9.','layer.10.','layer.11.']\n","    optimizer_grouped_parameters = [\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': args.weight_decay},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': args.weight_decay, 'lr': args.learning_rate/2.6},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': args.weight_decay, 'lr': args.learning_rate},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': args.weight_decay, 'lr': args.learning_rate*2.6},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': 0.0},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': 0.0, 'lr': args.learning_rate/2.6},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': 0.0, 'lr': args.learning_rate},\n","        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': 0.0, 'lr': args.learning_rate*2.6},\n","        {'params': [p for n, p in model.named_parameters() if args.model_type not in n], 'lr':args.learning_rate*20, \"weight_decay\": 0.0},\n","    ]\n","    return optimizer_grouped_parameters"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oXz_0fIQP4iI"},"source":["### Metric Logger"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-12T15:50:27.021442Z","iopub.execute_input":"2021-08-12T15:50:27.021952Z","iopub.status.idle":"2021-08-12T15:50:27.03234Z","shell.execute_reply.started":"2021-08-12T15:50:27.021912Z","shell.execute_reply":"2021-08-12T15:50:27.03156Z"},"id":"bkFB-iMcET34","trusted":true,"executionInfo":{"status":"ok","timestamp":1634288936722,"user_tz":-540,"elapsed":16,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["class AverageMeter(object):\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","        self.max = 0\n","        self.min = 1e5\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","        if val > self.max:\n","            self.max = val\n","        if val < self.min:\n","            self.min = val"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y946gQxtP4iJ"},"source":["### Utilities"]},{"cell_type":"code","metadata":{"id":"spFRutV0ET34","execution":{"iopub.status.busy":"2021-08-17T20:26:30.014223Z","iopub.execute_input":"2021-08-17T20:26:30.014623Z","iopub.status.idle":"2021-08-17T20:26:30.030566Z","shell.execute_reply.started":"2021-08-17T20:26:30.01459Z","shell.execute_reply":"2021-08-17T20:26:30.029723Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1634288936722,"user_tz":-540,"elapsed":16,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["def make_model(args):\n","    config = AutoConfig.from_pretrained(args.config_name)\n","    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name)\n","    model = Model(args.model_name_or_path, config=config)\n","    return config, tokenizer, model\n","\n","def make_optimizer(args, model):\n","    # optimizer_grouped_parameters = get_optimizer_grouped_parameters(args, model)\n","    no_decay = [\"bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\n","            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","            \"weight_decay\": args.weight_decay,\n","        },\n","        {\n","            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n","            \"weight_decay\": 0.0,\n","        },\n","    ]\n","    if args.optimizer_type == \"AdamW\":\n","        optimizer = AdamW(\n","            optimizer_grouped_parameters,\n","            lr=args.learning_rate,\n","            eps=args.epsilon,\n","            correct_bias=True\n","        )\n","        return optimizer\n","\n","def make_scheduler(\n","    args, optimizer, \n","    num_warmup_steps, \n","    num_training_steps\n","):\n","    if args.decay_name == \"cosine-warmup\":\n","        scheduler = get_cosine_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=num_warmup_steps,\n","            num_training_steps=num_training_steps\n","        )\n","    else:\n","        scheduler = get_linear_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=num_warmup_steps,\n","            num_training_steps=num_training_steps\n","        )\n","    return scheduler    \n","\n","def make_loader(\n","    args, data, \n","    tokenizer, fold\n","):\n","    train_set, valid_set = data[data['kfold']!=fold], data[data['kfold']==fold]\n","    \n","    train_features, valid_features = [[] for _ in range(2)]\n","    for i, row in train_set.iterrows():\n","        train_features += prepare_train_features(args, row, tokenizer)\n","    for i, row in valid_set.iterrows():\n","        valid_features += prepare_train_features(args, row, tokenizer)\n","\n","    train_dataset = DatasetRetriever(train_features)\n","    valid_dataset = DatasetRetriever(valid_features)\n","    print(f\"Num examples Train= {len(train_dataset)}, Num examples Valid={len(valid_dataset)}\")\n","    \n","    train_sampler = RandomSampler(train_dataset)\n","    valid_sampler = SequentialSampler(valid_dataset)\n","\n","    train_dataloader = DataLoader(\n","        train_dataset,\n","        batch_size=args.train_batch_size,\n","        sampler=train_sampler,\n","        num_workers=optimal_num_of_loader_workers(),\n","        pin_memory=True,\n","        drop_last=False \n","    )\n","\n","    valid_dataloader = DataLoader(\n","        valid_dataset,\n","        batch_size=args.eval_batch_size, \n","        sampler=valid_sampler,\n","        num_workers=optimal_num_of_loader_workers(),\n","        pin_memory=True, \n","        drop_last=False\n","    )\n","\n","    return train_dataloader, valid_dataloader"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i-zfYJGxP4iK"},"source":["### Trainer"]},{"cell_type":"code","metadata":{"id":"iFLvh1VQET35","execution":{"iopub.status.busy":"2021-08-17T20:26:36.310455Z","iopub.execute_input":"2021-08-17T20:26:36.310827Z","iopub.status.idle":"2021-08-17T20:26:36.325575Z","shell.execute_reply.started":"2021-08-17T20:26:36.310796Z","shell.execute_reply":"2021-08-17T20:26:36.324417Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1634288936723,"user_tz":-540,"elapsed":14,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["class Trainer:\n","    def __init__(\n","        self, model, tokenizer, \n","        optimizer, scheduler\n","    ):\n","        self.model = model\n","        self.tokenizer = tokenizer\n","        self.optimizer = optimizer\n","        self.scheduler = scheduler\n","\n","    def train(\n","        self, args, \n","        train_dataloader, \n","        epoch, result_dict\n","    ):\n","        count = 0\n","        losses = AverageMeter()\n","        \n","        self.model.zero_grad()\n","        self.model.train()\n","        \n","        fix_all_seeds(args.seed)\n","        \n","        for batch_idx, batch_data in enumerate(train_dataloader):\n","            input_ids, attention_mask, targets_start, targets_end = \\\n","                batch_data['input_ids'], batch_data['attention_mask'], \\\n","                    batch_data['start_position'], batch_data['end_position']\n","            \n","            input_ids, attention_mask, targets_start, targets_end = \\\n","                input_ids.cuda(), attention_mask.cuda(), targets_start.cuda(), targets_end.cuda()\n","\n","            outputs_start, outputs_end = self.model(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","            )\n","            \n","            loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n","            loss = loss / args.gradient_accumulation_steps\n","\n","            if args.fp16:\n","                with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n","                    scaled_loss.backward()\n","            else:\n","                loss.backward()\n","\n","            count += input_ids.size(0)\n","            losses.update(loss.item(), input_ids.size(0))\n","\n","            # if args.fp16:\n","            #     torch.nn.utils.clip_grad_norm_(amp.master_params(self.optimizer), args.max_grad_norm)\n","            # else:\n","            #     torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.max_grad_norm)\n","\n","            if batch_idx % args.gradient_accumulation_steps == 0 or batch_idx == len(train_dataloader) - 1:\n","                self.optimizer.step()\n","                self.scheduler.step()\n","                self.optimizer.zero_grad()\n","\n","            if (batch_idx % args.logging_steps == 0) or (batch_idx+1)==len(train_dataloader):\n","                _s = str(len(str(len(train_dataloader.sampler))))\n","                ret = [\n","                    ('Epoch: {:0>2} [{: >' + _s + '}/{} ({: >3.0f}%)]').format(epoch, count, len(train_dataloader.sampler), 100 * count / len(train_dataloader.sampler)),\n","                    'Train Loss: {: >4.5f}'.format(losses.avg),\n","                ]\n","                print(', '.join(ret))\n","\n","        result_dict['train_loss'].append(losses.avg)\n","        return result_dict"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7pmOF0dxP4iL"},"source":["### Evaluator"]},{"cell_type":"code","metadata":{"id":"1a8kG2UYET36","execution":{"iopub.status.busy":"2021-08-17T20:26:39.517516Z","iopub.execute_input":"2021-08-17T20:26:39.517886Z","iopub.status.idle":"2021-08-17T20:26:39.528591Z","shell.execute_reply.started":"2021-08-17T20:26:39.517856Z","shell.execute_reply":"2021-08-17T20:26:39.527569Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1634288937092,"user_tz":-540,"elapsed":382,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["class Evaluator:\n","    def __init__(self, model):\n","        self.model = model\n","    \n","    def save(self, result, output_dir):\n","        with open(f'{output_dir}/result_dict.json', 'w') as f:\n","            f.write(json.dumps(result, sort_keys=True, indent=4, ensure_ascii=False))\n","\n","    def evaluate(self, valid_dataloader, epoch, result_dict):\n","        losses = AverageMeter()\n","        for batch_idx, batch_data in enumerate(valid_dataloader):\n","            self.model = self.model.eval()\n","            input_ids, attention_mask, targets_start, targets_end = \\\n","                batch_data['input_ids'], batch_data['attention_mask'], \\\n","                    batch_data['start_position'], batch_data['end_position']\n","            \n","            input_ids, attention_mask, targets_start, targets_end = \\\n","                input_ids.cuda(), attention_mask.cuda(), targets_start.cuda(), targets_end.cuda()\n","            \n","            with torch.no_grad():            \n","                outputs_start, outputs_end = self.model(\n","                    input_ids=input_ids,\n","                    attention_mask=attention_mask,\n","                )\n","                \n","                loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n","                losses.update(loss.item(), input_ids.size(0))\n","                \n","        print('----Validation Results Summary----')\n","        print('Epoch: [{}] Valid Loss: {: >4.5f}'.format(epoch, losses.avg))\n","        result_dict['val_loss'].append(losses.avg)        \n","        return result_dict"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cAZPoRJeP4iM"},"source":["### Initialize Training"]},{"cell_type":"code","metadata":{"id":"v-gUDyq2ET37","execution":{"iopub.status.busy":"2021-08-17T20:26:44.41837Z","iopub.execute_input":"2021-08-17T20:26:44.418722Z","iopub.status.idle":"2021-08-17T20:26:44.428918Z","shell.execute_reply.started":"2021-08-17T20:26:44.418691Z","shell.execute_reply":"2021-08-17T20:26:44.428086Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1634288937093,"user_tz":-540,"elapsed":11,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["def init_training(args, data, fold):\n","    fix_all_seeds(args.seed)\n","    \n","    if not os.path.exists(args.output_dir):\n","        os.makedirs(args.output_dir)\n","    \n","    # model\n","    model_config, tokenizer, model = make_model(args)\n","    if torch.cuda.device_count() >= 1:\n","        print('Model pushed to {} GPU(s), type {}.'.format(\n","            torch.cuda.device_count(), \n","            torch.cuda.get_device_name(0))\n","        )\n","        model = model.cuda() \n","    else:\n","        raise ValueError('CPU training is not supported')\n","    \n","    # data loaders\n","    train_dataloader, valid_dataloader = make_loader(args, data, tokenizer, fold)\n","\n","    # optimizer\n","    optimizer = make_optimizer(args, model)\n","\n","    # scheduler\n","    num_training_steps = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps) * args.epochs\n","    if args.warmup_ratio > 0:\n","        num_warmup_steps = int(args.warmup_ratio * num_training_steps)\n","    else:\n","        num_warmup_steps = 0\n","    print(f\"Total Training Steps: {num_training_steps}, Total Warmup Steps: {num_warmup_steps}\")\n","    scheduler = make_scheduler(args, optimizer, num_warmup_steps, num_training_steps)\n","\n","    # mixed precision training with NVIDIA Apex\n","    if args.fp16:\n","        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n","    \n","    result_dict = {\n","        'epoch':[], \n","        'train_loss': [], \n","        'val_loss' : [], \n","        'best_val_loss': np.inf\n","    }\n","\n","    return (\n","        model, model_config, tokenizer, optimizer, scheduler, \n","        train_dataloader, valid_dataloader, result_dict\n","    )"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g6ZBX-5aP4iN"},"source":["### Run"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-08-12T15:50:27.097353Z","iopub.execute_input":"2021-08-12T15:50:27.097724Z","iopub.status.idle":"2021-08-12T15:50:27.112113Z","shell.execute_reply.started":"2021-08-12T15:50:27.097688Z","shell.execute_reply":"2021-08-12T15:50:27.111224Z"},"id":"39ei5Bm5ET37","trusted":true,"executionInfo":{"status":"ok","timestamp":1634288937094,"user_tz":-540,"elapsed":10,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["def run(data, fold):\n","    args = Config()\n","    model, model_config, tokenizer, optimizer, scheduler, train_dataloader, \\\n","        valid_dataloader, result_dict = init_training(args, data, fold)\n","    \n","    trainer = Trainer(model, tokenizer, optimizer, scheduler)\n","    evaluator = Evaluator(model)\n","\n","    train_time_list = []\n","    valid_time_list = []\n","\n","    for epoch in range(args.epochs):\n","        result_dict['epoch'].append(epoch)\n","\n","        # Train\n","        torch.cuda.synchronize()\n","        tic1 = time.time()\n","        result_dict = trainer.train(\n","            args, train_dataloader, \n","            epoch, result_dict\n","        )\n","        torch.cuda.synchronize()\n","        tic2 = time.time() \n","        train_time_list.append(tic2 - tic1)\n","        \n","        # Evaluate\n","        torch.cuda.synchronize()\n","        tic3 = time.time()\n","        result_dict = evaluator.evaluate(\n","            valid_dataloader, epoch, result_dict\n","        )\n","        torch.cuda.synchronize()\n","        tic4 = time.time() \n","        valid_time_list.append(tic4 - tic3)\n","            \n","        output_dir = os.path.join(args.output_dir, f\"checkpoint-fold-{fold}\")\n","        if result_dict['val_loss'][-1] < result_dict['best_val_loss']:\n","            print(\"{} Epoch, Best epoch was updated! Valid Loss: {: >4.5f}\".format(epoch, result_dict['val_loss'][-1]))\n","            result_dict[\"best_val_loss\"] = result_dict['val_loss'][-1]        \n","            \n","            os.makedirs(output_dir, exist_ok=True)\n","            torch.save(model.state_dict(), f\"{output_dir}/pytorch_model.bin\")\n","            model_config.save_pretrained(output_dir)\n","            tokenizer.save_pretrained(output_dir)\n","            print(f\"Saving model checkpoint to {output_dir}.\")\n","            \n","        print()\n","\n","    evaluator.save(result_dict, output_dir)\n","    \n","    print(f\"Total Training Time: {np.sum(train_time_list)}secs, Average Training Time per Epoch: {np.mean(train_time_list)}secs.\")\n","    print(f\"Total Validation Time: {np.sum(valid_time_list)}secs, Average Validation Time per Epoch: {np.mean(valid_time_list)}secs.\")\n","    \n","    torch.cuda.empty_cache()\n","    del trainer, evaluator\n","    del model, model_config, tokenizer\n","    del optimizer, scheduler\n","    del train_dataloader, valid_dataloader, result_dict\n","    gc.collect()"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"mPaGnnCnbhWl","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["1ed761b35d894a36a13c1986f9d837a3","1e1ecde8ac064c57ac087b29b06fb979","8f2bd6a7231040968d8bdedecde490f3","f29dd3deb91f409591ec9e5aa0dba32c","e56d0468a5b24381add3b36cd9ae92aa","0c88ec9eb5b7460ca3a98e28c5b81cce","75f284b37dc84699be5c3d0aa6e13829","a3c36816782e4d88b451a45475beaa2f","cab351b3db6e477e804fd7ded192504c","4742a4c2606442ec8006cb006b64c16a","fdcc7b8d44b047a2ac009c676be81cd5","9540e562331a4998a283cf307cfebaf5","5eab18ad529a40579e05d6e2962e38e9","4507666b269b412b8d2ee8c5fcc1867d","18b3ba1f830f42f68dd5b7fbf01c4b4a","f38f5aa16dbd499d9f916fc1df444f3b","114aac99910943bf9fc1e91eb3c0d445","cd72891e782f47c0ada163ddad2ff643","1579d6dc429f46dbba3b880e1f932f6a","0d7853852cd84783adb9d2bbe53e4975","4953261ee00049c9b46836dc1974f09a","6cbd253c4d144bcea0217711da6f5b3e","c67f52a3927044a9aaf51c68fe3719e4","1a89cf47828e40e09eef5c45dffbc714","31bdba3c546241a0ae5195eebaa4c794","033b9dc011d349dcb88a935f9f8272d0","f9355da31910408ca235969111c560fb","246dcbdfd95a4700bbdc076043c60d46","5273bde35b274c21bfad5f00c1655783","f569cc05edad456d84ff74e0dc76baab","b9e623d28a21481bbd96254d1a58eb14","67873897abe04b8e9b206e0654925b78","d984a8ebb3264584b51df7844e871948","3e618e44d092437c87ba6de016c3f718","d7e48be142274ea1abaefb57b6dc4a5d","0c051f29b1384525b9faa022a6d27985","ccb769c80c4b4ca789d26535416f88d4","59d5054f1b164cdc9f9893caf0cb22b0","b4e79f62594f4864bb143ddbb2b5b99d","7870f91887d64f46a08250b4fa9ada9f","3acf9ba8808941f1907f460e338c289b","582d6f7f4cf54d4a80967d07da1d4211","c9079ed3fb4c4dc9920dc60bb059d308","290f5e7b64514ebfa57558bc32d8aac4","4f399f2a8c574689981ff074ec39bb38","a7549a037ce2468f95f5364bb36dfa6c","2629affb557744cfbac1a9b9c7f19fc5","007f2dfb6aa946af83d44f97c7551df7","78a20a72bca349bc9b3e977ff96f1519","1df40521bc5d465681b3ab61ec9a1d1c","d27f09d18b85487cb6cf5bb8270f2d19","5866ae6a39fa46fc8a3044937a976d15","e2fdce5401134b458f901a7b90349bd6","01e6a4ce3a994f318577a02090ec9054","e58188acc732452c8a11ff3a0ca7a02c"]},"executionInfo":{"status":"ok","timestamp":1634304054355,"user_tz":-540,"elapsed":53923,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}},"outputId":"d7105c21-e625-446a-833d-33e4d711bb13"},"source":["for fold in range(1,6):\n","    print();print()\n","    print('-'*50)\n","    print(f'FOLD: {fold}')\n","    print('-'*50)\n","    run(train, fold)"],"execution_count":19,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","\n","--------------------------------------------------\n","FOLD: 1\n","--------------------------------------------------\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1ed761b35d894a36a13c1986f9d837a3","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/606 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9540e562331a4998a283cf307cfebaf5","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/179 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c67f52a3927044a9aaf51c68fe3719e4","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/4.83M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3e618e44d092437c87ba6de016c3f718","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/150 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4f399f2a8c574689981ff074ec39bb38","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/2.09G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"output_type":"stream","name":"stdout","text":["Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n","Num examples Train= 18449, Num examples Valid=4620\n","Total Training Steps: 2307, Total Warmup Steps: 230\n","Epoch: 00 [    4/18449 (  0%)], Train Loss: 2.92419\n","Epoch: 00 [   44/18449 (  0%)], Train Loss: 2.88910\n","Epoch: 00 [   84/18449 (  0%)], Train Loss: 2.89202\n","Epoch: 00 [  124/18449 (  1%)], Train Loss: 2.85582\n","Epoch: 00 [  164/18449 (  1%)], Train Loss: 2.80896\n","Epoch: 00 [  204/18449 (  1%)], Train Loss: 2.75654\n","Epoch: 00 [  244/18449 (  1%)], Train Loss: 2.67990\n","Epoch: 00 [  284/18449 (  2%)], Train Loss: 2.59635\n","Epoch: 00 [  324/18449 (  2%)], Train Loss: 2.48523\n","Epoch: 00 [  364/18449 (  2%)], Train Loss: 2.36173\n","Epoch: 00 [  404/18449 (  2%)], Train Loss: 2.24968\n","Epoch: 00 [  444/18449 (  2%)], Train Loss: 2.12581\n","Epoch: 00 [  484/18449 (  3%)], Train Loss: 2.01404\n","Epoch: 00 [  524/18449 (  3%)], Train Loss: 1.92947\n","Epoch: 00 [  564/18449 (  3%)], Train Loss: 1.83347\n","Epoch: 00 [  604/18449 (  3%)], Train Loss: 1.74409\n","Epoch: 00 [  644/18449 (  3%)], Train Loss: 1.65593\n","Epoch: 00 [  684/18449 (  4%)], Train Loss: 1.59861\n","Epoch: 00 [  724/18449 (  4%)], Train Loss: 1.53868\n","Epoch: 00 [  764/18449 (  4%)], Train Loss: 1.48134\n","Epoch: 00 [  804/18449 (  4%)], Train Loss: 1.42322\n","Epoch: 00 [  844/18449 (  5%)], Train Loss: 1.37600\n","Epoch: 00 [  884/18449 (  5%)], Train Loss: 1.33627\n","Epoch: 00 [  924/18449 (  5%)], Train Loss: 1.29290\n","Epoch: 00 [  964/18449 (  5%)], Train Loss: 1.25244\n","Epoch: 00 [ 1004/18449 (  5%)], Train Loss: 1.21515\n","Epoch: 00 [ 1044/18449 (  6%)], Train Loss: 1.18190\n","Epoch: 00 [ 1084/18449 (  6%)], Train Loss: 1.15584\n","Epoch: 00 [ 1124/18449 (  6%)], Train Loss: 1.13387\n","Epoch: 00 [ 1164/18449 (  6%)], Train Loss: 1.10633\n","Epoch: 00 [ 1204/18449 (  7%)], Train Loss: 1.08284\n","Epoch: 00 [ 1244/18449 (  7%)], Train Loss: 1.05306\n","Epoch: 00 [ 1284/18449 (  7%)], Train Loss: 1.02946\n","Epoch: 00 [ 1324/18449 (  7%)], Train Loss: 1.01136\n","Epoch: 00 [ 1364/18449 (  7%)], Train Loss: 0.99270\n","Epoch: 00 [ 1404/18449 (  8%)], Train Loss: 0.98247\n","Epoch: 00 [ 1444/18449 (  8%)], Train Loss: 0.96428\n","Epoch: 00 [ 1484/18449 (  8%)], Train Loss: 0.95103\n","Epoch: 00 [ 1524/18449 (  8%)], Train Loss: 0.93661\n","Epoch: 00 [ 1564/18449 (  8%)], Train Loss: 0.92211\n","Epoch: 00 [ 1604/18449 (  9%)], Train Loss: 0.91026\n","Epoch: 00 [ 1644/18449 (  9%)], Train Loss: 0.89558\n","Epoch: 00 [ 1684/18449 (  9%)], Train Loss: 0.89138\n","Epoch: 00 [ 1724/18449 (  9%)], Train Loss: 0.87956\n","Epoch: 00 [ 1764/18449 ( 10%)], Train Loss: 0.87065\n","Epoch: 00 [ 1804/18449 ( 10%)], Train Loss: 0.86331\n","Epoch: 00 [ 1844/18449 ( 10%)], Train Loss: 0.85483\n","Epoch: 00 [ 1884/18449 ( 10%)], Train Loss: 0.84073\n","Epoch: 00 [ 1924/18449 ( 10%)], Train Loss: 0.83417\n","Epoch: 00 [ 1964/18449 ( 11%)], Train Loss: 0.82874\n","Epoch: 00 [ 2004/18449 ( 11%)], Train Loss: 0.81797\n","Epoch: 00 [ 2044/18449 ( 11%)], Train Loss: 0.80954\n","Epoch: 00 [ 2084/18449 ( 11%)], Train Loss: 0.79961\n","Epoch: 00 [ 2124/18449 ( 12%)], Train Loss: 0.79254\n","Epoch: 00 [ 2164/18449 ( 12%)], Train Loss: 0.78140\n","Epoch: 00 [ 2204/18449 ( 12%)], Train Loss: 0.77246\n","Epoch: 00 [ 2244/18449 ( 12%)], Train Loss: 0.76832\n","Epoch: 00 [ 2284/18449 ( 12%)], Train Loss: 0.76094\n","Epoch: 00 [ 2324/18449 ( 13%)], Train Loss: 0.75769\n","Epoch: 00 [ 2364/18449 ( 13%)], Train Loss: 0.75169\n","Epoch: 00 [ 2404/18449 ( 13%)], Train Loss: 0.74647\n","Epoch: 00 [ 2444/18449 ( 13%)], Train Loss: 0.73742\n","Epoch: 00 [ 2484/18449 ( 13%)], Train Loss: 0.73031\n","Epoch: 00 [ 2524/18449 ( 14%)], Train Loss: 0.72506\n","Epoch: 00 [ 2564/18449 ( 14%)], Train Loss: 0.72057\n","Epoch: 00 [ 2604/18449 ( 14%)], Train Loss: 0.71518\n","Epoch: 00 [ 2644/18449 ( 14%)], Train Loss: 0.71096\n","Epoch: 00 [ 2684/18449 ( 15%)], Train Loss: 0.70540\n","Epoch: 00 [ 2724/18449 ( 15%)], Train Loss: 0.69853\n","Epoch: 00 [ 2764/18449 ( 15%)], Train Loss: 0.69472\n","Epoch: 00 [ 2804/18449 ( 15%)], Train Loss: 0.69021\n","Epoch: 00 [ 2844/18449 ( 15%)], Train Loss: 0.68360\n","Epoch: 00 [ 2884/18449 ( 16%)], Train Loss: 0.68202\n","Epoch: 00 [ 2924/18449 ( 16%)], Train Loss: 0.67819\n","Epoch: 00 [ 2964/18449 ( 16%)], Train Loss: 0.67611\n","Epoch: 00 [ 3004/18449 ( 16%)], Train Loss: 0.67483\n","Epoch: 00 [ 3044/18449 ( 16%)], Train Loss: 0.67072\n","Epoch: 00 [ 3084/18449 ( 17%)], Train Loss: 0.66808\n","Epoch: 00 [ 3124/18449 ( 17%)], Train Loss: 0.66404\n","Epoch: 00 [ 3164/18449 ( 17%)], Train Loss: 0.66180\n","Epoch: 00 [ 3204/18449 ( 17%)], Train Loss: 0.66020\n","Epoch: 00 [ 3244/18449 ( 18%)], Train Loss: 0.65549\n","Epoch: 00 [ 3284/18449 ( 18%)], Train Loss: 0.64865\n","Epoch: 00 [ 3324/18449 ( 18%)], Train Loss: 0.64640\n","Epoch: 00 [ 3364/18449 ( 18%)], Train Loss: 0.64129\n","Epoch: 00 [ 3404/18449 ( 18%)], Train Loss: 0.63783\n","Epoch: 00 [ 3444/18449 ( 19%)], Train Loss: 0.63383\n","Epoch: 00 [ 3484/18449 ( 19%)], Train Loss: 0.63206\n","Epoch: 00 [ 3524/18449 ( 19%)], Train Loss: 0.62892\n","Epoch: 00 [ 3564/18449 ( 19%)], Train Loss: 0.62430\n","Epoch: 00 [ 3604/18449 ( 20%)], Train Loss: 0.62030\n","Epoch: 00 [ 3644/18449 ( 20%)], Train Loss: 0.61626\n","Epoch: 00 [ 3684/18449 ( 20%)], Train Loss: 0.61319\n","Epoch: 00 [ 3724/18449 ( 20%)], Train Loss: 0.61052\n","Epoch: 00 [ 3764/18449 ( 20%)], Train Loss: 0.60890\n","Epoch: 00 [ 3804/18449 ( 21%)], Train Loss: 0.60444\n","Epoch: 00 [ 3844/18449 ( 21%)], Train Loss: 0.60014\n","Epoch: 00 [ 3884/18449 ( 21%)], Train Loss: 0.59881\n","Epoch: 00 [ 3924/18449 ( 21%)], Train Loss: 0.59539\n","Epoch: 00 [ 3964/18449 ( 21%)], Train Loss: 0.59198\n","Epoch: 00 [ 4004/18449 ( 22%)], Train Loss: 0.58933\n","Epoch: 00 [ 4044/18449 ( 22%)], Train Loss: 0.58734\n","Epoch: 00 [ 4084/18449 ( 22%)], Train Loss: 0.58611\n","Epoch: 00 [ 4124/18449 ( 22%)], Train Loss: 0.58395\n","Epoch: 00 [ 4164/18449 ( 23%)], Train Loss: 0.58169\n","Epoch: 00 [ 4204/18449 ( 23%)], Train Loss: 0.57949\n","Epoch: 00 [ 4244/18449 ( 23%)], Train Loss: 0.57882\n","Epoch: 00 [ 4284/18449 ( 23%)], Train Loss: 0.57654\n","Epoch: 00 [ 4324/18449 ( 23%)], Train Loss: 0.57646\n","Epoch: 00 [ 4364/18449 ( 24%)], Train Loss: 0.57621\n","Epoch: 00 [ 4404/18449 ( 24%)], Train Loss: 0.57462\n","Epoch: 00 [ 4444/18449 ( 24%)], Train Loss: 0.57202\n","Epoch: 00 [ 4484/18449 ( 24%)], Train Loss: 0.57076\n","Epoch: 00 [ 4524/18449 ( 25%)], Train Loss: 0.57118\n","Epoch: 00 [ 4564/18449 ( 25%)], Train Loss: 0.57005\n","Epoch: 00 [ 4604/18449 ( 25%)], Train Loss: 0.56721\n","Epoch: 00 [ 4644/18449 ( 25%)], Train Loss: 0.56421\n","Epoch: 00 [ 4684/18449 ( 25%)], Train Loss: 0.56079\n","Epoch: 00 [ 4724/18449 ( 26%)], Train Loss: 0.56021\n","Epoch: 00 [ 4764/18449 ( 26%)], Train Loss: 0.55732\n","Epoch: 00 [ 4804/18449 ( 26%)], Train Loss: 0.55562\n","Epoch: 00 [ 4844/18449 ( 26%)], Train Loss: 0.55446\n","Epoch: 00 [ 4884/18449 ( 26%)], Train Loss: 0.55265\n","Epoch: 00 [ 4924/18449 ( 27%)], Train Loss: 0.55032\n","Epoch: 00 [ 4964/18449 ( 27%)], Train Loss: 0.54844\n","Epoch: 00 [ 5004/18449 ( 27%)], Train Loss: 0.54784\n","Epoch: 00 [ 5044/18449 ( 27%)], Train Loss: 0.54687\n","Epoch: 00 [ 5084/18449 ( 28%)], Train Loss: 0.54444\n","Epoch: 00 [ 5124/18449 ( 28%)], Train Loss: 0.54192\n","Epoch: 00 [ 5164/18449 ( 28%)], Train Loss: 0.54033\n","Epoch: 00 [ 5204/18449 ( 28%)], Train Loss: 0.53908\n","Epoch: 00 [ 5244/18449 ( 28%)], Train Loss: 0.53793\n","Epoch: 00 [ 5284/18449 ( 29%)], Train Loss: 0.53605\n","Epoch: 00 [ 5324/18449 ( 29%)], Train Loss: 0.53508\n","Epoch: 00 [ 5364/18449 ( 29%)], Train Loss: 0.53384\n","Epoch: 00 [ 5404/18449 ( 29%)], Train Loss: 0.53100\n","Epoch: 00 [ 5444/18449 ( 30%)], Train Loss: 0.52995\n","Epoch: 00 [ 5484/18449 ( 30%)], Train Loss: 0.52865\n","Epoch: 00 [ 5524/18449 ( 30%)], Train Loss: 0.52776\n","Epoch: 00 [ 5564/18449 ( 30%)], Train Loss: 0.52684\n","Epoch: 00 [ 5604/18449 ( 30%)], Train Loss: 0.52578\n","Epoch: 00 [ 5644/18449 ( 31%)], Train Loss: 0.52601\n","Epoch: 00 [ 5684/18449 ( 31%)], Train Loss: 0.52412\n","Epoch: 00 [ 5724/18449 ( 31%)], Train Loss: 0.52252\n","Epoch: 00 [ 5764/18449 ( 31%)], Train Loss: 0.52256\n","Epoch: 00 [ 5804/18449 ( 31%)], Train Loss: 0.52115\n","Epoch: 00 [ 5844/18449 ( 32%)], Train Loss: 0.51825\n","Epoch: 00 [ 5884/18449 ( 32%)], Train Loss: 0.51706\n","Epoch: 00 [ 5924/18449 ( 32%)], Train Loss: 0.51495\n","Epoch: 00 [ 5964/18449 ( 32%)], Train Loss: 0.51257\n","Epoch: 00 [ 6004/18449 ( 33%)], Train Loss: 0.51063\n","Epoch: 00 [ 6044/18449 ( 33%)], Train Loss: 0.50951\n","Epoch: 00 [ 6084/18449 ( 33%)], Train Loss: 0.50811\n","Epoch: 00 [ 6124/18449 ( 33%)], Train Loss: 0.50660\n","Epoch: 00 [ 6164/18449 ( 33%)], Train Loss: 0.50498\n","Epoch: 00 [ 6204/18449 ( 34%)], Train Loss: 0.50512\n","Epoch: 00 [ 6244/18449 ( 34%)], Train Loss: 0.50269\n","Epoch: 00 [ 6284/18449 ( 34%)], Train Loss: 0.50112\n","Epoch: 00 [ 6324/18449 ( 34%)], Train Loss: 0.50041\n","Epoch: 00 [ 6364/18449 ( 34%)], Train Loss: 0.49888\n","Epoch: 00 [ 6404/18449 ( 35%)], Train Loss: 0.49838\n","Epoch: 00 [ 6444/18449 ( 35%)], Train Loss: 0.49826\n","Epoch: 00 [ 6484/18449 ( 35%)], Train Loss: 0.49654\n","Epoch: 00 [ 6524/18449 ( 35%)], Train Loss: 0.49542\n","Epoch: 00 [ 6564/18449 ( 36%)], Train Loss: 0.49405\n","Epoch: 00 [ 6604/18449 ( 36%)], Train Loss: 0.49311\n","Epoch: 00 [ 6644/18449 ( 36%)], Train Loss: 0.49204\n","Epoch: 00 [ 6684/18449 ( 36%)], Train Loss: 0.49147\n","Epoch: 00 [ 6724/18449 ( 36%)], Train Loss: 0.49037\n","Epoch: 00 [ 6764/18449 ( 37%)], Train Loss: 0.48937\n","Epoch: 00 [ 6804/18449 ( 37%)], Train Loss: 0.48899\n","Epoch: 00 [ 6844/18449 ( 37%)], Train Loss: 0.48870\n","Epoch: 00 [ 6884/18449 ( 37%)], Train Loss: 0.48676\n","Epoch: 00 [ 6924/18449 ( 38%)], Train Loss: 0.48568\n","Epoch: 00 [ 6964/18449 ( 38%)], Train Loss: 0.48394\n","Epoch: 00 [ 7004/18449 ( 38%)], Train Loss: 0.48340\n","Epoch: 00 [ 7044/18449 ( 38%)], Train Loss: 0.48316\n","Epoch: 00 [ 7084/18449 ( 38%)], Train Loss: 0.48160\n","Epoch: 00 [ 7124/18449 ( 39%)], Train Loss: 0.48001\n","Epoch: 00 [ 7164/18449 ( 39%)], Train Loss: 0.47992\n","Epoch: 00 [ 7204/18449 ( 39%)], Train Loss: 0.47911\n","Epoch: 00 [ 7244/18449 ( 39%)], Train Loss: 0.47843\n","Epoch: 00 [ 7284/18449 ( 39%)], Train Loss: 0.47841\n","Epoch: 00 [ 7324/18449 ( 40%)], Train Loss: 0.47854\n","Epoch: 00 [ 7364/18449 ( 40%)], Train Loss: 0.47737\n","Epoch: 00 [ 7404/18449 ( 40%)], Train Loss: 0.47671\n","Epoch: 00 [ 7444/18449 ( 40%)], Train Loss: 0.47629\n","Epoch: 00 [ 7484/18449 ( 41%)], Train Loss: 0.47583\n","Epoch: 00 [ 7524/18449 ( 41%)], Train Loss: 0.47450\n","Epoch: 00 [ 7564/18449 ( 41%)], Train Loss: 0.47336\n","Epoch: 00 [ 7604/18449 ( 41%)], Train Loss: 0.47394\n","Epoch: 00 [ 7644/18449 ( 41%)], Train Loss: 0.47373\n","Epoch: 00 [ 7684/18449 ( 42%)], Train Loss: 0.47314\n","Epoch: 00 [ 7724/18449 ( 42%)], Train Loss: 0.47214\n","Epoch: 00 [ 7764/18449 ( 42%)], Train Loss: 0.47151\n","Epoch: 00 [ 7804/18449 ( 42%)], Train Loss: 0.47036\n","Epoch: 00 [ 7844/18449 ( 43%)], Train Loss: 0.47007\n","Epoch: 00 [ 7884/18449 ( 43%)], Train Loss: 0.46902\n","Epoch: 00 [ 7924/18449 ( 43%)], Train Loss: 0.46762\n","Epoch: 00 [ 7964/18449 ( 43%)], Train Loss: 0.46686\n","Epoch: 00 [ 8004/18449 ( 43%)], Train Loss: 0.46657\n","Epoch: 00 [ 8044/18449 ( 44%)], Train Loss: 0.46642\n","Epoch: 00 [ 8084/18449 ( 44%)], Train Loss: 0.46656\n","Epoch: 00 [ 8124/18449 ( 44%)], Train Loss: 0.46622\n","Epoch: 00 [ 8164/18449 ( 44%)], Train Loss: 0.46538\n","Epoch: 00 [ 8204/18449 ( 44%)], Train Loss: 0.46452\n","Epoch: 00 [ 8244/18449 ( 45%)], Train Loss: 0.46439\n","Epoch: 00 [ 8284/18449 ( 45%)], Train Loss: 0.46302\n","Epoch: 00 [ 8324/18449 ( 45%)], Train Loss: 0.46274\n","Epoch: 00 [ 8364/18449 ( 45%)], Train Loss: 0.46279\n","Epoch: 00 [ 8404/18449 ( 46%)], Train Loss: 0.46230\n","Epoch: 00 [ 8444/18449 ( 46%)], Train Loss: 0.46178\n","Epoch: 00 [ 8484/18449 ( 46%)], Train Loss: 0.46042\n","Epoch: 00 [ 8524/18449 ( 46%)], Train Loss: 0.46053\n","Epoch: 00 [ 8564/18449 ( 46%)], Train Loss: 0.45991\n","Epoch: 00 [ 8604/18449 ( 47%)], Train Loss: 0.45891\n","Epoch: 00 [ 8644/18449 ( 47%)], Train Loss: 0.45908\n","Epoch: 00 [ 8684/18449 ( 47%)], Train Loss: 0.45979\n","Epoch: 00 [ 8724/18449 ( 47%)], Train Loss: 0.45949\n","Epoch: 00 [ 8764/18449 ( 48%)], Train Loss: 0.45879\n","Epoch: 00 [ 8804/18449 ( 48%)], Train Loss: 0.45886\n","Epoch: 00 [ 8844/18449 ( 48%)], Train Loss: 0.45794\n","Epoch: 00 [ 8884/18449 ( 48%)], Train Loss: 0.45665\n","Epoch: 00 [ 8924/18449 ( 48%)], Train Loss: 0.45612\n","Epoch: 00 [ 8964/18449 ( 49%)], Train Loss: 0.45560\n","Epoch: 00 [ 9004/18449 ( 49%)], Train Loss: 0.45471\n","Epoch: 00 [ 9044/18449 ( 49%)], Train Loss: 0.45373\n","Epoch: 00 [ 9084/18449 ( 49%)], Train Loss: 0.45322\n","Epoch: 00 [ 9124/18449 ( 49%)], Train Loss: 0.45296\n","Epoch: 00 [ 9164/18449 ( 50%)], Train Loss: 0.45192\n","Epoch: 00 [ 9204/18449 ( 50%)], Train Loss: 0.45153\n","Epoch: 00 [ 9244/18449 ( 50%)], Train Loss: 0.45161\n","Epoch: 00 [ 9284/18449 ( 50%)], Train Loss: 0.45102\n","Epoch: 00 [ 9324/18449 ( 51%)], Train Loss: 0.45104\n","Epoch: 00 [ 9364/18449 ( 51%)], Train Loss: 0.45041\n","Epoch: 00 [ 9404/18449 ( 51%)], Train Loss: 0.44979\n","Epoch: 00 [ 9444/18449 ( 51%)], Train Loss: 0.44967\n","Epoch: 00 [ 9484/18449 ( 51%)], Train Loss: 0.44910\n","Epoch: 00 [ 9524/18449 ( 52%)], Train Loss: 0.44808\n","Epoch: 00 [ 9564/18449 ( 52%)], Train Loss: 0.44783\n","Epoch: 00 [ 9604/18449 ( 52%)], Train Loss: 0.44774\n","Epoch: 00 [ 9644/18449 ( 52%)], Train Loss: 0.44784\n","Epoch: 00 [ 9684/18449 ( 52%)], Train Loss: 0.44830\n","Epoch: 00 [ 9724/18449 ( 53%)], Train Loss: 0.44813\n","Epoch: 00 [ 9764/18449 ( 53%)], Train Loss: 0.44726\n","Epoch: 00 [ 9804/18449 ( 53%)], Train Loss: 0.44654\n","Epoch: 00 [ 9844/18449 ( 53%)], Train Loss: 0.44580\n","Epoch: 00 [ 9884/18449 ( 54%)], Train Loss: 0.44503\n","Epoch: 00 [ 9924/18449 ( 54%)], Train Loss: 0.44467\n","Epoch: 00 [ 9964/18449 ( 54%)], Train Loss: 0.44416\n","Epoch: 00 [10004/18449 ( 54%)], Train Loss: 0.44392\n","Epoch: 00 [10044/18449 ( 54%)], Train Loss: 0.44314\n","Epoch: 00 [10084/18449 ( 55%)], Train Loss: 0.44310\n","Epoch: 00 [10124/18449 ( 55%)], Train Loss: 0.44234\n","Epoch: 00 [10164/18449 ( 55%)], Train Loss: 0.44225\n","Epoch: 00 [10204/18449 ( 55%)], Train Loss: 0.44224\n","Epoch: 00 [10244/18449 ( 56%)], Train Loss: 0.44151\n","Epoch: 00 [10284/18449 ( 56%)], Train Loss: 0.44117\n","Epoch: 00 [10324/18449 ( 56%)], Train Loss: 0.44103\n","Epoch: 00 [10364/18449 ( 56%)], Train Loss: 0.44116\n","Epoch: 00 [10404/18449 ( 56%)], Train Loss: 0.44030\n","Epoch: 00 [10444/18449 ( 57%)], Train Loss: 0.44010\n","Epoch: 00 [10484/18449 ( 57%)], Train Loss: 0.43939\n","Epoch: 00 [10524/18449 ( 57%)], Train Loss: 0.43942\n","Epoch: 00 [10564/18449 ( 57%)], Train Loss: 0.43856\n","Epoch: 00 [10604/18449 ( 57%)], Train Loss: 0.43850\n","Epoch: 00 [10644/18449 ( 58%)], Train Loss: 0.43819\n","Epoch: 00 [10684/18449 ( 58%)], Train Loss: 0.43754\n","Epoch: 00 [10724/18449 ( 58%)], Train Loss: 0.43772\n","Epoch: 00 [10764/18449 ( 58%)], Train Loss: 0.43690\n","Epoch: 00 [10804/18449 ( 59%)], Train Loss: 0.43628\n","Epoch: 00 [10844/18449 ( 59%)], Train Loss: 0.43640\n","Epoch: 00 [10884/18449 ( 59%)], Train Loss: 0.43584\n","Epoch: 00 [10924/18449 ( 59%)], Train Loss: 0.43558\n","Epoch: 00 [10964/18449 ( 59%)], Train Loss: 0.43548\n","Epoch: 00 [11004/18449 ( 60%)], Train Loss: 0.43551\n","Epoch: 00 [11044/18449 ( 60%)], Train Loss: 0.43515\n","Epoch: 00 [11084/18449 ( 60%)], Train Loss: 0.43525\n","Epoch: 00 [11124/18449 ( 60%)], Train Loss: 0.43489\n","Epoch: 00 [11164/18449 ( 61%)], Train Loss: 0.43466\n","Epoch: 00 [11204/18449 ( 61%)], Train Loss: 0.43488\n","Epoch: 00 [11244/18449 ( 61%)], Train Loss: 0.43448\n","Epoch: 00 [11284/18449 ( 61%)], Train Loss: 0.43468\n","Epoch: 00 [11324/18449 ( 61%)], Train Loss: 0.43413\n","Epoch: 00 [11364/18449 ( 62%)], Train Loss: 0.43373\n","Epoch: 00 [11404/18449 ( 62%)], Train Loss: 0.43367\n","Epoch: 00 [11444/18449 ( 62%)], Train Loss: 0.43379\n","Epoch: 00 [11484/18449 ( 62%)], Train Loss: 0.43340\n","Epoch: 00 [11524/18449 ( 62%)], Train Loss: 0.43255\n","Epoch: 00 [11564/18449 ( 63%)], Train Loss: 0.43174\n","Epoch: 00 [11604/18449 ( 63%)], Train Loss: 0.43183\n","Epoch: 00 [11644/18449 ( 63%)], Train Loss: 0.43168\n","Epoch: 00 [11684/18449 ( 63%)], Train Loss: 0.43125\n","Epoch: 00 [11724/18449 ( 64%)], Train Loss: 0.43067\n","Epoch: 00 [11764/18449 ( 64%)], Train Loss: 0.43045\n","Epoch: 00 [11804/18449 ( 64%)], Train Loss: 0.42962\n","Epoch: 00 [11844/18449 ( 64%)], Train Loss: 0.42891\n","Epoch: 00 [11884/18449 ( 64%)], Train Loss: 0.42895\n","Epoch: 00 [11924/18449 ( 65%)], Train Loss: 0.42896\n","Epoch: 00 [11964/18449 ( 65%)], Train Loss: 0.42805\n","Epoch: 00 [12004/18449 ( 65%)], Train Loss: 0.42799\n","Epoch: 00 [12044/18449 ( 65%)], Train Loss: 0.42721\n","Epoch: 00 [12084/18449 ( 65%)], Train Loss: 0.42708\n","Epoch: 00 [12124/18449 ( 66%)], Train Loss: 0.42658\n","Epoch: 00 [12164/18449 ( 66%)], Train Loss: 0.42601\n","Epoch: 00 [12204/18449 ( 66%)], Train Loss: 0.42599\n","Epoch: 00 [12244/18449 ( 66%)], Train Loss: 0.42531\n","Epoch: 00 [12284/18449 ( 67%)], Train Loss: 0.42462\n","Epoch: 00 [12324/18449 ( 67%)], Train Loss: 0.42440\n","Epoch: 00 [12364/18449 ( 67%)], Train Loss: 0.42428\n","Epoch: 00 [12404/18449 ( 67%)], Train Loss: 0.42449\n","Epoch: 00 [12444/18449 ( 67%)], Train Loss: 0.42458\n","Epoch: 00 [12484/18449 ( 68%)], Train Loss: 0.42457\n","Epoch: 00 [12524/18449 ( 68%)], Train Loss: 0.42422\n","Epoch: 00 [12564/18449 ( 68%)], Train Loss: 0.42365\n","Epoch: 00 [12604/18449 ( 68%)], Train Loss: 0.42363\n","Epoch: 00 [12644/18449 ( 69%)], Train Loss: 0.42328\n","Epoch: 00 [12684/18449 ( 69%)], Train Loss: 0.42326\n","Epoch: 00 [12724/18449 ( 69%)], Train Loss: 0.42344\n","Epoch: 00 [12764/18449 ( 69%)], Train Loss: 0.42285\n","Epoch: 00 [12804/18449 ( 69%)], Train Loss: 0.42316\n","Epoch: 00 [12844/18449 ( 70%)], Train Loss: 0.42323\n","Epoch: 00 [12884/18449 ( 70%)], Train Loss: 0.42305\n","Epoch: 00 [12924/18449 ( 70%)], Train Loss: 0.42290\n","Epoch: 00 [12964/18449 ( 70%)], Train Loss: 0.42293\n","Epoch: 00 [13004/18449 ( 70%)], Train Loss: 0.42242\n","Epoch: 00 [13044/18449 ( 71%)], Train Loss: 0.42224\n","Epoch: 00 [13084/18449 ( 71%)], Train Loss: 0.42219\n","Epoch: 00 [13124/18449 ( 71%)], Train Loss: 0.42209\n","Epoch: 00 [13164/18449 ( 71%)], Train Loss: 0.42175\n","Epoch: 00 [13204/18449 ( 72%)], Train Loss: 0.42141\n","Epoch: 00 [13244/18449 ( 72%)], Train Loss: 0.42104\n","Epoch: 00 [13284/18449 ( 72%)], Train Loss: 0.42098\n","Epoch: 00 [13324/18449 ( 72%)], Train Loss: 0.42072\n","Epoch: 00 [13364/18449 ( 72%)], Train Loss: 0.42026\n","Epoch: 00 [13404/18449 ( 73%)], Train Loss: 0.41955\n","Epoch: 00 [13444/18449 ( 73%)], Train Loss: 0.41907\n","Epoch: 00 [13484/18449 ( 73%)], Train Loss: 0.41913\n","Epoch: 00 [13524/18449 ( 73%)], Train Loss: 0.41927\n","Epoch: 00 [13564/18449 ( 74%)], Train Loss: 0.41873\n","Epoch: 00 [13604/18449 ( 74%)], Train Loss: 0.41857\n","Epoch: 00 [13644/18449 ( 74%)], Train Loss: 0.41820\n","Epoch: 00 [13684/18449 ( 74%)], Train Loss: 0.41821\n","Epoch: 00 [13724/18449 ( 74%)], Train Loss: 0.41828\n","Epoch: 00 [13764/18449 ( 75%)], Train Loss: 0.41807\n","Epoch: 00 [13804/18449 ( 75%)], Train Loss: 0.41758\n","Epoch: 00 [13844/18449 ( 75%)], Train Loss: 0.41758\n","Epoch: 00 [13884/18449 ( 75%)], Train Loss: 0.41723\n","Epoch: 00 [13924/18449 ( 75%)], Train Loss: 0.41700\n","Epoch: 00 [13964/18449 ( 76%)], Train Loss: 0.41678\n","Epoch: 00 [14004/18449 ( 76%)], Train Loss: 0.41616\n","Epoch: 00 [14044/18449 ( 76%)], Train Loss: 0.41585\n","Epoch: 00 [14084/18449 ( 76%)], Train Loss: 0.41544\n","Epoch: 00 [14124/18449 ( 77%)], Train Loss: 0.41524\n","Epoch: 00 [14164/18449 ( 77%)], Train Loss: 0.41472\n","Epoch: 00 [14204/18449 ( 77%)], Train Loss: 0.41474\n","Epoch: 00 [14244/18449 ( 77%)], Train Loss: 0.41463\n","Epoch: 00 [14284/18449 ( 77%)], Train Loss: 0.41428\n","Epoch: 00 [14324/18449 ( 78%)], Train Loss: 0.41381\n","Epoch: 00 [14364/18449 ( 78%)], Train Loss: 0.41310\n","Epoch: 00 [14404/18449 ( 78%)], Train Loss: 0.41281\n","Epoch: 00 [14444/18449 ( 78%)], Train Loss: 0.41228\n","Epoch: 00 [14484/18449 ( 79%)], Train Loss: 0.41258\n","Epoch: 00 [14524/18449 ( 79%)], Train Loss: 0.41268\n","Epoch: 00 [14564/18449 ( 79%)], Train Loss: 0.41300\n","Epoch: 00 [14604/18449 ( 79%)], Train Loss: 0.41231\n","Epoch: 00 [14644/18449 ( 79%)], Train Loss: 0.41212\n","Epoch: 00 [14684/18449 ( 80%)], Train Loss: 0.41171\n","Epoch: 00 [14724/18449 ( 80%)], Train Loss: 0.41158\n","Epoch: 00 [14764/18449 ( 80%)], Train Loss: 0.41143\n","Epoch: 00 [14804/18449 ( 80%)], Train Loss: 0.41144\n","Epoch: 00 [14844/18449 ( 80%)], Train Loss: 0.41113\n","Epoch: 00 [14884/18449 ( 81%)], Train Loss: 0.41110\n","Epoch: 00 [14924/18449 ( 81%)], Train Loss: 0.41077\n","Epoch: 00 [14964/18449 ( 81%)], Train Loss: 0.41042\n","Epoch: 00 [15004/18449 ( 81%)], Train Loss: 0.40966\n","Epoch: 00 [15044/18449 ( 82%)], Train Loss: 0.40930\n","Epoch: 00 [15084/18449 ( 82%)], Train Loss: 0.40974\n","Epoch: 00 [15124/18449 ( 82%)], Train Loss: 0.40933\n","Epoch: 00 [15164/18449 ( 82%)], Train Loss: 0.40904\n","Epoch: 00 [15204/18449 ( 82%)], Train Loss: 0.40861\n","Epoch: 00 [15244/18449 ( 83%)], Train Loss: 0.40809\n","Epoch: 00 [15284/18449 ( 83%)], Train Loss: 0.40796\n","Epoch: 00 [15324/18449 ( 83%)], Train Loss: 0.40772\n","Epoch: 00 [15364/18449 ( 83%)], Train Loss: 0.40731\n","Epoch: 00 [15404/18449 ( 83%)], Train Loss: 0.40677\n","Epoch: 00 [15444/18449 ( 84%)], Train Loss: 0.40632\n","Epoch: 00 [15484/18449 ( 84%)], Train Loss: 0.40576\n","Epoch: 00 [15524/18449 ( 84%)], Train Loss: 0.40578\n","Epoch: 00 [15564/18449 ( 84%)], Train Loss: 0.40552\n","Epoch: 00 [15604/18449 ( 85%)], Train Loss: 0.40539\n","Epoch: 00 [15644/18449 ( 85%)], Train Loss: 0.40518\n","Epoch: 00 [15684/18449 ( 85%)], Train Loss: 0.40503\n","Epoch: 00 [15724/18449 ( 85%)], Train Loss: 0.40464\n","Epoch: 00 [15764/18449 ( 85%)], Train Loss: 0.40414\n","Epoch: 00 [15804/18449 ( 86%)], Train Loss: 0.40367\n","Epoch: 00 [15844/18449 ( 86%)], Train Loss: 0.40358\n","Epoch: 00 [15884/18449 ( 86%)], Train Loss: 0.40313\n","Epoch: 00 [15924/18449 ( 86%)], Train Loss: 0.40254\n","Epoch: 00 [15964/18449 ( 87%)], Train Loss: 0.40214\n","Epoch: 00 [16004/18449 ( 87%)], Train Loss: 0.40247\n","Epoch: 00 [16044/18449 ( 87%)], Train Loss: 0.40217\n","Epoch: 00 [16084/18449 ( 87%)], Train Loss: 0.40219\n","Epoch: 00 [16124/18449 ( 87%)], Train Loss: 0.40176\n","Epoch: 00 [16164/18449 ( 88%)], Train Loss: 0.40117\n","Epoch: 00 [16204/18449 ( 88%)], Train Loss: 0.40098\n","Epoch: 00 [16244/18449 ( 88%)], Train Loss: 0.40060\n","Epoch: 00 [16284/18449 ( 88%)], Train Loss: 0.40047\n","Epoch: 00 [16324/18449 ( 88%)], Train Loss: 0.39996\n","Epoch: 00 [16364/18449 ( 89%)], Train Loss: 0.39965\n","Epoch: 00 [16404/18449 ( 89%)], Train Loss: 0.39971\n","Epoch: 00 [16444/18449 ( 89%)], Train Loss: 0.39908\n","Epoch: 00 [16484/18449 ( 89%)], Train Loss: 0.39871\n","Epoch: 00 [16524/18449 ( 90%)], Train Loss: 0.39865\n","Epoch: 00 [16564/18449 ( 90%)], Train Loss: 0.39863\n","Epoch: 00 [16604/18449 ( 90%)], Train Loss: 0.39821\n","Epoch: 00 [16644/18449 ( 90%)], Train Loss: 0.39767\n","Epoch: 00 [16684/18449 ( 90%)], Train Loss: 0.39734\n","Epoch: 00 [16724/18449 ( 91%)], Train Loss: 0.39689\n","Epoch: 00 [16764/18449 ( 91%)], Train Loss: 0.39655\n","Epoch: 00 [16804/18449 ( 91%)], Train Loss: 0.39663\n","Epoch: 00 [16844/18449 ( 91%)], Train Loss: 0.39603\n","Epoch: 00 [16884/18449 ( 92%)], Train Loss: 0.39583\n","Epoch: 00 [16924/18449 ( 92%)], Train Loss: 0.39567\n","Epoch: 00 [16964/18449 ( 92%)], Train Loss: 0.39544\n","Epoch: 00 [17004/18449 ( 92%)], Train Loss: 0.39524\n","Epoch: 00 [17044/18449 ( 92%)], Train Loss: 0.39506\n","Epoch: 00 [17084/18449 ( 93%)], Train Loss: 0.39515\n","Epoch: 00 [17124/18449 ( 93%)], Train Loss: 0.39507\n","Epoch: 00 [17164/18449 ( 93%)], Train Loss: 0.39509\n","Epoch: 00 [17204/18449 ( 93%)], Train Loss: 0.39479\n","Epoch: 00 [17244/18449 ( 93%)], Train Loss: 0.39462\n","Epoch: 00 [17284/18449 ( 94%)], Train Loss: 0.39439\n","Epoch: 00 [17324/18449 ( 94%)], Train Loss: 0.39431\n","Epoch: 00 [17364/18449 ( 94%)], Train Loss: 0.39387\n","Epoch: 00 [17404/18449 ( 94%)], Train Loss: 0.39357\n","Epoch: 00 [17444/18449 ( 95%)], Train Loss: 0.39314\n","Epoch: 00 [17484/18449 ( 95%)], Train Loss: 0.39296\n","Epoch: 00 [17524/18449 ( 95%)], Train Loss: 0.39307\n","Epoch: 00 [17564/18449 ( 95%)], Train Loss: 0.39287\n","Epoch: 00 [17604/18449 ( 95%)], Train Loss: 0.39239\n","Epoch: 00 [17644/18449 ( 96%)], Train Loss: 0.39242\n","Epoch: 00 [17684/18449 ( 96%)], Train Loss: 0.39238\n","Epoch: 00 [17724/18449 ( 96%)], Train Loss: 0.39239\n","Epoch: 00 [17764/18449 ( 96%)], Train Loss: 0.39212\n","Epoch: 00 [17804/18449 ( 97%)], Train Loss: 0.39191\n","Epoch: 00 [17844/18449 ( 97%)], Train Loss: 0.39202\n","Epoch: 00 [17884/18449 ( 97%)], Train Loss: 0.39193\n","Epoch: 00 [17924/18449 ( 97%)], Train Loss: 0.39165\n","Epoch: 00 [17964/18449 ( 97%)], Train Loss: 0.39131\n","Epoch: 00 [18004/18449 ( 98%)], Train Loss: 0.39080\n","Epoch: 00 [18044/18449 ( 98%)], Train Loss: 0.39032\n","Epoch: 00 [18084/18449 ( 98%)], Train Loss: 0.39054\n","Epoch: 00 [18124/18449 ( 98%)], Train Loss: 0.39038\n","Epoch: 00 [18164/18449 ( 98%)], Train Loss: 0.39027\n","Epoch: 00 [18204/18449 ( 99%)], Train Loss: 0.38991\n","Epoch: 00 [18244/18449 ( 99%)], Train Loss: 0.38973\n","Epoch: 00 [18284/18449 ( 99%)], Train Loss: 0.38961\n","Epoch: 00 [18324/18449 ( 99%)], Train Loss: 0.38945\n","Epoch: 00 [18364/18449 (100%)], Train Loss: 0.38928\n","Epoch: 00 [18404/18449 (100%)], Train Loss: 0.38936\n","Epoch: 00 [18444/18449 (100%)], Train Loss: 0.38905\n","Epoch: 00 [18449/18449 (100%)], Train Loss: 0.38920\n","----Validation Results Summary----\n","Epoch: [0] Valid Loss: 0.57609\n","0 Epoch, Best epoch was updated! Valid Loss: 0.57609\n","Saving model checkpoint to chaii-qa-5-fold-xlmroberta-torch-fit/checkpoint-fold-1.\n","\n","Total Training Time: 2562.9789555072784secs, Average Training Time per Epoch: 2562.9789555072784secs.\n","Total Validation Time: 194.2873730659485secs, Average Validation Time per Epoch: 194.2873730659485secs.\n","\n","\n","--------------------------------------------------\n","FOLD: 2\n","--------------------------------------------------\n","Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n","Num examples Train= 17967, Num examples Valid=5102\n","Total Training Steps: 2246, Total Warmup Steps: 224\n","Epoch: 00 [    4/17967 (  0%)], Train Loss: 2.90929\n","Epoch: 00 [   44/17967 (  0%)], Train Loss: 2.91411\n","Epoch: 00 [   84/17967 (  0%)], Train Loss: 2.88139\n","Epoch: 00 [  124/17967 (  1%)], Train Loss: 2.84500\n","Epoch: 00 [  164/17967 (  1%)], Train Loss: 2.78751\n","Epoch: 00 [  204/17967 (  1%)], Train Loss: 2.71465\n","Epoch: 00 [  244/17967 (  1%)], Train Loss: 2.64382\n","Epoch: 00 [  284/17967 (  2%)], Train Loss: 2.54815\n","Epoch: 00 [  324/17967 (  2%)], Train Loss: 2.43396\n","Epoch: 00 [  364/17967 (  2%)], Train Loss: 2.33777\n","Epoch: 00 [  404/17967 (  2%)], Train Loss: 2.22875\n","Epoch: 00 [  444/17967 (  2%)], Train Loss: 2.10182\n","Epoch: 00 [  484/17967 (  3%)], Train Loss: 1.98746\n","Epoch: 00 [  524/17967 (  3%)], Train Loss: 1.87933\n","Epoch: 00 [  564/17967 (  3%)], Train Loss: 1.77613\n","Epoch: 00 [  604/17967 (  3%)], Train Loss: 1.67730\n","Epoch: 00 [  644/17967 (  4%)], Train Loss: 1.61557\n","Epoch: 00 [  684/17967 (  4%)], Train Loss: 1.56225\n","Epoch: 00 [  724/17967 (  4%)], Train Loss: 1.50002\n","Epoch: 00 [  764/17967 (  4%)], Train Loss: 1.43997\n","Epoch: 00 [  804/17967 (  4%)], Train Loss: 1.38315\n","Epoch: 00 [  844/17967 (  5%)], Train Loss: 1.33153\n","Epoch: 00 [  884/17967 (  5%)], Train Loss: 1.28989\n","Epoch: 00 [  924/17967 (  5%)], Train Loss: 1.25289\n","Epoch: 00 [  964/17967 (  5%)], Train Loss: 1.21435\n","Epoch: 00 [ 1004/17967 (  6%)], Train Loss: 1.17707\n","Epoch: 00 [ 1044/17967 (  6%)], Train Loss: 1.14505\n","Epoch: 00 [ 1084/17967 (  6%)], Train Loss: 1.11880\n","Epoch: 00 [ 1124/17967 (  6%)], Train Loss: 1.09153\n","Epoch: 00 [ 1164/17967 (  6%)], Train Loss: 1.06642\n","Epoch: 00 [ 1204/17967 (  7%)], Train Loss: 1.05202\n","Epoch: 00 [ 1244/17967 (  7%)], Train Loss: 1.04164\n","Epoch: 00 [ 1284/17967 (  7%)], Train Loss: 1.02907\n","Epoch: 00 [ 1324/17967 (  7%)], Train Loss: 1.01402\n","Epoch: 00 [ 1364/17967 (  8%)], Train Loss: 0.99625\n","Epoch: 00 [ 1404/17967 (  8%)], Train Loss: 0.97855\n","Epoch: 00 [ 1444/17967 (  8%)], Train Loss: 0.96243\n","Epoch: 00 [ 1484/17967 (  8%)], Train Loss: 0.94817\n","Epoch: 00 [ 1524/17967 (  8%)], Train Loss: 0.93737\n","Epoch: 00 [ 1564/17967 (  9%)], Train Loss: 0.91921\n","Epoch: 00 [ 1604/17967 (  9%)], Train Loss: 0.90260\n","Epoch: 00 [ 1644/17967 (  9%)], Train Loss: 0.89245\n","Epoch: 00 [ 1684/17967 (  9%)], Train Loss: 0.88266\n","Epoch: 00 [ 1724/17967 ( 10%)], Train Loss: 0.87140\n","Epoch: 00 [ 1764/17967 ( 10%)], Train Loss: 0.85804\n","Epoch: 00 [ 1804/17967 ( 10%)], Train Loss: 0.85294\n","Epoch: 00 [ 1844/17967 ( 10%)], Train Loss: 0.84092\n","Epoch: 00 [ 1884/17967 ( 10%)], Train Loss: 0.82984\n","Epoch: 00 [ 1924/17967 ( 11%)], Train Loss: 0.82115\n","Epoch: 00 [ 1964/17967 ( 11%)], Train Loss: 0.81171\n","Epoch: 00 [ 2004/17967 ( 11%)], Train Loss: 0.80273\n","Epoch: 00 [ 2044/17967 ( 11%)], Train Loss: 0.79115\n","Epoch: 00 [ 2084/17967 ( 12%)], Train Loss: 0.77994\n","Epoch: 00 [ 2124/17967 ( 12%)], Train Loss: 0.77501\n","Epoch: 00 [ 2164/17967 ( 12%)], Train Loss: 0.76626\n","Epoch: 00 [ 2204/17967 ( 12%)], Train Loss: 0.76281\n","Epoch: 00 [ 2244/17967 ( 12%)], Train Loss: 0.75521\n","Epoch: 00 [ 2284/17967 ( 13%)], Train Loss: 0.74793\n","Epoch: 00 [ 2324/17967 ( 13%)], Train Loss: 0.74027\n","Epoch: 00 [ 2364/17967 ( 13%)], Train Loss: 0.73455\n","Epoch: 00 [ 2404/17967 ( 13%)], Train Loss: 0.72661\n","Epoch: 00 [ 2444/17967 ( 14%)], Train Loss: 0.72309\n","Epoch: 00 [ 2484/17967 ( 14%)], Train Loss: 0.71672\n","Epoch: 00 [ 2524/17967 ( 14%)], Train Loss: 0.71157\n","Epoch: 00 [ 2564/17967 ( 14%)], Train Loss: 0.70500\n","Epoch: 00 [ 2604/17967 ( 14%)], Train Loss: 0.70008\n","Epoch: 00 [ 2644/17967 ( 15%)], Train Loss: 0.69519\n","Epoch: 00 [ 2684/17967 ( 15%)], Train Loss: 0.69277\n","Epoch: 00 [ 2724/17967 ( 15%)], Train Loss: 0.68993\n","Epoch: 00 [ 2764/17967 ( 15%)], Train Loss: 0.68383\n","Epoch: 00 [ 2804/17967 ( 16%)], Train Loss: 0.68043\n","Epoch: 00 [ 2844/17967 ( 16%)], Train Loss: 0.67825\n","Epoch: 00 [ 2884/17967 ( 16%)], Train Loss: 0.67762\n","Epoch: 00 [ 2924/17967 ( 16%)], Train Loss: 0.67395\n","Epoch: 00 [ 2964/17967 ( 16%)], Train Loss: 0.67073\n","Epoch: 00 [ 3004/17967 ( 17%)], Train Loss: 0.66971\n","Epoch: 00 [ 3044/17967 ( 17%)], Train Loss: 0.66386\n","Epoch: 00 [ 3084/17967 ( 17%)], Train Loss: 0.66511\n","Epoch: 00 [ 3124/17967 ( 17%)], Train Loss: 0.66239\n","Epoch: 00 [ 3164/17967 ( 18%)], Train Loss: 0.66025\n","Epoch: 00 [ 3204/17967 ( 18%)], Train Loss: 0.65645\n","Epoch: 00 [ 3244/17967 ( 18%)], Train Loss: 0.65396\n","Epoch: 00 [ 3284/17967 ( 18%)], Train Loss: 0.64894\n","Epoch: 00 [ 3324/17967 ( 19%)], Train Loss: 0.64622\n","Epoch: 00 [ 3364/17967 ( 19%)], Train Loss: 0.64372\n","Epoch: 00 [ 3404/17967 ( 19%)], Train Loss: 0.63875\n","Epoch: 00 [ 3444/17967 ( 19%)], Train Loss: 0.63579\n","Epoch: 00 [ 3484/17967 ( 19%)], Train Loss: 0.63428\n","Epoch: 00 [ 3524/17967 ( 20%)], Train Loss: 0.63246\n","Epoch: 00 [ 3564/17967 ( 20%)], Train Loss: 0.63084\n","Epoch: 00 [ 3604/17967 ( 20%)], Train Loss: 0.62917\n","Epoch: 00 [ 3644/17967 ( 20%)], Train Loss: 0.62616\n","Epoch: 00 [ 3684/17967 ( 21%)], Train Loss: 0.62176\n","Epoch: 00 [ 3724/17967 ( 21%)], Train Loss: 0.61877\n","Epoch: 00 [ 3764/17967 ( 21%)], Train Loss: 0.61705\n","Epoch: 00 [ 3804/17967 ( 21%)], Train Loss: 0.61446\n","Epoch: 00 [ 3844/17967 ( 21%)], Train Loss: 0.61155\n","Epoch: 00 [ 3884/17967 ( 22%)], Train Loss: 0.60717\n","Epoch: 00 [ 3924/17967 ( 22%)], Train Loss: 0.60415\n","Epoch: 00 [ 3964/17967 ( 22%)], Train Loss: 0.60079\n","Epoch: 00 [ 4004/17967 ( 22%)], Train Loss: 0.59750\n","Epoch: 00 [ 4044/17967 ( 23%)], Train Loss: 0.59409\n","Epoch: 00 [ 4084/17967 ( 23%)], Train Loss: 0.59195\n","Epoch: 00 [ 4124/17967 ( 23%)], Train Loss: 0.59082\n","Epoch: 00 [ 4164/17967 ( 23%)], Train Loss: 0.58861\n","Epoch: 00 [ 4204/17967 ( 23%)], Train Loss: 0.58499\n","Epoch: 00 [ 4244/17967 ( 24%)], Train Loss: 0.58315\n","Epoch: 00 [ 4284/17967 ( 24%)], Train Loss: 0.57990\n","Epoch: 00 [ 4324/17967 ( 24%)], Train Loss: 0.57800\n","Epoch: 00 [ 4364/17967 ( 24%)], Train Loss: 0.57691\n","Epoch: 00 [ 4404/17967 ( 25%)], Train Loss: 0.57526\n","Epoch: 00 [ 4444/17967 ( 25%)], Train Loss: 0.57376\n","Epoch: 00 [ 4484/17967 ( 25%)], Train Loss: 0.57108\n","Epoch: 00 [ 4524/17967 ( 25%)], Train Loss: 0.56855\n","Epoch: 00 [ 4564/17967 ( 25%)], Train Loss: 0.56597\n","Epoch: 00 [ 4604/17967 ( 26%)], Train Loss: 0.56391\n","Epoch: 00 [ 4644/17967 ( 26%)], Train Loss: 0.56129\n","Epoch: 00 [ 4684/17967 ( 26%)], Train Loss: 0.55887\n","Epoch: 00 [ 4724/17967 ( 26%)], Train Loss: 0.55793\n","Epoch: 00 [ 4764/17967 ( 27%)], Train Loss: 0.55653\n","Epoch: 00 [ 4804/17967 ( 27%)], Train Loss: 0.55600\n","Epoch: 00 [ 4844/17967 ( 27%)], Train Loss: 0.55579\n","Epoch: 00 [ 4884/17967 ( 27%)], Train Loss: 0.55352\n","Epoch: 00 [ 4924/17967 ( 27%)], Train Loss: 0.55324\n","Epoch: 00 [ 4964/17967 ( 28%)], Train Loss: 0.55252\n","Epoch: 00 [ 5004/17967 ( 28%)], Train Loss: 0.55195\n","Epoch: 00 [ 5044/17967 ( 28%)], Train Loss: 0.55204\n","Epoch: 00 [ 5084/17967 ( 28%)], Train Loss: 0.54951\n","Epoch: 00 [ 5124/17967 ( 29%)], Train Loss: 0.54885\n","Epoch: 00 [ 5164/17967 ( 29%)], Train Loss: 0.54889\n","Epoch: 00 [ 5204/17967 ( 29%)], Train Loss: 0.54716\n","Epoch: 00 [ 5244/17967 ( 29%)], Train Loss: 0.54606\n","Epoch: 00 [ 5284/17967 ( 29%)], Train Loss: 0.54490\n","Epoch: 00 [ 5324/17967 ( 30%)], Train Loss: 0.54405\n","Epoch: 00 [ 5364/17967 ( 30%)], Train Loss: 0.54290\n","Epoch: 00 [ 5404/17967 ( 30%)], Train Loss: 0.54271\n","Epoch: 00 [ 5444/17967 ( 30%)], Train Loss: 0.54103\n","Epoch: 00 [ 5484/17967 ( 31%)], Train Loss: 0.53909\n","Epoch: 00 [ 5524/17967 ( 31%)], Train Loss: 0.53652\n","Epoch: 00 [ 5564/17967 ( 31%)], Train Loss: 0.53400\n","Epoch: 00 [ 5604/17967 ( 31%)], Train Loss: 0.53256\n","Epoch: 00 [ 5644/17967 ( 31%)], Train Loss: 0.53208\n","Epoch: 00 [ 5684/17967 ( 32%)], Train Loss: 0.53042\n","Epoch: 00 [ 5724/17967 ( 32%)], Train Loss: 0.52889\n","Epoch: 00 [ 5764/17967 ( 32%)], Train Loss: 0.52830\n","Epoch: 00 [ 5804/17967 ( 32%)], Train Loss: 0.52790\n","Epoch: 00 [ 5844/17967 ( 33%)], Train Loss: 0.52645\n","Epoch: 00 [ 5884/17967 ( 33%)], Train Loss: 0.52549\n","Epoch: 00 [ 5924/17967 ( 33%)], Train Loss: 0.52371\n","Epoch: 00 [ 5964/17967 ( 33%)], Train Loss: 0.52178\n","Epoch: 00 [ 6004/17967 ( 33%)], Train Loss: 0.52002\n","Epoch: 00 [ 6044/17967 ( 34%)], Train Loss: 0.51905\n","Epoch: 00 [ 6084/17967 ( 34%)], Train Loss: 0.51838\n","Epoch: 00 [ 6124/17967 ( 34%)], Train Loss: 0.51708\n","Epoch: 00 [ 6164/17967 ( 34%)], Train Loss: 0.51685\n","Epoch: 00 [ 6204/17967 ( 35%)], Train Loss: 0.51630\n","Epoch: 00 [ 6244/17967 ( 35%)], Train Loss: 0.51588\n","Epoch: 00 [ 6284/17967 ( 35%)], Train Loss: 0.51520\n","Epoch: 00 [ 6324/17967 ( 35%)], Train Loss: 0.51374\n","Epoch: 00 [ 6364/17967 ( 35%)], Train Loss: 0.51430\n","Epoch: 00 [ 6404/17967 ( 36%)], Train Loss: 0.51376\n","Epoch: 00 [ 6444/17967 ( 36%)], Train Loss: 0.51370\n","Epoch: 00 [ 6484/17967 ( 36%)], Train Loss: 0.51252\n","Epoch: 00 [ 6524/17967 ( 36%)], Train Loss: 0.51114\n","Epoch: 00 [ 6564/17967 ( 37%)], Train Loss: 0.51035\n","Epoch: 00 [ 6604/17967 ( 37%)], Train Loss: 0.50890\n","Epoch: 00 [ 6644/17967 ( 37%)], Train Loss: 0.50795\n","Epoch: 00 [ 6684/17967 ( 37%)], Train Loss: 0.50612\n","Epoch: 00 [ 6724/17967 ( 37%)], Train Loss: 0.50502\n","Epoch: 00 [ 6764/17967 ( 38%)], Train Loss: 0.50404\n","Epoch: 00 [ 6804/17967 ( 38%)], Train Loss: 0.50294\n","Epoch: 00 [ 6844/17967 ( 38%)], Train Loss: 0.50173\n","Epoch: 00 [ 6884/17967 ( 38%)], Train Loss: 0.50039\n","Epoch: 00 [ 6924/17967 ( 39%)], Train Loss: 0.50114\n","Epoch: 00 [ 6964/17967 ( 39%)], Train Loss: 0.49964\n","Epoch: 00 [ 7004/17967 ( 39%)], Train Loss: 0.49905\n","Epoch: 00 [ 7044/17967 ( 39%)], Train Loss: 0.49811\n","Epoch: 00 [ 7084/17967 ( 39%)], Train Loss: 0.49633\n","Epoch: 00 [ 7124/17967 ( 40%)], Train Loss: 0.49465\n","Epoch: 00 [ 7164/17967 ( 40%)], Train Loss: 0.49365\n","Epoch: 00 [ 7204/17967 ( 40%)], Train Loss: 0.49295\n","Epoch: 00 [ 7244/17967 ( 40%)], Train Loss: 0.49173\n","Epoch: 00 [ 7284/17967 ( 41%)], Train Loss: 0.49118\n","Epoch: 00 [ 7324/17967 ( 41%)], Train Loss: 0.48983\n","Epoch: 00 [ 7364/17967 ( 41%)], Train Loss: 0.48949\n","Epoch: 00 [ 7404/17967 ( 41%)], Train Loss: 0.48878\n","Epoch: 00 [ 7444/17967 ( 41%)], Train Loss: 0.48852\n","Epoch: 00 [ 7484/17967 ( 42%)], Train Loss: 0.48706\n","Epoch: 00 [ 7524/17967 ( 42%)], Train Loss: 0.48621\n","Epoch: 00 [ 7564/17967 ( 42%)], Train Loss: 0.48596\n","Epoch: 00 [ 7604/17967 ( 42%)], Train Loss: 0.48476\n","Epoch: 00 [ 7644/17967 ( 43%)], Train Loss: 0.48392\n","Epoch: 00 [ 7684/17967 ( 43%)], Train Loss: 0.48368\n","Epoch: 00 [ 7724/17967 ( 43%)], Train Loss: 0.48203\n","Epoch: 00 [ 7764/17967 ( 43%)], Train Loss: 0.48185\n","Epoch: 00 [ 7804/17967 ( 43%)], Train Loss: 0.48112\n","Epoch: 00 [ 7844/17967 ( 44%)], Train Loss: 0.48003\n","Epoch: 00 [ 7884/17967 ( 44%)], Train Loss: 0.47830\n","Epoch: 00 [ 7924/17967 ( 44%)], Train Loss: 0.47777\n","Epoch: 00 [ 7964/17967 ( 44%)], Train Loss: 0.47767\n","Epoch: 00 [ 8004/17967 ( 45%)], Train Loss: 0.47732\n","Epoch: 00 [ 8044/17967 ( 45%)], Train Loss: 0.47732\n","Epoch: 00 [ 8084/17967 ( 45%)], Train Loss: 0.47675\n","Epoch: 00 [ 8124/17967 ( 45%)], Train Loss: 0.47650\n","Epoch: 00 [ 8164/17967 ( 45%)], Train Loss: 0.47643\n","Epoch: 00 [ 8204/17967 ( 46%)], Train Loss: 0.47608\n","Epoch: 00 [ 8244/17967 ( 46%)], Train Loss: 0.47644\n","Epoch: 00 [ 8284/17967 ( 46%)], Train Loss: 0.47476\n","Epoch: 00 [ 8324/17967 ( 46%)], Train Loss: 0.47362\n","Epoch: 00 [ 8364/17967 ( 47%)], Train Loss: 0.47348\n","Epoch: 00 [ 8404/17967 ( 47%)], Train Loss: 0.47273\n","Epoch: 00 [ 8444/17967 ( 47%)], Train Loss: 0.47151\n","Epoch: 00 [ 8484/17967 ( 47%)], Train Loss: 0.47075\n","Epoch: 00 [ 8524/17967 ( 47%)], Train Loss: 0.47012\n","Epoch: 00 [ 8564/17967 ( 48%)], Train Loss: 0.46968\n","Epoch: 00 [ 8604/17967 ( 48%)], Train Loss: 0.46936\n","Epoch: 00 [ 8644/17967 ( 48%)], Train Loss: 0.46851\n","Epoch: 00 [ 8684/17967 ( 48%)], Train Loss: 0.46782\n","Epoch: 00 [ 8724/17967 ( 49%)], Train Loss: 0.46704\n","Epoch: 00 [ 8764/17967 ( 49%)], Train Loss: 0.46716\n","Epoch: 00 [ 8804/17967 ( 49%)], Train Loss: 0.46590\n","Epoch: 00 [ 8844/17967 ( 49%)], Train Loss: 0.46531\n","Epoch: 00 [ 8884/17967 ( 49%)], Train Loss: 0.46423\n","Epoch: 00 [ 8924/17967 ( 50%)], Train Loss: 0.46426\n","Epoch: 00 [ 8964/17967 ( 50%)], Train Loss: 0.46411\n","Epoch: 00 [ 9004/17967 ( 50%)], Train Loss: 0.46364\n","Epoch: 00 [ 9044/17967 ( 50%)], Train Loss: 0.46248\n","Epoch: 00 [ 9084/17967 ( 51%)], Train Loss: 0.46262\n","Epoch: 00 [ 9124/17967 ( 51%)], Train Loss: 0.46269\n","Epoch: 00 [ 9164/17967 ( 51%)], Train Loss: 0.46295\n","Epoch: 00 [ 9204/17967 ( 51%)], Train Loss: 0.46216\n","Epoch: 00 [ 9244/17967 ( 51%)], Train Loss: 0.46123\n","Epoch: 00 [ 9284/17967 ( 52%)], Train Loss: 0.46117\n","Epoch: 00 [ 9324/17967 ( 52%)], Train Loss: 0.46034\n","Epoch: 00 [ 9364/17967 ( 52%)], Train Loss: 0.45940\n","Epoch: 00 [ 9404/17967 ( 52%)], Train Loss: 0.45904\n","Epoch: 00 [ 9444/17967 ( 53%)], Train Loss: 0.45908\n","Epoch: 00 [ 9484/17967 ( 53%)], Train Loss: 0.45836\n","Epoch: 00 [ 9524/17967 ( 53%)], Train Loss: 0.45908\n","Epoch: 00 [ 9564/17967 ( 53%)], Train Loss: 0.45796\n","Epoch: 00 [ 9604/17967 ( 53%)], Train Loss: 0.45730\n","Epoch: 00 [ 9644/17967 ( 54%)], Train Loss: 0.45671\n","Epoch: 00 [ 9684/17967 ( 54%)], Train Loss: 0.45626\n","Epoch: 00 [ 9724/17967 ( 54%)], Train Loss: 0.45579\n","Epoch: 00 [ 9764/17967 ( 54%)], Train Loss: 0.45527\n","Epoch: 00 [ 9804/17967 ( 55%)], Train Loss: 0.45461\n","Epoch: 00 [ 9844/17967 ( 55%)], Train Loss: 0.45350\n","Epoch: 00 [ 9884/17967 ( 55%)], Train Loss: 0.45261\n","Epoch: 00 [ 9924/17967 ( 55%)], Train Loss: 0.45258\n","Epoch: 00 [ 9964/17967 ( 55%)], Train Loss: 0.45255\n","Epoch: 00 [10004/17967 ( 56%)], Train Loss: 0.45173\n","Epoch: 00 [10044/17967 ( 56%)], Train Loss: 0.45173\n","Epoch: 00 [10084/17967 ( 56%)], Train Loss: 0.45137\n","Epoch: 00 [10124/17967 ( 56%)], Train Loss: 0.45097\n","Epoch: 00 [10164/17967 ( 57%)], Train Loss: 0.45034\n","Epoch: 00 [10204/17967 ( 57%)], Train Loss: 0.44969\n","Epoch: 00 [10244/17967 ( 57%)], Train Loss: 0.44902\n","Epoch: 00 [10284/17967 ( 57%)], Train Loss: 0.44836\n","Epoch: 00 [10324/17967 ( 57%)], Train Loss: 0.44760\n","Epoch: 00 [10364/17967 ( 58%)], Train Loss: 0.44707\n","Epoch: 00 [10404/17967 ( 58%)], Train Loss: 0.44632\n","Epoch: 00 [10444/17967 ( 58%)], Train Loss: 0.44662\n","Epoch: 00 [10484/17967 ( 58%)], Train Loss: 0.44582\n","Epoch: 00 [10524/17967 ( 59%)], Train Loss: 0.44529\n","Epoch: 00 [10564/17967 ( 59%)], Train Loss: 0.44481\n","Epoch: 00 [10604/17967 ( 59%)], Train Loss: 0.44435\n","Epoch: 00 [10644/17967 ( 59%)], Train Loss: 0.44436\n","Epoch: 00 [10684/17967 ( 59%)], Train Loss: 0.44347\n","Epoch: 00 [10724/17967 ( 60%)], Train Loss: 0.44262\n","Epoch: 00 [10764/17967 ( 60%)], Train Loss: 0.44221\n","Epoch: 00 [10804/17967 ( 60%)], Train Loss: 0.44182\n","Epoch: 00 [10844/17967 ( 60%)], Train Loss: 0.44124\n","Epoch: 00 [10884/17967 ( 61%)], Train Loss: 0.44082\n","Epoch: 00 [10924/17967 ( 61%)], Train Loss: 0.44126\n","Epoch: 00 [10964/17967 ( 61%)], Train Loss: 0.44126\n","Epoch: 00 [11004/17967 ( 61%)], Train Loss: 0.44124\n","Epoch: 00 [11044/17967 ( 61%)], Train Loss: 0.44108\n","Epoch: 00 [11084/17967 ( 62%)], Train Loss: 0.44126\n","Epoch: 00 [11124/17967 ( 62%)], Train Loss: 0.44063\n","Epoch: 00 [11164/17967 ( 62%)], Train Loss: 0.43951\n","Epoch: 00 [11204/17967 ( 62%)], Train Loss: 0.43925\n","Epoch: 00 [11244/17967 ( 63%)], Train Loss: 0.43896\n","Epoch: 00 [11284/17967 ( 63%)], Train Loss: 0.43840\n","Epoch: 00 [11324/17967 ( 63%)], Train Loss: 0.43871\n","Epoch: 00 [11364/17967 ( 63%)], Train Loss: 0.43830\n","Epoch: 00 [11404/17967 ( 63%)], Train Loss: 0.43796\n","Epoch: 00 [11444/17967 ( 64%)], Train Loss: 0.43762\n","Epoch: 00 [11484/17967 ( 64%)], Train Loss: 0.43637\n","Epoch: 00 [11524/17967 ( 64%)], Train Loss: 0.43651\n","Epoch: 00 [11564/17967 ( 64%)], Train Loss: 0.43637\n","Epoch: 00 [11604/17967 ( 65%)], Train Loss: 0.43683\n","Epoch: 00 [11644/17967 ( 65%)], Train Loss: 0.43600\n","Epoch: 00 [11684/17967 ( 65%)], Train Loss: 0.43558\n","Epoch: 00 [11724/17967 ( 65%)], Train Loss: 0.43535\n","Epoch: 00 [11764/17967 ( 65%)], Train Loss: 0.43470\n","Epoch: 00 [11804/17967 ( 66%)], Train Loss: 0.43470\n","Epoch: 00 [11844/17967 ( 66%)], Train Loss: 0.43410\n","Epoch: 00 [11884/17967 ( 66%)], Train Loss: 0.43369\n","Epoch: 00 [11924/17967 ( 66%)], Train Loss: 0.43316\n","Epoch: 00 [11964/17967 ( 67%)], Train Loss: 0.43373\n","Epoch: 00 [12004/17967 ( 67%)], Train Loss: 0.43361\n","Epoch: 00 [12044/17967 ( 67%)], Train Loss: 0.43271\n","Epoch: 00 [12084/17967 ( 67%)], Train Loss: 0.43252\n","Epoch: 00 [12124/17967 ( 67%)], Train Loss: 0.43172\n","Epoch: 00 [12164/17967 ( 68%)], Train Loss: 0.43191\n","Epoch: 00 [12204/17967 ( 68%)], Train Loss: 0.43200\n","Epoch: 00 [12244/17967 ( 68%)], Train Loss: 0.43104\n","Epoch: 00 [12284/17967 ( 68%)], Train Loss: 0.43063\n","Epoch: 00 [12324/17967 ( 69%)], Train Loss: 0.43024\n","Epoch: 00 [12364/17967 ( 69%)], Train Loss: 0.42949\n","Epoch: 00 [12404/17967 ( 69%)], Train Loss: 0.42906\n","Epoch: 00 [12444/17967 ( 69%)], Train Loss: 0.42902\n","Epoch: 00 [12484/17967 ( 69%)], Train Loss: 0.42850\n","Epoch: 00 [12524/17967 ( 70%)], Train Loss: 0.42835\n","Epoch: 00 [12564/17967 ( 70%)], Train Loss: 0.42818\n","Epoch: 00 [12604/17967 ( 70%)], Train Loss: 0.42829\n","Epoch: 00 [12644/17967 ( 70%)], Train Loss: 0.42812\n","Epoch: 00 [12684/17967 ( 71%)], Train Loss: 0.42763\n","Epoch: 00 [12724/17967 ( 71%)], Train Loss: 0.42704\n","Epoch: 00 [12764/17967 ( 71%)], Train Loss: 0.42706\n","Epoch: 00 [12804/17967 ( 71%)], Train Loss: 0.42668\n","Epoch: 00 [12844/17967 ( 71%)], Train Loss: 0.42633\n","Epoch: 00 [12884/17967 ( 72%)], Train Loss: 0.42588\n","Epoch: 00 [12924/17967 ( 72%)], Train Loss: 0.42536\n","Epoch: 00 [12964/17967 ( 72%)], Train Loss: 0.42522\n","Epoch: 00 [13004/17967 ( 72%)], Train Loss: 0.42538\n","Epoch: 00 [13044/17967 ( 73%)], Train Loss: 0.42510\n","Epoch: 00 [13084/17967 ( 73%)], Train Loss: 0.42453\n","Epoch: 00 [13124/17967 ( 73%)], Train Loss: 0.42415\n","Epoch: 00 [13164/17967 ( 73%)], Train Loss: 0.42430\n","Epoch: 00 [13204/17967 ( 73%)], Train Loss: 0.42415\n","Epoch: 00 [13244/17967 ( 74%)], Train Loss: 0.42346\n","Epoch: 00 [13284/17967 ( 74%)], Train Loss: 0.42314\n","Epoch: 00 [13324/17967 ( 74%)], Train Loss: 0.42248\n","Epoch: 00 [13364/17967 ( 74%)], Train Loss: 0.42250\n","Epoch: 00 [13404/17967 ( 75%)], Train Loss: 0.42205\n","Epoch: 00 [13444/17967 ( 75%)], Train Loss: 0.42174\n","Epoch: 00 [13484/17967 ( 75%)], Train Loss: 0.42153\n","Epoch: 00 [13524/17967 ( 75%)], Train Loss: 0.42164\n","Epoch: 00 [13564/17967 ( 75%)], Train Loss: 0.42205\n","Epoch: 00 [13604/17967 ( 76%)], Train Loss: 0.42143\n","Epoch: 00 [13644/17967 ( 76%)], Train Loss: 0.42161\n","Epoch: 00 [13684/17967 ( 76%)], Train Loss: 0.42152\n","Epoch: 00 [13724/17967 ( 76%)], Train Loss: 0.42213\n","Epoch: 00 [13764/17967 ( 77%)], Train Loss: 0.42131\n","Epoch: 00 [13804/17967 ( 77%)], Train Loss: 0.42074\n","Epoch: 00 [13844/17967 ( 77%)], Train Loss: 0.42043\n","Epoch: 00 [13884/17967 ( 77%)], Train Loss: 0.41981\n","Epoch: 00 [13924/17967 ( 77%)], Train Loss: 0.41917\n","Epoch: 00 [13964/17967 ( 78%)], Train Loss: 0.41862\n","Epoch: 00 [14004/17967 ( 78%)], Train Loss: 0.41884\n","Epoch: 00 [14044/17967 ( 78%)], Train Loss: 0.41860\n","Epoch: 00 [14084/17967 ( 78%)], Train Loss: 0.41846\n","Epoch: 00 [14124/17967 ( 79%)], Train Loss: 0.41817\n","Epoch: 00 [14164/17967 ( 79%)], Train Loss: 0.41766\n","Epoch: 00 [14204/17967 ( 79%)], Train Loss: 0.41675\n","Epoch: 00 [14244/17967 ( 79%)], Train Loss: 0.41683\n","Epoch: 00 [14284/17967 ( 80%)], Train Loss: 0.41658\n","Epoch: 00 [14324/17967 ( 80%)], Train Loss: 0.41598\n","Epoch: 00 [14364/17967 ( 80%)], Train Loss: 0.41557\n","Epoch: 00 [14404/17967 ( 80%)], Train Loss: 0.41501\n","Epoch: 00 [14444/17967 ( 80%)], Train Loss: 0.41462\n","Epoch: 00 [14484/17967 ( 81%)], Train Loss: 0.41489\n","Epoch: 00 [14524/17967 ( 81%)], Train Loss: 0.41515\n","Epoch: 00 [14564/17967 ( 81%)], Train Loss: 0.41450\n","Epoch: 00 [14604/17967 ( 81%)], Train Loss: 0.41445\n","Epoch: 00 [14644/17967 ( 82%)], Train Loss: 0.41397\n","Epoch: 00 [14684/17967 ( 82%)], Train Loss: 0.41351\n","Epoch: 00 [14724/17967 ( 82%)], Train Loss: 0.41322\n","Epoch: 00 [14764/17967 ( 82%)], Train Loss: 0.41254\n","Epoch: 00 [14804/17967 ( 82%)], Train Loss: 0.41222\n","Epoch: 00 [14844/17967 ( 83%)], Train Loss: 0.41138\n","Epoch: 00 [14884/17967 ( 83%)], Train Loss: 0.41232\n","Epoch: 00 [14924/17967 ( 83%)], Train Loss: 0.41235\n","Epoch: 00 [14964/17967 ( 83%)], Train Loss: 0.41209\n","Epoch: 00 [15004/17967 ( 84%)], Train Loss: 0.41192\n","Epoch: 00 [15044/17967 ( 84%)], Train Loss: 0.41152\n","Epoch: 00 [15084/17967 ( 84%)], Train Loss: 0.41124\n","Epoch: 00 [15124/17967 ( 84%)], Train Loss: 0.41083\n","Epoch: 00 [15164/17967 ( 84%)], Train Loss: 0.41056\n","Epoch: 00 [15204/17967 ( 85%)], Train Loss: 0.41046\n","Epoch: 00 [15244/17967 ( 85%)], Train Loss: 0.41033\n","Epoch: 00 [15284/17967 ( 85%)], Train Loss: 0.41009\n","Epoch: 00 [15324/17967 ( 85%)], Train Loss: 0.41000\n","Epoch: 00 [15364/17967 ( 86%)], Train Loss: 0.40972\n","Epoch: 00 [15404/17967 ( 86%)], Train Loss: 0.40969\n","Epoch: 00 [15444/17967 ( 86%)], Train Loss: 0.40948\n","Epoch: 00 [15484/17967 ( 86%)], Train Loss: 0.40914\n","Epoch: 00 [15524/17967 ( 86%)], Train Loss: 0.40864\n","Epoch: 00 [15564/17967 ( 87%)], Train Loss: 0.40860\n","Epoch: 00 [15604/17967 ( 87%)], Train Loss: 0.40850\n","Epoch: 00 [15644/17967 ( 87%)], Train Loss: 0.40847\n","Epoch: 00 [15684/17967 ( 87%)], Train Loss: 0.40866\n","Epoch: 00 [15724/17967 ( 88%)], Train Loss: 0.40828\n","Epoch: 00 [15764/17967 ( 88%)], Train Loss: 0.40797\n","Epoch: 00 [15804/17967 ( 88%)], Train Loss: 0.40727\n","Epoch: 00 [15844/17967 ( 88%)], Train Loss: 0.40657\n","Epoch: 00 [15884/17967 ( 88%)], Train Loss: 0.40638\n","Epoch: 00 [15924/17967 ( 89%)], Train Loss: 0.40618\n","Epoch: 00 [15964/17967 ( 89%)], Train Loss: 0.40557\n","Epoch: 00 [16004/17967 ( 89%)], Train Loss: 0.40520\n","Epoch: 00 [16044/17967 ( 89%)], Train Loss: 0.40533\n","Epoch: 00 [16084/17967 ( 90%)], Train Loss: 0.40554\n","Epoch: 00 [16124/17967 ( 90%)], Train Loss: 0.40515\n","Epoch: 00 [16164/17967 ( 90%)], Train Loss: 0.40503\n","Epoch: 00 [16204/17967 ( 90%)], Train Loss: 0.40514\n","Epoch: 00 [16244/17967 ( 90%)], Train Loss: 0.40492\n","Epoch: 00 [16284/17967 ( 91%)], Train Loss: 0.40473\n","Epoch: 00 [16324/17967 ( 91%)], Train Loss: 0.40436\n","Epoch: 00 [16364/17967 ( 91%)], Train Loss: 0.40413\n","Epoch: 00 [16404/17967 ( 91%)], Train Loss: 0.40407\n","Epoch: 00 [16444/17967 ( 92%)], Train Loss: 0.40338\n","Epoch: 00 [16484/17967 ( 92%)], Train Loss: 0.40288\n","Epoch: 00 [16524/17967 ( 92%)], Train Loss: 0.40271\n","Epoch: 00 [16564/17967 ( 92%)], Train Loss: 0.40242\n","Epoch: 00 [16604/17967 ( 92%)], Train Loss: 0.40251\n","Epoch: 00 [16644/17967 ( 93%)], Train Loss: 0.40241\n","Epoch: 00 [16684/17967 ( 93%)], Train Loss: 0.40239\n","Epoch: 00 [16724/17967 ( 93%)], Train Loss: 0.40202\n","Epoch: 00 [16764/17967 ( 93%)], Train Loss: 0.40131\n","Epoch: 00 [16804/17967 ( 94%)], Train Loss: 0.40119\n","Epoch: 00 [16844/17967 ( 94%)], Train Loss: 0.40098\n","Epoch: 00 [16884/17967 ( 94%)], Train Loss: 0.40082\n","Epoch: 00 [16924/17967 ( 94%)], Train Loss: 0.40113\n","Epoch: 00 [16964/17967 ( 94%)], Train Loss: 0.40072\n","Epoch: 00 [17004/17967 ( 95%)], Train Loss: 0.40077\n","Epoch: 00 [17044/17967 ( 95%)], Train Loss: 0.40029\n","Epoch: 00 [17084/17967 ( 95%)], Train Loss: 0.39995\n","Epoch: 00 [17124/17967 ( 95%)], Train Loss: 0.39965\n","Epoch: 00 [17164/17967 ( 96%)], Train Loss: 0.39940\n","Epoch: 00 [17204/17967 ( 96%)], Train Loss: 0.39922\n","Epoch: 00 [17244/17967 ( 96%)], Train Loss: 0.39908\n","Epoch: 00 [17284/17967 ( 96%)], Train Loss: 0.39887\n","Epoch: 00 [17324/17967 ( 96%)], Train Loss: 0.39841\n","Epoch: 00 [17364/17967 ( 97%)], Train Loss: 0.39796\n","Epoch: 00 [17404/17967 ( 97%)], Train Loss: 0.39765\n","Epoch: 00 [17444/17967 ( 97%)], Train Loss: 0.39743\n","Epoch: 00 [17484/17967 ( 97%)], Train Loss: 0.39756\n","Epoch: 00 [17524/17967 ( 98%)], Train Loss: 0.39709\n","Epoch: 00 [17564/17967 ( 98%)], Train Loss: 0.39674\n","Epoch: 00 [17604/17967 ( 98%)], Train Loss: 0.39644\n","Epoch: 00 [17644/17967 ( 98%)], Train Loss: 0.39631\n","Epoch: 00 [17684/17967 ( 98%)], Train Loss: 0.39612\n","Epoch: 00 [17724/17967 ( 99%)], Train Loss: 0.39573\n","Epoch: 00 [17764/17967 ( 99%)], Train Loss: 0.39532\n","Epoch: 00 [17804/17967 ( 99%)], Train Loss: 0.39498\n","Epoch: 00 [17844/17967 ( 99%)], Train Loss: 0.39476\n","Epoch: 00 [17884/17967 (100%)], Train Loss: 0.39429\n","Epoch: 00 [17924/17967 (100%)], Train Loss: 0.39421\n","Epoch: 00 [17964/17967 (100%)], Train Loss: 0.39402\n","Epoch: 00 [17967/17967 (100%)], Train Loss: 0.39405\n","----Validation Results Summary----\n","Epoch: [0] Valid Loss: 0.55720\n","0 Epoch, Best epoch was updated! Valid Loss: 0.55720\n","Saving model checkpoint to chaii-qa-5-fold-xlmroberta-torch-fit/checkpoint-fold-2.\n","\n","Total Training Time: 2496.9487297534943secs, Average Training Time per Epoch: 2496.9487297534943secs.\n","Total Validation Time: 214.7231900691986secs, Average Validation Time per Epoch: 214.7231900691986secs.\n","\n","\n","--------------------------------------------------\n","FOLD: 3\n","--------------------------------------------------\n","Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n","Num examples Train= 18765, Num examples Valid=4304\n","Total Training Steps: 2346, Total Warmup Steps: 234\n","Epoch: 00 [    4/18765 (  0%)], Train Loss: 2.83550\n","Epoch: 00 [   44/18765 (  0%)], Train Loss: 2.92596\n","Epoch: 00 [   84/18765 (  0%)], Train Loss: 2.88841\n","Epoch: 00 [  124/18765 (  1%)], Train Loss: 2.83932\n","Epoch: 00 [  164/18765 (  1%)], Train Loss: 2.77994\n","Epoch: 00 [  204/18765 (  1%)], Train Loss: 2.71142\n","Epoch: 00 [  244/18765 (  1%)], Train Loss: 2.63200\n","Epoch: 00 [  284/18765 (  2%)], Train Loss: 2.54595\n","Epoch: 00 [  324/18765 (  2%)], Train Loss: 2.43741\n","Epoch: 00 [  364/18765 (  2%)], Train Loss: 2.33767\n","Epoch: 00 [  404/18765 (  2%)], Train Loss: 2.23204\n","Epoch: 00 [  444/18765 (  2%)], Train Loss: 2.10473\n","Epoch: 00 [  484/18765 (  3%)], Train Loss: 1.97282\n","Epoch: 00 [  524/18765 (  3%)], Train Loss: 1.88409\n","Epoch: 00 [  564/18765 (  3%)], Train Loss: 1.79097\n","Epoch: 00 [  604/18765 (  3%)], Train Loss: 1.70640\n","Epoch: 00 [  644/18765 (  3%)], Train Loss: 1.62915\n","Epoch: 00 [  684/18765 (  4%)], Train Loss: 1.57151\n","Epoch: 00 [  724/18765 (  4%)], Train Loss: 1.50584\n","Epoch: 00 [  764/18765 (  4%)], Train Loss: 1.44642\n","Epoch: 00 [  804/18765 (  4%)], Train Loss: 1.38910\n","Epoch: 00 [  844/18765 (  4%)], Train Loss: 1.34622\n","Epoch: 00 [  884/18765 (  5%)], Train Loss: 1.30582\n","Epoch: 00 [  924/18765 (  5%)], Train Loss: 1.26315\n","Epoch: 00 [  964/18765 (  5%)], Train Loss: 1.22938\n","Epoch: 00 [ 1004/18765 (  5%)], Train Loss: 1.18790\n","Epoch: 00 [ 1044/18765 (  6%)], Train Loss: 1.16610\n","Epoch: 00 [ 1084/18765 (  6%)], Train Loss: 1.13105\n","Epoch: 00 [ 1124/18765 (  6%)], Train Loss: 1.10795\n","Epoch: 00 [ 1164/18765 (  6%)], Train Loss: 1.08452\n","Epoch: 00 [ 1204/18765 (  6%)], Train Loss: 1.06333\n","Epoch: 00 [ 1244/18765 (  7%)], Train Loss: 1.04228\n","Epoch: 00 [ 1284/18765 (  7%)], Train Loss: 1.03298\n","Epoch: 00 [ 1324/18765 (  7%)], Train Loss: 1.01009\n","Epoch: 00 [ 1364/18765 (  7%)], Train Loss: 0.98998\n","Epoch: 00 [ 1404/18765 (  7%)], Train Loss: 0.96647\n","Epoch: 00 [ 1444/18765 (  8%)], Train Loss: 0.95119\n","Epoch: 00 [ 1484/18765 (  8%)], Train Loss: 0.93403\n","Epoch: 00 [ 1524/18765 (  8%)], Train Loss: 0.92049\n","Epoch: 00 [ 1564/18765 (  8%)], Train Loss: 0.90720\n","Epoch: 00 [ 1604/18765 (  9%)], Train Loss: 0.89861\n","Epoch: 00 [ 1644/18765 (  9%)], Train Loss: 0.88476\n","Epoch: 00 [ 1684/18765 (  9%)], Train Loss: 0.87244\n","Epoch: 00 [ 1724/18765 (  9%)], Train Loss: 0.86275\n","Epoch: 00 [ 1764/18765 (  9%)], Train Loss: 0.85258\n","Epoch: 00 [ 1804/18765 ( 10%)], Train Loss: 0.84263\n","Epoch: 00 [ 1844/18765 ( 10%)], Train Loss: 0.83539\n","Epoch: 00 [ 1884/18765 ( 10%)], Train Loss: 0.82742\n","Epoch: 00 [ 1924/18765 ( 10%)], Train Loss: 0.81781\n","Epoch: 00 [ 1964/18765 ( 10%)], Train Loss: 0.80635\n","Epoch: 00 [ 2004/18765 ( 11%)], Train Loss: 0.79851\n","Epoch: 00 [ 2044/18765 ( 11%)], Train Loss: 0.78893\n","Epoch: 00 [ 2084/18765 ( 11%)], Train Loss: 0.77961\n","Epoch: 00 [ 2124/18765 ( 11%)], Train Loss: 0.77760\n","Epoch: 00 [ 2164/18765 ( 12%)], Train Loss: 0.77135\n","Epoch: 00 [ 2204/18765 ( 12%)], Train Loss: 0.76419\n","Epoch: 00 [ 2244/18765 ( 12%)], Train Loss: 0.75696\n","Epoch: 00 [ 2284/18765 ( 12%)], Train Loss: 0.75121\n","Epoch: 00 [ 2324/18765 ( 12%)], Train Loss: 0.74474\n","Epoch: 00 [ 2364/18765 ( 13%)], Train Loss: 0.73838\n","Epoch: 00 [ 2404/18765 ( 13%)], Train Loss: 0.73239\n","Epoch: 00 [ 2444/18765 ( 13%)], Train Loss: 0.72863\n","Epoch: 00 [ 2484/18765 ( 13%)], Train Loss: 0.72390\n","Epoch: 00 [ 2524/18765 ( 13%)], Train Loss: 0.71568\n","Epoch: 00 [ 2564/18765 ( 14%)], Train Loss: 0.70920\n","Epoch: 00 [ 2604/18765 ( 14%)], Train Loss: 0.70131\n","Epoch: 00 [ 2644/18765 ( 14%)], Train Loss: 0.69665\n","Epoch: 00 [ 2684/18765 ( 14%)], Train Loss: 0.69056\n","Epoch: 00 [ 2724/18765 ( 15%)], Train Loss: 0.68386\n","Epoch: 00 [ 2764/18765 ( 15%)], Train Loss: 0.67966\n","Epoch: 00 [ 2804/18765 ( 15%)], Train Loss: 0.67435\n","Epoch: 00 [ 2844/18765 ( 15%)], Train Loss: 0.66874\n","Epoch: 00 [ 2884/18765 ( 15%)], Train Loss: 0.66421\n","Epoch: 00 [ 2924/18765 ( 16%)], Train Loss: 0.66270\n","Epoch: 00 [ 2964/18765 ( 16%)], Train Loss: 0.66019\n","Epoch: 00 [ 3004/18765 ( 16%)], Train Loss: 0.65886\n","Epoch: 00 [ 3044/18765 ( 16%)], Train Loss: 0.65670\n","Epoch: 00 [ 3084/18765 ( 16%)], Train Loss: 0.65309\n","Epoch: 00 [ 3124/18765 ( 17%)], Train Loss: 0.64993\n","Epoch: 00 [ 3164/18765 ( 17%)], Train Loss: 0.64773\n","Epoch: 00 [ 3204/18765 ( 17%)], Train Loss: 0.64327\n","Epoch: 00 [ 3244/18765 ( 17%)], Train Loss: 0.63932\n","Epoch: 00 [ 3284/18765 ( 18%)], Train Loss: 0.63651\n","Epoch: 00 [ 3324/18765 ( 18%)], Train Loss: 0.63184\n","Epoch: 00 [ 3364/18765 ( 18%)], Train Loss: 0.62691\n","Epoch: 00 [ 3404/18765 ( 18%)], Train Loss: 0.62408\n","Epoch: 00 [ 3444/18765 ( 18%)], Train Loss: 0.61991\n","Epoch: 00 [ 3484/18765 ( 19%)], Train Loss: 0.61617\n","Epoch: 00 [ 3524/18765 ( 19%)], Train Loss: 0.61228\n","Epoch: 00 [ 3564/18765 ( 19%)], Train Loss: 0.61084\n","Epoch: 00 [ 3604/18765 ( 19%)], Train Loss: 0.60961\n","Epoch: 00 [ 3644/18765 ( 19%)], Train Loss: 0.60564\n","Epoch: 00 [ 3684/18765 ( 20%)], Train Loss: 0.60369\n","Epoch: 00 [ 3724/18765 ( 20%)], Train Loss: 0.60025\n","Epoch: 00 [ 3764/18765 ( 20%)], Train Loss: 0.59931\n","Epoch: 00 [ 3804/18765 ( 20%)], Train Loss: 0.59653\n","Epoch: 00 [ 3844/18765 ( 20%)], Train Loss: 0.59341\n","Epoch: 00 [ 3884/18765 ( 21%)], Train Loss: 0.59161\n","Epoch: 00 [ 3924/18765 ( 21%)], Train Loss: 0.58820\n","Epoch: 00 [ 3964/18765 ( 21%)], Train Loss: 0.58637\n","Epoch: 00 [ 4004/18765 ( 21%)], Train Loss: 0.58463\n","Epoch: 00 [ 4044/18765 ( 22%)], Train Loss: 0.58101\n","Epoch: 00 [ 4084/18765 ( 22%)], Train Loss: 0.57780\n","Epoch: 00 [ 4124/18765 ( 22%)], Train Loss: 0.57426\n","Epoch: 00 [ 4164/18765 ( 22%)], Train Loss: 0.57170\n","Epoch: 00 [ 4204/18765 ( 22%)], Train Loss: 0.56979\n","Epoch: 00 [ 4244/18765 ( 23%)], Train Loss: 0.56802\n","Epoch: 00 [ 4284/18765 ( 23%)], Train Loss: 0.56597\n","Epoch: 00 [ 4324/18765 ( 23%)], Train Loss: 0.56324\n","Epoch: 00 [ 4364/18765 ( 23%)], Train Loss: 0.56012\n","Epoch: 00 [ 4404/18765 ( 23%)], Train Loss: 0.55844\n","Epoch: 00 [ 4444/18765 ( 24%)], Train Loss: 0.55650\n","Epoch: 00 [ 4484/18765 ( 24%)], Train Loss: 0.55629\n","Epoch: 00 [ 4524/18765 ( 24%)], Train Loss: 0.55374\n","Epoch: 00 [ 4564/18765 ( 24%)], Train Loss: 0.55166\n","Epoch: 00 [ 4604/18765 ( 25%)], Train Loss: 0.54937\n","Epoch: 00 [ 4644/18765 ( 25%)], Train Loss: 0.54673\n","Epoch: 00 [ 4684/18765 ( 25%)], Train Loss: 0.54750\n","Epoch: 00 [ 4724/18765 ( 25%)], Train Loss: 0.54697\n","Epoch: 00 [ 4764/18765 ( 25%)], Train Loss: 0.54461\n","Epoch: 00 [ 4804/18765 ( 26%)], Train Loss: 0.54482\n","Epoch: 00 [ 4844/18765 ( 26%)], Train Loss: 0.54353\n","Epoch: 00 [ 4884/18765 ( 26%)], Train Loss: 0.54216\n","Epoch: 00 [ 4924/18765 ( 26%)], Train Loss: 0.54031\n","Epoch: 00 [ 4964/18765 ( 26%)], Train Loss: 0.54142\n","Epoch: 00 [ 5004/18765 ( 27%)], Train Loss: 0.54049\n","Epoch: 00 [ 5044/18765 ( 27%)], Train Loss: 0.53851\n","Epoch: 00 [ 5084/18765 ( 27%)], Train Loss: 0.53737\n","Epoch: 00 [ 5124/18765 ( 27%)], Train Loss: 0.53543\n","Epoch: 00 [ 5164/18765 ( 28%)], Train Loss: 0.53406\n","Epoch: 00 [ 5204/18765 ( 28%)], Train Loss: 0.53173\n","Epoch: 00 [ 5244/18765 ( 28%)], Train Loss: 0.53086\n","Epoch: 00 [ 5284/18765 ( 28%)], Train Loss: 0.53075\n","Epoch: 00 [ 5324/18765 ( 28%)], Train Loss: 0.52973\n","Epoch: 00 [ 5364/18765 ( 29%)], Train Loss: 0.52884\n","Epoch: 00 [ 5404/18765 ( 29%)], Train Loss: 0.52854\n","Epoch: 00 [ 5444/18765 ( 29%)], Train Loss: 0.52729\n","Epoch: 00 [ 5484/18765 ( 29%)], Train Loss: 0.52575\n","Epoch: 00 [ 5524/18765 ( 29%)], Train Loss: 0.52414\n","Epoch: 00 [ 5564/18765 ( 30%)], Train Loss: 0.52422\n","Epoch: 00 [ 5604/18765 ( 30%)], Train Loss: 0.52228\n","Epoch: 00 [ 5644/18765 ( 30%)], Train Loss: 0.52018\n","Epoch: 00 [ 5684/18765 ( 30%)], Train Loss: 0.51902\n","Epoch: 00 [ 5724/18765 ( 31%)], Train Loss: 0.51826\n","Epoch: 00 [ 5764/18765 ( 31%)], Train Loss: 0.51781\n","Epoch: 00 [ 5804/18765 ( 31%)], Train Loss: 0.51597\n","Epoch: 00 [ 5844/18765 ( 31%)], Train Loss: 0.51440\n","Epoch: 00 [ 5884/18765 ( 31%)], Train Loss: 0.51199\n","Epoch: 00 [ 5924/18765 ( 32%)], Train Loss: 0.51055\n","Epoch: 00 [ 5964/18765 ( 32%)], Train Loss: 0.50934\n","Epoch: 00 [ 6004/18765 ( 32%)], Train Loss: 0.50705\n","Epoch: 00 [ 6044/18765 ( 32%)], Train Loss: 0.50715\n","Epoch: 00 [ 6084/18765 ( 32%)], Train Loss: 0.50540\n","Epoch: 00 [ 6124/18765 ( 33%)], Train Loss: 0.50375\n","Epoch: 00 [ 6164/18765 ( 33%)], Train Loss: 0.50320\n","Epoch: 00 [ 6204/18765 ( 33%)], Train Loss: 0.50171\n","Epoch: 00 [ 6244/18765 ( 33%)], Train Loss: 0.50081\n","Epoch: 00 [ 6284/18765 ( 33%)], Train Loss: 0.50086\n","Epoch: 00 [ 6324/18765 ( 34%)], Train Loss: 0.49995\n","Epoch: 00 [ 6364/18765 ( 34%)], Train Loss: 0.49934\n","Epoch: 00 [ 6404/18765 ( 34%)], Train Loss: 0.49866\n","Epoch: 00 [ 6444/18765 ( 34%)], Train Loss: 0.49706\n","Epoch: 00 [ 6484/18765 ( 35%)], Train Loss: 0.49593\n","Epoch: 00 [ 6524/18765 ( 35%)], Train Loss: 0.49451\n","Epoch: 00 [ 6564/18765 ( 35%)], Train Loss: 0.49374\n","Epoch: 00 [ 6604/18765 ( 35%)], Train Loss: 0.49292\n","Epoch: 00 [ 6644/18765 ( 35%)], Train Loss: 0.49137\n","Epoch: 00 [ 6684/18765 ( 36%)], Train Loss: 0.49014\n","Epoch: 00 [ 6724/18765 ( 36%)], Train Loss: 0.49037\n","Epoch: 00 [ 6764/18765 ( 36%)], Train Loss: 0.49027\n","Epoch: 00 [ 6804/18765 ( 36%)], Train Loss: 0.48993\n","Epoch: 00 [ 6844/18765 ( 36%)], Train Loss: 0.48968\n","Epoch: 00 [ 6884/18765 ( 37%)], Train Loss: 0.48853\n","Epoch: 00 [ 6924/18765 ( 37%)], Train Loss: 0.48748\n","Epoch: 00 [ 6964/18765 ( 37%)], Train Loss: 0.48794\n","Epoch: 00 [ 7004/18765 ( 37%)], Train Loss: 0.48710\n","Epoch: 00 [ 7044/18765 ( 38%)], Train Loss: 0.48642\n","Epoch: 00 [ 7084/18765 ( 38%)], Train Loss: 0.48533\n","Epoch: 00 [ 7124/18765 ( 38%)], Train Loss: 0.48362\n","Epoch: 00 [ 7164/18765 ( 38%)], Train Loss: 0.48206\n","Epoch: 00 [ 7204/18765 ( 38%)], Train Loss: 0.48035\n","Epoch: 00 [ 7244/18765 ( 39%)], Train Loss: 0.48037\n","Epoch: 00 [ 7284/18765 ( 39%)], Train Loss: 0.47895\n","Epoch: 00 [ 7324/18765 ( 39%)], Train Loss: 0.47786\n","Epoch: 00 [ 7364/18765 ( 39%)], Train Loss: 0.47793\n","Epoch: 00 [ 7404/18765 ( 39%)], Train Loss: 0.47755\n","Epoch: 00 [ 7444/18765 ( 40%)], Train Loss: 0.47656\n","Epoch: 00 [ 7484/18765 ( 40%)], Train Loss: 0.47582\n","Epoch: 00 [ 7524/18765 ( 40%)], Train Loss: 0.47578\n","Epoch: 00 [ 7564/18765 ( 40%)], Train Loss: 0.47415\n","Epoch: 00 [ 7604/18765 ( 41%)], Train Loss: 0.47357\n","Epoch: 00 [ 7644/18765 ( 41%)], Train Loss: 0.47210\n","Epoch: 00 [ 7684/18765 ( 41%)], Train Loss: 0.47095\n","Epoch: 00 [ 7724/18765 ( 41%)], Train Loss: 0.47045\n","Epoch: 00 [ 7764/18765 ( 41%)], Train Loss: 0.47077\n","Epoch: 00 [ 7804/18765 ( 42%)], Train Loss: 0.47133\n","Epoch: 00 [ 7844/18765 ( 42%)], Train Loss: 0.47038\n","Epoch: 00 [ 7884/18765 ( 42%)], Train Loss: 0.46977\n","Epoch: 00 [ 7924/18765 ( 42%)], Train Loss: 0.46918\n","Epoch: 00 [ 7964/18765 ( 42%)], Train Loss: 0.46841\n","Epoch: 00 [ 8004/18765 ( 43%)], Train Loss: 0.46773\n","Epoch: 00 [ 8044/18765 ( 43%)], Train Loss: 0.46740\n","Epoch: 00 [ 8084/18765 ( 43%)], Train Loss: 0.46684\n","Epoch: 00 [ 8124/18765 ( 43%)], Train Loss: 0.46647\n","Epoch: 00 [ 8164/18765 ( 44%)], Train Loss: 0.46580\n","Epoch: 00 [ 8204/18765 ( 44%)], Train Loss: 0.46630\n","Epoch: 00 [ 8244/18765 ( 44%)], Train Loss: 0.46525\n","Epoch: 00 [ 8284/18765 ( 44%)], Train Loss: 0.46414\n","Epoch: 00 [ 8324/18765 ( 44%)], Train Loss: 0.46449\n","Epoch: 00 [ 8364/18765 ( 45%)], Train Loss: 0.46355\n","Epoch: 00 [ 8404/18765 ( 45%)], Train Loss: 0.46283\n","Epoch: 00 [ 8444/18765 ( 45%)], Train Loss: 0.46277\n","Epoch: 00 [ 8484/18765 ( 45%)], Train Loss: 0.46251\n","Epoch: 00 [ 8524/18765 ( 45%)], Train Loss: 0.46169\n","Epoch: 00 [ 8564/18765 ( 46%)], Train Loss: 0.46036\n","Epoch: 00 [ 8604/18765 ( 46%)], Train Loss: 0.45939\n","Epoch: 00 [ 8644/18765 ( 46%)], Train Loss: 0.45903\n","Epoch: 00 [ 8684/18765 ( 46%)], Train Loss: 0.45792\n","Epoch: 00 [ 8724/18765 ( 46%)], Train Loss: 0.45732\n","Epoch: 00 [ 8764/18765 ( 47%)], Train Loss: 0.45706\n","Epoch: 00 [ 8804/18765 ( 47%)], Train Loss: 0.45624\n","Epoch: 00 [ 8844/18765 ( 47%)], Train Loss: 0.45677\n","Epoch: 00 [ 8884/18765 ( 47%)], Train Loss: 0.45657\n","Epoch: 00 [ 8924/18765 ( 48%)], Train Loss: 0.45576\n","Epoch: 00 [ 8964/18765 ( 48%)], Train Loss: 0.45643\n","Epoch: 00 [ 9004/18765 ( 48%)], Train Loss: 0.45669\n","Epoch: 00 [ 9044/18765 ( 48%)], Train Loss: 0.45586\n","Epoch: 00 [ 9084/18765 ( 48%)], Train Loss: 0.45471\n","Epoch: 00 [ 9124/18765 ( 49%)], Train Loss: 0.45458\n","Epoch: 00 [ 9164/18765 ( 49%)], Train Loss: 0.45365\n","Epoch: 00 [ 9204/18765 ( 49%)], Train Loss: 0.45271\n","Epoch: 00 [ 9244/18765 ( 49%)], Train Loss: 0.45230\n","Epoch: 00 [ 9284/18765 ( 49%)], Train Loss: 0.45204\n","Epoch: 00 [ 9324/18765 ( 50%)], Train Loss: 0.45134\n","Epoch: 00 [ 9364/18765 ( 50%)], Train Loss: 0.45072\n","Epoch: 00 [ 9404/18765 ( 50%)], Train Loss: 0.45017\n","Epoch: 00 [ 9444/18765 ( 50%)], Train Loss: 0.44981\n","Epoch: 00 [ 9484/18765 ( 51%)], Train Loss: 0.44944\n","Epoch: 00 [ 9524/18765 ( 51%)], Train Loss: 0.44949\n","Epoch: 00 [ 9564/18765 ( 51%)], Train Loss: 0.44851\n","Epoch: 00 [ 9604/18765 ( 51%)], Train Loss: 0.44789\n","Epoch: 00 [ 9644/18765 ( 51%)], Train Loss: 0.44686\n","Epoch: 00 [ 9684/18765 ( 52%)], Train Loss: 0.44657\n","Epoch: 00 [ 9724/18765 ( 52%)], Train Loss: 0.44660\n","Epoch: 00 [ 9764/18765 ( 52%)], Train Loss: 0.44566\n","Epoch: 00 [ 9804/18765 ( 52%)], Train Loss: 0.44475\n","Epoch: 00 [ 9844/18765 ( 52%)], Train Loss: 0.44431\n","Epoch: 00 [ 9884/18765 ( 53%)], Train Loss: 0.44420\n","Epoch: 00 [ 9924/18765 ( 53%)], Train Loss: 0.44364\n","Epoch: 00 [ 9964/18765 ( 53%)], Train Loss: 0.44276\n","Epoch: 00 [10004/18765 ( 53%)], Train Loss: 0.44229\n","Epoch: 00 [10044/18765 ( 54%)], Train Loss: 0.44237\n","Epoch: 00 [10084/18765 ( 54%)], Train Loss: 0.44179\n","Epoch: 00 [10124/18765 ( 54%)], Train Loss: 0.44062\n","Epoch: 00 [10164/18765 ( 54%)], Train Loss: 0.43960\n","Epoch: 00 [10204/18765 ( 54%)], Train Loss: 0.43855\n","Epoch: 00 [10244/18765 ( 55%)], Train Loss: 0.43790\n","Epoch: 00 [10284/18765 ( 55%)], Train Loss: 0.43812\n","Epoch: 00 [10324/18765 ( 55%)], Train Loss: 0.43744\n","Epoch: 00 [10364/18765 ( 55%)], Train Loss: 0.43652\n","Epoch: 00 [10404/18765 ( 55%)], Train Loss: 0.43679\n","Epoch: 00 [10444/18765 ( 56%)], Train Loss: 0.43647\n","Epoch: 00 [10484/18765 ( 56%)], Train Loss: 0.43556\n","Epoch: 00 [10524/18765 ( 56%)], Train Loss: 0.43603\n","Epoch: 00 [10564/18765 ( 56%)], Train Loss: 0.43627\n","Epoch: 00 [10604/18765 ( 57%)], Train Loss: 0.43553\n","Epoch: 00 [10644/18765 ( 57%)], Train Loss: 0.43527\n","Epoch: 00 [10684/18765 ( 57%)], Train Loss: 0.43458\n","Epoch: 00 [10724/18765 ( 57%)], Train Loss: 0.43456\n","Epoch: 00 [10764/18765 ( 57%)], Train Loss: 0.43404\n","Epoch: 00 [10804/18765 ( 58%)], Train Loss: 0.43368\n","Epoch: 00 [10844/18765 ( 58%)], Train Loss: 0.43315\n","Epoch: 00 [10884/18765 ( 58%)], Train Loss: 0.43267\n","Epoch: 00 [10924/18765 ( 58%)], Train Loss: 0.43214\n","Epoch: 00 [10964/18765 ( 58%)], Train Loss: 0.43145\n","Epoch: 00 [11004/18765 ( 59%)], Train Loss: 0.43206\n","Epoch: 00 [11044/18765 ( 59%)], Train Loss: 0.43207\n","Epoch: 00 [11084/18765 ( 59%)], Train Loss: 0.43186\n","Epoch: 00 [11124/18765 ( 59%)], Train Loss: 0.43128\n","Epoch: 00 [11164/18765 ( 59%)], Train Loss: 0.43131\n","Epoch: 00 [11204/18765 ( 60%)], Train Loss: 0.43055\n","Epoch: 00 [11244/18765 ( 60%)], Train Loss: 0.43007\n","Epoch: 00 [11284/18765 ( 60%)], Train Loss: 0.42954\n","Epoch: 00 [11324/18765 ( 60%)], Train Loss: 0.42870\n","Epoch: 00 [11364/18765 ( 61%)], Train Loss: 0.42863\n","Epoch: 00 [11404/18765 ( 61%)], Train Loss: 0.42832\n","Epoch: 00 [11444/18765 ( 61%)], Train Loss: 0.42789\n","Epoch: 00 [11484/18765 ( 61%)], Train Loss: 0.42753\n","Epoch: 00 [11524/18765 ( 61%)], Train Loss: 0.42719\n","Epoch: 00 [11564/18765 ( 62%)], Train Loss: 0.42709\n","Epoch: 00 [11604/18765 ( 62%)], Train Loss: 0.42729\n","Epoch: 00 [11644/18765 ( 62%)], Train Loss: 0.42673\n","Epoch: 00 [11684/18765 ( 62%)], Train Loss: 0.42621\n","Epoch: 00 [11724/18765 ( 62%)], Train Loss: 0.42605\n","Epoch: 00 [11764/18765 ( 63%)], Train Loss: 0.42662\n","Epoch: 00 [11804/18765 ( 63%)], Train Loss: 0.42621\n","Epoch: 00 [11844/18765 ( 63%)], Train Loss: 0.42625\n","Epoch: 00 [11884/18765 ( 63%)], Train Loss: 0.42656\n","Epoch: 00 [11924/18765 ( 64%)], Train Loss: 0.42635\n","Epoch: 00 [11964/18765 ( 64%)], Train Loss: 0.42598\n","Epoch: 00 [12004/18765 ( 64%)], Train Loss: 0.42551\n","Epoch: 00 [12044/18765 ( 64%)], Train Loss: 0.42508\n","Epoch: 00 [12084/18765 ( 64%)], Train Loss: 0.42436\n","Epoch: 00 [12124/18765 ( 65%)], Train Loss: 0.42368\n","Epoch: 00 [12164/18765 ( 65%)], Train Loss: 0.42304\n","Epoch: 00 [12204/18765 ( 65%)], Train Loss: 0.42298\n","Epoch: 00 [12244/18765 ( 65%)], Train Loss: 0.42305\n","Epoch: 00 [12284/18765 ( 65%)], Train Loss: 0.42255\n","Epoch: 00 [12324/18765 ( 66%)], Train Loss: 0.42232\n","Epoch: 00 [12364/18765 ( 66%)], Train Loss: 0.42191\n","Epoch: 00 [12404/18765 ( 66%)], Train Loss: 0.42138\n","Epoch: 00 [12444/18765 ( 66%)], Train Loss: 0.42140\n","Epoch: 00 [12484/18765 ( 67%)], Train Loss: 0.42174\n","Epoch: 00 [12524/18765 ( 67%)], Train Loss: 0.42070\n","Epoch: 00 [12564/18765 ( 67%)], Train Loss: 0.42036\n","Epoch: 00 [12604/18765 ( 67%)], Train Loss: 0.41948\n","Epoch: 00 [12644/18765 ( 67%)], Train Loss: 0.41987\n","Epoch: 00 [12684/18765 ( 68%)], Train Loss: 0.41930\n","Epoch: 00 [12724/18765 ( 68%)], Train Loss: 0.41889\n","Epoch: 00 [12764/18765 ( 68%)], Train Loss: 0.41878\n","Epoch: 00 [12804/18765 ( 68%)], Train Loss: 0.41860\n","Epoch: 00 [12844/18765 ( 68%)], Train Loss: 0.41827\n","Epoch: 00 [12884/18765 ( 69%)], Train Loss: 0.41763\n","Epoch: 00 [12924/18765 ( 69%)], Train Loss: 0.41727\n","Epoch: 00 [12964/18765 ( 69%)], Train Loss: 0.41656\n","Epoch: 00 [13004/18765 ( 69%)], Train Loss: 0.41660\n","Epoch: 00 [13044/18765 ( 70%)], Train Loss: 0.41619\n","Epoch: 00 [13084/18765 ( 70%)], Train Loss: 0.41580\n","Epoch: 00 [13124/18765 ( 70%)], Train Loss: 0.41549\n","Epoch: 00 [13164/18765 ( 70%)], Train Loss: 0.41484\n","Epoch: 00 [13204/18765 ( 70%)], Train Loss: 0.41472\n","Epoch: 00 [13244/18765 ( 71%)], Train Loss: 0.41443\n","Epoch: 00 [13284/18765 ( 71%)], Train Loss: 0.41417\n","Epoch: 00 [13324/18765 ( 71%)], Train Loss: 0.41428\n","Epoch: 00 [13364/18765 ( 71%)], Train Loss: 0.41380\n","Epoch: 00 [13404/18765 ( 71%)], Train Loss: 0.41374\n","Epoch: 00 [13444/18765 ( 72%)], Train Loss: 0.41324\n","Epoch: 00 [13484/18765 ( 72%)], Train Loss: 0.41287\n","Epoch: 00 [13524/18765 ( 72%)], Train Loss: 0.41279\n","Epoch: 00 [13564/18765 ( 72%)], Train Loss: 0.41258\n","Epoch: 00 [13604/18765 ( 72%)], Train Loss: 0.41284\n","Epoch: 00 [13644/18765 ( 73%)], Train Loss: 0.41200\n","Epoch: 00 [13684/18765 ( 73%)], Train Loss: 0.41182\n","Epoch: 00 [13724/18765 ( 73%)], Train Loss: 0.41120\n","Epoch: 00 [13764/18765 ( 73%)], Train Loss: 0.41111\n","Epoch: 00 [13804/18765 ( 74%)], Train Loss: 0.41073\n","Epoch: 00 [13844/18765 ( 74%)], Train Loss: 0.41042\n","Epoch: 00 [13884/18765 ( 74%)], Train Loss: 0.40990\n","Epoch: 00 [13924/18765 ( 74%)], Train Loss: 0.40971\n","Epoch: 00 [13964/18765 ( 74%)], Train Loss: 0.40915\n","Epoch: 00 [14004/18765 ( 75%)], Train Loss: 0.40900\n","Epoch: 00 [14044/18765 ( 75%)], Train Loss: 0.40863\n","Epoch: 00 [14084/18765 ( 75%)], Train Loss: 0.40805\n","Epoch: 00 [14124/18765 ( 75%)], Train Loss: 0.40822\n","Epoch: 00 [14164/18765 ( 75%)], Train Loss: 0.40771\n","Epoch: 00 [14204/18765 ( 76%)], Train Loss: 0.40723\n","Epoch: 00 [14244/18765 ( 76%)], Train Loss: 0.40665\n","Epoch: 00 [14284/18765 ( 76%)], Train Loss: 0.40602\n","Epoch: 00 [14324/18765 ( 76%)], Train Loss: 0.40582\n","Epoch: 00 [14364/18765 ( 77%)], Train Loss: 0.40521\n","Epoch: 00 [14404/18765 ( 77%)], Train Loss: 0.40477\n","Epoch: 00 [14444/18765 ( 77%)], Train Loss: 0.40446\n","Epoch: 00 [14484/18765 ( 77%)], Train Loss: 0.40466\n","Epoch: 00 [14524/18765 ( 77%)], Train Loss: 0.40489\n","Epoch: 00 [14564/18765 ( 78%)], Train Loss: 0.40465\n","Epoch: 00 [14604/18765 ( 78%)], Train Loss: 0.40426\n","Epoch: 00 [14644/18765 ( 78%)], Train Loss: 0.40450\n","Epoch: 00 [14684/18765 ( 78%)], Train Loss: 0.40464\n","Epoch: 00 [14724/18765 ( 78%)], Train Loss: 0.40427\n","Epoch: 00 [14764/18765 ( 79%)], Train Loss: 0.40406\n","Epoch: 00 [14804/18765 ( 79%)], Train Loss: 0.40377\n","Epoch: 00 [14844/18765 ( 79%)], Train Loss: 0.40347\n","Epoch: 00 [14884/18765 ( 79%)], Train Loss: 0.40294\n","Epoch: 00 [14924/18765 ( 80%)], Train Loss: 0.40255\n","Epoch: 00 [14964/18765 ( 80%)], Train Loss: 0.40197\n","Epoch: 00 [15004/18765 ( 80%)], Train Loss: 0.40161\n","Epoch: 00 [15044/18765 ( 80%)], Train Loss: 0.40104\n","Epoch: 00 [15084/18765 ( 80%)], Train Loss: 0.40080\n","Epoch: 00 [15124/18765 ( 81%)], Train Loss: 0.40076\n","Epoch: 00 [15164/18765 ( 81%)], Train Loss: 0.40089\n","Epoch: 00 [15204/18765 ( 81%)], Train Loss: 0.40158\n","Epoch: 00 [15244/18765 ( 81%)], Train Loss: 0.40103\n","Epoch: 00 [15284/18765 ( 81%)], Train Loss: 0.40141\n","Epoch: 00 [15324/18765 ( 82%)], Train Loss: 0.40091\n","Epoch: 00 [15364/18765 ( 82%)], Train Loss: 0.40054\n","Epoch: 00 [15404/18765 ( 82%)], Train Loss: 0.40036\n","Epoch: 00 [15444/18765 ( 82%)], Train Loss: 0.39993\n","Epoch: 00 [15484/18765 ( 83%)], Train Loss: 0.39976\n","Epoch: 00 [15524/18765 ( 83%)], Train Loss: 0.39991\n","Epoch: 00 [15564/18765 ( 83%)], Train Loss: 0.39991\n","Epoch: 00 [15604/18765 ( 83%)], Train Loss: 0.39944\n","Epoch: 00 [15644/18765 ( 83%)], Train Loss: 0.39897\n","Epoch: 00 [15684/18765 ( 84%)], Train Loss: 0.39874\n","Epoch: 00 [15724/18765 ( 84%)], Train Loss: 0.39929\n","Epoch: 00 [15764/18765 ( 84%)], Train Loss: 0.39901\n","Epoch: 00 [15804/18765 ( 84%)], Train Loss: 0.39880\n","Epoch: 00 [15844/18765 ( 84%)], Train Loss: 0.39843\n","Epoch: 00 [15884/18765 ( 85%)], Train Loss: 0.39805\n","Epoch: 00 [15924/18765 ( 85%)], Train Loss: 0.39772\n","Epoch: 00 [15964/18765 ( 85%)], Train Loss: 0.39775\n","Epoch: 00 [16004/18765 ( 85%)], Train Loss: 0.39756\n","Epoch: 00 [16044/18765 ( 85%)], Train Loss: 0.39743\n","Epoch: 00 [16084/18765 ( 86%)], Train Loss: 0.39740\n","Epoch: 00 [16124/18765 ( 86%)], Train Loss: 0.39675\n","Epoch: 00 [16164/18765 ( 86%)], Train Loss: 0.39646\n","Epoch: 00 [16204/18765 ( 86%)], Train Loss: 0.39600\n","Epoch: 00 [16244/18765 ( 87%)], Train Loss: 0.39573\n","Epoch: 00 [16284/18765 ( 87%)], Train Loss: 0.39595\n","Epoch: 00 [16324/18765 ( 87%)], Train Loss: 0.39578\n","Epoch: 00 [16364/18765 ( 87%)], Train Loss: 0.39536\n","Epoch: 00 [16404/18765 ( 87%)], Train Loss: 0.39528\n","Epoch: 00 [16444/18765 ( 88%)], Train Loss: 0.39487\n","Epoch: 00 [16484/18765 ( 88%)], Train Loss: 0.39467\n","Epoch: 00 [16524/18765 ( 88%)], Train Loss: 0.39479\n","Epoch: 00 [16564/18765 ( 88%)], Train Loss: 0.39487\n","Epoch: 00 [16604/18765 ( 88%)], Train Loss: 0.39422\n","Epoch: 00 [16644/18765 ( 89%)], Train Loss: 0.39371\n","Epoch: 00 [16684/18765 ( 89%)], Train Loss: 0.39338\n","Epoch: 00 [16724/18765 ( 89%)], Train Loss: 0.39317\n","Epoch: 00 [16764/18765 ( 89%)], Train Loss: 0.39283\n","Epoch: 00 [16804/18765 ( 90%)], Train Loss: 0.39261\n","Epoch: 00 [16844/18765 ( 90%)], Train Loss: 0.39254\n","Epoch: 00 [16884/18765 ( 90%)], Train Loss: 0.39244\n","Epoch: 00 [16924/18765 ( 90%)], Train Loss: 0.39200\n","Epoch: 00 [16964/18765 ( 90%)], Train Loss: 0.39166\n","Epoch: 00 [17004/18765 ( 91%)], Train Loss: 0.39139\n","Epoch: 00 [17044/18765 ( 91%)], Train Loss: 0.39105\n","Epoch: 00 [17084/18765 ( 91%)], Train Loss: 0.39072\n","Epoch: 00 [17124/18765 ( 91%)], Train Loss: 0.39068\n","Epoch: 00 [17164/18765 ( 91%)], Train Loss: 0.39020\n","Epoch: 00 [17204/18765 ( 92%)], Train Loss: 0.38993\n","Epoch: 00 [17244/18765 ( 92%)], Train Loss: 0.38984\n","Epoch: 00 [17284/18765 ( 92%)], Train Loss: 0.38996\n","Epoch: 00 [17324/18765 ( 92%)], Train Loss: 0.39027\n","Epoch: 00 [17364/18765 ( 93%)], Train Loss: 0.38985\n","Epoch: 00 [17404/18765 ( 93%)], Train Loss: 0.39012\n","Epoch: 00 [17444/18765 ( 93%)], Train Loss: 0.39002\n","Epoch: 00 [17484/18765 ( 93%)], Train Loss: 0.39003\n","Epoch: 00 [17524/18765 ( 93%)], Train Loss: 0.38992\n","Epoch: 00 [17564/18765 ( 94%)], Train Loss: 0.38976\n","Epoch: 00 [17604/18765 ( 94%)], Train Loss: 0.38938\n","Epoch: 00 [17644/18765 ( 94%)], Train Loss: 0.38907\n","Epoch: 00 [17684/18765 ( 94%)], Train Loss: 0.38932\n","Epoch: 00 [17724/18765 ( 94%)], Train Loss: 0.38904\n","Epoch: 00 [17764/18765 ( 95%)], Train Loss: 0.38914\n","Epoch: 00 [17804/18765 ( 95%)], Train Loss: 0.38878\n","Epoch: 00 [17844/18765 ( 95%)], Train Loss: 0.38845\n","Epoch: 00 [17884/18765 ( 95%)], Train Loss: 0.38808\n","Epoch: 00 [17924/18765 ( 96%)], Train Loss: 0.38785\n","Epoch: 00 [17964/18765 ( 96%)], Train Loss: 0.38763\n","Epoch: 00 [18004/18765 ( 96%)], Train Loss: 0.38733\n","Epoch: 00 [18044/18765 ( 96%)], Train Loss: 0.38685\n","Epoch: 00 [18084/18765 ( 96%)], Train Loss: 0.38647\n","Epoch: 00 [18124/18765 ( 97%)], Train Loss: 0.38609\n","Epoch: 00 [18164/18765 ( 97%)], Train Loss: 0.38583\n","Epoch: 00 [18204/18765 ( 97%)], Train Loss: 0.38544\n","Epoch: 00 [18244/18765 ( 97%)], Train Loss: 0.38498\n","Epoch: 00 [18284/18765 ( 97%)], Train Loss: 0.38442\n","Epoch: 00 [18324/18765 ( 98%)], Train Loss: 0.38408\n","Epoch: 00 [18364/18765 ( 98%)], Train Loss: 0.38417\n","Epoch: 00 [18404/18765 ( 98%)], Train Loss: 0.38426\n","Epoch: 00 [18444/18765 ( 98%)], Train Loss: 0.38448\n","Epoch: 00 [18484/18765 ( 99%)], Train Loss: 0.38455\n","Epoch: 00 [18524/18765 ( 99%)], Train Loss: 0.38422\n","Epoch: 00 [18564/18765 ( 99%)], Train Loss: 0.38389\n","Epoch: 00 [18604/18765 ( 99%)], Train Loss: 0.38335\n","Epoch: 00 [18644/18765 ( 99%)], Train Loss: 0.38297\n","Epoch: 00 [18684/18765 (100%)], Train Loss: 0.38297\n","Epoch: 00 [18724/18765 (100%)], Train Loss: 0.38305\n","Epoch: 00 [18764/18765 (100%)], Train Loss: 0.38329\n","Epoch: 00 [18765/18765 (100%)], Train Loss: 0.38336\n","----Validation Results Summary----\n","Epoch: [0] Valid Loss: 0.61498\n","0 Epoch, Best epoch was updated! Valid Loss: 0.61498\n","Saving model checkpoint to chaii-qa-5-fold-xlmroberta-torch-fit/checkpoint-fold-3.\n","\n","Total Training Time: 2607.9383203983307secs, Average Training Time per Epoch: 2607.9383203983307secs.\n","Total Validation Time: 180.9925675392151secs, Average Validation Time per Epoch: 180.9925675392151secs.\n","\n","\n","--------------------------------------------------\n","FOLD: 4\n","--------------------------------------------------\n","Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n","Num examples Train= 18701, Num examples Valid=4368\n","Total Training Steps: 2338, Total Warmup Steps: 233\n","Epoch: 00 [    4/18701 (  0%)], Train Loss: 2.97787\n","Epoch: 00 [   44/18701 (  0%)], Train Loss: 2.90730\n","Epoch: 00 [   84/18701 (  0%)], Train Loss: 2.88351\n","Epoch: 00 [  124/18701 (  1%)], Train Loss: 2.84616\n","Epoch: 00 [  164/18701 (  1%)], Train Loss: 2.80570\n","Epoch: 00 [  204/18701 (  1%)], Train Loss: 2.74120\n","Epoch: 00 [  244/18701 (  1%)], Train Loss: 2.65955\n","Epoch: 00 [  284/18701 (  2%)], Train Loss: 2.56138\n","Epoch: 00 [  324/18701 (  2%)], Train Loss: 2.43368\n","Epoch: 00 [  364/18701 (  2%)], Train Loss: 2.31393\n","Epoch: 00 [  404/18701 (  2%)], Train Loss: 2.19521\n","Epoch: 00 [  444/18701 (  2%)], Train Loss: 2.08758\n","Epoch: 00 [  484/18701 (  3%)], Train Loss: 1.94848\n","Epoch: 00 [  524/18701 (  3%)], Train Loss: 1.84774\n","Epoch: 00 [  564/18701 (  3%)], Train Loss: 1.75949\n","Epoch: 00 [  604/18701 (  3%)], Train Loss: 1.68607\n","Epoch: 00 [  644/18701 (  3%)], Train Loss: 1.61019\n","Epoch: 00 [  684/18701 (  4%)], Train Loss: 1.55311\n","Epoch: 00 [  724/18701 (  4%)], Train Loss: 1.51078\n","Epoch: 00 [  764/18701 (  4%)], Train Loss: 1.46481\n","Epoch: 00 [  804/18701 (  4%)], Train Loss: 1.41960\n","Epoch: 00 [  844/18701 (  5%)], Train Loss: 1.37494\n","Epoch: 00 [  884/18701 (  5%)], Train Loss: 1.33249\n","Epoch: 00 [  924/18701 (  5%)], Train Loss: 1.30294\n","Epoch: 00 [  964/18701 (  5%)], Train Loss: 1.27226\n","Epoch: 00 [ 1004/18701 (  5%)], Train Loss: 1.23214\n","Epoch: 00 [ 1044/18701 (  6%)], Train Loss: 1.19770\n","Epoch: 00 [ 1084/18701 (  6%)], Train Loss: 1.17387\n","Epoch: 00 [ 1124/18701 (  6%)], Train Loss: 1.15230\n","Epoch: 00 [ 1164/18701 (  6%)], Train Loss: 1.12917\n","Epoch: 00 [ 1204/18701 (  6%)], Train Loss: 1.10658\n","Epoch: 00 [ 1244/18701 (  7%)], Train Loss: 1.08897\n","Epoch: 00 [ 1284/18701 (  7%)], Train Loss: 1.06941\n","Epoch: 00 [ 1324/18701 (  7%)], Train Loss: 1.05124\n","Epoch: 00 [ 1364/18701 (  7%)], Train Loss: 1.03145\n","Epoch: 00 [ 1404/18701 (  8%)], Train Loss: 1.01425\n","Epoch: 00 [ 1444/18701 (  8%)], Train Loss: 1.00021\n","Epoch: 00 [ 1484/18701 (  8%)], Train Loss: 0.98224\n","Epoch: 00 [ 1524/18701 (  8%)], Train Loss: 0.96781\n","Epoch: 00 [ 1564/18701 (  8%)], Train Loss: 0.95665\n","Epoch: 00 [ 1604/18701 (  9%)], Train Loss: 0.94600\n","Epoch: 00 [ 1644/18701 (  9%)], Train Loss: 0.93223\n","Epoch: 00 [ 1684/18701 (  9%)], Train Loss: 0.92134\n","Epoch: 00 [ 1724/18701 (  9%)], Train Loss: 0.90770\n","Epoch: 00 [ 1764/18701 (  9%)], Train Loss: 0.90237\n","Epoch: 00 [ 1804/18701 ( 10%)], Train Loss: 0.88904\n","Epoch: 00 [ 1844/18701 ( 10%)], Train Loss: 0.87918\n","Epoch: 00 [ 1884/18701 ( 10%)], Train Loss: 0.87525\n","Epoch: 00 [ 1924/18701 ( 10%)], Train Loss: 0.86821\n","Epoch: 00 [ 1964/18701 ( 11%)], Train Loss: 0.85815\n","Epoch: 00 [ 2004/18701 ( 11%)], Train Loss: 0.84671\n","Epoch: 00 [ 2044/18701 ( 11%)], Train Loss: 0.84098\n","Epoch: 00 [ 2084/18701 ( 11%)], Train Loss: 0.83254\n","Epoch: 00 [ 2124/18701 ( 11%)], Train Loss: 0.82152\n","Epoch: 00 [ 2164/18701 ( 12%)], Train Loss: 0.81647\n","Epoch: 00 [ 2204/18701 ( 12%)], Train Loss: 0.80551\n","Epoch: 00 [ 2244/18701 ( 12%)], Train Loss: 0.80020\n","Epoch: 00 [ 2284/18701 ( 12%)], Train Loss: 0.79038\n","Epoch: 00 [ 2324/18701 ( 12%)], Train Loss: 0.78117\n","Epoch: 00 [ 2364/18701 ( 13%)], Train Loss: 0.77660\n","Epoch: 00 [ 2404/18701 ( 13%)], Train Loss: 0.77122\n","Epoch: 00 [ 2444/18701 ( 13%)], Train Loss: 0.76163\n","Epoch: 00 [ 2484/18701 ( 13%)], Train Loss: 0.75682\n","Epoch: 00 [ 2524/18701 ( 13%)], Train Loss: 0.75460\n","Epoch: 00 [ 2564/18701 ( 14%)], Train Loss: 0.75127\n","Epoch: 00 [ 2604/18701 ( 14%)], Train Loss: 0.74677\n","Epoch: 00 [ 2644/18701 ( 14%)], Train Loss: 0.74009\n","Epoch: 00 [ 2684/18701 ( 14%)], Train Loss: 0.73800\n","Epoch: 00 [ 2724/18701 ( 15%)], Train Loss: 0.73152\n","Epoch: 00 [ 2764/18701 ( 15%)], Train Loss: 0.72606\n","Epoch: 00 [ 2804/18701 ( 15%)], Train Loss: 0.72197\n","Epoch: 00 [ 2844/18701 ( 15%)], Train Loss: 0.72331\n","Epoch: 00 [ 2884/18701 ( 15%)], Train Loss: 0.71823\n","Epoch: 00 [ 2924/18701 ( 16%)], Train Loss: 0.71298\n","Epoch: 00 [ 2964/18701 ( 16%)], Train Loss: 0.70943\n","Epoch: 00 [ 3004/18701 ( 16%)], Train Loss: 0.70377\n","Epoch: 00 [ 3044/18701 ( 16%)], Train Loss: 0.70041\n","Epoch: 00 [ 3084/18701 ( 16%)], Train Loss: 0.69542\n","Epoch: 00 [ 3124/18701 ( 17%)], Train Loss: 0.69074\n","Epoch: 00 [ 3164/18701 ( 17%)], Train Loss: 0.68794\n","Epoch: 00 [ 3204/18701 ( 17%)], Train Loss: 0.68095\n","Epoch: 00 [ 3244/18701 ( 17%)], Train Loss: 0.67511\n","Epoch: 00 [ 3284/18701 ( 18%)], Train Loss: 0.67041\n","Epoch: 00 [ 3324/18701 ( 18%)], Train Loss: 0.66788\n","Epoch: 00 [ 3364/18701 ( 18%)], Train Loss: 0.66411\n","Epoch: 00 [ 3404/18701 ( 18%)], Train Loss: 0.65874\n","Epoch: 00 [ 3444/18701 ( 18%)], Train Loss: 0.65647\n","Epoch: 00 [ 3484/18701 ( 19%)], Train Loss: 0.65283\n","Epoch: 00 [ 3524/18701 ( 19%)], Train Loss: 0.64974\n","Epoch: 00 [ 3564/18701 ( 19%)], Train Loss: 0.64787\n","Epoch: 00 [ 3604/18701 ( 19%)], Train Loss: 0.64321\n","Epoch: 00 [ 3644/18701 ( 19%)], Train Loss: 0.63814\n","Epoch: 00 [ 3684/18701 ( 20%)], Train Loss: 0.63813\n","Epoch: 00 [ 3724/18701 ( 20%)], Train Loss: 0.63552\n","Epoch: 00 [ 3764/18701 ( 20%)], Train Loss: 0.63251\n","Epoch: 00 [ 3804/18701 ( 20%)], Train Loss: 0.62916\n","Epoch: 00 [ 3844/18701 ( 21%)], Train Loss: 0.62637\n","Epoch: 00 [ 3884/18701 ( 21%)], Train Loss: 0.62253\n","Epoch: 00 [ 3924/18701 ( 21%)], Train Loss: 0.62062\n","Epoch: 00 [ 3964/18701 ( 21%)], Train Loss: 0.61896\n","Epoch: 00 [ 4004/18701 ( 21%)], Train Loss: 0.61728\n","Epoch: 00 [ 4044/18701 ( 22%)], Train Loss: 0.61266\n","Epoch: 00 [ 4084/18701 ( 22%)], Train Loss: 0.60961\n","Epoch: 00 [ 4124/18701 ( 22%)], Train Loss: 0.60689\n","Epoch: 00 [ 4164/18701 ( 22%)], Train Loss: 0.60404\n","Epoch: 00 [ 4204/18701 ( 22%)], Train Loss: 0.60066\n","Epoch: 00 [ 4244/18701 ( 23%)], Train Loss: 0.59809\n","Epoch: 00 [ 4284/18701 ( 23%)], Train Loss: 0.59728\n","Epoch: 00 [ 4324/18701 ( 23%)], Train Loss: 0.59339\n","Epoch: 00 [ 4364/18701 ( 23%)], Train Loss: 0.59066\n","Epoch: 00 [ 4404/18701 ( 24%)], Train Loss: 0.58832\n","Epoch: 00 [ 4444/18701 ( 24%)], Train Loss: 0.58421\n","Epoch: 00 [ 4484/18701 ( 24%)], Train Loss: 0.58402\n","Epoch: 00 [ 4524/18701 ( 24%)], Train Loss: 0.58348\n","Epoch: 00 [ 4564/18701 ( 24%)], Train Loss: 0.58063\n","Epoch: 00 [ 4604/18701 ( 25%)], Train Loss: 0.57792\n","Epoch: 00 [ 4644/18701 ( 25%)], Train Loss: 0.57545\n","Epoch: 00 [ 4684/18701 ( 25%)], Train Loss: 0.57217\n","Epoch: 00 [ 4724/18701 ( 25%)], Train Loss: 0.57046\n","Epoch: 00 [ 4764/18701 ( 25%)], Train Loss: 0.56897\n","Epoch: 00 [ 4804/18701 ( 26%)], Train Loss: 0.56802\n","Epoch: 00 [ 4844/18701 ( 26%)], Train Loss: 0.56529\n","Epoch: 00 [ 4884/18701 ( 26%)], Train Loss: 0.56352\n","Epoch: 00 [ 4924/18701 ( 26%)], Train Loss: 0.56155\n","Epoch: 00 [ 4964/18701 ( 27%)], Train Loss: 0.55952\n","Epoch: 00 [ 5004/18701 ( 27%)], Train Loss: 0.55766\n","Epoch: 00 [ 5044/18701 ( 27%)], Train Loss: 0.55751\n","Epoch: 00 [ 5084/18701 ( 27%)], Train Loss: 0.55665\n","Epoch: 00 [ 5124/18701 ( 27%)], Train Loss: 0.55524\n","Epoch: 00 [ 5164/18701 ( 28%)], Train Loss: 0.55289\n","Epoch: 00 [ 5204/18701 ( 28%)], Train Loss: 0.55120\n","Epoch: 00 [ 5244/18701 ( 28%)], Train Loss: 0.55075\n","Epoch: 00 [ 5284/18701 ( 28%)], Train Loss: 0.54839\n","Epoch: 00 [ 5324/18701 ( 28%)], Train Loss: 0.54845\n","Epoch: 00 [ 5364/18701 ( 29%)], Train Loss: 0.54789\n","Epoch: 00 [ 5404/18701 ( 29%)], Train Loss: 0.54636\n","Epoch: 00 [ 5444/18701 ( 29%)], Train Loss: 0.54358\n","Epoch: 00 [ 5484/18701 ( 29%)], Train Loss: 0.54147\n","Epoch: 00 [ 5524/18701 ( 30%)], Train Loss: 0.53900\n","Epoch: 00 [ 5564/18701 ( 30%)], Train Loss: 0.53848\n","Epoch: 00 [ 5604/18701 ( 30%)], Train Loss: 0.53794\n","Epoch: 00 [ 5644/18701 ( 30%)], Train Loss: 0.53638\n","Epoch: 00 [ 5684/18701 ( 30%)], Train Loss: 0.53503\n","Epoch: 00 [ 5724/18701 ( 31%)], Train Loss: 0.53375\n","Epoch: 00 [ 5764/18701 ( 31%)], Train Loss: 0.53283\n","Epoch: 00 [ 5804/18701 ( 31%)], Train Loss: 0.53186\n","Epoch: 00 [ 5844/18701 ( 31%)], Train Loss: 0.53070\n","Epoch: 00 [ 5884/18701 ( 31%)], Train Loss: 0.52911\n","Epoch: 00 [ 5924/18701 ( 32%)], Train Loss: 0.52745\n","Epoch: 00 [ 5964/18701 ( 32%)], Train Loss: 0.52582\n","Epoch: 00 [ 6004/18701 ( 32%)], Train Loss: 0.52501\n","Epoch: 00 [ 6044/18701 ( 32%)], Train Loss: 0.52500\n","Epoch: 00 [ 6084/18701 ( 33%)], Train Loss: 0.52375\n","Epoch: 00 [ 6124/18701 ( 33%)], Train Loss: 0.52171\n","Epoch: 00 [ 6164/18701 ( 33%)], Train Loss: 0.52103\n","Epoch: 00 [ 6204/18701 ( 33%)], Train Loss: 0.51961\n","Epoch: 00 [ 6244/18701 ( 33%)], Train Loss: 0.51820\n","Epoch: 00 [ 6284/18701 ( 34%)], Train Loss: 0.51690\n","Epoch: 00 [ 6324/18701 ( 34%)], Train Loss: 0.51636\n","Epoch: 00 [ 6364/18701 ( 34%)], Train Loss: 0.51367\n","Epoch: 00 [ 6404/18701 ( 34%)], Train Loss: 0.51337\n","Epoch: 00 [ 6444/18701 ( 34%)], Train Loss: 0.51242\n","Epoch: 00 [ 6484/18701 ( 35%)], Train Loss: 0.51141\n","Epoch: 00 [ 6524/18701 ( 35%)], Train Loss: 0.51091\n","Epoch: 00 [ 6564/18701 ( 35%)], Train Loss: 0.51131\n","Epoch: 00 [ 6604/18701 ( 35%)], Train Loss: 0.51033\n","Epoch: 00 [ 6644/18701 ( 36%)], Train Loss: 0.51105\n","Epoch: 00 [ 6684/18701 ( 36%)], Train Loss: 0.50969\n","Epoch: 00 [ 6724/18701 ( 36%)], Train Loss: 0.50809\n","Epoch: 00 [ 6764/18701 ( 36%)], Train Loss: 0.50726\n","Epoch: 00 [ 6804/18701 ( 36%)], Train Loss: 0.50612\n","Epoch: 00 [ 6844/18701 ( 37%)], Train Loss: 0.50467\n","Epoch: 00 [ 6884/18701 ( 37%)], Train Loss: 0.50450\n","Epoch: 00 [ 6924/18701 ( 37%)], Train Loss: 0.50387\n","Epoch: 00 [ 6964/18701 ( 37%)], Train Loss: 0.50386\n","Epoch: 00 [ 7004/18701 ( 37%)], Train Loss: 0.50351\n","Epoch: 00 [ 7044/18701 ( 38%)], Train Loss: 0.50280\n","Epoch: 00 [ 7084/18701 ( 38%)], Train Loss: 0.50197\n","Epoch: 00 [ 7124/18701 ( 38%)], Train Loss: 0.50069\n","Epoch: 00 [ 7164/18701 ( 38%)], Train Loss: 0.50092\n","Epoch: 00 [ 7204/18701 ( 39%)], Train Loss: 0.50053\n","Epoch: 00 [ 7244/18701 ( 39%)], Train Loss: 0.50078\n","Epoch: 00 [ 7284/18701 ( 39%)], Train Loss: 0.50049\n","Epoch: 00 [ 7324/18701 ( 39%)], Train Loss: 0.50011\n","Epoch: 00 [ 7364/18701 ( 39%)], Train Loss: 0.49928\n","Epoch: 00 [ 7404/18701 ( 40%)], Train Loss: 0.49799\n","Epoch: 00 [ 7444/18701 ( 40%)], Train Loss: 0.49642\n","Epoch: 00 [ 7484/18701 ( 40%)], Train Loss: 0.49527\n","Epoch: 00 [ 7524/18701 ( 40%)], Train Loss: 0.49555\n","Epoch: 00 [ 7564/18701 ( 40%)], Train Loss: 0.49456\n","Epoch: 00 [ 7604/18701 ( 41%)], Train Loss: 0.49414\n","Epoch: 00 [ 7644/18701 ( 41%)], Train Loss: 0.49346\n","Epoch: 00 [ 7684/18701 ( 41%)], Train Loss: 0.49159\n","Epoch: 00 [ 7724/18701 ( 41%)], Train Loss: 0.49066\n","Epoch: 00 [ 7764/18701 ( 42%)], Train Loss: 0.49034\n","Epoch: 00 [ 7804/18701 ( 42%)], Train Loss: 0.48912\n","Epoch: 00 [ 7844/18701 ( 42%)], Train Loss: 0.48826\n","Epoch: 00 [ 7884/18701 ( 42%)], Train Loss: 0.48690\n","Epoch: 00 [ 7924/18701 ( 42%)], Train Loss: 0.48746\n","Epoch: 00 [ 7964/18701 ( 43%)], Train Loss: 0.48746\n","Epoch: 00 [ 8004/18701 ( 43%)], Train Loss: 0.48655\n","Epoch: 00 [ 8044/18701 ( 43%)], Train Loss: 0.48597\n","Epoch: 00 [ 8084/18701 ( 43%)], Train Loss: 0.48467\n","Epoch: 00 [ 8124/18701 ( 43%)], Train Loss: 0.48461\n","Epoch: 00 [ 8164/18701 ( 44%)], Train Loss: 0.48363\n","Epoch: 00 [ 8204/18701 ( 44%)], Train Loss: 0.48285\n","Epoch: 00 [ 8244/18701 ( 44%)], Train Loss: 0.48233\n","Epoch: 00 [ 8284/18701 ( 44%)], Train Loss: 0.48267\n","Epoch: 00 [ 8324/18701 ( 45%)], Train Loss: 0.48164\n","Epoch: 00 [ 8364/18701 ( 45%)], Train Loss: 0.48045\n","Epoch: 00 [ 8404/18701 ( 45%)], Train Loss: 0.47992\n","Epoch: 00 [ 8444/18701 ( 45%)], Train Loss: 0.47868\n","Epoch: 00 [ 8484/18701 ( 45%)], Train Loss: 0.47681\n","Epoch: 00 [ 8524/18701 ( 46%)], Train Loss: 0.47607\n","Epoch: 00 [ 8564/18701 ( 46%)], Train Loss: 0.47617\n","Epoch: 00 [ 8604/18701 ( 46%)], Train Loss: 0.47511\n","Epoch: 00 [ 8644/18701 ( 46%)], Train Loss: 0.47375\n","Epoch: 00 [ 8684/18701 ( 46%)], Train Loss: 0.47383\n","Epoch: 00 [ 8724/18701 ( 47%)], Train Loss: 0.47335\n","Epoch: 00 [ 8764/18701 ( 47%)], Train Loss: 0.47220\n","Epoch: 00 [ 8804/18701 ( 47%)], Train Loss: 0.47119\n","Epoch: 00 [ 8844/18701 ( 47%)], Train Loss: 0.47061\n","Epoch: 00 [ 8884/18701 ( 48%)], Train Loss: 0.46921\n","Epoch: 00 [ 8924/18701 ( 48%)], Train Loss: 0.46813\n","Epoch: 00 [ 8964/18701 ( 48%)], Train Loss: 0.46744\n","Epoch: 00 [ 9004/18701 ( 48%)], Train Loss: 0.46716\n","Epoch: 00 [ 9044/18701 ( 48%)], Train Loss: 0.46650\n","Epoch: 00 [ 9084/18701 ( 49%)], Train Loss: 0.46624\n","Epoch: 00 [ 9124/18701 ( 49%)], Train Loss: 0.46527\n","Epoch: 00 [ 9164/18701 ( 49%)], Train Loss: 0.46421\n","Epoch: 00 [ 9204/18701 ( 49%)], Train Loss: 0.46390\n","Epoch: 00 [ 9244/18701 ( 49%)], Train Loss: 0.46370\n","Epoch: 00 [ 9284/18701 ( 50%)], Train Loss: 0.46341\n","Epoch: 00 [ 9324/18701 ( 50%)], Train Loss: 0.46302\n","Epoch: 00 [ 9364/18701 ( 50%)], Train Loss: 0.46236\n","Epoch: 00 [ 9404/18701 ( 50%)], Train Loss: 0.46247\n","Epoch: 00 [ 9444/18701 ( 50%)], Train Loss: 0.46203\n","Epoch: 00 [ 9484/18701 ( 51%)], Train Loss: 0.46179\n","Epoch: 00 [ 9524/18701 ( 51%)], Train Loss: 0.46201\n","Epoch: 00 [ 9564/18701 ( 51%)], Train Loss: 0.46158\n","Epoch: 00 [ 9604/18701 ( 51%)], Train Loss: 0.46063\n","Epoch: 00 [ 9644/18701 ( 52%)], Train Loss: 0.45998\n","Epoch: 00 [ 9684/18701 ( 52%)], Train Loss: 0.45911\n","Epoch: 00 [ 9724/18701 ( 52%)], Train Loss: 0.45875\n","Epoch: 00 [ 9764/18701 ( 52%)], Train Loss: 0.45844\n","Epoch: 00 [ 9804/18701 ( 52%)], Train Loss: 0.45774\n","Epoch: 00 [ 9844/18701 ( 53%)], Train Loss: 0.45744\n","Epoch: 00 [ 9884/18701 ( 53%)], Train Loss: 0.45662\n","Epoch: 00 [ 9924/18701 ( 53%)], Train Loss: 0.45636\n","Epoch: 00 [ 9964/18701 ( 53%)], Train Loss: 0.45580\n","Epoch: 00 [10004/18701 ( 53%)], Train Loss: 0.45566\n","Epoch: 00 [10044/18701 ( 54%)], Train Loss: 0.45556\n","Epoch: 00 [10084/18701 ( 54%)], Train Loss: 0.45509\n","Epoch: 00 [10124/18701 ( 54%)], Train Loss: 0.45464\n","Epoch: 00 [10164/18701 ( 54%)], Train Loss: 0.45421\n","Epoch: 00 [10204/18701 ( 55%)], Train Loss: 0.45344\n","Epoch: 00 [10244/18701 ( 55%)], Train Loss: 0.45350\n","Epoch: 00 [10284/18701 ( 55%)], Train Loss: 0.45310\n","Epoch: 00 [10324/18701 ( 55%)], Train Loss: 0.45279\n","Epoch: 00 [10364/18701 ( 55%)], Train Loss: 0.45213\n","Epoch: 00 [10404/18701 ( 56%)], Train Loss: 0.45179\n","Epoch: 00 [10444/18701 ( 56%)], Train Loss: 0.45142\n","Epoch: 00 [10484/18701 ( 56%)], Train Loss: 0.45082\n","Epoch: 00 [10524/18701 ( 56%)], Train Loss: 0.45045\n","Epoch: 00 [10564/18701 ( 56%)], Train Loss: 0.44988\n","Epoch: 00 [10604/18701 ( 57%)], Train Loss: 0.44918\n","Epoch: 00 [10644/18701 ( 57%)], Train Loss: 0.44870\n","Epoch: 00 [10684/18701 ( 57%)], Train Loss: 0.44818\n","Epoch: 00 [10724/18701 ( 57%)], Train Loss: 0.44848\n","Epoch: 00 [10764/18701 ( 58%)], Train Loss: 0.44783\n","Epoch: 00 [10804/18701 ( 58%)], Train Loss: 0.44776\n","Epoch: 00 [10844/18701 ( 58%)], Train Loss: 0.44755\n","Epoch: 00 [10884/18701 ( 58%)], Train Loss: 0.44679\n","Epoch: 00 [10924/18701 ( 58%)], Train Loss: 0.44618\n","Epoch: 00 [10964/18701 ( 59%)], Train Loss: 0.44598\n","Epoch: 00 [11004/18701 ( 59%)], Train Loss: 0.44537\n","Epoch: 00 [11044/18701 ( 59%)], Train Loss: 0.44504\n","Epoch: 00 [11084/18701 ( 59%)], Train Loss: 0.44452\n","Epoch: 00 [11124/18701 ( 59%)], Train Loss: 0.44428\n","Epoch: 00 [11164/18701 ( 60%)], Train Loss: 0.44364\n","Epoch: 00 [11204/18701 ( 60%)], Train Loss: 0.44257\n","Epoch: 00 [11244/18701 ( 60%)], Train Loss: 0.44172\n","Epoch: 00 [11284/18701 ( 60%)], Train Loss: 0.44069\n","Epoch: 00 [11324/18701 ( 61%)], Train Loss: 0.44092\n","Epoch: 00 [11364/18701 ( 61%)], Train Loss: 0.44088\n","Epoch: 00 [11404/18701 ( 61%)], Train Loss: 0.44032\n","Epoch: 00 [11444/18701 ( 61%)], Train Loss: 0.43981\n","Epoch: 00 [11484/18701 ( 61%)], Train Loss: 0.43948\n","Epoch: 00 [11524/18701 ( 62%)], Train Loss: 0.43890\n","Epoch: 00 [11564/18701 ( 62%)], Train Loss: 0.43870\n","Epoch: 00 [11604/18701 ( 62%)], Train Loss: 0.43804\n","Epoch: 00 [11644/18701 ( 62%)], Train Loss: 0.43763\n","Epoch: 00 [11684/18701 ( 62%)], Train Loss: 0.43678\n","Epoch: 00 [11724/18701 ( 63%)], Train Loss: 0.43645\n","Epoch: 00 [11764/18701 ( 63%)], Train Loss: 0.43583\n","Epoch: 00 [11804/18701 ( 63%)], Train Loss: 0.43557\n","Epoch: 00 [11844/18701 ( 63%)], Train Loss: 0.43504\n","Epoch: 00 [11884/18701 ( 64%)], Train Loss: 0.43474\n","Epoch: 00 [11924/18701 ( 64%)], Train Loss: 0.43432\n","Epoch: 00 [11964/18701 ( 64%)], Train Loss: 0.43351\n","Epoch: 00 [12004/18701 ( 64%)], Train Loss: 0.43306\n","Epoch: 00 [12044/18701 ( 64%)], Train Loss: 0.43242\n","Epoch: 00 [12084/18701 ( 65%)], Train Loss: 0.43208\n","Epoch: 00 [12124/18701 ( 65%)], Train Loss: 0.43171\n","Epoch: 00 [12164/18701 ( 65%)], Train Loss: 0.43156\n","Epoch: 00 [12204/18701 ( 65%)], Train Loss: 0.43091\n","Epoch: 00 [12244/18701 ( 65%)], Train Loss: 0.43052\n","Epoch: 00 [12284/18701 ( 66%)], Train Loss: 0.43020\n","Epoch: 00 [12324/18701 ( 66%)], Train Loss: 0.42979\n","Epoch: 00 [12364/18701 ( 66%)], Train Loss: 0.42953\n","Epoch: 00 [12404/18701 ( 66%)], Train Loss: 0.42910\n","Epoch: 00 [12444/18701 ( 67%)], Train Loss: 0.42840\n","Epoch: 00 [12484/18701 ( 67%)], Train Loss: 0.42786\n","Epoch: 00 [12524/18701 ( 67%)], Train Loss: 0.42728\n","Epoch: 00 [12564/18701 ( 67%)], Train Loss: 0.42666\n","Epoch: 00 [12604/18701 ( 67%)], Train Loss: 0.42633\n","Epoch: 00 [12644/18701 ( 68%)], Train Loss: 0.42591\n","Epoch: 00 [12684/18701 ( 68%)], Train Loss: 0.42605\n","Epoch: 00 [12724/18701 ( 68%)], Train Loss: 0.42573\n","Epoch: 00 [12764/18701 ( 68%)], Train Loss: 0.42524\n","Epoch: 00 [12804/18701 ( 68%)], Train Loss: 0.42467\n","Epoch: 00 [12844/18701 ( 69%)], Train Loss: 0.42422\n","Epoch: 00 [12884/18701 ( 69%)], Train Loss: 0.42388\n","Epoch: 00 [12924/18701 ( 69%)], Train Loss: 0.42351\n","Epoch: 00 [12964/18701 ( 69%)], Train Loss: 0.42326\n","Epoch: 00 [13004/18701 ( 70%)], Train Loss: 0.42292\n","Epoch: 00 [13044/18701 ( 70%)], Train Loss: 0.42242\n","Epoch: 00 [13084/18701 ( 70%)], Train Loss: 0.42237\n","Epoch: 00 [13124/18701 ( 70%)], Train Loss: 0.42187\n","Epoch: 00 [13164/18701 ( 70%)], Train Loss: 0.42181\n","Epoch: 00 [13204/18701 ( 71%)], Train Loss: 0.42139\n","Epoch: 00 [13244/18701 ( 71%)], Train Loss: 0.42073\n","Epoch: 00 [13284/18701 ( 71%)], Train Loss: 0.42008\n","Epoch: 00 [13324/18701 ( 71%)], Train Loss: 0.41953\n","Epoch: 00 [13364/18701 ( 71%)], Train Loss: 0.41903\n","Epoch: 00 [13404/18701 ( 72%)], Train Loss: 0.41866\n","Epoch: 00 [13444/18701 ( 72%)], Train Loss: 0.41818\n","Epoch: 00 [13484/18701 ( 72%)], Train Loss: 0.41772\n","Epoch: 00 [13524/18701 ( 72%)], Train Loss: 0.41694\n","Epoch: 00 [13564/18701 ( 73%)], Train Loss: 0.41647\n","Epoch: 00 [13604/18701 ( 73%)], Train Loss: 0.41629\n","Epoch: 00 [13644/18701 ( 73%)], Train Loss: 0.41565\n","Epoch: 00 [13684/18701 ( 73%)], Train Loss: 0.41512\n","Epoch: 00 [13724/18701 ( 73%)], Train Loss: 0.41485\n","Epoch: 00 [13764/18701 ( 74%)], Train Loss: 0.41452\n","Epoch: 00 [13804/18701 ( 74%)], Train Loss: 0.41411\n","Epoch: 00 [13844/18701 ( 74%)], Train Loss: 0.41382\n","Epoch: 00 [13884/18701 ( 74%)], Train Loss: 0.41350\n","Epoch: 00 [13924/18701 ( 74%)], Train Loss: 0.41271\n","Epoch: 00 [13964/18701 ( 75%)], Train Loss: 0.41324\n","Epoch: 00 [14004/18701 ( 75%)], Train Loss: 0.41257\n","Epoch: 00 [14044/18701 ( 75%)], Train Loss: 0.41202\n","Epoch: 00 [14084/18701 ( 75%)], Train Loss: 0.41144\n","Epoch: 00 [14124/18701 ( 76%)], Train Loss: 0.41114\n","Epoch: 00 [14164/18701 ( 76%)], Train Loss: 0.41051\n","Epoch: 00 [14204/18701 ( 76%)], Train Loss: 0.41040\n","Epoch: 00 [14244/18701 ( 76%)], Train Loss: 0.40992\n","Epoch: 00 [14284/18701 ( 76%)], Train Loss: 0.40979\n","Epoch: 00 [14324/18701 ( 77%)], Train Loss: 0.40954\n","Epoch: 00 [14364/18701 ( 77%)], Train Loss: 0.40938\n","Epoch: 00 [14404/18701 ( 77%)], Train Loss: 0.40860\n","Epoch: 00 [14444/18701 ( 77%)], Train Loss: 0.40812\n","Epoch: 00 [14484/18701 ( 77%)], Train Loss: 0.40866\n","Epoch: 00 [14524/18701 ( 78%)], Train Loss: 0.40857\n","Epoch: 00 [14564/18701 ( 78%)], Train Loss: 0.40852\n","Epoch: 00 [14604/18701 ( 78%)], Train Loss: 0.40799\n","Epoch: 00 [14644/18701 ( 78%)], Train Loss: 0.40763\n","Epoch: 00 [14684/18701 ( 79%)], Train Loss: 0.40742\n","Epoch: 00 [14724/18701 ( 79%)], Train Loss: 0.40721\n","Epoch: 00 [14764/18701 ( 79%)], Train Loss: 0.40717\n","Epoch: 00 [14804/18701 ( 79%)], Train Loss: 0.40680\n","Epoch: 00 [14844/18701 ( 79%)], Train Loss: 0.40665\n","Epoch: 00 [14884/18701 ( 80%)], Train Loss: 0.40649\n","Epoch: 00 [14924/18701 ( 80%)], Train Loss: 0.40691\n","Epoch: 00 [14964/18701 ( 80%)], Train Loss: 0.40686\n","Epoch: 00 [15004/18701 ( 80%)], Train Loss: 0.40653\n","Epoch: 00 [15044/18701 ( 80%)], Train Loss: 0.40588\n","Epoch: 00 [15084/18701 ( 81%)], Train Loss: 0.40557\n","Epoch: 00 [15124/18701 ( 81%)], Train Loss: 0.40542\n","Epoch: 00 [15164/18701 ( 81%)], Train Loss: 0.40488\n","Epoch: 00 [15204/18701 ( 81%)], Train Loss: 0.40459\n","Epoch: 00 [15244/18701 ( 82%)], Train Loss: 0.40449\n","Epoch: 00 [15284/18701 ( 82%)], Train Loss: 0.40392\n","Epoch: 00 [15324/18701 ( 82%)], Train Loss: 0.40400\n","Epoch: 00 [15364/18701 ( 82%)], Train Loss: 0.40440\n","Epoch: 00 [15404/18701 ( 82%)], Train Loss: 0.40385\n","Epoch: 00 [15444/18701 ( 83%)], Train Loss: 0.40389\n","Epoch: 00 [15484/18701 ( 83%)], Train Loss: 0.40402\n","Epoch: 00 [15524/18701 ( 83%)], Train Loss: 0.40359\n","Epoch: 00 [15564/18701 ( 83%)], Train Loss: 0.40322\n","Epoch: 00 [15604/18701 ( 83%)], Train Loss: 0.40259\n","Epoch: 00 [15644/18701 ( 84%)], Train Loss: 0.40234\n","Epoch: 00 [15684/18701 ( 84%)], Train Loss: 0.40210\n","Epoch: 00 [15724/18701 ( 84%)], Train Loss: 0.40218\n","Epoch: 00 [15764/18701 ( 84%)], Train Loss: 0.40216\n","Epoch: 00 [15804/18701 ( 85%)], Train Loss: 0.40227\n","Epoch: 00 [15844/18701 ( 85%)], Train Loss: 0.40183\n","Epoch: 00 [15884/18701 ( 85%)], Train Loss: 0.40164\n","Epoch: 00 [15924/18701 ( 85%)], Train Loss: 0.40137\n","Epoch: 00 [15964/18701 ( 85%)], Train Loss: 0.40095\n","Epoch: 00 [16004/18701 ( 86%)], Train Loss: 0.40101\n","Epoch: 00 [16044/18701 ( 86%)], Train Loss: 0.40123\n","Epoch: 00 [16084/18701 ( 86%)], Train Loss: 0.40118\n","Epoch: 00 [16124/18701 ( 86%)], Train Loss: 0.40119\n","Epoch: 00 [16164/18701 ( 86%)], Train Loss: 0.40117\n","Epoch: 00 [16204/18701 ( 87%)], Train Loss: 0.40130\n","Epoch: 00 [16244/18701 ( 87%)], Train Loss: 0.40096\n","Epoch: 00 [16284/18701 ( 87%)], Train Loss: 0.40064\n","Epoch: 00 [16324/18701 ( 87%)], Train Loss: 0.40043\n","Epoch: 00 [16364/18701 ( 88%)], Train Loss: 0.40079\n","Epoch: 00 [16404/18701 ( 88%)], Train Loss: 0.40070\n","Epoch: 00 [16444/18701 ( 88%)], Train Loss: 0.40033\n","Epoch: 00 [16484/18701 ( 88%)], Train Loss: 0.40023\n","Epoch: 00 [16524/18701 ( 88%)], Train Loss: 0.39987\n","Epoch: 00 [16564/18701 ( 89%)], Train Loss: 0.39987\n","Epoch: 00 [16604/18701 ( 89%)], Train Loss: 0.39952\n","Epoch: 00 [16644/18701 ( 89%)], Train Loss: 0.39975\n","Epoch: 00 [16684/18701 ( 89%)], Train Loss: 0.39984\n","Epoch: 00 [16724/18701 ( 89%)], Train Loss: 0.39975\n","Epoch: 00 [16764/18701 ( 90%)], Train Loss: 0.39961\n","Epoch: 00 [16804/18701 ( 90%)], Train Loss: 0.39903\n","Epoch: 00 [16844/18701 ( 90%)], Train Loss: 0.39875\n","Epoch: 00 [16884/18701 ( 90%)], Train Loss: 0.39847\n","Epoch: 00 [16924/18701 ( 90%)], Train Loss: 0.39819\n","Epoch: 00 [16964/18701 ( 91%)], Train Loss: 0.39794\n","Epoch: 00 [17004/18701 ( 91%)], Train Loss: 0.39733\n","Epoch: 00 [17044/18701 ( 91%)], Train Loss: 0.39784\n","Epoch: 00 [17084/18701 ( 91%)], Train Loss: 0.39738\n","Epoch: 00 [17124/18701 ( 92%)], Train Loss: 0.39705\n","Epoch: 00 [17164/18701 ( 92%)], Train Loss: 0.39681\n","Epoch: 00 [17204/18701 ( 92%)], Train Loss: 0.39644\n","Epoch: 00 [17244/18701 ( 92%)], Train Loss: 0.39633\n","Epoch: 00 [17284/18701 ( 92%)], Train Loss: 0.39625\n","Epoch: 00 [17324/18701 ( 93%)], Train Loss: 0.39582\n","Epoch: 00 [17364/18701 ( 93%)], Train Loss: 0.39548\n","Epoch: 00 [17404/18701 ( 93%)], Train Loss: 0.39542\n","Epoch: 00 [17444/18701 ( 93%)], Train Loss: 0.39514\n","Epoch: 00 [17484/18701 ( 93%)], Train Loss: 0.39496\n","Epoch: 00 [17524/18701 ( 94%)], Train Loss: 0.39443\n","Epoch: 00 [17564/18701 ( 94%)], Train Loss: 0.39423\n","Epoch: 00 [17604/18701 ( 94%)], Train Loss: 0.39428\n","Epoch: 00 [17644/18701 ( 94%)], Train Loss: 0.39377\n","Epoch: 00 [17684/18701 ( 95%)], Train Loss: 0.39329\n","Epoch: 00 [17724/18701 ( 95%)], Train Loss: 0.39293\n","Epoch: 00 [17764/18701 ( 95%)], Train Loss: 0.39287\n","Epoch: 00 [17804/18701 ( 95%)], Train Loss: 0.39281\n","Epoch: 00 [17844/18701 ( 95%)], Train Loss: 0.39248\n","Epoch: 00 [17884/18701 ( 96%)], Train Loss: 0.39219\n","Epoch: 00 [17924/18701 ( 96%)], Train Loss: 0.39160\n","Epoch: 00 [17964/18701 ( 96%)], Train Loss: 0.39099\n","Epoch: 00 [18004/18701 ( 96%)], Train Loss: 0.39063\n","Epoch: 00 [18044/18701 ( 96%)], Train Loss: 0.39018\n","Epoch: 00 [18084/18701 ( 97%)], Train Loss: 0.38971\n","Epoch: 00 [18124/18701 ( 97%)], Train Loss: 0.38940\n","Epoch: 00 [18164/18701 ( 97%)], Train Loss: 0.38920\n","Epoch: 00 [18204/18701 ( 97%)], Train Loss: 0.38878\n","Epoch: 00 [18244/18701 ( 98%)], Train Loss: 0.38904\n","Epoch: 00 [18284/18701 ( 98%)], Train Loss: 0.38881\n","Epoch: 00 [18324/18701 ( 98%)], Train Loss: 0.38899\n","Epoch: 00 [18364/18701 ( 98%)], Train Loss: 0.38878\n","Epoch: 00 [18404/18701 ( 98%)], Train Loss: 0.38868\n","Epoch: 00 [18444/18701 ( 99%)], Train Loss: 0.38867\n","Epoch: 00 [18484/18701 ( 99%)], Train Loss: 0.38863\n","Epoch: 00 [18524/18701 ( 99%)], Train Loss: 0.38879\n","Epoch: 00 [18564/18701 ( 99%)], Train Loss: 0.38844\n","Epoch: 00 [18604/18701 ( 99%)], Train Loss: 0.38824\n","Epoch: 00 [18644/18701 (100%)], Train Loss: 0.38775\n","Epoch: 00 [18684/18701 (100%)], Train Loss: 0.38771\n","Epoch: 00 [18701/18701 (100%)], Train Loss: 0.38760\n","----Validation Results Summary----\n","Epoch: [0] Valid Loss: 0.58509\n","0 Epoch, Best epoch was updated! Valid Loss: 0.58509\n","Saving model checkpoint to chaii-qa-5-fold-xlmroberta-torch-fit/checkpoint-fold-4.\n","\n","Total Training Time: 2598.0124225616455secs, Average Training Time per Epoch: 2598.0124225616455secs.\n","Total Validation Time: 183.68438744544983secs, Average Validation Time per Epoch: 183.68438744544983secs.\n","\n","\n","--------------------------------------------------\n","FOLD: 5\n","--------------------------------------------------\n","Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n","Num examples Train= 23069, Num examples Valid=0\n","Total Training Steps: 2884, Total Warmup Steps: 288\n","Epoch: 00 [    4/23069 (  0%)], Train Loss: 2.92595\n","Epoch: 00 [   44/23069 (  0%)], Train Loss: 2.90102\n","Epoch: 00 [   84/23069 (  0%)], Train Loss: 2.87705\n","Epoch: 00 [  124/23069 (  1%)], Train Loss: 2.84823\n","Epoch: 00 [  164/23069 (  1%)], Train Loss: 2.81063\n","Epoch: 00 [  204/23069 (  1%)], Train Loss: 2.76044\n","Epoch: 00 [  244/23069 (  1%)], Train Loss: 2.70359\n","Epoch: 00 [  284/23069 (  1%)], Train Loss: 2.62044\n","Epoch: 00 [  324/23069 (  1%)], Train Loss: 2.52563\n","Epoch: 00 [  364/23069 (  2%)], Train Loss: 2.44480\n","Epoch: 00 [  404/23069 (  2%)], Train Loss: 2.33778\n","Epoch: 00 [  444/23069 (  2%)], Train Loss: 2.22460\n","Epoch: 00 [  484/23069 (  2%)], Train Loss: 2.11602\n","Epoch: 00 [  524/23069 (  2%)], Train Loss: 2.00113\n","Epoch: 00 [  564/23069 (  2%)], Train Loss: 1.89111\n","Epoch: 00 [  604/23069 (  3%)], Train Loss: 1.79100\n","Epoch: 00 [  644/23069 (  3%)], Train Loss: 1.71909\n","Epoch: 00 [  684/23069 (  3%)], Train Loss: 1.64372\n","Epoch: 00 [  724/23069 (  3%)], Train Loss: 1.58388\n","Epoch: 00 [  764/23069 (  3%)], Train Loss: 1.53190\n","Epoch: 00 [  804/23069 (  3%)], Train Loss: 1.47848\n","Epoch: 00 [  844/23069 (  4%)], Train Loss: 1.42825\n","Epoch: 00 [  884/23069 (  4%)], Train Loss: 1.37702\n","Epoch: 00 [  924/23069 (  4%)], Train Loss: 1.34172\n","Epoch: 00 [  964/23069 (  4%)], Train Loss: 1.30115\n","Epoch: 00 [ 1004/23069 (  4%)], Train Loss: 1.26790\n","Epoch: 00 [ 1044/23069 (  5%)], Train Loss: 1.22711\n","Epoch: 00 [ 1084/23069 (  5%)], Train Loss: 1.20627\n","Epoch: 00 [ 1124/23069 (  5%)], Train Loss: 1.18250\n","Epoch: 00 [ 1164/23069 (  5%)], Train Loss: 1.16074\n","Epoch: 00 [ 1204/23069 (  5%)], Train Loss: 1.13439\n","Epoch: 00 [ 1244/23069 (  5%)], Train Loss: 1.10710\n","Epoch: 00 [ 1284/23069 (  6%)], Train Loss: 1.08647\n","Epoch: 00 [ 1324/23069 (  6%)], Train Loss: 1.06405\n","Epoch: 00 [ 1364/23069 (  6%)], Train Loss: 1.04598\n","Epoch: 00 [ 1404/23069 (  6%)], Train Loss: 1.02811\n","Epoch: 00 [ 1444/23069 (  6%)], Train Loss: 1.01619\n","Epoch: 00 [ 1484/23069 (  6%)], Train Loss: 0.99852\n","Epoch: 00 [ 1524/23069 (  7%)], Train Loss: 0.98686\n","Epoch: 00 [ 1564/23069 (  7%)], Train Loss: 0.97369\n","Epoch: 00 [ 1604/23069 (  7%)], Train Loss: 0.96544\n","Epoch: 00 [ 1644/23069 (  7%)], Train Loss: 0.95208\n","Epoch: 00 [ 1684/23069 (  7%)], Train Loss: 0.93891\n","Epoch: 00 [ 1724/23069 (  7%)], Train Loss: 0.93081\n","Epoch: 00 [ 1764/23069 (  8%)], Train Loss: 0.92143\n","Epoch: 00 [ 1804/23069 (  8%)], Train Loss: 0.90885\n","Epoch: 00 [ 1844/23069 (  8%)], Train Loss: 0.89775\n","Epoch: 00 [ 1884/23069 (  8%)], Train Loss: 0.88948\n","Epoch: 00 [ 1924/23069 (  8%)], Train Loss: 0.88065\n","Epoch: 00 [ 1964/23069 (  9%)], Train Loss: 0.86974\n","Epoch: 00 [ 2004/23069 (  9%)], Train Loss: 0.86197\n","Epoch: 00 [ 2044/23069 (  9%)], Train Loss: 0.85305\n","Epoch: 00 [ 2084/23069 (  9%)], Train Loss: 0.84635\n","Epoch: 00 [ 2124/23069 (  9%)], Train Loss: 0.84025\n","Epoch: 00 [ 2164/23069 (  9%)], Train Loss: 0.83594\n","Epoch: 00 [ 2204/23069 ( 10%)], Train Loss: 0.83456\n","Epoch: 00 [ 2244/23069 ( 10%)], Train Loss: 0.83017\n","Epoch: 00 [ 2284/23069 ( 10%)], Train Loss: 0.82537\n","Epoch: 00 [ 2324/23069 ( 10%)], Train Loss: 0.81891\n","Epoch: 00 [ 2364/23069 ( 10%)], Train Loss: 0.81158\n","Epoch: 00 [ 2404/23069 ( 10%)], Train Loss: 0.80307\n","Epoch: 00 [ 2444/23069 ( 11%)], Train Loss: 0.79766\n","Epoch: 00 [ 2484/23069 ( 11%)], Train Loss: 0.79405\n","Epoch: 00 [ 2524/23069 ( 11%)], Train Loss: 0.78805\n","Epoch: 00 [ 2564/23069 ( 11%)], Train Loss: 0.78114\n","Epoch: 00 [ 2604/23069 ( 11%)], Train Loss: 0.77313\n","Epoch: 00 [ 2644/23069 ( 11%)], Train Loss: 0.76840\n","Epoch: 00 [ 2684/23069 ( 12%)], Train Loss: 0.76262\n","Epoch: 00 [ 2724/23069 ( 12%)], Train Loss: 0.75653\n","Epoch: 00 [ 2764/23069 ( 12%)], Train Loss: 0.75230\n","Epoch: 00 [ 2804/23069 ( 12%)], Train Loss: 0.74696\n","Epoch: 00 [ 2844/23069 ( 12%)], Train Loss: 0.74177\n","Epoch: 00 [ 2884/23069 ( 13%)], Train Loss: 0.73602\n","Epoch: 00 [ 2924/23069 ( 13%)], Train Loss: 0.73124\n","Epoch: 00 [ 2964/23069 ( 13%)], Train Loss: 0.72466\n","Epoch: 00 [ 3004/23069 ( 13%)], Train Loss: 0.72257\n","Epoch: 00 [ 3044/23069 ( 13%)], Train Loss: 0.71861\n","Epoch: 00 [ 3084/23069 ( 13%)], Train Loss: 0.71292\n","Epoch: 00 [ 3124/23069 ( 14%)], Train Loss: 0.70644\n","Epoch: 00 [ 3164/23069 ( 14%)], Train Loss: 0.70018\n","Epoch: 00 [ 3204/23069 ( 14%)], Train Loss: 0.69442\n","Epoch: 00 [ 3244/23069 ( 14%)], Train Loss: 0.69317\n","Epoch: 00 [ 3284/23069 ( 14%)], Train Loss: 0.69053\n","Epoch: 00 [ 3324/23069 ( 14%)], Train Loss: 0.68667\n","Epoch: 00 [ 3364/23069 ( 15%)], Train Loss: 0.68319\n","Epoch: 00 [ 3404/23069 ( 15%)], Train Loss: 0.68090\n","Epoch: 00 [ 3444/23069 ( 15%)], Train Loss: 0.67708\n","Epoch: 00 [ 3484/23069 ( 15%)], Train Loss: 0.67360\n","Epoch: 00 [ 3524/23069 ( 15%)], Train Loss: 0.67033\n","Epoch: 00 [ 3564/23069 ( 15%)], Train Loss: 0.66879\n","Epoch: 00 [ 3604/23069 ( 16%)], Train Loss: 0.66857\n","Epoch: 00 [ 3644/23069 ( 16%)], Train Loss: 0.66706\n","Epoch: 00 [ 3684/23069 ( 16%)], Train Loss: 0.66420\n","Epoch: 00 [ 3724/23069 ( 16%)], Train Loss: 0.66127\n","Epoch: 00 [ 3764/23069 ( 16%)], Train Loss: 0.65794\n","Epoch: 00 [ 3804/23069 ( 16%)], Train Loss: 0.65470\n","Epoch: 00 [ 3844/23069 ( 17%)], Train Loss: 0.65092\n","Epoch: 00 [ 3884/23069 ( 17%)], Train Loss: 0.64953\n","Epoch: 00 [ 3924/23069 ( 17%)], Train Loss: 0.64702\n","Epoch: 00 [ 3964/23069 ( 17%)], Train Loss: 0.64391\n","Epoch: 00 [ 4004/23069 ( 17%)], Train Loss: 0.64082\n","Epoch: 00 [ 4044/23069 ( 18%)], Train Loss: 0.63952\n","Epoch: 00 [ 4084/23069 ( 18%)], Train Loss: 0.63800\n","Epoch: 00 [ 4124/23069 ( 18%)], Train Loss: 0.63651\n","Epoch: 00 [ 4164/23069 ( 18%)], Train Loss: 0.63444\n","Epoch: 00 [ 4204/23069 ( 18%)], Train Loss: 0.63180\n","Epoch: 00 [ 4244/23069 ( 18%)], Train Loss: 0.63045\n","Epoch: 00 [ 4284/23069 ( 19%)], Train Loss: 0.62824\n","Epoch: 00 [ 4324/23069 ( 19%)], Train Loss: 0.62563\n","Epoch: 00 [ 4364/23069 ( 19%)], Train Loss: 0.62212\n","Epoch: 00 [ 4404/23069 ( 19%)], Train Loss: 0.62054\n","Epoch: 00 [ 4444/23069 ( 19%)], Train Loss: 0.61846\n","Epoch: 00 [ 4484/23069 ( 19%)], Train Loss: 0.61490\n","Epoch: 00 [ 4524/23069 ( 20%)], Train Loss: 0.61236\n","Epoch: 00 [ 4564/23069 ( 20%)], Train Loss: 0.60869\n","Epoch: 00 [ 4604/23069 ( 20%)], Train Loss: 0.60764\n","Epoch: 00 [ 4644/23069 ( 20%)], Train Loss: 0.60477\n","Epoch: 00 [ 4684/23069 ( 20%)], Train Loss: 0.60292\n","Epoch: 00 [ 4724/23069 ( 20%)], Train Loss: 0.59997\n","Epoch: 00 [ 4764/23069 ( 21%)], Train Loss: 0.59782\n","Epoch: 00 [ 4804/23069 ( 21%)], Train Loss: 0.59531\n","Epoch: 00 [ 4844/23069 ( 21%)], Train Loss: 0.59168\n","Epoch: 00 [ 4884/23069 ( 21%)], Train Loss: 0.58797\n","Epoch: 00 [ 4924/23069 ( 21%)], Train Loss: 0.58575\n","Epoch: 00 [ 4964/23069 ( 22%)], Train Loss: 0.58307\n","Epoch: 00 [ 5004/23069 ( 22%)], Train Loss: 0.58187\n","Epoch: 00 [ 5044/23069 ( 22%)], Train Loss: 0.58100\n","Epoch: 00 [ 5084/23069 ( 22%)], Train Loss: 0.58004\n","Epoch: 00 [ 5124/23069 ( 22%)], Train Loss: 0.57789\n","Epoch: 00 [ 5164/23069 ( 22%)], Train Loss: 0.57777\n","Epoch: 00 [ 5204/23069 ( 23%)], Train Loss: 0.57554\n","Epoch: 00 [ 5244/23069 ( 23%)], Train Loss: 0.57420\n","Epoch: 00 [ 5284/23069 ( 23%)], Train Loss: 0.57177\n","Epoch: 00 [ 5324/23069 ( 23%)], Train Loss: 0.56929\n","Epoch: 00 [ 5364/23069 ( 23%)], Train Loss: 0.56663\n","Epoch: 00 [ 5404/23069 ( 23%)], Train Loss: 0.56423\n","Epoch: 00 [ 5444/23069 ( 24%)], Train Loss: 0.56254\n","Epoch: 00 [ 5484/23069 ( 24%)], Train Loss: 0.56320\n","Epoch: 00 [ 5524/23069 ( 24%)], Train Loss: 0.56178\n","Epoch: 00 [ 5564/23069 ( 24%)], Train Loss: 0.56033\n","Epoch: 00 [ 5604/23069 ( 24%)], Train Loss: 0.55904\n","Epoch: 00 [ 5644/23069 ( 24%)], Train Loss: 0.55630\n","Epoch: 00 [ 5684/23069 ( 25%)], Train Loss: 0.55637\n","Epoch: 00 [ 5724/23069 ( 25%)], Train Loss: 0.55456\n","Epoch: 00 [ 5764/23069 ( 25%)], Train Loss: 0.55385\n","Epoch: 00 [ 5804/23069 ( 25%)], Train Loss: 0.55389\n","Epoch: 00 [ 5844/23069 ( 25%)], Train Loss: 0.55169\n","Epoch: 00 [ 5884/23069 ( 26%)], Train Loss: 0.55073\n","Epoch: 00 [ 5924/23069 ( 26%)], Train Loss: 0.54933\n","Epoch: 00 [ 5964/23069 ( 26%)], Train Loss: 0.54808\n","Epoch: 00 [ 6004/23069 ( 26%)], Train Loss: 0.54647\n","Epoch: 00 [ 6044/23069 ( 26%)], Train Loss: 0.54515\n","Epoch: 00 [ 6084/23069 ( 26%)], Train Loss: 0.54320\n","Epoch: 00 [ 6124/23069 ( 27%)], Train Loss: 0.54101\n","Epoch: 00 [ 6164/23069 ( 27%)], Train Loss: 0.54051\n","Epoch: 00 [ 6204/23069 ( 27%)], Train Loss: 0.53906\n","Epoch: 00 [ 6244/23069 ( 27%)], Train Loss: 0.53861\n","Epoch: 00 [ 6284/23069 ( 27%)], Train Loss: 0.53686\n","Epoch: 00 [ 6324/23069 ( 27%)], Train Loss: 0.53436\n","Epoch: 00 [ 6364/23069 ( 28%)], Train Loss: 0.53353\n","Epoch: 00 [ 6404/23069 ( 28%)], Train Loss: 0.53491\n","Epoch: 00 [ 6444/23069 ( 28%)], Train Loss: 0.53286\n","Epoch: 00 [ 6484/23069 ( 28%)], Train Loss: 0.53278\n","Epoch: 00 [ 6524/23069 ( 28%)], Train Loss: 0.53161\n","Epoch: 00 [ 6564/23069 ( 28%)], Train Loss: 0.53103\n","Epoch: 00 [ 6604/23069 ( 29%)], Train Loss: 0.53028\n","Epoch: 00 [ 6644/23069 ( 29%)], Train Loss: 0.52911\n","Epoch: 00 [ 6684/23069 ( 29%)], Train Loss: 0.52892\n","Epoch: 00 [ 6724/23069 ( 29%)], Train Loss: 0.52798\n","Epoch: 00 [ 6764/23069 ( 29%)], Train Loss: 0.52695\n","Epoch: 00 [ 6804/23069 ( 29%)], Train Loss: 0.52623\n","Epoch: 00 [ 6844/23069 ( 30%)], Train Loss: 0.52544\n","Epoch: 00 [ 6884/23069 ( 30%)], Train Loss: 0.52463\n","Epoch: 00 [ 6924/23069 ( 30%)], Train Loss: 0.52270\n","Epoch: 00 [ 6964/23069 ( 30%)], Train Loss: 0.52108\n","Epoch: 00 [ 7004/23069 ( 30%)], Train Loss: 0.51975\n","Epoch: 00 [ 7044/23069 ( 31%)], Train Loss: 0.51769\n","Epoch: 00 [ 7084/23069 ( 31%)], Train Loss: 0.51639\n","Epoch: 00 [ 7124/23069 ( 31%)], Train Loss: 0.51483\n","Epoch: 00 [ 7164/23069 ( 31%)], Train Loss: 0.51316\n","Epoch: 00 [ 7204/23069 ( 31%)], Train Loss: 0.51253\n","Epoch: 00 [ 7244/23069 ( 31%)], Train Loss: 0.51158\n","Epoch: 00 [ 7284/23069 ( 32%)], Train Loss: 0.51005\n","Epoch: 00 [ 7324/23069 ( 32%)], Train Loss: 0.50958\n","Epoch: 00 [ 7364/23069 ( 32%)], Train Loss: 0.50922\n","Epoch: 00 [ 7404/23069 ( 32%)], Train Loss: 0.50779\n","Epoch: 00 [ 7444/23069 ( 32%)], Train Loss: 0.50759\n","Epoch: 00 [ 7484/23069 ( 32%)], Train Loss: 0.50635\n","Epoch: 00 [ 7524/23069 ( 33%)], Train Loss: 0.50534\n","Epoch: 00 [ 7564/23069 ( 33%)], Train Loss: 0.50404\n","Epoch: 00 [ 7604/23069 ( 33%)], Train Loss: 0.50256\n","Epoch: 00 [ 7644/23069 ( 33%)], Train Loss: 0.50104\n","Epoch: 00 [ 7684/23069 ( 33%)], Train Loss: 0.50164\n","Epoch: 00 [ 7724/23069 ( 33%)], Train Loss: 0.50036\n","Epoch: 00 [ 7764/23069 ( 34%)], Train Loss: 0.50011\n","Epoch: 00 [ 7804/23069 ( 34%)], Train Loss: 0.50023\n","Epoch: 00 [ 7844/23069 ( 34%)], Train Loss: 0.49917\n","Epoch: 00 [ 7884/23069 ( 34%)], Train Loss: 0.49833\n","Epoch: 00 [ 7924/23069 ( 34%)], Train Loss: 0.49851\n","Epoch: 00 [ 7964/23069 ( 35%)], Train Loss: 0.49859\n","Epoch: 00 [ 8004/23069 ( 35%)], Train Loss: 0.49873\n","Epoch: 00 [ 8044/23069 ( 35%)], Train Loss: 0.49805\n","Epoch: 00 [ 8084/23069 ( 35%)], Train Loss: 0.49866\n","Epoch: 00 [ 8124/23069 ( 35%)], Train Loss: 0.49875\n","Epoch: 00 [ 8164/23069 ( 35%)], Train Loss: 0.49866\n","Epoch: 00 [ 8204/23069 ( 36%)], Train Loss: 0.49758\n","Epoch: 00 [ 8244/23069 ( 36%)], Train Loss: 0.49627\n","Epoch: 00 [ 8284/23069 ( 36%)], Train Loss: 0.49554\n","Epoch: 00 [ 8324/23069 ( 36%)], Train Loss: 0.49444\n","Epoch: 00 [ 8364/23069 ( 36%)], Train Loss: 0.49315\n","Epoch: 00 [ 8404/23069 ( 36%)], Train Loss: 0.49261\n","Epoch: 00 [ 8444/23069 ( 37%)], Train Loss: 0.49208\n","Epoch: 00 [ 8484/23069 ( 37%)], Train Loss: 0.49148\n","Epoch: 00 [ 8524/23069 ( 37%)], Train Loss: 0.49117\n","Epoch: 00 [ 8564/23069 ( 37%)], Train Loss: 0.49043\n","Epoch: 00 [ 8604/23069 ( 37%)], Train Loss: 0.48972\n","Epoch: 00 [ 8644/23069 ( 37%)], Train Loss: 0.48955\n","Epoch: 00 [ 8684/23069 ( 38%)], Train Loss: 0.48852\n","Epoch: 00 [ 8724/23069 ( 38%)], Train Loss: 0.48814\n","Epoch: 00 [ 8764/23069 ( 38%)], Train Loss: 0.48772\n","Epoch: 00 [ 8804/23069 ( 38%)], Train Loss: 0.48676\n","Epoch: 00 [ 8844/23069 ( 38%)], Train Loss: 0.48662\n","Epoch: 00 [ 8884/23069 ( 39%)], Train Loss: 0.48581\n","Epoch: 00 [ 8924/23069 ( 39%)], Train Loss: 0.48482\n","Epoch: 00 [ 8964/23069 ( 39%)], Train Loss: 0.48407\n","Epoch: 00 [ 9004/23069 ( 39%)], Train Loss: 0.48347\n","Epoch: 00 [ 9044/23069 ( 39%)], Train Loss: 0.48281\n","Epoch: 00 [ 9084/23069 ( 39%)], Train Loss: 0.48283\n","Epoch: 00 [ 9124/23069 ( 40%)], Train Loss: 0.48172\n","Epoch: 00 [ 9164/23069 ( 40%)], Train Loss: 0.48126\n","Epoch: 00 [ 9204/23069 ( 40%)], Train Loss: 0.48108\n","Epoch: 00 [ 9244/23069 ( 40%)], Train Loss: 0.48116\n","Epoch: 00 [ 9284/23069 ( 40%)], Train Loss: 0.48054\n","Epoch: 00 [ 9324/23069 ( 40%)], Train Loss: 0.47992\n","Epoch: 00 [ 9364/23069 ( 41%)], Train Loss: 0.47881\n","Epoch: 00 [ 9404/23069 ( 41%)], Train Loss: 0.47847\n","Epoch: 00 [ 9444/23069 ( 41%)], Train Loss: 0.47834\n","Epoch: 00 [ 9484/23069 ( 41%)], Train Loss: 0.47742\n","Epoch: 00 [ 9524/23069 ( 41%)], Train Loss: 0.47690\n","Epoch: 00 [ 9564/23069 ( 41%)], Train Loss: 0.47577\n","Epoch: 00 [ 9604/23069 ( 42%)], Train Loss: 0.47509\n","Epoch: 00 [ 9644/23069 ( 42%)], Train Loss: 0.47402\n","Epoch: 00 [ 9684/23069 ( 42%)], Train Loss: 0.47367\n","Epoch: 00 [ 9724/23069 ( 42%)], Train Loss: 0.47339\n","Epoch: 00 [ 9764/23069 ( 42%)], Train Loss: 0.47253\n","Epoch: 00 [ 9804/23069 ( 42%)], Train Loss: 0.47155\n","Epoch: 00 [ 9844/23069 ( 43%)], Train Loss: 0.47027\n","Epoch: 00 [ 9884/23069 ( 43%)], Train Loss: 0.46945\n","Epoch: 00 [ 9924/23069 ( 43%)], Train Loss: 0.46914\n","Epoch: 00 [ 9964/23069 ( 43%)], Train Loss: 0.46959\n","Epoch: 00 [10004/23069 ( 43%)], Train Loss: 0.46939\n","Epoch: 00 [10044/23069 ( 44%)], Train Loss: 0.46875\n","Epoch: 00 [10084/23069 ( 44%)], Train Loss: 0.46866\n","Epoch: 00 [10124/23069 ( 44%)], Train Loss: 0.46788\n","Epoch: 00 [10164/23069 ( 44%)], Train Loss: 0.46786\n","Epoch: 00 [10204/23069 ( 44%)], Train Loss: 0.46808\n","Epoch: 00 [10244/23069 ( 44%)], Train Loss: 0.46708\n","Epoch: 00 [10284/23069 ( 45%)], Train Loss: 0.46652\n","Epoch: 00 [10324/23069 ( 45%)], Train Loss: 0.46555\n","Epoch: 00 [10364/23069 ( 45%)], Train Loss: 0.46565\n","Epoch: 00 [10404/23069 ( 45%)], Train Loss: 0.46511\n","Epoch: 00 [10444/23069 ( 45%)], Train Loss: 0.46533\n","Epoch: 00 [10484/23069 ( 45%)], Train Loss: 0.46458\n","Epoch: 00 [10524/23069 ( 46%)], Train Loss: 0.46407\n","Epoch: 00 [10564/23069 ( 46%)], Train Loss: 0.46376\n","Epoch: 00 [10604/23069 ( 46%)], Train Loss: 0.46276\n","Epoch: 00 [10644/23069 ( 46%)], Train Loss: 0.46215\n","Epoch: 00 [10684/23069 ( 46%)], Train Loss: 0.46099\n","Epoch: 00 [10724/23069 ( 46%)], Train Loss: 0.46061\n","Epoch: 00 [10764/23069 ( 47%)], Train Loss: 0.46026\n","Epoch: 00 [10804/23069 ( 47%)], Train Loss: 0.46021\n","Epoch: 00 [10844/23069 ( 47%)], Train Loss: 0.46028\n","Epoch: 00 [10884/23069 ( 47%)], Train Loss: 0.46050\n","Epoch: 00 [10924/23069 ( 47%)], Train Loss: 0.45999\n","Epoch: 00 [10964/23069 ( 48%)], Train Loss: 0.45929\n","Epoch: 00 [11004/23069 ( 48%)], Train Loss: 0.45888\n","Epoch: 00 [11044/23069 ( 48%)], Train Loss: 0.45879\n","Epoch: 00 [11084/23069 ( 48%)], Train Loss: 0.45852\n","Epoch: 00 [11124/23069 ( 48%)], Train Loss: 0.45829\n","Epoch: 00 [11164/23069 ( 48%)], Train Loss: 0.45782\n","Epoch: 00 [11204/23069 ( 49%)], Train Loss: 0.45745\n","Epoch: 00 [11244/23069 ( 49%)], Train Loss: 0.45648\n","Epoch: 00 [11284/23069 ( 49%)], Train Loss: 0.45604\n","Epoch: 00 [11324/23069 ( 49%)], Train Loss: 0.45610\n","Epoch: 00 [11364/23069 ( 49%)], Train Loss: 0.45573\n","Epoch: 00 [11404/23069 ( 49%)], Train Loss: 0.45561\n","Epoch: 00 [11444/23069 ( 50%)], Train Loss: 0.45523\n","Epoch: 00 [11484/23069 ( 50%)], Train Loss: 0.45505\n","Epoch: 00 [11524/23069 ( 50%)], Train Loss: 0.45439\n","Epoch: 00 [11564/23069 ( 50%)], Train Loss: 0.45356\n","Epoch: 00 [11604/23069 ( 50%)], Train Loss: 0.45328\n","Epoch: 00 [11644/23069 ( 50%)], Train Loss: 0.45329\n","Epoch: 00 [11684/23069 ( 51%)], Train Loss: 0.45337\n","Epoch: 00 [11724/23069 ( 51%)], Train Loss: 0.45278\n","Epoch: 00 [11764/23069 ( 51%)], Train Loss: 0.45257\n","Epoch: 00 [11804/23069 ( 51%)], Train Loss: 0.45157\n","Epoch: 00 [11844/23069 ( 51%)], Train Loss: 0.45072\n","Epoch: 00 [11884/23069 ( 52%)], Train Loss: 0.44984\n","Epoch: 00 [11924/23069 ( 52%)], Train Loss: 0.44918\n","Epoch: 00 [11964/23069 ( 52%)], Train Loss: 0.44850\n","Epoch: 00 [12004/23069 ( 52%)], Train Loss: 0.44840\n","Epoch: 00 [12044/23069 ( 52%)], Train Loss: 0.44783\n","Epoch: 00 [12084/23069 ( 52%)], Train Loss: 0.44721\n","Epoch: 00 [12124/23069 ( 53%)], Train Loss: 0.44707\n","Epoch: 00 [12164/23069 ( 53%)], Train Loss: 0.44713\n","Epoch: 00 [12204/23069 ( 53%)], Train Loss: 0.44656\n","Epoch: 00 [12244/23069 ( 53%)], Train Loss: 0.44620\n","Epoch: 00 [12284/23069 ( 53%)], Train Loss: 0.44575\n","Epoch: 00 [12324/23069 ( 53%)], Train Loss: 0.44515\n","Epoch: 00 [12364/23069 ( 54%)], Train Loss: 0.44473\n","Epoch: 00 [12404/23069 ( 54%)], Train Loss: 0.44461\n","Epoch: 00 [12444/23069 ( 54%)], Train Loss: 0.44455\n","Epoch: 00 [12484/23069 ( 54%)], Train Loss: 0.44376\n","Epoch: 00 [12524/23069 ( 54%)], Train Loss: 0.44378\n","Epoch: 00 [12564/23069 ( 54%)], Train Loss: 0.44319\n","Epoch: 00 [12604/23069 ( 55%)], Train Loss: 0.44327\n","Epoch: 00 [12644/23069 ( 55%)], Train Loss: 0.44313\n","Epoch: 00 [12684/23069 ( 55%)], Train Loss: 0.44244\n","Epoch: 00 [12724/23069 ( 55%)], Train Loss: 0.44229\n","Epoch: 00 [12764/23069 ( 55%)], Train Loss: 0.44186\n","Epoch: 00 [12804/23069 ( 56%)], Train Loss: 0.44141\n","Epoch: 00 [12844/23069 ( 56%)], Train Loss: 0.44087\n","Epoch: 00 [12884/23069 ( 56%)], Train Loss: 0.44054\n","Epoch: 00 [12924/23069 ( 56%)], Train Loss: 0.44024\n","Epoch: 00 [12964/23069 ( 56%)], Train Loss: 0.43957\n","Epoch: 00 [13004/23069 ( 56%)], Train Loss: 0.43955\n","Epoch: 00 [13044/23069 ( 57%)], Train Loss: 0.43939\n","Epoch: 00 [13084/23069 ( 57%)], Train Loss: 0.43952\n","Epoch: 00 [13124/23069 ( 57%)], Train Loss: 0.43900\n","Epoch: 00 [13164/23069 ( 57%)], Train Loss: 0.43855\n","Epoch: 00 [13204/23069 ( 57%)], Train Loss: 0.43829\n","Epoch: 00 [13244/23069 ( 57%)], Train Loss: 0.43783\n","Epoch: 00 [13284/23069 ( 58%)], Train Loss: 0.43755\n","Epoch: 00 [13324/23069 ( 58%)], Train Loss: 0.43712\n","Epoch: 00 [13364/23069 ( 58%)], Train Loss: 0.43719\n","Epoch: 00 [13404/23069 ( 58%)], Train Loss: 0.43721\n","Epoch: 00 [13444/23069 ( 58%)], Train Loss: 0.43688\n","Epoch: 00 [13484/23069 ( 58%)], Train Loss: 0.43679\n","Epoch: 00 [13524/23069 ( 59%)], Train Loss: 0.43647\n","Epoch: 00 [13564/23069 ( 59%)], Train Loss: 0.43629\n","Epoch: 00 [13604/23069 ( 59%)], Train Loss: 0.43610\n","Epoch: 00 [13644/23069 ( 59%)], Train Loss: 0.43574\n","Epoch: 00 [13684/23069 ( 59%)], Train Loss: 0.43539\n","Epoch: 00 [13724/23069 ( 59%)], Train Loss: 0.43450\n","Epoch: 00 [13764/23069 ( 60%)], Train Loss: 0.43412\n","Epoch: 00 [13804/23069 ( 60%)], Train Loss: 0.43370\n","Epoch: 00 [13844/23069 ( 60%)], Train Loss: 0.43271\n","Epoch: 00 [13884/23069 ( 60%)], Train Loss: 0.43280\n","Epoch: 00 [13924/23069 ( 60%)], Train Loss: 0.43283\n","Epoch: 00 [13964/23069 ( 61%)], Train Loss: 0.43272\n","Epoch: 00 [14004/23069 ( 61%)], Train Loss: 0.43219\n","Epoch: 00 [14044/23069 ( 61%)], Train Loss: 0.43161\n","Epoch: 00 [14084/23069 ( 61%)], Train Loss: 0.43153\n","Epoch: 00 [14124/23069 ( 61%)], Train Loss: 0.43091\n","Epoch: 00 [14164/23069 ( 61%)], Train Loss: 0.43092\n","Epoch: 00 [14204/23069 ( 62%)], Train Loss: 0.43014\n","Epoch: 00 [14244/23069 ( 62%)], Train Loss: 0.42947\n","Epoch: 00 [14284/23069 ( 62%)], Train Loss: 0.42902\n","Epoch: 00 [14324/23069 ( 62%)], Train Loss: 0.42922\n","Epoch: 00 [14364/23069 ( 62%)], Train Loss: 0.42876\n","Epoch: 00 [14404/23069 ( 62%)], Train Loss: 0.42832\n","Epoch: 00 [14444/23069 ( 63%)], Train Loss: 0.42835\n","Epoch: 00 [14484/23069 ( 63%)], Train Loss: 0.42789\n","Epoch: 00 [14524/23069 ( 63%)], Train Loss: 0.42772\n","Epoch: 00 [14564/23069 ( 63%)], Train Loss: 0.42721\n","Epoch: 00 [14604/23069 ( 63%)], Train Loss: 0.42684\n","Epoch: 00 [14644/23069 ( 63%)], Train Loss: 0.42698\n","Epoch: 00 [14684/23069 ( 64%)], Train Loss: 0.42658\n","Epoch: 00 [14724/23069 ( 64%)], Train Loss: 0.42643\n","Epoch: 00 [14764/23069 ( 64%)], Train Loss: 0.42612\n","Epoch: 00 [14804/23069 ( 64%)], Train Loss: 0.42589\n","Epoch: 00 [14844/23069 ( 64%)], Train Loss: 0.42579\n","Epoch: 00 [14884/23069 ( 65%)], Train Loss: 0.42601\n","Epoch: 00 [14924/23069 ( 65%)], Train Loss: 0.42565\n","Epoch: 00 [14964/23069 ( 65%)], Train Loss: 0.42531\n","Epoch: 00 [15004/23069 ( 65%)], Train Loss: 0.42515\n","Epoch: 00 [15044/23069 ( 65%)], Train Loss: 0.42460\n","Epoch: 00 [15084/23069 ( 65%)], Train Loss: 0.42465\n","Epoch: 00 [15124/23069 ( 66%)], Train Loss: 0.42435\n","Epoch: 00 [15164/23069 ( 66%)], Train Loss: 0.42384\n","Epoch: 00 [15204/23069 ( 66%)], Train Loss: 0.42384\n","Epoch: 00 [15244/23069 ( 66%)], Train Loss: 0.42319\n","Epoch: 00 [15284/23069 ( 66%)], Train Loss: 0.42301\n","Epoch: 00 [15324/23069 ( 66%)], Train Loss: 0.42303\n","Epoch: 00 [15364/23069 ( 67%)], Train Loss: 0.42314\n","Epoch: 00 [15404/23069 ( 67%)], Train Loss: 0.42284\n","Epoch: 00 [15444/23069 ( 67%)], Train Loss: 0.42282\n","Epoch: 00 [15484/23069 ( 67%)], Train Loss: 0.42247\n","Epoch: 00 [15524/23069 ( 67%)], Train Loss: 0.42185\n","Epoch: 00 [15564/23069 ( 67%)], Train Loss: 0.42133\n","Epoch: 00 [15604/23069 ( 68%)], Train Loss: 0.42103\n","Epoch: 00 [15644/23069 ( 68%)], Train Loss: 0.42031\n","Epoch: 00 [15684/23069 ( 68%)], Train Loss: 0.42003\n","Epoch: 00 [15724/23069 ( 68%)], Train Loss: 0.42024\n","Epoch: 00 [15764/23069 ( 68%)], Train Loss: 0.42014\n","Epoch: 00 [15804/23069 ( 69%)], Train Loss: 0.41991\n","Epoch: 00 [15844/23069 ( 69%)], Train Loss: 0.41964\n","Epoch: 00 [15884/23069 ( 69%)], Train Loss: 0.41954\n","Epoch: 00 [15924/23069 ( 69%)], Train Loss: 0.41958\n","Epoch: 00 [15964/23069 ( 69%)], Train Loss: 0.41916\n","Epoch: 00 [16004/23069 ( 69%)], Train Loss: 0.41894\n","Epoch: 00 [16044/23069 ( 70%)], Train Loss: 0.41843\n","Epoch: 00 [16084/23069 ( 70%)], Train Loss: 0.41793\n","Epoch: 00 [16124/23069 ( 70%)], Train Loss: 0.41735\n","Epoch: 00 [16164/23069 ( 70%)], Train Loss: 0.41744\n","Epoch: 00 [16204/23069 ( 70%)], Train Loss: 0.41680\n","Epoch: 00 [16244/23069 ( 70%)], Train Loss: 0.41644\n","Epoch: 00 [16284/23069 ( 71%)], Train Loss: 0.41634\n","Epoch: 00 [16324/23069 ( 71%)], Train Loss: 0.41619\n","Epoch: 00 [16364/23069 ( 71%)], Train Loss: 0.41586\n","Epoch: 00 [16404/23069 ( 71%)], Train Loss: 0.41525\n","Epoch: 00 [16444/23069 ( 71%)], Train Loss: 0.41492\n","Epoch: 00 [16484/23069 ( 71%)], Train Loss: 0.41453\n","Epoch: 00 [16524/23069 ( 72%)], Train Loss: 0.41427\n","Epoch: 00 [16564/23069 ( 72%)], Train Loss: 0.41394\n","Epoch: 00 [16604/23069 ( 72%)], Train Loss: 0.41362\n","Epoch: 00 [16644/23069 ( 72%)], Train Loss: 0.41356\n","Epoch: 00 [16684/23069 ( 72%)], Train Loss: 0.41327\n","Epoch: 00 [16724/23069 ( 72%)], Train Loss: 0.41276\n","Epoch: 00 [16764/23069 ( 73%)], Train Loss: 0.41231\n","Epoch: 00 [16804/23069 ( 73%)], Train Loss: 0.41190\n","Epoch: 00 [16844/23069 ( 73%)], Train Loss: 0.41172\n","Epoch: 00 [16884/23069 ( 73%)], Train Loss: 0.41149\n","Epoch: 00 [16924/23069 ( 73%)], Train Loss: 0.41138\n","Epoch: 00 [16964/23069 ( 74%)], Train Loss: 0.41120\n","Epoch: 00 [17004/23069 ( 74%)], Train Loss: 0.41084\n","Epoch: 00 [17044/23069 ( 74%)], Train Loss: 0.41060\n","Epoch: 00 [17084/23069 ( 74%)], Train Loss: 0.40995\n","Epoch: 00 [17124/23069 ( 74%)], Train Loss: 0.40963\n","Epoch: 00 [17164/23069 ( 74%)], Train Loss: 0.40913\n","Epoch: 00 [17204/23069 ( 75%)], Train Loss: 0.40849\n","Epoch: 00 [17244/23069 ( 75%)], Train Loss: 0.40819\n","Epoch: 00 [17284/23069 ( 75%)], Train Loss: 0.40796\n","Epoch: 00 [17324/23069 ( 75%)], Train Loss: 0.40777\n","Epoch: 00 [17364/23069 ( 75%)], Train Loss: 0.40800\n","Epoch: 00 [17404/23069 ( 75%)], Train Loss: 0.40738\n","Epoch: 00 [17444/23069 ( 76%)], Train Loss: 0.40696\n","Epoch: 00 [17484/23069 ( 76%)], Train Loss: 0.40681\n","Epoch: 00 [17524/23069 ( 76%)], Train Loss: 0.40655\n","Epoch: 00 [17564/23069 ( 76%)], Train Loss: 0.40621\n","Epoch: 00 [17604/23069 ( 76%)], Train Loss: 0.40573\n","Epoch: 00 [17644/23069 ( 76%)], Train Loss: 0.40582\n","Epoch: 00 [17684/23069 ( 77%)], Train Loss: 0.40577\n","Epoch: 00 [17724/23069 ( 77%)], Train Loss: 0.40538\n","Epoch: 00 [17764/23069 ( 77%)], Train Loss: 0.40548\n","Epoch: 00 [17804/23069 ( 77%)], Train Loss: 0.40529\n","Epoch: 00 [17844/23069 ( 77%)], Train Loss: 0.40547\n","Epoch: 00 [17884/23069 ( 78%)], Train Loss: 0.40523\n","Epoch: 00 [17924/23069 ( 78%)], Train Loss: 0.40477\n","Epoch: 00 [17964/23069 ( 78%)], Train Loss: 0.40438\n","Epoch: 00 [18004/23069 ( 78%)], Train Loss: 0.40434\n","Epoch: 00 [18044/23069 ( 78%)], Train Loss: 0.40380\n","Epoch: 00 [18084/23069 ( 78%)], Train Loss: 0.40354\n","Epoch: 00 [18124/23069 ( 79%)], Train Loss: 0.40295\n","Epoch: 00 [18164/23069 ( 79%)], Train Loss: 0.40264\n","Epoch: 00 [18204/23069 ( 79%)], Train Loss: 0.40270\n","Epoch: 00 [18244/23069 ( 79%)], Train Loss: 0.40226\n","Epoch: 00 [18284/23069 ( 79%)], Train Loss: 0.40178\n","Epoch: 00 [18324/23069 ( 79%)], Train Loss: 0.40171\n","Epoch: 00 [18364/23069 ( 80%)], Train Loss: 0.40131\n","Epoch: 00 [18404/23069 ( 80%)], Train Loss: 0.40096\n","Epoch: 00 [18444/23069 ( 80%)], Train Loss: 0.40088\n","Epoch: 00 [18484/23069 ( 80%)], Train Loss: 0.40058\n","Epoch: 00 [18524/23069 ( 80%)], Train Loss: 0.40029\n","Epoch: 00 [18564/23069 ( 80%)], Train Loss: 0.39983\n","Epoch: 00 [18604/23069 ( 81%)], Train Loss: 0.39982\n","Epoch: 00 [18644/23069 ( 81%)], Train Loss: 0.39945\n","Epoch: 00 [18684/23069 ( 81%)], Train Loss: 0.39894\n","Epoch: 00 [18724/23069 ( 81%)], Train Loss: 0.39867\n","Epoch: 00 [18764/23069 ( 81%)], Train Loss: 0.39841\n","Epoch: 00 [18804/23069 ( 82%)], Train Loss: 0.39811\n","Epoch: 00 [18844/23069 ( 82%)], Train Loss: 0.39818\n","Epoch: 00 [18884/23069 ( 82%)], Train Loss: 0.39792\n","Epoch: 00 [18924/23069 ( 82%)], Train Loss: 0.39742\n","Epoch: 00 [18964/23069 ( 82%)], Train Loss: 0.39721\n","Epoch: 00 [19004/23069 ( 82%)], Train Loss: 0.39726\n","Epoch: 00 [19044/23069 ( 83%)], Train Loss: 0.39718\n","Epoch: 00 [19084/23069 ( 83%)], Train Loss: 0.39672\n","Epoch: 00 [19124/23069 ( 83%)], Train Loss: 0.39635\n","Epoch: 00 [19164/23069 ( 83%)], Train Loss: 0.39630\n","Epoch: 00 [19204/23069 ( 83%)], Train Loss: 0.39597\n","Epoch: 00 [19244/23069 ( 83%)], Train Loss: 0.39552\n","Epoch: 00 [19284/23069 ( 84%)], Train Loss: 0.39509\n","Epoch: 00 [19324/23069 ( 84%)], Train Loss: 0.39496\n","Epoch: 00 [19364/23069 ( 84%)], Train Loss: 0.39492\n","Epoch: 00 [19404/23069 ( 84%)], Train Loss: 0.39475\n","Epoch: 00 [19444/23069 ( 84%)], Train Loss: 0.39463\n","Epoch: 00 [19484/23069 ( 84%)], Train Loss: 0.39416\n","Epoch: 00 [19524/23069 ( 85%)], Train Loss: 0.39403\n","Epoch: 00 [19564/23069 ( 85%)], Train Loss: 0.39379\n","Epoch: 00 [19604/23069 ( 85%)], Train Loss: 0.39338\n","Epoch: 00 [19644/23069 ( 85%)], Train Loss: 0.39301\n","Epoch: 00 [19684/23069 ( 85%)], Train Loss: 0.39283\n","Epoch: 00 [19724/23069 ( 86%)], Train Loss: 0.39263\n","Epoch: 00 [19764/23069 ( 86%)], Train Loss: 0.39255\n","Epoch: 00 [19804/23069 ( 86%)], Train Loss: 0.39268\n","Epoch: 00 [19844/23069 ( 86%)], Train Loss: 0.39256\n","Epoch: 00 [19884/23069 ( 86%)], Train Loss: 0.39215\n","Epoch: 00 [19924/23069 ( 86%)], Train Loss: 0.39227\n","Epoch: 00 [19964/23069 ( 87%)], Train Loss: 0.39184\n","Epoch: 00 [20004/23069 ( 87%)], Train Loss: 0.39174\n","Epoch: 00 [20044/23069 ( 87%)], Train Loss: 0.39157\n","Epoch: 00 [20084/23069 ( 87%)], Train Loss: 0.39144\n","Epoch: 00 [20124/23069 ( 87%)], Train Loss: 0.39132\n","Epoch: 00 [20164/23069 ( 87%)], Train Loss: 0.39127\n","Epoch: 00 [20204/23069 ( 88%)], Train Loss: 0.39116\n","Epoch: 00 [20244/23069 ( 88%)], Train Loss: 0.39102\n","Epoch: 00 [20284/23069 ( 88%)], Train Loss: 0.39080\n","Epoch: 00 [20324/23069 ( 88%)], Train Loss: 0.39077\n","Epoch: 00 [20364/23069 ( 88%)], Train Loss: 0.39042\n","Epoch: 00 [20404/23069 ( 88%)], Train Loss: 0.39002\n","Epoch: 00 [20444/23069 ( 89%)], Train Loss: 0.38984\n","Epoch: 00 [20484/23069 ( 89%)], Train Loss: 0.38959\n","Epoch: 00 [20524/23069 ( 89%)], Train Loss: 0.38968\n","Epoch: 00 [20564/23069 ( 89%)], Train Loss: 0.38983\n","Epoch: 00 [20604/23069 ( 89%)], Train Loss: 0.38961\n","Epoch: 00 [20644/23069 ( 89%)], Train Loss: 0.38940\n","Epoch: 00 [20684/23069 ( 90%)], Train Loss: 0.38886\n","Epoch: 00 [20724/23069 ( 90%)], Train Loss: 0.38862\n","Epoch: 00 [20764/23069 ( 90%)], Train Loss: 0.38822\n","Epoch: 00 [20804/23069 ( 90%)], Train Loss: 0.38822\n","Epoch: 00 [20844/23069 ( 90%)], Train Loss: 0.38808\n","Epoch: 00 [20884/23069 ( 91%)], Train Loss: 0.38779\n","Epoch: 00 [20924/23069 ( 91%)], Train Loss: 0.38757\n","Epoch: 00 [20964/23069 ( 91%)], Train Loss: 0.38728\n","Epoch: 00 [21004/23069 ( 91%)], Train Loss: 0.38688\n","Epoch: 00 [21044/23069 ( 91%)], Train Loss: 0.38664\n","Epoch: 00 [21084/23069 ( 91%)], Train Loss: 0.38655\n","Epoch: 00 [21124/23069 ( 92%)], Train Loss: 0.38656\n","Epoch: 00 [21164/23069 ( 92%)], Train Loss: 0.38654\n","Epoch: 00 [21204/23069 ( 92%)], Train Loss: 0.38640\n","Epoch: 00 [21244/23069 ( 92%)], Train Loss: 0.38623\n","Epoch: 00 [21284/23069 ( 92%)], Train Loss: 0.38590\n","Epoch: 00 [21324/23069 ( 92%)], Train Loss: 0.38542\n","Epoch: 00 [21364/23069 ( 93%)], Train Loss: 0.38530\n","Epoch: 00 [21404/23069 ( 93%)], Train Loss: 0.38549\n","Epoch: 00 [21444/23069 ( 93%)], Train Loss: 0.38520\n","Epoch: 00 [21484/23069 ( 93%)], Train Loss: 0.38499\n","Epoch: 00 [21524/23069 ( 93%)], Train Loss: 0.38472\n","Epoch: 00 [21564/23069 ( 93%)], Train Loss: 0.38433\n","Epoch: 00 [21604/23069 ( 94%)], Train Loss: 0.38397\n","Epoch: 00 [21644/23069 ( 94%)], Train Loss: 0.38362\n","Epoch: 00 [21684/23069 ( 94%)], Train Loss: 0.38355\n","Epoch: 00 [21724/23069 ( 94%)], Train Loss: 0.38334\n","Epoch: 00 [21764/23069 ( 94%)], Train Loss: 0.38295\n","Epoch: 00 [21804/23069 ( 95%)], Train Loss: 0.38289\n","Epoch: 00 [21844/23069 ( 95%)], Train Loss: 0.38266\n","Epoch: 00 [21884/23069 ( 95%)], Train Loss: 0.38263\n","Epoch: 00 [21924/23069 ( 95%)], Train Loss: 0.38236\n","Epoch: 00 [21964/23069 ( 95%)], Train Loss: 0.38233\n","Epoch: 00 [22004/23069 ( 95%)], Train Loss: 0.38194\n","Epoch: 00 [22044/23069 ( 96%)], Train Loss: 0.38159\n","Epoch: 00 [22084/23069 ( 96%)], Train Loss: 0.38147\n","Epoch: 00 [22124/23069 ( 96%)], Train Loss: 0.38100\n","Epoch: 00 [22164/23069 ( 96%)], Train Loss: 0.38095\n","Epoch: 00 [22204/23069 ( 96%)], Train Loss: 0.38088\n","Epoch: 00 [22244/23069 ( 96%)], Train Loss: 0.38050\n","Epoch: 00 [22284/23069 ( 97%)], Train Loss: 0.38051\n","Epoch: 00 [22324/23069 ( 97%)], Train Loss: 0.38022\n","Epoch: 00 [22364/23069 ( 97%)], Train Loss: 0.38000\n","Epoch: 00 [22404/23069 ( 97%)], Train Loss: 0.37993\n","Epoch: 00 [22444/23069 ( 97%)], Train Loss: 0.37968\n","Epoch: 00 [22484/23069 ( 97%)], Train Loss: 0.37931\n","Epoch: 00 [22524/23069 ( 98%)], Train Loss: 0.37917\n","Epoch: 00 [22564/23069 ( 98%)], Train Loss: 0.37875\n","Epoch: 00 [22604/23069 ( 98%)], Train Loss: 0.37868\n","Epoch: 00 [22644/23069 ( 98%)], Train Loss: 0.37859\n","Epoch: 00 [22684/23069 ( 98%)], Train Loss: 0.37847\n","Epoch: 00 [22724/23069 ( 99%)], Train Loss: 0.37834\n","Epoch: 00 [22764/23069 ( 99%)], Train Loss: 0.37856\n","Epoch: 00 [22804/23069 ( 99%)], Train Loss: 0.37822\n","Epoch: 00 [22844/23069 ( 99%)], Train Loss: 0.37812\n","Epoch: 00 [22884/23069 ( 99%)], Train Loss: 0.37797\n","Epoch: 00 [22924/23069 ( 99%)], Train Loss: 0.37771\n","Epoch: 00 [22964/23069 (100%)], Train Loss: 0.37737\n","Epoch: 00 [23004/23069 (100%)], Train Loss: 0.37693\n","Epoch: 00 [23044/23069 (100%)], Train Loss: 0.37695\n","Epoch: 00 [23069/23069 (100%)], Train Loss: 0.37693\n","----Validation Results Summary----\n","Epoch: [0] Valid Loss: 0.00000\n","0 Epoch, Best epoch was updated! Valid Loss: 0.00000\n","Saving model checkpoint to chaii-qa-5-fold-xlmroberta-torch-fit/checkpoint-fold-5.\n","\n","Total Training Time: 3205.2202792167664secs, Average Training Time per Epoch: 3205.2202792167664secs.\n","Total Validation Time: 0.5017268657684326secs, Average Validation Time per Epoch: 0.5017268657684326secs.\n"]}]},{"cell_type":"code","metadata":{"id":"DkjRIhdbwjHx","execution":{"iopub.status.busy":"2021-08-17T20:36:49.722586Z","iopub.execute_input":"2021-08-17T20:36:49.723011Z","iopub.status.idle":"2021-08-17T20:36:49.727659Z","shell.execute_reply.started":"2021-08-17T20:36:49.722978Z","shell.execute_reply":"2021-08-17T20:36:49.726609Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1634304054361,"user_tz":-540,"elapsed":18,"user":{"displayName":"Ryosuke Horiuchi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRDRSQ3DBbQ81iOVzkYeSWlBKjBWw5tKBmtXzVlw=s64","userId":"18359745667022839599"}}},"source":["# example for training second fold\n","\n","# for fold in range(1, 2):\n","#     print();print()\n","#     print('-'*50)\n","#     print(f'FOLD: {fold}')\n","#     print('-'*50)\n","#     run(train, fold)"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7uIwhUECP4iP"},"source":["### Thanks and please do Upvote!"]}]}