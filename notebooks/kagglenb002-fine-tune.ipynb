{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import BertTokenizerFast, XLMRobertaTokenizerFast\nfrom transformers import BertForQuestionAnswering, XLMRobertaForQuestionAnswering\n\n\nUSE_PIPELINE = True\nMODEL_PATH = \"../input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2\" #\"../input/localnb001-export-transformers\"\nMODEL_PATH_TO_OBJECT = {\n    \"../input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2\": [XLMRobertaTokenizerFast, XLMRobertaForQuestionAnswering],\n    \"../input/localnb001-export-transformers\": [BertTokenizerFast, BertForQuestionAnswering]\n}\nMODEL_NAME_FINETUNED = \"model_deepset_xlm_roberta_large_squad2\" #\"model\"\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nfrom tqdm import tqdm\nfrom transformers import pipeline\nimport torch","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-25T01:14:54.015244Z","iopub.execute_input":"2021-09-25T01:14:54.016138Z","iopub.status.idle":"2021-09-25T01:15:02.964352Z","shell.execute_reply.started":"2021-09-25T01:14:54.016019Z","shell.execute_reply":"2021-09-25T01:15:02.963293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from transformers import pipeline\n# if DISABLE_INTERNET:\n#     model_path = \"../input/localnb001-export-transformers\"\n#     model = pipeline('question-answering', model=model_path, tokenizer=model_path, device=0)\n# else:\n#     model = pipeline('question-answering', model='bert-base-multilingual-cased', device=0)\n\n\ntokenizer = MODEL_PATH_TO_OBJECT[MODEL_PATH][0].from_pretrained(MODEL_PATH)\nmodel = MODEL_PATH_TO_OBJECT[MODEL_PATH][1].from_pretrained(MODEL_PATH)\n\n\n# Load model weights and optimizer state\noutput_model = f\"../input/localnb002-fine-tune/{MODEL_NAME_FINETUNED}.pth\"\ncheckpoint = torch.load(output_model, map_location='cpu')\nmodel.load_state_dict(checkpoint['model_state_dict'])\n\nif USE_PIPELINE:\n    #model = pipeline('question-answering', model=model_path, tokenizer=model_path, device=0)\n    model = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, device=0)\nelse:\n    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n    model.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T01:15:02.966385Z","iopub.execute_input":"2021-09-25T01:15:02.966794Z","iopub.status.idle":"2021-09-25T01:16:15.949168Z","shell.execute_reply.started":"2021-09-25T01:15:02.966752Z","shell.execute_reply":"2021-09-25T01:16:15.948049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv(\"../input/chaii-hindi-and-tamil-question-answering/test.csv\")\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T01:16:15.951622Z","iopub.execute_input":"2021-09-25T01:16:15.952015Z","iopub.status.idle":"2021-09-25T01:16:16.011951Z","shell.execute_reply.started":"2021-09-25T01:16:15.951974Z","shell.execute_reply":"2021-09-25T01:16:16.010952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_fn(use_pipeline=False):\n    test[\"PredictionString\"] = \"\"\n    tqdm_df_itertuples = tqdm(test.itertuples(), total=len(test))\n    for row in tqdm_df_itertuples:\n        i = row[0]\n        context = row[2]\n        question = row[3]\n        \n        if use_pipeline:\n            output = model(question=question, context=context)\n            pred = output[\"answer\"]\n        else:\n            inputs = tokenizer(question, \n                               context, \n                               add_special_tokens=True,\n                               max_length=512,\n                               padding=True, \n                               truncation=True, \n                               return_tensors=\"pt\")\n            inputs.to(device)\n            input_ids = inputs[\"input_ids\"].tolist()[0]\n            outputs = model(**inputs)\n            answer_start_scores = outputs.start_logits\n            answer_end_scores = outputs.end_logits\n\n            # Get the most likely beginning of answer with the argmax of the score\n            answer_start = torch.argmax(answer_start_scores)\n            # Get the most likely end of answer with the argmax of the score\n            answer_end = torch.argmax(answer_end_scores) + 1\n\n            pred = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n\n        test.loc[i, \"PredictionString\"] = pred\n        \n    return test","metadata":{"execution":{"iopub.status.busy":"2021-09-25T01:16:16.015486Z","iopub.execute_input":"2021-09-25T01:16:16.015772Z","iopub.status.idle":"2021-09-25T01:16:16.025125Z","shell.execute_reply.started":"2021-09-25T01:16:16.015743Z","shell.execute_reply":"2021-09-25T01:16:16.023931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test_fn(use_pipeline=USE_PIPELINE)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T01:17:26.125943Z","iopub.execute_input":"2021-09-25T01:17:26.126516Z","iopub.status.idle":"2021-09-25T01:17:30.558556Z","shell.execute_reply.started":"2021-09-25T01:17:26.126483Z","shell.execute_reply":"2021-09-25T01:17:30.557002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(test)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T01:17:33.145907Z","iopub.execute_input":"2021-09-25T01:17:33.146277Z","iopub.status.idle":"2021-09-25T01:17:33.164054Z","shell.execute_reply.started":"2021-09-25T01:17:33.14622Z","shell.execute_reply":"2021-09-25T01:17:33.162481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[[\"id\", \"PredictionString\"]].to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T01:17:41.200827Z","iopub.execute_input":"2021-09-25T01:17:41.201242Z","iopub.status.idle":"2021-09-25T01:17:41.216146Z","shell.execute_reply.started":"2021-09-25T01:17:41.201197Z","shell.execute_reply":"2021-09-25T01:17:41.214413Z"},"trusted":true},"execution_count":null,"outputs":[]}]}