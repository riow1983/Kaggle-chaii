{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"DISABLE_INTERNET = True\nINFERENCE = True\nUSE_PIPELINE = False\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-15T01:02:04.498136Z","iopub.execute_input":"2021-09-15T01:02:04.498555Z","iopub.status.idle":"2021-09-15T01:02:04.525272Z","shell.execute_reply.started":"2021-09-15T01:02:04.498472Z","shell.execute_reply":"2021-09-15T01:02:04.524485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2021-09-15T01:02:04.526707Z","iopub.execute_input":"2021-09-15T01:02:04.527039Z","iopub.status.idle":"2021-09-15T01:02:04.532389Z","shell.execute_reply.started":"2021-09-15T01:02:04.527004Z","shell.execute_reply":"2021-09-15T01:02:04.531297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from transformers import pipeline\n# if DISABLE_INTERNET:\n#     model_path = \"../input/localnb001-export-transformers\"\n#     model = pipeline('question-answering', model=model_path, tokenizer=model_path, device=0)\n# else:\n#     model = pipeline('question-answering', model='bert-base-multilingual-cased', device=0)\n\nfrom transformers import pipeline, BertForQuestionAnswering, BertTokenizerFast\nimport torch\nif DISABLE_INTERNET:\n    model_path = \"../input/localnb001-export-transformers\"\n    model = BertForQuestionAnswering.from_pretrained(model_path)\n    tokenizer = BertTokenizerFast.from_pretrained(model_path)\n    \n    # Load model weights and optimizer state\n    output_model = \"../input/localnb002-fine-tune/model.pth\"\n    checkpoint = torch.load(output_model, map_location='cpu')\n    model.load_state_dict(checkpoint['model_state_dict'])\n    \n    if USE_PIPELINE:\n        #model = pipeline('question-answering', model=model_path, tokenizer=model_path, device=0)\n        model = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, device=0)\n    else:\n        device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n        model.to(device)\nelse:\n    if USE_PIPELINE:\n        model = pipeline('question-answering', model='bert-base-multilingual-cased', device=0)\n    else:\n        model = BertForQuestionAnswering.from_pretrained('bert-base-multilingual-cased')\n        tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')\n        \n        # Load model weights and optimizer state\n        output_model = \"../input/localnb002-fine-tune/model.pth\"\n        checkpoint = torch.load(output_model, map_location='cpu')\n        model.load_state_dict(checkpoint['model_state_dict'])\n        \n        device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n        model.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-09-15T01:02:04.534903Z","iopub.execute_input":"2021-09-15T01:02:04.53543Z","iopub.status.idle":"2021-09-15T01:02:42.749495Z","shell.execute_reply.started":"2021-09-15T01:02:04.535394Z","shell.execute_reply":"2021-09-15T01:02:42.748559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv(\"../input/chaii-hindi-and-tamil-question-answering/test.csv\")\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-15T01:02:42.751122Z","iopub.execute_input":"2021-09-15T01:02:42.751468Z","iopub.status.idle":"2021-09-15T01:02:42.796605Z","shell.execute_reply.started":"2021-09-15T01:02:42.751433Z","shell.execute_reply":"2021-09-15T01:02:42.795847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_fn(use_pipeline=False):\n    test[\"PredictionString\"] = \"\"\n    tqdm_df_itertuples = tqdm(test.itertuples(), total=len(test))\n    for row in tqdm_df_itertuples:\n        i = row[0]\n        context = row[2]\n        question = row[3]\n        \n        if use_pipeline:\n            output = model(question=question, context=context)\n            pred = output[\"answer\"]\n        else:\n            inputs = tokenizer(question, \n                               context, \n                               add_special_tokens=True,\n                               max_length=512,\n                               padding=True, \n                               truncation=True, \n                               return_tensors=\"pt\")\n            inputs.to(device)\n            input_ids = inputs[\"input_ids\"].tolist()[0]\n            outputs = model(**inputs)\n            answer_start_scores = outputs.start_logits\n            answer_end_scores = outputs.end_logits\n\n            # Get the most likely beginning of answer with the argmax of the score\n            answer_start = torch.argmax(answer_start_scores)\n            # Get the most likely end of answer with the argmax of the score\n            answer_end = torch.argmax(answer_end_scores) + 1\n\n            pred = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n\n        test.loc[i, \"PredictionString\"] = pred\n        \n    return test","metadata":{"execution":{"iopub.status.busy":"2021-09-15T01:02:42.799421Z","iopub.execute_input":"2021-09-15T01:02:42.79966Z","iopub.status.idle":"2021-09-15T01:02:42.80948Z","shell.execute_reply.started":"2021-09-15T01:02:42.799637Z","shell.execute_reply":"2021-09-15T01:02:42.807888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test_fn(use_pipeline=USE_PIPELINE)","metadata":{"execution":{"iopub.status.busy":"2021-09-15T01:02:45.171675Z","iopub.execute_input":"2021-09-15T01:02:45.172006Z","iopub.status.idle":"2021-09-15T01:02:46.194275Z","shell.execute_reply.started":"2021-09-15T01:02:45.171977Z","shell.execute_reply":"2021-09-15T01:02:46.193333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(test)","metadata":{"execution":{"iopub.status.busy":"2021-09-15T01:02:49.057818Z","iopub.execute_input":"2021-09-15T01:02:49.05816Z","iopub.status.idle":"2021-09-15T01:02:49.071047Z","shell.execute_reply.started":"2021-09-15T01:02:49.058125Z","shell.execute_reply":"2021-09-15T01:02:49.070102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[[\"id\", \"PredictionString\"]].to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-15T01:02:54.411826Z","iopub.execute_input":"2021-09-15T01:02:54.41215Z","iopub.status.idle":"2021-09-15T01:02:54.422213Z","shell.execute_reply.started":"2021-09-15T01:02:54.412119Z","shell.execute_reply":"2021-09-15T01:02:54.421386Z"},"trusted":true},"execution_count":null,"outputs":[]}]}